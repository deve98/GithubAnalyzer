,Title,State,Created_at,Updated_at,Closed_at,Author,Details,Comments,Id
0,How to get name of file from chunk name?,open,2020-03-21T16:25:12Z,2020-03-21T16:25:12Z,,NONE,"Hello,

I have chunk name like this ""chunk_00000000030072BF_00000001.mfs"", How can I trace back to find file name?

Thanks.",,9800119
1,LizardFS Chunkserver size and data real size is different,open,2020-03-19T11:26:12Z,2020-03-19T11:32:43Z,,NONE,"Hi,

I have the problems with size of LizardFS. The data real size is just 1.6TB with chunk replicate = 2, so i think Lizardfs total chunk server will ~4TB, but total size which I counted from my chunk servers is **16TB**. I don't know why? Anybody can help me? Is it a bug?

**# Info**
> Version: 3.12
> File size: Almost < 10MB ( static file: css, image, js....)

Regards,","Update:

The status of cluster I see on LizardFS WebUI is normal. All chunks is safe and no file in trash folder. :(",9800119
2,lizardfs-3.10.4-0el7 RPMs at packages.lizardfs.com corrupted,open,2020-03-02T13:00:07Z,2020-03-02T13:00:07Z,,NONE,"Hi,

The RPMs at http://packages.lizardfs.com/yum/el7/ for version 3.10.4-0el7 are corrupted. Other versions are OK.

For example:
NOK
file lizardfs-3.10.4-0el7.src.rpm
lizardfs-3.10.4-0el7.src.rpm: **data**
OK
file lizardfs-3.10.6-0el7.src.rpm
lizardfs-3.10.6-0el7.src.rpm: **RPM v3.0 src**",,9800119
3,"Feature Requests: 2020 - IB, RDMA, Dedup, Compression, Caching",open,2020-02-24T23:47:34Z,2020-02-24T23:47:34Z,,NONE,"Hello, Hope all is well!

After reading through #278, I figured I would bump the issue as well as request a few other features I would love to eventually see.

I have been working around the lack of direct IB support from lizardfs by using Infiniband + ISER via ISCSI exporting each block, and running both (multiple chunkserver processes, one per disk) chunk servers and master from a single machine.
Basically, no lizardfs IB support has been worked around by using IB to first make all devices appear local to the server, then setting up lizardfs, but it makes for one hell of a confusing setup process trying to manage it all. Then you layer Multipath, VDO, and caching, and good lord is it a nightmare to keep all the WWNs and UUIDs in order.

Since I'm voicing feature requests:
VDO is the block level deduplication solutions I've settled on. For my use case it not only recovers my EC{} lost storage, but deduplication actually saves me an additional 15%. (My use case being 100% 1GB .gz files from the common crawl corpus. 
Again, this is a layer of complexity in management during setup which would be awesome to see asn an integration // native option. Transparent disk deduplication and compression.
--> Anyone following in these steps down the road, consider exporting the VDO active block storage via ISCSI, instead of running VDO on the master/chunkserver machine, to ensure each node in the cluster handles the processing and ram requirements of deduplication and compression, instead of centralizing this computing on the master server.

Lastly, Tiering or Caching. This has a popular issue or two, #524 , #710 both pop to mind. 

I have three m.2 NVME drives, currently sitting around, as for the life of my I have not been able to find a method to get the above working with any form of caching.
As Lizardfs doesn't support storage teiring, and my preference is EC{}, I turned to Caching.

The caches I've tried (and failed for one reason or another) are dm-writeboost, EnhanceIO (the still maintained fork), bcache. Each wants to work on blocks, meaning trying to repeat a task nearly 30 times across each block. Would love to see a method to leverage 1.5TB of screaming fast NVME to speed up lizardfs. (Open to ideas if anyone has anything..?)

I even considered IOnotify, to simply write to the SSD's, and automatically move the files to lizardfs, but this has it's own issues (directory recursion, as an example). 

Ideal setup would be to ""assign"" a device, directory, etc. as an SSD write cache. and otherwise not have to worry about it.

Deduplication, compression, caching, Infiniband. <3




",,9800119
4,LizardFS on CentOS: no logs are appearing after running tests - Fix,open,2020-02-10T11:26:11Z,2020-02-10T11:28:11Z,,NONE,,,9800119
5,Refactor code session header,open,2020-02-08T08:27:18Z,2020-02-10T11:28:15Z,,NONE,,,9800119
6,Incoherency between throughput displayed by cgiserv and collectd/bwm-ng,open,2020-02-05T08:41:28Z,2020-02-11T08:08:51Z,,NONE,"Hello.

To discover why Lizardfs is so slow on my 10 SSD, I'm looking at some monitoring.

In addition to the `cgiserv` provided by Lizardfs, I setup a `collectd` (with `graphite` and `grafana` on a dedicated computer) and I'm looking at the output of `bwm-ng` to get some live monitoring.

I have incoherent results between the disk throughput shown by `cgiserv` and the stats I got with `collectd` and `bwm-ng`.

As you will see below:

- I only tested an standalone server to measure the stripping perf of Lizardfs without any replication
- I modified the [Storage Tuner Benchmark](https://github.com/Korkman/storage-tuner-benchmark) to report result against a single SSD access
  ``` diff
  diff --git a/storage-tuner-benchmark b/storage-tuner-benchmark
  index 3f60413..0b0617a 100755
  --- a/storage-tuner-benchmark
  +++ b/storage-tuner-benchmark
  @@ -174,9 +174,42 @@ Testgroup ""current""
   16 files, 16 threads each, rnd 16k reads, native aio: 494 read iops (123% HDD)
   HEREDOC
   )
  -	
  +
  +	# HGST Ultrastar SS300 HUSMR3280ASS200 (800GB SAS 12GB/s)
   	myBaselineSSD=$(cat <<'HEREDOC'
  -TODO
  +storage-tuner-benchmark version 2.1.0
  +Testgroup ""current""
  +===   1 file series   ===
  +1 file, 1 thread, seq 1M writes, simple: 672 write iops (371% HDD)
  +1 file, 1 thread, rnd 16k writes, simple: 19084 write iops (2680% HDD)
  +1 file, 1 thread, rnd 16k writes, simple, take 2: 19215 write iops (2920% HDD)
  +1 file, 16 threads, rnd 4k writes, posixaio: 66057 write iops (18819% HDD)
  +1 file, 16 threads, rnd 8k writes, posixaio: 46166 write iops (10888% HDD)
  +1 file, 16 threads, rnd 16k writes, posixaio: 28990 write iops (6485% HDD)
  +1 file, 16 threads, rnd 16k writes, posixaio, take 2: 29104 write iops (5774% HDD)
  +===  16 file series   ===
  +16 files, 1 thread each, seq 1M writes, simple: 994 write iops (662% HDD)
  +16 files, 1 thread each, rnd 16k writes, simple: 55165 write iops (8435% HDD)
  +16 files, 1 thread each, rnd 16k writes, simple, take 2: 55277 write iops (8816% HDD)
  +16 files, 1 thread each, rnd 16k writes, posixaio: 53785 write iops (11541% HDD)
  +16 files, 16 threads each, rnd 16k writes, posixaio: 54710 write iops (10188% HDD)
  +16 files, 16 threads each, rnd 16k writes, posixaio, take 2: 54462 write iops (10870% HDD)
  +===   O_SYNC series   ===
  +1 file, 1 thread, rnd 16k writes, simple, o_sync: 18798 write iops (12127% HDD)
  +1 file, 16 threads, rnd 16k writes, posixaio, o_sync: 18012 write iops (7473% HDD)
  +16 files, 1 thread each, rnd 16k writes, simple, o_sync: 53479 write iops (10589% HDD)
  +16 files, 16 threads each, rnd 16k writes, posixaio, o_sync: 53158 write iops (10443% HDD)
  +===    read series    ===
  +1 file, 1 thread, seq 1M reads, simple: 781 read iops (20% HDD)
  +1 file, 16 threads, rnd 16k reads, posixaio: 5142 read iops (3007% HDD)
  +16 files, 1 thread each, seq 1M reads, simple: 1077 read iops (26% HDD)
  +16 files, 1 thread each, rnd 16k reads, posixaio: 48398 read iops (10297% HDD)
  +16 files, 16 threads each, rnd 16k reads, posixaio: 48936 read iops (9866% HDD)
  +=== native aio series ===
  +1 file, 16 threads, rnd 16k writes, native aio: 18903 write iops (3083% HDD)
  +16 files, 16 threads each, rnd 16k writes, native aio: 55334 write iops (11044% HDD)
  +1 file, 16 threads, rnd 16k reads, native aio: 5062 read iops (3864% HDD)
  +16 files, 16 threads each, rnd 16k reads, native aio: 49661 read iops (10052% HDD)
   HEREDOC
   )
  ```
- I made the same tests against a software `RAID0` (`mdadm`)



## Setup

I'm testing a new infrastructure composed of 5 servers, each one is:

- Debian Buster
- 72 cores Xeon(R) Gold 5220 CPU @ 2.20GHz
- 396GB of RAM
- 10 * 800GB SSD SAS 12GB/s
- Kernel `4.19.0-6-amd64`
- Lizardfs `3.12.0+dfsg-3+b10` from distribution



## Tests

I tried several things to test the IO:

- put 335GB of ISO DVD in a `tmpfs` (under `/mnt/ramfs/isos`) and
  - copy the source directory to the mount point
  - copy the source directory using `parallel` with 20 concurrent jobs
  - `rsync` the source directory to the mount point
- run [storage-tuner-benchmark](https://github.com/Korkman/storage-tuner-benchmark)
- run ``bonnie++``
  - a single process
  ```
  bonnie++ -d /mnt/test/ -u bonnie -m $(hostname)-lizardfs -n 64:100000:16:64 -b -q > /root/lizardfs/bonnie++-single 2>&1
  ```
  - 10 processes
  ```
  bonnie++ -d /mnt/test -u bonnie -p10

  for i in {1..10}
  do
      (nohup bonnie++ -ys -d /mnt/test/ -u bonnie -m $(hostname)-lizardfs-${i} -r 24g -s 400G -n 64:100000:16:64 -b -q > /root/lizardfs/bonnie++-parallel-${i} 2>&1 & )
  done
  ```

To get statistics:

- I installed `collectd` (5 seconds measurement interval) and send everything to a dedicated machine with `graphite` and `grafana`. This is very light on each server for a very good accuracy.
- I installed `bwm-ng` (`bwm-ng -i disk -T avg -d 1 -t 2000`)


## Lizardfs stripping perf (standalone server without any replication)

Now, I'm testing without any replication just to see how lizardfs perform locally, on each server I have:

- 10 SSD without RAID formatted as XFS
  - each one has `/etc/fstab` options `rw,noexec,nodev,noatime,nodiratime,noquota,largeio,inode64,barrier=0`
  - when mounted, each one has `/proc/mounts` options `rw,relatime,attr2,inode64,noquota` (other options are not in [xfs(5)](https://manpages.debian.org/buster/xfsprogs/xfs.5.en.html))
- a local master
- a local chunkserver connected to the local master and using the 10 SSD (so stripping data between each one if I understand correctly)

So each server has a lizardfs of 7.2TB available space used only locally with the mount point `/mnt/test`:

- `/etc/fstab` entry 
  ```
  mfsmount		/mnt/test	fuse	big_writes,cacheexpirationtime=0,mfsdelayedinit,mfsmaster=127.0.0.1,_netdev	0 0
  ```
- `/proc/mounts` (`big_writes` has no effect? It's not in [mount.fuse(8)](https://manpages.debian.org/buster/fuse/mount.fuse.8.en.html) manpage)
  ```
  mfs#127.0.0.1:9421 /mnt/test fuse rw,relatime,user_id=0,group_id=0,default_permissions,allow_other 0 0
  ```

### Copy of 335GB from RAM

I just run `cp -r /mnt/ramfs/isos /mnt/test/`:

- `cgiserv`: around 900MiB/s on each SSD
- `collectd`: around 40MB/s on each SSD (60 to 90 IO/s per SSD in 5 minutes average)
- `bwm-ng`: between 30MB/s to 40MB/s on each SSD

According to `cgiserv` the copy should take 40 seconds (335GB at 9GB/s) but it took 16 minutes (350MB/s which is 5 times slower than `RAID0`).


### Copy of 335GB from RAM with 20 parallel jobs

I run `find . | parallel -j 20 cp {} /mnt/test/isos/`

- `cgiserv`: around 1.1GiB/s on each SSD
- `collectd`: around 130MB/s

According to `cgiserv` the copy should take 30 seconds (335GB at 11GB/s) but it took 4 minutes 20 seconds (1.29GB/s which is 6 to 8 times slower than `RAID0`).


### rsync of 335GB from RAM

I run `rysnc -rvWP /mnt/ramfs/isos/ /mnt/test/isos/`:

- `cgiserv`: around 900MB/s on each SSD
- `collectd`: around 25MB/s on each SSD (50 IO/s in 5 minutes average)
- `bwm-ng`: around 25MB/s on each SSD

According to `cgiserv` the `rsync` should take 40 seconds (335GB at 9GB/s) but it took 26 minutes (214MB/s which is 2 times slower than `RAID0`).

Here is the output of `rsync` itself:

```
sending incremental file list
created directory /mnt/test/isos
./
dvd1.iso

  5,441,959,936 100%  201.73MB/s    0:00:25 (xfr#1, to-chk=65/67)
dvd10.iso

  5,441,959,936 100%  206.52MB/s    0:00:25 (xfr#2, to-chk=64/67)
dvd11.iso

  5,441,959,936 100%  220.23MB/s    0:00:23 (xfr#3, to-chk=63/67)
dvd12.iso

  5,441,959,936 100%  215.45MB/s    0:00:24 (xfr#4, to-chk=62/67)
dvd13.iso

  5,441,959,936 100%  222.06MB/s    0:00:23 (xfr#5, to-chk=61/67)
dvd14.iso

  5,441,959,936 100%  228.87MB/s    0:00:22 (xfr#6, to-chk=60/67)
dvd15.iso

  5,441,959,936 100%  257.97MB/s    0:00:20 (xfr#7, to-chk=59/67)
dvd16.iso

  5,441,959,936 100%  219.18MB/s    0:00:23 (xfr#8, to-chk=58/67)
dvd17.iso

  5,441,959,936 100%  210.99MB/s    0:00:24 (xfr#9, to-chk=57/67)
dvd18.iso

  5,441,959,936 100%  212.12MB/s    0:00:24 (xfr#10, to-chk=56/67)
dvd19.iso

  5,441,959,936 100%  209.28MB/s    0:00:24 (xfr#11, to-chk=55/67)
dvd2.iso

  5,441,959,936 100%  218.19MB/s    0:00:23 (xfr#12, to-chk=54/67)
dvd20.iso

  5,441,959,936 100%  207.30MB/s    0:00:25 (xfr#13, to-chk=53/67)
dvd21.iso

  5,441,959,936 100%  222.25MB/s    0:00:23 (xfr#14, to-chk=52/67)
dvd22.iso

  5,441,959,936 100%  218.25MB/s    0:00:23 (xfr#15, to-chk=51/67)
dvd23.iso

  5,441,959,936 100%  214.62MB/s    0:00:24 (xfr#16, to-chk=50/67)
dvd24.iso

  5,441,959,936 100%  215.16MB/s    0:00:24 (xfr#17, to-chk=49/67)
dvd25.iso

  5,441,959,936 100%  227.52MB/s    0:00:22 (xfr#18, to-chk=48/67)
dvd26.iso

  5,441,959,936 100%  207.30MB/s    0:00:25 (xfr#19, to-chk=47/67)
dvd27.iso

  5,441,959,936 100%  227.74MB/s    0:00:22 (xfr#20, to-chk=46/67)
dvd28.iso

  5,441,959,936 100%  208.60MB/s    0:00:24 (xfr#21, to-chk=45/67)
dvd29.iso

  5,441,959,936 100%  205.47MB/s    0:00:25 (xfr#22, to-chk=44/67)
dvd3.iso

  5,441,959,936 100%  212.07MB/s    0:00:24 (xfr#23, to-chk=43/67)
dvd30.iso

  5,441,959,936 100%  216.21MB/s    0:00:24 (xfr#24, to-chk=42/67)
dvd31.iso

  5,441,959,936 100%  230.70MB/s    0:00:22 (xfr#25, to-chk=41/67)
dvd32.iso

  5,441,959,936 100%  217.72MB/s    0:00:23 (xfr#26, to-chk=40/67)
dvd33.iso

  5,441,959,936 100%  207.39MB/s    0:00:25 (xfr#27, to-chk=39/67)
dvd34.iso

  5,441,959,936 100%  231.04MB/s    0:00:22 (xfr#28, to-chk=38/67)
dvd35.iso

  5,441,959,936 100%  220.92MB/s    0:00:23 (xfr#29, to-chk=37/67)
dvd36.iso

  5,441,959,936 100%  204.09MB/s    0:00:25 (xfr#30, to-chk=36/67)
dvd37.iso

  5,441,959,936 100%  206.99MB/s    0:00:25 (xfr#31, to-chk=35/67)
dvd38.iso

  5,441,959,936 100%  215.03MB/s    0:00:24 (xfr#32, to-chk=34/67)
dvd39.iso

  5,441,959,936 100%  229.02MB/s    0:00:22 (xfr#33, to-chk=33/67)
dvd4.iso

  5,441,959,936 100%  241.97MB/s    0:00:21 (xfr#34, to-chk=32/67)
dvd40.iso

  5,441,959,936 100%  217.49MB/s    0:00:23 (xfr#35, to-chk=31/67)
dvd41.iso

  5,441,959,936 100%  219.97MB/s    0:00:23 (xfr#36, to-chk=30/67)
dvd42.iso

  5,441,959,936 100%  216.25MB/s    0:00:23 (xfr#37, to-chk=29/67)
dvd43.iso

  5,441,959,936 100%  235.31MB/s    0:00:22 (xfr#38, to-chk=28/67)
dvd44.iso

  5,441,959,936 100%  233.85MB/s    0:00:22 (xfr#39, to-chk=27/67)
dvd45.iso

  5,441,959,936 100%  214.33MB/s    0:00:24 (xfr#40, to-chk=26/67)
dvd46.iso

  5,441,959,936 100%  208.43MB/s    0:00:24 (xfr#41, to-chk=25/67)
dvd47.iso

  5,441,959,936 100%  207.77MB/s    0:00:24 (xfr#42, to-chk=24/67)
dvd48.iso

  5,441,959,936 100%  203.35MB/s    0:00:25 (xfr#43, to-chk=23/67)
dvd49.iso

  5,441,959,936 100%  210.28MB/s    0:00:24 (xfr#44, to-chk=22/67)
dvd5.iso

  5,441,959,936 100%  214.17MB/s    0:00:24 (xfr#45, to-chk=21/67)
dvd50.iso

  5,441,959,936 100%  221.94MB/s    0:00:23 (xfr#46, to-chk=20/67)
dvd51.iso

  5,441,959,936 100%  218.02MB/s    0:00:23 (xfr#47, to-chk=19/67)
dvd52.iso

  5,441,959,936 100%  218.07MB/s    0:00:23 (xfr#48, to-chk=18/67)
dvd53.iso

  5,441,959,936 100%  223.86MB/s    0:00:23 (xfr#49, to-chk=17/67)
dvd54.iso

  5,441,959,936 100%  215.84MB/s    0:00:24 (xfr#50, to-chk=16/67)
dvd55.iso

  5,441,959,936 100%  213.72MB/s    0:00:24 (xfr#51, to-chk=15/67)
dvd56.iso

  5,441,959,936 100%  212.62MB/s    0:00:24 (xfr#52, to-chk=14/67)
dvd57.iso

  5,441,959,936 100%  214.78MB/s    0:00:24 (xfr#53, to-chk=13/67)
dvd58.iso

  5,441,959,936 100%  213.39MB/s    0:00:24 (xfr#54, to-chk=12/67)
dvd59.iso

  5,441,959,936 100%  217.41MB/s    0:00:23 (xfr#55, to-chk=11/67)
dvd6.iso

  5,441,959,936 100%  202.61MB/s    0:00:25 (xfr#56, to-chk=10/67)
dvd60.iso

  5,441,959,936 100%  201.11MB/s    0:00:25 (xfr#57, to-chk=9/67)
dvd61.iso

  5,441,959,936 100%  198.87MB/s    0:00:26 (xfr#58, to-chk=8/67)
dvd62.iso

  5,441,959,936 100%  209.06MB/s    0:00:24 (xfr#59, to-chk=7/67)
dvd63.iso

  5,441,959,936 100%  202.95MB/s    0:00:25 (xfr#60, to-chk=6/67)
dvd64.iso

  5,441,959,936 100%  219.84MB/s    0:00:23 (xfr#61, to-chk=5/67)
dvd65.iso

  5,441,959,936 100%  212.42MB/s    0:00:24 (xfr#62, to-chk=4/67)
dvd66.iso

  5,441,959,936 100%  211.74MB/s    0:00:24 (xfr#63, to-chk=3/67)
dvd7.iso

  5,441,959,936 100%  207.44MB/s    0:00:25 (xfr#64, to-chk=2/67)
dvd8.iso

  5,441,959,936 100%  213.68MB/s    0:00:24 (xfr#65, to-chk=1/67)
dvd9.iso

  5,441,959,936 100%  217.59MB/s    0:00:23 (xfr#66, to-chk=0/67)

sent 359,257,047,755 bytes  received 1,310 bytes  230,366,815.69 bytes/sec
total size is 359,169,355,776  speedup is 1.00
```


### Storage Tuner Benchmark

No need to compare against `RAID0` with the `SSD` percentage initialised with one of the disk:

```
Creating test directory ""stb-testdir""
Running tests in ""/mnt/test/stb-testdir"" on nebula80 @ 2020-02-04 14:58:12 ...
storage-tuner-benchmark version 2.1.0
Testgroup ""current""
===   1 file series   ===
1 file, 1 thread, seq 1M writes, simple: 371 write iops (204% HDD / 55% SSD)
1 file, 1 thread, rnd 16k writes, simple: 1159 write iops (162% HDD / 6% SSD)
1 file, 1 thread, rnd 16k writes, simple, take 2: 1197 write iops (181% HDD / 6% SSD)
1 file, 16 threads, rnd 4k writes, posixaio: 1786 write iops (508% HDD / 2% SSD)
1 file, 16 threads, rnd 8k writes, posixaio: 1792 write iops (422% HDD / 3% SSD)
1 file, 16 threads, rnd 16k writes, posixaio: 1774 write iops (396% HDD / 6% SSD)
1 file, 16 threads, rnd 16k writes, posixaio, take 2: 1786 write iops (354% HDD / 6% SSD)
===  16 file series   ===
16 files, 1 thread each, seq 1M writes, simple: 1145 write iops (763% HDD / 115% SSD)
16 files, 1 thread each, rnd 16k writes, simple: 15893 write iops (2430% HDD / 28% SSD)
16 files, 1 thread each, rnd 16k writes, simple, take 2: 15740 write iops (2510% HDD / 28% SSD)
16 files, 1 thread each, rnd 16k writes, posixaio: 15811 write iops (3392% HDD / 29% SSD)
16 files, 16 threads each, rnd 16k writes, posixaio: 53541 write iops (9970% HDD / 97% SSD)
16 files, 16 threads each, rnd 16k writes, posixaio, take 2: 51728 write iops (10324% HDD / 94% SSD)
===   O_SYNC series   ===
1 file, 1 thread, rnd 16k writes, simple, o_sync: 772 write iops (498% HDD / 4% SSD)
1 file, 16 threads, rnd 16k writes, posixaio, o_sync: 829 write iops (343% HDD / 4% SSD)
16 files, 1 thread each, rnd 16k writes, simple, o_sync: 15956 write iops (3159% HDD / 29% SSD)
16 files, 16 threads each, rnd 16k writes, posixaio, o_sync: 15625 write iops (3069% HDD / 29% SSD)
===    read series    ===
1 file, 1 thread, seq 1M reads, simple: 294 read iops (7% HDD / 37% SSD)
1 file, 16 threads, rnd 16k reads, posixaio: 1992 read iops (1164% HDD / 38% SSD)
16 files, 1 thread each, seq 1M reads, simple: 3779 read iops (93% HDD / 350% SSD)
16 files, 1 thread each, rnd 16k reads, posixaio: 55924 read iops (11898% HDD / 115% SSD)
16 files, 16 threads each, rnd 16k reads, posixaio: 56470 read iops (11385% HDD / 115% SSD)
=== native aio series ===
1 file, 16 threads, rnd 16k writes, native aio: 820 write iops (133% HDD / 4% SSD)
16 files, 16 threads each, rnd 16k writes, native aio: 16161 write iops (3225% HDD / 29% SSD)
1 file, 16 threads, rnd 16k reads, native aio: 1633 read iops (1246% HDD / 32% SSD)
16 files, 16 threads each, rnd 16k reads, native aio: 55845 read iops (11304% HDD / 112% SSD)
Tests complete on nebula80 @ 2020-02-04 15:00:30.
Files remain. To clean up, add argument ""cleanup"".
Cleanup
Done.
```


### Bonnie++

It takes ages to complete, during the running, at one point in time I had:

- `cgiserv`: around 900MB/s for each SSD
- `collectd`: around 8MB/s for each SSD
- `bwm-ng`: around 7MB/s for each SSD

`bonnie++` finally failed after hours (more than 10hours) with the following output:

```
Bonnie: drastic I/O error (rmdir): Directory not empty
Cleaning up test directory after error.
```



## Raw perf without Lizardfs

To make a comparison, I started with a software `RAID0` with `mdadm` using all the 10 SSD.

The monitoring is against the `RAID0` device `/dev/md0`.

### Copy of 335GB from RAM

I just run `cp -r /mnt/ramfs/isos /mnt/test/`, `collectd` tells me:

⮕ around 2GB/s
⮕ between 3.5k IOPS to 4.5k IOPS

Which is coherent with the output of `bwm-ng`.

### Copy of 335GB from RAM with 20 parallel jobs

I run `find . | parallel -j 20 cp {} /mnt/test/isos/`, `collectd` tells me:

⮕ between 6GB/s to 8GB/s
⮕ around 20k IOPS

It's consistent with the output of `bwm-ng`

### rsync of 335GB from RAM

`rsync` is quite slow in itself compared to `cp`, `collectd` tells me:

⮕ around 500MB/s
⮕ between 700 IOPS to 1.2k IOPS

Here is the `rsync` output itself:

```
sending incremental file list
created directory /mnt/test/isos
./
dvd1.iso

  5,441,959,936 100%  502.16MB/s    0:00:10 (xfr#1, to-chk=65/67)
dvd10.iso

  5,441,959,936 100%  473.96MB/s    0:00:10 (xfr#2, to-chk=64/67)
dvd11.iso

  5,441,959,936 100%  455.45MB/s    0:00:11 (xfr#3, to-chk=63/67)
dvd12.iso

  5,441,959,936 100%  474.22MB/s    0:00:10 (xfr#4, to-chk=62/67)
dvd13.iso

  5,441,959,936 100%  441.69MB/s    0:00:11 (xfr#5, to-chk=61/67)
dvd14.iso

  5,441,959,936 100%  451.84MB/s    0:00:11 (xfr#6, to-chk=60/67)
dvd15.iso

  5,441,959,936 100%  465.75MB/s    0:00:11 (xfr#7, to-chk=59/67)
dvd16.iso

  5,441,959,936 100%  484.31MB/s    0:00:10 (xfr#8, to-chk=58/67)
dvd17.iso

  5,441,959,936 100%  461.44MB/s    0:00:11 (xfr#9, to-chk=57/67)
dvd18.iso

  5,441,959,936 100%  484.40MB/s    0:00:10 (xfr#10, to-chk=56/67)
dvd19.iso

  5,441,959,936 100%  459.40MB/s    0:00:11 (xfr#11, to-chk=55/67)
dvd2.iso

  5,441,959,936 100%  470.01MB/s    0:00:11 (xfr#12, to-chk=54/67)
dvd20.iso

  5,441,959,936 100%  489.01MB/s    0:00:10 (xfr#13, to-chk=53/67)
dvd21.iso

  5,441,959,936 100%  463.09MB/s    0:00:11 (xfr#14, to-chk=52/67)
dvd22.iso

  5,441,959,936 100%  478.02MB/s    0:00:10 (xfr#15, to-chk=51/67)
dvd23.iso

  5,441,959,936 100%  451.53MB/s    0:00:11 (xfr#16, to-chk=50/67)
dvd24.iso

  5,441,959,936 100%  475.31MB/s    0:00:10 (xfr#17, to-chk=49/67)
dvd25.iso

  5,441,959,936 100%  456.77MB/s    0:00:11 (xfr#18, to-chk=48/67)
dvd26.iso

  5,441,959,936 100%  480.41MB/s    0:00:10 (xfr#19, to-chk=47/67)
dvd27.iso

  5,441,959,936 100%  457.05MB/s    0:00:11 (xfr#20, to-chk=46/67)
dvd28.iso

  5,441,959,936 100%  472.54MB/s    0:00:10 (xfr#21, to-chk=45/67)
dvd29.iso

  5,441,959,936 100%  449.11MB/s    0:00:11 (xfr#22, to-chk=44/67)
dvd3.iso

  5,441,959,936 100%  457.30MB/s    0:00:11 (xfr#23, to-chk=43/67)
dvd30.iso

  5,441,959,936 100%  473.96MB/s    0:00:10 (xfr#24, to-chk=42/67)
dvd31.iso

  5,441,959,936 100%  455.17MB/s    0:00:11 (xfr#25, to-chk=41/67)
dvd32.iso

  5,441,959,936 100%  479.61MB/s    0:00:10 (xfr#26, to-chk=40/67)
dvd33.iso

  5,441,959,936 100%  464.04MB/s    0:00:11 (xfr#27, to-chk=39/67)
dvd34.iso

  5,441,959,936 100%  487.17MB/s    0:00:10 (xfr#28, to-chk=38/67)
dvd35.iso

  5,441,959,936 100%  463.13MB/s    0:00:11 (xfr#29, to-chk=37/67)
dvd36.iso

  5,441,959,936 100%  481.79MB/s    0:00:10 (xfr#30, to-chk=36/67)
dvd37.iso

  5,441,959,936 100%  460.79MB/s    0:00:11 (xfr#31, to-chk=35/67)
dvd38.iso

  5,441,959,936 100%  483.32MB/s    0:00:10 (xfr#32, to-chk=34/67)
dvd39.iso

  5,441,959,936 100%  456.29MB/s    0:00:11 (xfr#33, to-chk=33/67)
dvd4.iso

  5,441,959,936 100%  461.65MB/s    0:00:11 (xfr#34, to-chk=32/67)
dvd40.iso

  5,441,959,936 100%  486.35MB/s    0:00:10 (xfr#35, to-chk=31/67)
dvd41.iso

  5,441,959,936 100%  461.57MB/s    0:00:11 (xfr#36, to-chk=30/67)
dvd42.iso

  5,441,959,936 100%  484.76MB/s    0:00:10 (xfr#37, to-chk=29/67)
dvd43.iso

  5,441,959,936 100%  465.54MB/s    0:00:11 (xfr#38, to-chk=28/67)
dvd44.iso

  5,441,959,936 100%  475.04MB/s    0:00:10 (xfr#39, to-chk=27/67)
dvd45.iso

  5,441,959,936 100%  445.75MB/s    0:00:11 (xfr#40, to-chk=26/67)
dvd46.iso

  5,441,959,936 100%  463.38MB/s    0:00:11 (xfr#41, to-chk=25/67)
dvd47.iso

  5,441,959,936 100%  474.31MB/s    0:00:10 (xfr#42, to-chk=24/67)
dvd48.iso

  5,441,959,936 100%  446.90MB/s    0:00:11 (xfr#43, to-chk=23/67)
dvd49.iso

  5,441,959,936 100%  464.67MB/s    0:00:11 (xfr#44, to-chk=22/67)
dvd5.iso

  5,441,959,936 100%  485.67MB/s    0:00:10 (xfr#45, to-chk=21/67)
dvd50.iso

  5,441,959,936 100%  458.51MB/s    0:00:11 (xfr#46, to-chk=20/67)
dvd51.iso

  5,441,959,936 100%  481.43MB/s    0:00:10 (xfr#47, to-chk=19/67)
dvd52.iso

  5,441,959,936 100%  457.05MB/s    0:00:11 (xfr#48, to-chk=18/67)
dvd53.iso

  5,441,959,936 100%  476.57MB/s    0:00:10 (xfr#49, to-chk=17/67)
dvd54.iso

  5,441,959,936 100%  452.79MB/s    0:00:11 (xfr#50, to-chk=16/67)
dvd55.iso

  5,441,959,936 100%  467.30MB/s    0:00:11 (xfr#51, to-chk=15/67)
dvd56.iso

  5,441,959,936 100%  473.01MB/s    0:00:10 (xfr#52, to-chk=14/67)
dvd57.iso

  5,441,959,936 100%  442.86MB/s    0:00:11 (xfr#53, to-chk=13/67)
dvd58.iso

  5,441,959,936 100%  453.78MB/s    0:00:11 (xfr#54, to-chk=12/67)
dvd59.iso

  5,441,959,936 100%  464.83MB/s    0:00:11 (xfr#55, to-chk=11/67)
dvd6.iso

  5,441,959,936 100%  473.31MB/s    0:00:10 (xfr#56, to-chk=10/67)
dvd60.iso

  5,441,959,936 100%  444.45MB/s    0:00:11 (xfr#57, to-chk=9/67)
dvd61.iso

  5,441,959,936 100%  454.81MB/s    0:00:11 (xfr#58, to-chk=8/67)
dvd62.iso

  5,441,959,936 100%  463.50MB/s    0:00:11 (xfr#59, to-chk=7/67)
dvd63.iso

  5,441,959,936 100%  474.00MB/s    0:00:10 (xfr#60, to-chk=6/67)
dvd64.iso

  5,441,959,936 100%  443.84MB/s    0:00:11 (xfr#61, to-chk=5/67)
dvd65.iso

  5,441,959,936 100%  455.13MB/s    0:00:11 (xfr#62, to-chk=4/67)
dvd66.iso

  5,441,959,936 100%  465.79MB/s    0:00:11 (xfr#63, to-chk=3/67)
dvd7.iso

  5,441,959,936 100%  481.48MB/s    0:00:10 (xfr#64, to-chk=2/67)
dvd8.iso

  5,441,959,936 100%  456.61MB/s    0:00:11 (xfr#65, to-chk=1/67)
dvd9.iso

  5,441,959,936 100%  478.33MB/s    0:00:10 (xfr#66, to-chk=0/67)

sent 359,257,047,755 bytes  received 1,310 bytes  513,591,206.67 bytes/sec
total size is 359,169,355,776  speedup is 1.00
```

### Storage Tuner Benchmark

```
Creating test directory ""stb-testdir""
Running tests in ""/mnt/test/stb-testdir"" on nebula80 @ 2020-02-04 08:46:58 ...
storage-tuner-benchmark version 2.1.0
Testgroup ""current""
===   1 file series   ===
1 file, 1 thread, seq 1M writes, simple: 1426 write iops (787% HDD / 212% SSD)
1 file, 1 thread, rnd 16k writes, simple: 17856 write iops (2507% HDD / 93% SSD)
1 file, 1 thread, rnd 16k writes, simple, take 2: 17927 write iops (2724% HDD / 93% SSD)
1 file, 16 threads, rnd 4k writes, posixaio: 61295 write iops (17462% HDD / 92% SSD)
1 file, 16 threads, rnd 8k writes, posixaio: 51210 write iops (12077% HDD / 110% SSD)
1 file, 16 threads, rnd 16k writes, posixaio: 36122 write iops (8080% HDD / 124% SSD)
1 file, 16 threads, rnd 16k writes, posixaio, take 2: 36721 write iops (7285% HDD / 126% SSD)
===  16 file series   ===
16 files, 1 thread each, seq 1M writes, simple: 4830 write iops (3220% HDD / 485% SSD)
16 files, 1 thread each, rnd 16k writes, simple: 179819 write iops (27495% HDD / 325% SSD)
16 files, 1 thread each, rnd 16k writes, simple, take 2: 180410 write iops (28773% HDD / 326% SSD)
16 files, 1 thread each, rnd 16k writes, posixaio: 146310 write iops (31396% HDD / 272% SSD)
16 files, 16 threads each, rnd 16k writes, posixaio: 309495 write iops (57634% HDD / 565% SSD)
16 files, 16 threads each, rnd 16k writes, posixaio, take 2: 310399 write iops (61955% HDD / 569% SSD)
===   O_SYNC series   ===
1 file, 1 thread, rnd 16k writes, simple, o_sync: 16672 write iops (10756% HDD / 88% SSD)
1 file, 16 threads, rnd 16k writes, posixaio, o_sync: 16675 write iops (6919% HDD / 92% SSD)
16 files, 1 thread each, rnd 16k writes, simple, o_sync: 175915 write iops (34834% HDD / 328% SSD)
16 files, 16 threads each, rnd 16k writes, posixaio, o_sync: 177008 write iops (34775% HDD / 332% SSD)
===    read series    ===
1 file, 1 thread, seq 1M reads, simple: 2900 read iops (77% HDD / 371% SSD)
1 file, 16 threads, rnd 16k reads, posixaio: 5105 read iops (2985% HDD / 99% SSD)
16 files, 1 thread each, seq 1M reads, simple: 5797 read iops (143% HDD / 538% SSD)
16 files, 1 thread each, rnd 16k reads, posixaio: 66565 read iops (14162% HDD / 137% SSD)
16 files, 16 threads each, rnd 16k reads, posixaio: 69165 read iops (13944% HDD / 141% SSD)
=== native aio series ===
1 file, 16 threads, rnd 16k writes, native aio: 17903 write iops (2920% HDD / 94% SSD)
16 files, 16 threads each, rnd 16k writes, native aio: 178593 write iops (35647% HDD / 322% SSD)
1 file, 16 threads, rnd 16k reads, native aio: 5023 read iops (3834% HDD / 99% SSD)
16 files, 16 threads each, rnd 16k reads, native aio: 70406 read iops (14252% HDD / 141% SSD)
Tests complete on nebula80 @ 2020-02-04 08:49:15.
Files remain. To clean up, add argument ""cleanup"".
Cleanup
Done.
```

### Bonnie++

```
Version  1.98       ------Sequential Output------ --Sequential Input- --Random-
                    -Per Chr- --Block-- -Rewrite- -Per Chr- --Block-- --Seeks--
Name:Size etc        /sec %CP  /sec %CP  /sec %CP  /sec %CP  /sec %CP  /sec %CP
nebula80-raid0 753G 2694k  99  1.7g  99  1.6g  91 4665k  99  3.0g  99 +++++ +++
Latency              6370us    1943us   67110us    2430us    5762us    1696us
Version  1.98       ------Sequential Create------ --------Random Create--------
nebula80-raid0      -Create-- --Read--- -Delete-- -Create-- --Read--- -Delete--
files:max:min        /sec %CP  /sec %CP  /sec %CP  /sec %CP  /sec %CP  /sec %CP
    64:100000:16/64 65536  32 65536  99 65536  46 65536  32 65536  99 65536  46
Latency              5662us    1528us   62180us    2861us    1628us   77778us
```
",I found that the `bonnie++` error is already solved by #662.,9800119
7,Add test to check that acls are being honored,open,2020-02-04T13:01:08Z,2020-02-07T13:50:37Z,,NONE,"I added a test that checks if acl’s are being honored. Your existing test ""test_acl_behaviour"" doesn't seem to check it in the right way.
It is associated with commit https://github.com/lizardfs/lizardfs/commit/e391f55014c263ab09b9c223c45bf23bb82ad961
 
It's very strange that this has not been created by LFS team already, as I believe it’s important to see in the future if it is broken and you didn't provide any documentation about your fix. It depends on what version of fuse you use/have and this is turn affects the OS that the user uses.
 
You continue with 3.13 that is unstable but don't provide tests for your fixes and documentation. When looking at your jenkins (http://jenkins.lizardfs.org/) it frightens me.",,9800119
8,mfsmetadump doesn't do UTF-8,open,2020-01-26T16:33:47Z,2020-02-06T13:39:55Z,,CONTRIBUTOR,"```
# mfsmetadump /var/lib/lizardfs/metadata.mfs|grep --line-buffered :B.*cher|head -1|xxd
00000000: 457c 703a 2020 2020 2020 3535 3339 7c63  E|p:      5539|c
00000010: 3a20 2020 2020 2035 3530 397c 6e3a 422e  :      5509|n:B.
00000020: 2e63 6865 720a                           .cher.
```
Uhh … '**What**? That's supposed to be ""Bücher"", i.e. ""Books"".

I *need* mfsmetadump/restore  to actually *work* because metadata.mfs is not architecture independent.",,9800119
9,Metadata Master RAM Usage,open,2020-01-24T19:32:35Z,2020-01-24T19:32:35Z,,NONE,"I have two clusters for comparison. The first is running 3.13rc1 with 11,116,931 and is using 4.1GB of RAM and a metadata.mfs size of 1.4GB. The second is running 3.13rc2 with 10,105,536 and is using 13GB of RAM and a metadata.mfs size of 5.5GB.

My understanding is that the Metadata Master's RAM usage was directly related to the number of chunks. So I'm curious why on one cluster I'm using 4.1GB of RAM with 11m chunks and on another I'm using 13GB of RAM with 10m chunks.

First, is my understanding of this correct, and that in one of these clusters I'm somehow using too much RAM, or is this right and I don't actually have an issue.

Second, if there is an issue, where would I start to troubleshoot this?",,9800119
10,Setup CI Builds With GitHub Releases ( And Other Notes on Releases ),open,2019-12-26T17:50:55Z,2020-03-10T17:14:53Z,,NONE,"This is somewhat related to #440 so @onlyjob you might be interested.

---

I wanted to know how the LizardFS developers would feel about getting automated builds setup for LizardFS on [Drone](https://drone.io). Drone is an Open Source CI server that I have experience with that also has a public cloud hosted for free for Open Source projects on GitHub. There would be no cost associated with establishing the builds.

I would want to get CI setup to push the built packages to GitHub releases whenever a release is made. I think it would be great for community testing if we also setup builds that would run on every push to master. These builds could upload the built artifacts to a specific `latest-master` release where anybody could try out the current latest commit of LizardFS. Obviously this `latest-master` release would have no guarantees of its stability.

If this is something that the dev team is fine with then I can create a PR for the build script. I could also tweak the workflow however you would like.

For the release strategy, I'm assuming a workflow of:

- Make commits/PRs to master
  - Every time master is updated the `latest-master` release on GitHub releases is updated with freshly-built packages
- Get master to a point where it is ready for release
- Create a release tag on master with the new version number
- CI builds the release tag and pushes the built artifacts to the GitHub release

That is just the model I had in mind for the CI script but I'm not sure exactly how things are done currently so this could be adapted to whatever you need.","Anybody have any input on this?

@antonboretskyi I'm going to be working on the Juju deployment for LizardFS and, while this isn't *required*, GitHub releases would help automate the container builds that I have to make against the LizardFS release artifacts and would aid in allowing us to provide deployments for newer versions of LizardFS.",9800119
11,How do you use LizardFS?,open,2019-12-26T04:15:05Z,2020-02-12T17:04:40Z,,NONE,"I just wanted to open a general topic asking how the people that use LizardFS are using it today. It would be valuable to know things like how big your cluster is ( number of nodes, cluster storage capacity ), what hardware you are deploying to ( VMs, bare metal, SSD, etc. ), and if you are using any deployment tools such as Docker, Ansible, Chef, etc. Possibly another good thing to know would be how you plan to use LizardFS in the future, if at all.

Obviously nobody is obliged to answer, but I think that it would help us get an idea of what improvements to target with LizardFS if we knew how people use it today.

---

To answer my own questions, I'm not using LizardFS at the moment.

In the past I used my team's [Docker image](https://github.com/katharostech/docker_lizardfs) and [Docker plugin](https://github.com/katharostech/docker-plugin_lizardfs) to deploy LizardFS on Docker Swarm. This was a 3 node cluster where the apps ran on the same servers as LizardFS. The servers were VMs in the cloud with SSDs.

The cluster hosted database storage and the file storage for all of the apps we hosted on those servers. Databases were noticeably slower on LizardFS, but it worked. We were running LizardFS 3.12.

With LizardFS development getting back up again, me and my team very possibly will use it in the future, but we don't have a specific use-case in mind yet.","We use LizardFS for content storage for television delivery.  3.12
Bare metal boxes hold drives, as we free up hardware from old raid arrays, we try to integrate what doesn't fail as well as some new hardware.

7 Servers: 2x5TB, 2X15TB, 2x22TB 1x44TB, for 127TB, with ~8TB free. Expected to grow.

A 3 server Proxmox cluster holds master, and a shadow, cgi-serv, this aids in the event of needing to do maintenance, snapshots.

10G backbone is planned, but missing cards.

We use Debian, XFS, mostly Areca sata controllers.

Our DAM now has support for managing Goals dynamically, changes to command line clients commands, would introduce changes for us. Nothing horrible, but knowing about the changes in advance would be helpful. 

RMlint is used for de-duplication using extended file attributes for hash storage, we also have support in out DAM to use/create this same (md5) hash. Any changes that might break extended file attributes would be unfortunate.

Clients are Debian and OSX, that windows client, or build instructions, would be ""handy""

I had some issues with a 2 chunk goal ending with odd missing chunks when losing a drive. Try to keep 3 chunks and or parity now if the content matters.

Other than that it's been pretty good, MUCH easier to manage than CEPH.
Not having to rebuild a raid array in the middle of a work week has been amazing, for everyone involved.",9800119
12,An update on the project progress for 2019,open,2019-12-16T14:16:51Z,2019-12-17T04:42:24Z,,NONE,"As promised we fixed the first 2 bugs in 3.13rc, we have been upgrading our testing suite on Jenkins, unfortunately due to many unexpected issues, we will be unable to complete the remaining bugs in 3.13 that we hoped for by the end of this year.

However we have been recruiting new developers to add to the team, and expect to have another 2 or 3 from the beginning of next year. This should help speed up the whole development process and get LizardFS back to the level it used to be, then push it even further!

Thank you to all the community that have stepped up and provided some insight into how they use LizardFS, what they like about it, what they dislike about it and what they would like to see added. For anybody else that would like to take part, drop us an email and we will send you some questions as a guideline for the article. Check out what we have so far, currently we have articles to take us through to February, would be great to get as many of the community as possible involved in this initiative.
[Articles](https://lizardfs.com/category/blogposts/)

Also anybody that would like to test LizardFS 3.13-rc2 and provide feedback that would be greatly appreciated.

Let me take this opportunity to thank all of the community for sticking with us through the difficult times we had in the project/company, the future is looking awesome! 

We  wish you a Merry Christmas, Happy Holidays and look forward to a fantastic New Year together.

From

The LizardFS Team
![image](https://user-images.githubusercontent.com/25224921/70913790-f8936500-2016-11ea-9643-31a9c0f86fc2.png)

","Hi and thanks for the update, appreciate the status update.

When it first came out, I did some testing with the 3.13rc in a virtual environment and was quite impressed with its robustness, though I didn't check ec modes, which I gather have some issues.

Since then, I've been a bit busy :( but things have eased off now, so I'd be interested in doing some more testing if wanted.",9800119
13,Write speed extremely slow,open,2019-12-13T17:42:54Z,2019-12-27T15:47:09Z,,CONTRIBUTOR,"I've noticed I'm having some issues, and I've narrowed it down to my pool write speed being _extremely_ slow.

```
$ dd if=/dev/zero of=/data/test2.img bs=1M count=1 oflag=dsync
1+0 records in
1+0 records out
1048576 bytes (1.0 MB, 1.0 MiB) copied, 309.986 s, 3.4 kB/s
```

I'm not sure where to begin troubleshooting...","@19wolf 
What are you trying to do? Do you still have slow writing? in which of your disks? you have 3xdisks at 99% capacity ...
So u change xor3 goal to single (1 copy only)? using labels?
Show me your Chunks section 
and what are you trying to do exactly?",9800119
14,HA and Network split (split brain),open,2019-12-12T08:56:42Z,2019-12-13T01:57:02Z,,NONE,"How does Lizardfs handle an event like a Network split in a HA Setup with like a Master server, two shadow masters servers and x number of chunk servers.  Like in an event where we lose 1-2 switches and the master and shadow servers might be located on different no longer connected switched, but maybe have acccess to some chunk servers, and some clients.

","Yes - it gives you added safety. How much that safety is worth to you varies... with multiple shadow masters, metaloggers aren't /that/ useful, but they don't take much resources and there's no real reason not to run them if you can.

(That said, there's no point running a logger on a server that is already a master. They're mostly a good idea to run on a chunkserver...)",9800119
15,Can't write data in ec & xor modes. Only empty files can be created,open,2019-12-11T10:34:31Z,2020-02-04T03:04:37Z,,NONE,"Hello!

I've installed lizardfs 3.12 on VM with OS Ubuntu 16.04. Created 6 disks 1GB each with ext4 and set `HDD_LEAVE_SPACE_DEFAULT = 100MiB`. It works fine in standard goals of replication modes.
My mfsgoal.cfg:
```
root@mfs1:~# cat /etc/mfs/mfsgoals.cfg
18 first_ec : $ec(3,1)

```
But as soon as I make `lizardfs setgoal -r first_es /mymountfolder` I can't write data into file using `echo ""test"" > /mymountfolder/test.log`, can't write files using cp, dd, rsync. Only empty files can be created. 
In syslog I can see the following errors:
`mfsmount: write file error, inode: 43, index: 0 - error sent by master server (No chunk servers) (try counter: 1)`

What can be a reason of this issue?","Is there a guide on the process of running multiple chunkservers on a single machine? In my case I have a very uneven distribution of disks across the machines in my cluster, and would like to split up the single machine with a peak in disk/chunk count.

(e.g:
Server 1 Disks: 4
Server 2 Disks: 4
Server 3 Disks: 4
Server 4 Disks: 4
Server 5 Disks: 4
Server 6 Disks: 16

-->

Server 6.1 Disks: 4
Server 6.2 Disks: 4
Server 6.3 Disks: 4
Server 6.4 Disks: 4
)

Would allow a switch from ec(4,2) to ec(6,2) correct?",9800119
16,Agama code?,open,2019-11-07T20:26:38Z,2019-11-08T07:56:16Z,,NONE,"I welcome the new beginning, and I wish all the best for your effort. I have read on the blog post from Patryk, ( https://lizardfs.com/introduction-to-the-community/ )  and I would like to know if there is any interest in opening up the Agama tree as well. We would welcome the opportunity to help in bringing it forward (even knowing that they are not stable/complete).","The code name for the rewritten core of Lizardfs; originally expected in 2018: 
https://archive.fosdem.org/2018/schedule/event/lizardfs/attachments/paper/2458/export/events/attachments/lizardfs/paper/2458/FOSDEM_SDS_Devroom_Presentation_2018.pdf",9800119
17,[feature request] - more advanced HA implementation without the use of the floating IP,open,2019-11-04T15:48:59Z,2019-11-14T17:01:42Z,,NONE,"Hi,

I am trying to understand how lizardfs works in HA configuration and how could I use it on my three VPS cluster.

I understand that uraft will try to assign the defined floating IP to one of the three masters.
something very similar to how keepalived works.

Does it mean that the setup is pretty much dedicated for local network and will likely not work with the VPS? 
Mainly because I am not able to assign IP in the datacentre local IP nd public IP probably only in some and only invoking additional API. ","I had an idea earlier about a possible workaround. It might require the development of a small proxy utility, but it wouldn't require changes to LizardFS:

Using uRaft, the LizardFS masters already know how to elect a leader to handle the requests. The uRaft package comes with a CLI that can show you what the current acting master is. The trick is then to run a proxy service on each of the masters that takes in requests from the chunkservers and clients and forwards them to the active master.

Because the LizardFS clients and chunkservers are not setup to allow providing multiple master addresses you would also have to run HAProxy or an equivalent tool on each of the chunkservers and clients that would listen on localhost and proxy the requests to any one of the available masters. Each chunkserver would have the master set to localhost on the proxy's port.",9800119
18,Endangered chunks not replicated,open,2019-10-28T15:56:19Z,2019-11-13T11:32:21Z,,NONE,"Hello, 

I am having a problem with endangered chunks which are never replicated.  They have been endangered for over a week now.    I am not sure how to fix them.  

list-defective-files shows 9 files as undergoal.  
The goal for all the files is 
```
Id      Name    Definition
3       3       3: $std {_ _ _}```

```# lizardfs-admin info 10.x.x.x 9421    
LizardFS v3.12.0
Memory usage:   13GiB
Total space:    118TiB
Available space:        90TiB
Trash space:    204GiB
Trash files:    154138
Reserved space: 399KiB
Reserved files: 4
FS objects:     33162265
Directories:    882997
Files:  32277575
Chunks: 32157551
Chunk copies:   93624676
Regular copies (deprecated):    93624676```

```# lizardfs-admin chunks-health  10.x.x.x 9421    
Chunks availability state:
        Goal    Safe    Unsafe  Lost
        3       30733915        5       -
        ec_small        1420942 -       -

Chunks replication state:
        Goal    0       1       2       3       4       5       6       7       8       9       10+
        3       30733911        4       5       -       -       -       -       -       -       -       -
        ec_small        1420942 -       -       -       -       -       -       -       -       -       -

Chunks deletion state:
        Goal    0       1       2       3       4       5       6       7       8       9       10+
        3       30733920        -       -       -       -       -       -       -       -       -       -
        ec_small        1420942 -       -       -       -       -       -       -       -       -       -```

I have tried file repair on the files to no avail. 

lizardfs fileinfo on a file gives this sort of thing
```         chunk 0: 0000000004DA8236_00000003 / (id:81429046 ver:3)
                copy 1: 10.x.x.22:9422:hdd
                copy 2: 10.x.x.23:9422:hdd
```

On 10.x.x.23 I can see a single error which may be related
```mfschunkserver[38841]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 10.x.x.22:9422)```

On 10.x.x.22 I see
```mfschunkserver[8158]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 10.x.x.21:9422)```

On 10.x.x.21 I see 
```mfschunkserver[3762]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 10.x.x.23:9422)```
```

Other chunk servers report similar messages saying either 21 or 23 has sent wrong 'Wrong chunk version' 

Is anyone else having this problem? Any ideas?  
","@jbarrigafk 
There were some messages from 2 days ago when i finished restarting the chunk servers.  There are many of the ""No such chunk"" messages from each chunk server when it was restarted.  The last few lines of the logs are here.  
```
Nov 11 23:49:05 gslfsmaster1 mfsmaster: (10.100.8.190:9422) chunk: 00000000047BAC0C deletion status: No such chunk
Nov 11 23:49:33 gslfsmaster1 mfsmaster: (10.100.8.190:9422) chunk: 0000000004D76393 deletion status: No such chunk
Nov 11 23:51:36 gslfsmaster1 mfsmaster: (10.100.8.190:9422) chunk: 0000000004E4CFB5 deletion status: No such chunk
Nov 12 06:25:06 gslfsmaster1 liblogging-stdlog:  [origin software=""rsyslogd"" swVersion=""8.24.0"" x-pid=""22886"" x-info=""http://www.rsyslog.com""] rsyslogd was HUPed
Nov 13 06:25:05 gslfsmaster1 liblogging-stdlog:  [origin software=""rsyslogd"" swVersion=""8.24.0"" x-pid=""22886"" x-info=""http://www.rsyslog.com""] rsyslogd was HUPed
```

On chunk servers I have almost nothing.  After finishing the rescan on one I have
```
# start of scan
Nov 11 22:21:11 gslfschunk01 mfschunkserver[3762]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 10.100.8.191:9422)
# end of scanq
Nov 11 23:20:22 gslfschunk01 mfschunkserver[51844]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 10.100.8.22:9422)
```
No other errors before or after in the last week. 

This chunk server seems to have the most issues
```
Nov 11 22:50:59 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 22:51:38 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 22:51:48 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 22:52:27 gslfschunk03 mfschunkserver[6797]: Received invalid response for chunk get block
Nov 11 22:52:27 gslfschunk03 mfschunkserver[6797]: replication error: Status 'No such chunk' sent by chunkserver (server 10.100.8.21:9422)
Nov 11 22:52:41 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 22:53:28 gslfschunk03 mfschunkserver[6797]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 10.100.8.191:9422)
Nov 11 22:53:31 gslfschunk03 mfschunkserver[6797]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 10.100.8.190:9422)
Nov 11 22:53:47 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 22:53:48 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 22:56:43 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 22:57:29 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 22:57:50 gslfschunk03 mfschunkserver[6797]: Received invalid response for chunk get block
Nov 11 22:57:50 gslfschunk03 mfschunkserver[6797]: replication error: Status 'No such chunk' sent by chunkserver (server 10.100.8.21:9422)
Nov 11 22:58:08 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 22:58:45 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 23:00:11 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 23:03:14 gslfschunk03 mfschunkserver[6797]: Received invalid response for chunk get block
Nov 11 23:03:14 gslfschunk03 mfschunkserver[6797]: replication error: Status 'No such chunk' sent by chunkserver (server 10.100.8.21:9422)
Nov 11 23:08:36 gslfschunk03 mfschunkserver[6797]: Received invalid response for chunk get block
Nov 11 23:08:36 gslfschunk03 mfschunkserver[6797]: replication error: Status 'No such chunk' sent by chunkserver (server 10.100.8.21:9422)
Nov 11 23:09:24 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 23:09:29 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 23:12:56 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 23:13:23 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 23:20:25 gslfschunk03 mfschunkserver[6797]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 10.100.8.190:9422)
Nov 11 23:23:14 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 23:42:30 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
Nov 11 23:55:14 gslfschunk03 mfschunkserver[6797]: (forward) write error: Broken pipe
```

Only 3/5 chunk servers showed errors. 



",9800119
19,Get data from lizardfs cluster slow,open,2019-10-08T16:15:36Z,2019-10-10T17:57:29Z,,NONE,"default mfsmount on client 
mfsmount /lizardfs

making a tar.gz of 2000 small files from inside lizardfs cluster:
```
time tar -zcvf /backup/local/2000.tar.gz -T /root/2019-09-2000-from-lizardfs.csv 2> /root/2000.err
real    0m12.746s
ej: 2019-09-2000-from-lizardfs.csv = /lizardfs/small.file1...2000.txt
```

making a tar.gz with the same 2000 small files from local disk outside the lizardfs cluster:
```
time tar -zcvf /backup/local/2000.tar.gz -T /root/2019-09-2000-from-local.csv 2> /root/2000.err
real    0m0.688s
ej: 2019-09-2000-from-local.csv = /var/local/small.file1...2000.txt
```
Is there a way to get lizardfs data faster? 

Tar of files inside lizardfs
0m12.746s

Tar of files outside lizardfs
real    0m0.688s

lizardfs 3.13.0	
Mfsmaster config:
```
 PERSONALITY = master
 WORKING_USER = mfs
 WORKING_GROUP = mfs
 DATA_PATH = /data/mfs
 MATOML_LISTEN_HOST = *
 MATOML_LISTEN_PORT = 9419
 MATOCS_LISTEN_HOST = *
 MATOCS_LISTEN_PORT = 9420
 MATOCL_LISTEN_HOST = *
 MATOCL_LISTEN_PORT = 9421
 MATOTS_LISTEN_HOST = *
 MATOTS_LISTEN_PORT = 9424
 CHUNKS_SOFT_DEL_LIMIT = 1000
 CHUNKS_HARD_DEL_LIMIT = 1000
 CHUNKS_WRITE_REP_LIMIT = 200
 CHUNKS_READ_REP_LIMIT = 200
 NO_ATIME = 0
```
Chunkservers(x6) config
```
LABEL = sv
MASTER_HOST = mfsmaster
ENABLE_LOAD_FACTOR = 1
NR_OF_NETWORK_WORKERS = 2
PERFORM_FSYNC = 0
```


","```
Test  #1
#10,000 small files  
time tar -zcf /backup/local/10000.tar.gz -T /backup/local/10000.txt 2> /backup/local/10000.err
real	1m18.564s
user	0m4.597s
sys	0m1.652s

Test  #2
#Same 10,000 small files
[root@mfschunk10-sv xmls]# time tar -zcf /backup/local/10000-2nd.tar.gz -T /backup/local/10000.txt 2> /backup/local/10000.err

real	0m13.080s
user	0m4.461s
sys	0m0.967s

Test  #3
#Same 10,000 small files
[root@mfschunk10-sv xmls]# time tar -zcf /backup/local/10000-3.tar.gz -T /backup/local/10000.txt 2> /backup/local/10000.err

real	0m12.953s
user	0m4.427s
sys	0m1.096s

Test  #4
#Different 100,000 small files
[root@mfschunk10-sv xmls]# time tar -zcf /backup/local/100000.tar.gz -T /backup/local/100000.txt 2> /backup/local/100000.err

real	13m19.034s
user	0m41.285s
sys	0m16.306s

Test  #5
#Same 100,000 small files
[root@mfschunk10-sv xmls]# time tar -zcf /backup/local/100000-2.tar.gz -T /backup/local/100000.txt 2> /backup/local/100000.err

real	2m19.745s
user	0m39.803s
sys	0m12.823s

Test  #6
#Same 100,000 small files
[root@mfschunk10-sv xmls]# time tar -zcf /backup/local/100000-3.tar.gz -T /backup/local/100000.txt 2> /backup/local/100000.err

real	2m23.853s
user	0m40.225s
sys	0m13.227s

Test  #7
#Same 10,000 small files
[root@mfschunk10-sv xmls]# time tar -I pigz -cf /backup/local/100000-4.tar.gz -T /backup/local/100000.txt 2> /backup/local/100000.err

real	2m16.300s
user	0m41.651s
sys	0m14.254s
```
Testing with the same file list, only the first time is when it takes too long.
After using the same list more than 1 time, times improve, but I don't know if it's because something stays in memory and that's why I do it faster.

however, I never make backups more than 1 time based on the same list.
The backups are approx. 10 million files based on a list.

Will there be any way to accelerate the extraction of information from lizardfs? maybe it influences the types of disks of the chunks and also the writing speed of the client, etc.",9800119
20,Add TLS support to LizardFS,open,2019-09-28T23:30:18Z,2019-09-29T09:50:45Z,,NONE,"Hi, could TLS support be added to LizardFS please?

I saw #566 but was closed by the author.",,9800119
21,Status regarding a new GUI for LizardFS?,open,2019-09-27T11:38:18Z,2019-11-18T08:58:37Z,,NONE,"Hi, it's been a long while since https://lizardfs.com/lizardfs-gui-the-first-look/ was published. When will this be done? It looks really nice for the outdated ui!","I believe once the new management gets everything up and running 100% we shall start seeing more progress on this

But man that looks awesome!",9800119
22,New secondary master crashes after download,open,2019-09-18T18:25:34Z,2020-01-26T17:37:32Z,,CONTRIBUTOR,"The new secondary manages to download `metadata.mfs` but then immediately segfaults.
```
Core was generated by `/usr/sbin/mfsmaster -d start'.
Program terminated with signal SIGSEGV, Segmentation fault.
#0  detail::IdPoolBlock<unsigned long, unsigned int>::markAsAcquired (id=0, this=0xaaab06ee2f30)
    at ./src/common/compact_vector.h:477
477	./src/common/compact_vector.h: No such file or directory.
(gdb) inf thr
  Id   Target Id                        Frame 
* 1    Thread 0xffffa1ecc010 (LWP 8576) detail::IdPoolBlock<unsigned long, unsigned int>::markAsAcquired
    (id=0, this=0xaaab06ee2f30) at ./src/common/compact_vector.h:477
(gdb) whe
#0  detail::IdPoolBlock<unsigned long, unsigned int>::markAsAcquired (id=0, this=0xaaab06ee2f30)
    at ./src/common/compact_vector.h:477
#1  IdPool<unsigned int>::markAsAcquired (id=0, this=0xffff9f87e048) at ./src/common/id_pool.h:401
#2  IdPool<unsigned int>::IdPool (cache_size=8192, block_size=262144, max_size=4294967279, 
    this=0xffff9f87e048) at ./src/common/id_pool.h:293
#3  IdPoolDetainer<unsigned int, unsigned int>::IdPoolDetainer (release_count=10, cache_size=8192, 
    block_size=262144, max_detention_size=4294967279, max_size=4294967279, 
    bucket_count=<optimized out>, detain_time=<optimized out>, this=0xffff9f87e048)
    at ./src/master/id_pool_detainer.h:351
#4  FilesystemMetadata::FilesystemMetadata (this=0xffff9f3fe010)
    at ./src/master/filesystem_metadata.h:103
#5  fs_strinit () at ./src/master/filesystem.cc:190
#6  0x0000aaaae7222de0 in fs_loadall () at ./src/master/filesystem.cc:208
#7  0x0000aaaae7266774 in masterconn_download_next (eptr=0xaaab06e88e00)
    at ./src/master/masterconn.cc:562
#8  0x0000aaaae7266dd4 in masterconn_gotpacket (eptr=0xaaab06e88e00, type=63, data=<optimized out>, 
    length=<optimized out>) at ./src/master/masterconn.cc:758
#9  0x0000aaaae72670b8 in masterconn_read (eptr=eptr@entry=0xaaab06e88e00)
    at ./src/master/masterconn.cc:956
#10 0x0000aaaae7267c1c in masterconn_serve (pdesc=std::vector of length 6, capacity 8 = {...})
    at ./src/master/masterconn.cc:1065
#11 masterconn_serve (pdesc=std::vector of length 6, capacity 8 = {...})
    at ./src/master/masterconn.cc:1042
#12 0x0000aaaae72dbddc in eventloop_run () at ./src/common/event_loop.cc:245
#13 0x0000aaaae71ee220 in main (argc=<optimized out>, argv=<optimized out>) at ./src/main/main.cc:1047
```
Both servers are little-endian 64-bit systems (x86, aarch64). The primary is on 0.12 while the new secondary is current github master, but the crash did happen with 0.12 also.

Master config (both the same except no ""shadow"" on the primary obviously):
```
ADMIN_PASSWORD = ************
OPERATIONS_DELAY_INIT = 10
AUTO_RECOVERY = 1
CHUNKS_LOOP_MAX_CPS = 10000
CHUNKS_LOOP_MAX_CPU = 40
ENDANGERED_CHUNKS_PRIORITY = 0.2
CHUNKS_REBALANCING_BETWEEN_LABELS = 1
NO_ATIME = 1
USE_BDB_FOR_NAME_STORAGE = 1
BDB_NAME_STORAGE_CACHE_SIZE = 1
PERSONALITY = shadow
MASTER_RECONNECTION_DELAY = 10
MASTER_HOST = mfsmaster.alias.smurf.noris.de
```

This happens immediately after download. Last lines of strace:
```
9410  rt_sigreturn({mask=[]})           = 0
9410  close(14)                         = 0
9410  getpid()                          = 9410
9410  write(2, ""09/18/19 20:20:53.229 [info] [9410:9410] : sessions downloaded 1950B/0.001626s (1.199 MB/s)\n"", 92) = 92
9410  getpid()                          = 9410
9410  writev(2, [{iov_base=""mfsmaster[9410]: sessions downloaded 1950B/0.001626s (1.199 MB/s)"", iov_len=65}, {iov_base=""\n"", iov_len=1}], 2) = 66
9410  sendto(5, ""<14>Sep 18 20:20:53 mfsmaster[9410]: sessions downloaded 1950B/0.001626s (1.199 MB/s)"", 85, MSG_NOSIGNAL, NULL, 0) = 85
9410  renameat(AT_FDCWD, ""sessions.mfs.tmp"", AT_FDCWD, ""sessions.mfs"") = 0
9410  mmap(NULL, 38277120, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0xffffbbfdd000
9410  --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_MAPERR, si_addr=0x2aaacbe0d700} ---
9410  +++ killed by SIGSEGV (core dumped) +++
```
Any help is appreciated. I'm quite anxious here because right now I don't have a secondary master. :-/ ","Would it be remotely possible to answer my question?

I found mfsmetadump. It's somewhat broken (mangles UTF-8 filenames). I also found mfsmetarestore, but what I didn't find is any code that actually reads the dumps produced by mfsmetadump. So, is that even possible? do I build a new lizardfs storage and copy all my data? that'd take ages …",9800119
23,chunkserver maintenance mode,open,2019-09-15T16:20:46Z,2019-09-24T08:23:27Z,,NONE,"How do you put a chunk server in maintenance mode? I would like the chunks from a specific chunkserver to be ""readonly"" for a certain window of time.","@jkiebzak 
I think of some options of what you want to do.
1.- you have to stop the mfschunkserver service.
In this way, the node no longer accepts more data, but there is no availability of the chunks of that node, in addition, based on its goals, the chunks that are found that will undergoal a replication start with another node to fulfill its goals.

2- If you handle Labels in your chunkservers, change the Label of your chunkserver so that it has no relation to the rest of your chunkservers.

3.- In your clients, you can do the mfsmount as read only as follows:
`mfsmount -o ro /your/directory/
`
If you need to specify only specific directories within your lizard cluster, you must use:
`mfsmount -S -o ro  /lizard/readonly-directory   /directory/client`

as such a maintenance option i don't know.
",9800119
24,Adding shadow servers to an existing master,open,2019-06-27T06:10:50Z,2019-06-28T20:12:50Z,,NONE,"I am searching everywhere for the way to add new shadow servers to an existing master server once it's been already running for a while.
I have experienced a prolonged downtime due to lack of documentation on how to properly shut off the lizardfs HA set-up that uses lizardfs-uraft. I have stopped the cluster by unmounting all clients first (means no more meta-data changes), then I have shut off the secondary server and lastly the server where master resided. It was all done ""properly"" using systemctl commands. No errors reported at that point.
When I later started the lizardfs-uraft service on the nodes involved, it seemed that something corrupted the database (which should have been properly saved to hard drive on service stop). It took over an hour for the mfsmaster process to playback the metadata from its db and changelogs. In the process of troubleshooting I have destroyed metadata on one of the master servers, so I got left with one server running master, which I reconfigured to run as a single master to avoid further mishaps with uraft.
Now I wonder how to:
- make the database consistent again - each restart of mfsmaster takes over an hour to start
- add aditional shadow servers and reconfigure the whole thing to be managed by lizardfs-uraft again.

I have not found enough information in the documentation, to be able to make this happen. A new shadow instance just runs on its own, regardless of the master (it does not ""join"" and is not listed in the webui). When I copy over the metadata from the master and run that shadow it also takes an hour to start, but it does not get connected with the master itself.

In my experience using HA services should just be pretty straight forward. Recovery scenarios and steps are usualy pretty well documented. If there is some documentation that I have missed (short of reading the code itself), please let me know where to find it.

Thanks!
","hi
So, right now you using URAFT-HA or only Master/Shadow ?
What S.O have your cluster?

List your mfsmaster.cfg from shadow server",9800119
25,"Chunkserver error every day (wrong id/version/type in header (0000000001CF8698_00000000, typeId 0))",open,2019-06-24T09:13:32Z,2020-02-19T07:01:55Z,,NONE,"Hi,
I have 7 chunkservers for out backup system and have 3 mount point for each one.
Example:
/dev/sdd         64T   44T   21T  69% /mnt/backup3
/dev/sdb         64T   44T   21T  69% /mnt/backup1
/dev/sdc         64T   44T   21T  69% /mnt/backup2
As a recently month, I faced a lot of errors with 3 mount points in exactly chunkserver.
Th06 24 15:47:15 SERVER_NAME zamfschunkserver5[8841]: chunk_readcrc: file:/mnt/backup1/chunksCF/chunk_0000000001CF82C1_00000001.mfs - wrong id/version/type in header (0000000001CF82C1_00000000, typeId 0)
Th06 24 15:47:15 SERVER_NAME zamfschunkserver5[8841]: hdd_io_begin: file:/mnt/backup1/chunksCF/chunk_0000000001CF82C1_00000001.mfs - read error: Success
Th06 24 15:47:44 SERVER_NAME zamfschunkserver5[8841]: chunk_readcrc: file:/mnt/backup1/chunksCF/chunk_0000000001CF8698_00000001.mfs - wrong id/version/type in header (0000000001CF8698_00000000, typeId 0)
Th06 24 15:47:44 SERVER_NAME zamfschunkserver5[8841]: hdd_io_begin: file:/mnt/backup1/chunksCF/chunk_0000000001CF8698_00000001.mfs - read error: Success
Any helps?
","Hi guy,
Any comment for this issue.
After I did a xfs_repair in mount point /mnt/backup2, a lot of chunks has failed.

> Feb 19 13:57:11 9 mfschunkserver_9.20[16875]: test_chunk: file:/mnt/backup2/chunks94/chunk_000000000B940E36_00000001.mfs - crc error
Feb 19 13:57:11 9 mfschunkserver_9.20[16875]: Chunk 000000000b940e36_00000001 (std:0) corrupted (detected by a client)
Feb 19 13:57:23  mfschunkserver_9.20[16875]: test_chunk: file:/mnt/backup2/chunks1F/chunk_000000000E1F282F_00000001.mfs - crc error
Feb 19 13:57:23  mfschunkserver_9.20[16875]: Chunk 000000000e1f282f_00000001 (std:0) corrupted (detected by a client)

> ll -h /mnt/backup2/chunks1F/chunk_000000000E1F282F_00000001.mfs
> -rw-r----- 1 mfs mfs 24M 13:28 24 Th01 /mnt/backup2/chunks1F/chunk_000000000E1F282F_00000001.mfs

Have a way to check how mfs master check it crc error or how to repair this?",9800119
26,Best way to serve multiple shares?,open,2019-06-17T20:08:07Z,2019-06-28T19:59:38Z,,NONE,"I want to serve two filesystems as separate shares (e.g. clients can mount `/one` and `/two` served by lizardFS).

My setup right now is like this:
- 3 HA masters using uRAFT
- 3 chunkservers all replicating `/one`
- Clients mount from the HA master IP and see the contents of `/one`

So..what do I change to add in `/two`? Do I need new chunkservers? New masters?

Or should they both be subdirectories of the one lizardFS? (So clients mount it and get say `/lizard` which has subdirectories `one` and `two`)","you can have inside /one the /two directory or have a root directory called /lizardfs and inside have the `/lizardfs/one` and `/lizardfs/two` directories

editing `mfsgoals.cfg` you can specify how many copies you want from each subdirectory and mount each one separately in different clients

#if your tree directory = /lizardfs/one
#client1 with 
`mfsmount -S /one  /testclient1 
`
#if your tree directory = /lizardfs/one
#client2 with  
`mfsmount -S /two  /testclient2
`
you can also have chunkservers that store only the information of /one and have other chunkservers that store only information of /two using LABELS in the configuration of `mfschunkserver.cfg`, this more than anything to separate the workload of the disks of your chunkservers, If your workload is not high, you can have all your chunkservers to replicate and work together.

if your client nodes should be able to see all your /lizardfs information, you should mount /lizardfs on your clients and they can see all the content, for example: /lizardfs/one and /lizardfs/two

If you need to restrict access to /lizardfs/two on your clients then you must separately mount each of your subdirectories and make use of `mfsgoals.cfg` and `mfsexports.cfg`",9800119
27,Chunkserver wedged,open,2019-06-09T18:59:43Z,2020-02-14T08:04:40Z,,CONTRIBUTOR,"```
Jun 06 08:32:19 lizard mfschunkserver[329]: connection reset by Master
Jun 06 08:32:19 lizard mfschunkserver[329]: mfschunkserver[329]: connecting to Master
Jun 06 08:32:19 lizard mfschunkserver[329]: connecting to Master
Jun 06 08:32:19 lizard mfschunkserver[329]: mfschunkserver[329]: connected to Master
Jun 06 08:32:19 lizard mfschunkserver[329]: connected to Master
Jun 06 08:35:09 lizard mfschunkserver[329]: mfschunkserver[329]: connection reset by Master
Jun 06 08:35:09 lizard mfschunkserver[329]: connection reset by Master
Jun 06 08:35:09 lizard mfschunkserver[329]: mfschunkserver[329]: connecting to Master
Jun 06 08:35:09 lizard mfschunkserver[329]: connecting to Master
```

On the master:
```
Jun 09 08:32:42 s-base mfsmaster[99629]: mfsmaster[99629]: chunk 000000000019a0bb has not enough valid parts (1) consider repairing it manually
Jun 09 08:32:42 s-base mfsmaster[99629]: mfsmaster[99629]: chunk 000000000019a0bb_00000002 - invalid part on (10.107.6.30 - ver:00000001)
Jun 09 08:32:41 s-base mfsmaster[99629]: chunk 000000000019a0bb has not enough valid parts (1) consider repairing it manually
Jun 09 08:32:41 s-base mfsmaster[99629]: chunk 000000000019a0bb_00000002 - invalid part on (10.107.6.30 - ver:00000001)
Jun 09 08:33:52 s-base mfsmaster[99629]: mfsmaster[99629]: chunkserver disconnected - ip: 10.107.8.135, port: 0, usedspace: 0 (0.00 GiB), totalspace: 0 (0.00 GiB)
Jun 09 08:33:52 s-base mfsmaster[99629]: chunkserver disconnected - ip: 10.107.8.135, port: 0, usedspace: 0 (0.00 GiB), totalspace: 0 (0.00 GiB)
Jun 09 08:35:13 s-base systemd[1]: Starting [Cron] ""*/5 * * * * root touch /run/cronmonitor""...
Jun 09 08:35:13 s-base systemd[1]: Starting [Cron] ""5-55/10 * * * * root command -v debian-sa1 > /dev/null && debian-sa1 1 1""...
Jun 09 08:35:13 s-base systemd[1]: cron-smurf-server-root-1.service: Succeeded.
Jun 09 08:35:13 s-base systemd[1]: Started [Cron] ""*/5 * * * * root touch /run/cronmonitor"".
Jun 09 08:35:13 s-base systemd[1]: cron-sysstat-root-0.service: Succeeded.
Jun 09 08:35:13 s-base systemd[1]: Started [Cron] ""5-55/10 * * * * root command -v debian-sa1 > /dev/null && debian-sa1 1 1"".
Jun 09 08:36:30 s-base mfsmaster[99629]: mfsmaster[99629]: chunkserver disconnected - ip: 10.107.8.135, port: 0, usedspace: 0 (0.00 GiB), totalspace: 0 (0.00 GiB)
Jun 09 08:36:30 s-base mfsmaster[99629]: chunkserver disconnected - ip: 10.107.8.135, port: 0, usedspace: 0 (0.00 GiB), totalspace: 0 (0.00 GiB)
Jun 09 08:37:48 s-base mfsmaster[99629]: mfsmaster[99629]: chunk 000000000019a0bb has not enough valid parts (1) consider repairing it manually
Jun 09 08:37:48 s-base mfsmaster[99629]: mfsmaster[99629]: chunk 000000000019a0bb_00000002 - invalid part on (10.107.6.30 - ver:00000001)
Jun 09 08:37:48 s-base mfsmaster[99629]: chunk 000000000019a0bb has not enough valid parts (1) consider repairing it manually
Jun 09 08:37:48 s-base mfsmaster[99629]: chunk 000000000019a0bb_00000002 - invalid part on (10.107.6.30 - ver:00000001)
Jun 09 08:39:06 s-base mfsmaster[99629]: mfsmaster[99629]: chunkserver disconnected - ip: 10.107.8.135, port: 0, usedspace: 0 (0.00 GiB), totalspace: 0 (0.00 GiB)
```

The root problem of this seems to have been a stuck log in mfschunkserver. Restarting it required a ""kill -9"" but fixed the problem. For now. :-/

Unfortunately I don't have logs of the chunkserver output before this happened. Will increase log retention time and try to discover more.","Hi,
After xfs_repair a hdd , I have alot of message

> Feb 14 15:00:40 ZA_ZT_LFS-19 mfschunkserver_9.20[16875]: test_chunk: file:/mnt/backup2/chunks0E/chunk_000000000B0E6040_00000001.mfs - crc error
> Feb 14 15:00:40 ZA_ZT_LFS-19 mfschunkserver_9.20[16875]: Chunk 000000000b0e6040_00000001 (std:0) corrupted (detected by a client)
> Feb 14 15:02:20 ZA_ZT_LFS-19 mfschunkserver_9.20[16875]: test_chunk: file:/mnt/backup2/chunks94/chunk_000000000D94AA48_00000001.mfs - crc error
> Feb 14 15:02:20 ZA_ZT_LFS-19 mfschunkserver_9.20[16875]: Chunk 000000000d94aa48_00000001 (std:0) corrupted (detected by a client)
Any ways to fix this error?",9800119
28,lizardfs-uraft failed to start,open,2019-05-13T21:07:33Z,2019-09-27T15:02:39Z,,NONE,"Hi, I'm working with Lizardfs-uraft 3.13.0 & Centos 7.
I have 3 metadata servers, 8 chunkservers and 2 metaloggers.
I had worked well for a while but now I have a problem with millions of files and chunks on my metadata servers.
systemctl start lizardfs-uraft (master1-sv = working)
systemctl start lizardfs-uraft (master2-sv = error)
systemctl start lizardfs-uraft (master3-sv = error)

my metadata.mfs size:
[root@mfsmaster1-sv ~]# du -hs /data/mfs/metadata.mfs
23G	/data/mfs/metadata.mfs

**mfsmaster2-sv LOGS**
```May 10 17:07:17 mfsmaster2-sv mfsmaster: mfsmaster daemon initialized properly
May 10 17:07:17 mfsmaster2-sv systemd: Started LizardFS master server daemon.
May 10 17:07:17 mfsmaster2-sv mfsmaster[22996]: connected to Master
May 10 17:07:17 mfsmaster2-sv systemd: Started LizardFS uraft high availability daemon.
May 10 17:07:17 mfsmaster2-sv lizardfs-uraft: Lizardfs-uraft initialized properly
May 10 17:07:17 mfsmaster2-sv lizardfs-uraft: Node 'mfsmaster1-sv' is now a leader.
May 10 17:07:17 mfsmaster2-sv lizardfs-uraft: Metadata server is alive
May 10 17:10:01 mfsmaster2-sv systemd: Created slice User Slice of root.
May 10 17:10:01 mfsmaster2-sv systemd: Started Session 15008 of user root.
May 10 17:10:01 mfsmaster2-sv systemd: Removed slice User Slice of root.
May 10 17:12:50 mfsmaster2-sv mfsmaster[22996]: metadata downloaded 23689390616B/333.292241s (71.077 MB/s)
May 10 17:12:54 mfsmaster2-sv mfsmaster[22996]: changelog.mfs.1 downloaded 12138576B/0.183249s (66.241 MB/s)
May 10 17:12:54 mfsmaster2-sv mfsmaster[22996]: changelog.mfs.2 downloaded 60904845B/0.930901s (65.426 MB/s)
May 10 17:12:55 mfsmaster2-sv mfsmaster[22996]: sessions downloaded 2166B/0.001474s (1.469 MB/s)
May 10 17:12:55 mfsmaster2-sv mfsmaster[22996]: opened metadata file /var/lib/mfs/metadata.mfs
May 10 17:12:55 mfsmaster2-sv mfsmaster[22996]: loading objects (files,directories,etc.) from the metadata file
May 10 17:13:58 mfsmaster2-sv mfsmaster[22996]: loading names from the metadata file
May 10 17:20:01 mfsmaster2-sv systemd: Created slice User Slice of root.
May 10 17:20:01 mfsmaster2-sv systemd: Started Session 15009 of user root.
May 10 17:20:01 mfsmaster2-sv systemd: Removed slice User Slice of root.
May 10 17:21:05 mfsmaster2-sv mfsmaster[22996]: loading deletion timestamps from the metadata file
May 10 17:21:05 mfsmaster2-sv mfsmaster[22996]: loading extra attributes (xattr) from the metadata file
May 10 17:21:05 mfsmaster2-sv mfsmaster[22996]: loading access control lists from the metadata file
May 10 17:21:05 mfsmaster2-sv mfsmaster[22996]: loading quota entries from the metadata file
May 10 17:21:05 mfsmaster2-sv mfsmaster[22996]: loading file locks from the metadata file
May 10 17:21:05 mfsmaster2-sv mfsmaster[22996]: loading chunks data from the metadata file
May 10 17:22:23 mfsmaster2-sv mfsmaster[22996]: checking filesystem consistency of the metadata file
May 10 17:22:29 mfsmaster2-sv mfsmaster[22996]: connecting files and chunks
May 10 17:27:51 mfsmaster2-sv mfsmaster[22996]: calculating checksum of the metadata
May 10 17:28:17 mfsmaster2-sv mfsmaster[22996]: metadata file /var/lib/mfs/metadata.mfs read (196985237 inodes including 30975314 directory inodes and 136297216 file inodes, 136301197 chunks)
May 10 17:28:17 mfsmaster2-sv mfsmaster[22996]: running in shadow mode - applying changelogs from /var/lib/mfs
May 10 17:28:18 mfsmaster2-sv mfsmaster[22996]: /var/lib/mfs/changelog.mfs.1: 220353 changes applied (1254452902 to 1254673254), 0 skipped
May 10 17:28:18 mfsmaster2-sv mfsmaster[22996]: /var/lib/mfs/changelog.mfs: 255 changes applied (1254673255 to 1254673509), 269484 skipped
May 10 17:29:24 mfsmaster2-sv systemd: lizardfs-ha-master.service: main process exited, code=killed, status=27/PROF
May 10 17:29:24 mfsmaster2-sv mfsmaster: 05/10/19 17:29:24.278 [warning] [39131:39131] : can't find process to terminate
May 10 17:29:24 mfsmaster2-sv mfsmaster: can't find process to terminate
May 10 17:29:24 mfsmaster2-sv systemd: Unit lizardfs-ha-master.service entered failed state.
May 10 17:29:24 mfsmaster2-sv systemd: lizardfs-ha-master.service failed.
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: Metadata server is dead
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: LizardFS uraft helper script
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: Available commands:
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: isalive
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: metadata-version
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: quick-stop
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: promote
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: demote
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: assign-ip
May 10 17:29:24 mfsmaster2-sv lizardfs-uraft: drop-ip
```

Only 1 of the metadata servers works so far, I can not raise the lizardfs-uraft service in my master2-sv or mfsmaster3-sv, both present the same error:
```
May 10 17:29:24 mfsmaster2-sv systemd: lizardfs-ha-master.service: main process exited, code=killed, status=27/PROF
May 10 17:29:24 mfsmaster2-sv mfsmaster: 05/10/19 17:29:24.278 [warning] [39131:39131] : can't find process to terminate
May 10 17:29:24 mfsmaster2-sv mfsmaster: can't find process to terminate

```
we currently handle around 130 million files and chunks, maybe that's the problem? I have played with different mfsmaster parameters, but none of them works, I have increased the values ​​of CHUNKS_LOOP_MAX_CPS and METADATA_CHECKSUM_RECALCULATION_SPEED without good results.

**mfsmaster1 config.**
```
 PERSONALITY = ha-cluster-managed
 ADMIN_PASSWORD = SOMEPASS
 WORKING_USER = mfs
 WORKING_GROUP = mfs
 DATA_PATH = /data/mfs
 MATOML_LISTEN_HOST = *
 MATOML_LISTEN_PORT = 9419
 MATOCS_LISTEN_HOST = *
 MATOCS_LISTEN_PORT = 9420
 MATOCL_LISTEN_HOST = *
 MATOCL_LISTEN_PORT = 9421
 MATOTS_LISTEN_HOST = *
 MATOTS_LISTEN_PORT = 9424
 NO_ATIME = 0
 MASTER_HOST = mfsmaster
```

**mfsmaster2 config**
```
 PERSONALITY = ha-cluster-managed
 ADMIN_PASSWORD = SOMEPASS
 WORKING_USER = mfs
 WORKING_GROUP = mfs
 DATA_PATH = /var/lib/mfs
 MATOML_LISTEN_HOST = *
 MATOML_LISTEN_PORT = 9419
 MATOCS_LISTEN_HOST = *
 MATOCS_LISTEN_PORT = 9420
 MATOCL_LISTEN_HOST = *
 MATOCL_LISTEN_PORT = 9421
 MATOTS_LISTEN_HOST = *
 MATOTS_LISTEN_PORT = 9424
 NO_ATIME = 0
 MASTER_HOST = mfsmaster
```
**mfsmaster3 config**
```
 PERSONALITY = ha-cluster-managed
 ADMIN_PASSWORD = SOMEPASS
 WORKING_USER = mfs
 WORKING_GROUP = mfs
 DATA_PATH = /data/mfs
 MATOML_LISTEN_HOST = *
 MATOML_LISTEN_PORT = 9419
 MATOCS_LISTEN_HOST = *
 MATOCS_LISTEN_PORT = 9420
 MATOCL_LISTEN_HOST = *
 MATOCL_LISTEN_PORT = 9421
 MATOTS_LISTEN_HOST = *
 MATOTS_LISTEN_PORT = 9424
 NO_ATIME = 0
 MASTER_HOST = mfsmaster

```
**lizardfs-uraft.cfg** (master2-sv)
```
 URAFT_NODE_ADDRESS = mfsmaster2-sv
 URAFT_NODE_ADDRESS = mfsmaster1-sv
 URAFT_NODE_ADDRESS = mfsmaster3-sv
 URAFT_ID = 0
 URAFT_FLOATING_IP      = 10.40.40.140
 URAFT_FLOATING_NETMASK = 255.255.255.0
 URAFT_FLOATING_IFACE   = bond0
```

**lizardfs-uraft.cfg** (master1-sv)
```
URAFT_NODE_ADDRESS = mfsmaster2-sv
 URAFT_NODE_ADDRESS = mfsmaster1-sv
 URAFT_NODE_ADDRESS = mfsmaster3-sv
 URAFT_ID = 1
 URAFT_FLOATING_IP      = 10.40.40.140
 URAFT_FLOATING_NETMASK = 255.255.255.0
 URAFT_FLOATING_IFACE   = bond0
```

**lizardfs-uraft.cfg** (master3-sv)
```
 URAFT_NODE_ADDRESS = mfsmaster2-sv
 URAFT_NODE_ADDRESS = mfsmaster1-sv
 URAFT_NODE_ADDRESS = mfsmaster3-sv
 URAFT_ID = 2
 URAFT_FLOATING_IP      = 10.40.40.140
 URAFT_FLOATING_NETMASK = 255.255.255.0
 URAFT_FLOATING_IFACE   = bond0
```
Any suggest??? 
Thanks","> set 1 and your problem with unloading metadata by uraft will disappear.
> With uraft you can only play with BACK_LOGS variable.

I have editted BACK_META_KEEP_PREVIOUS  to default (1), but not luck.
Aug  6 17:34:00 hostname mfsmaster[53621]: /var/lib/mfs/changelog.mfs.1: 4778156 changes applied (26124369179 to 26129147334), 0 skipped
Aug  6 17:34:02 hostname mfsmaster[53621]: /var/lib/mfs/changelog.mfs: 38301 changes applied (26129147335 to 26129185635), 838880 skipped
Aug  6 17:35:13 hostname6 mfsmaster[53621]: all needed changelogs applied successfully
Aug  6 17:35:13 hostname  mfsmaster[53621]: synced at version = 26129185635
Aug  6 17:35:13 hostname mfsmaster[53621]: write to Master error: Connection reset by peer
Aug  6 17:35:14 hostname mfsmaster[53621]: connecting to Master
Aug  6 17:35:14 hostname mfsmaster[53621]: connected to Master
Aug  6 17:35:14 hostname mfsmaster[53621]: unloading filesystem at 26129185707

In master :
Aug  6 17:36:00 hostname mfsmaster: connection with ML(10.30.6.34) has been closed by peer
Aug  6 18:27:17 hostname mfsmaster: connection with ML(10.30.6.34) has been closed by peer
",9800119
29,3.13.0-rc1 + Samba 4.5,open,2019-04-08T04:28:21Z,2019-04-17T07:33:09Z,,NONE,"I have a cluster with LFS 3.9.4 serving files to clients with Samba 4.3, and it's working very well.

I've built a new cluster to test with LFS 3.13.0-rc1 and Samba 4.5. It kind of works, but I've run into a strange issue. Specifically, I have an LFS mount point at `/netfs/lfs/files`, which is owned by `user` and a member of the `group` group (not really the names). I also have `user2` which is a member of `group`. The `/netfs/lfs/files` directory's permissions are set to 770. It's shared out in Samba as `files`.

When I access it with `smbclient` (or mount it with mount.cifs, same issue occurs) as follows:

`smbclient //localhost/files -U user`

Everything works normally.

Then I try it with this user:

`smbclient //localhost/files -U user2`

And upon doing a directory listing, I receive `NT_STATUS_ACCESS_DENIED listing \*`. And if I do a directory listing anytime in the ~12 seconds within connecting, I get the same thing. After that, it starts working normally, then as far as I can tell keeps working normally until I exit. If I reconnect/remount, it does it again (gives me access denied errors for ~12 seconds, then works fine after that). Note, it works normally (without giving any errors) if I log in using the user that owns the folder being shared out. It's only when I use a user belonging to the group (but not the owner) that this occurs. If I set the permissions to 777, then it works from the start as well.

I wasn't sure if this was an issue with LFS, Samba, or something else. So I setup Samba to create a share outside of LFS on the local system, and it doesn't behave this way (it works fine).

I then tried mounting `/netfs/lfs/files` with the debug option. Then when I connect with `smbclient`, when I do my initial directory listings, I get this:

```
unique: 5, opcode: GETATTR (3), nodeid: 1, insize: 56, pid: 19680
   unique: 5, success, outsize: 120
unique: 6, opcode: LOOKUP (1), nodeid: 1, insize: 48, pid: 19680
   unique: 6, success, outsize: 144
unique: 7, opcode: LOOKUP (1), nodeid: 1057584, insize: 42, pid: 19680
   unique: 7, error: -13 (Permission denied), outsize: 16
```

If I keep doing `dir` repeatedly, I'll keep getting the same thing for ~12 seconds, then it starts working normally and I get this instead (note the ""Permission denied"" message is gone, and it gives me the directory listing):

```
unique: 11, opcode: GETATTR (3), nodeid: 1, insize: 56, pid: 19680
   unique: 11, success, outsize: 120
unique: 12, opcode: LOOKUP (1), nodeid: 1, insize: 48, pid: 19680
   unique: 12, success, outsize: 144
unique: 13, opcode: LOOKUP (1), nodeid: 1057584, insize: 42, pid: 19680
   unique: 13, error: -2 (No such file or directory), outsize: 16
unique: 14, opcode: GETXATTR (22), nodeid: 1057584, insize: 72, pid: 19680
   unique: 14, error: -61 (No data available), outsize: 16
unique: 15, opcode: GETXATTR (22), nodeid: 1057584, insize: 73, pid: 19680
   unique: 15, error: -61 (No data available), outsize: 16
unique: 16, opcode: OPENDIR (27), nodeid: 1057584, insize: 48, pid: 19680
   unique: 16, success, outsize: 32
unique: 17, opcode: READDIR (28), nodeid: 1057584, insize: 80, pid: 19680
   unique: 17, success, outsize: 152
unique: 18, opcode: GETATTR (3), nodeid: 1057584, insize: 56, pid: 19680
   unique: 18, success, outsize: 120
unique: 19, opcode: LOOKUP (1), nodeid: 1057584, insize: 48, pid: 19680
   unique: 19, success, outsize: 144
unique: 20, opcode: LOOKUP (1), nodeid: 1057584, insize: 52, pid: 19680
   unique: 20, success, outsize: 144
unique: 21, opcode: READDIR (28), nodeid: 1057584, insize: 80, pid: 19680
   unique: 21, success, outsize: 16
unique: 22, opcode: RELEASEDIR (29), nodeid: 1057584, insize: 64, pid: 0
   unique: 22, success, outsize: 16
unique: 23, opcode: GETATTR (3), nodeid: 1057584, insize: 56, pid: 19680
   unique: 23, success, outsize: 120
unique: 24, opcode: STATFS (17), nodeid: 1057584, insize: 40, pid: 19680
   unique: 24, success, outsize: 96
unique: 25, opcode: LOOKUP (1), nodeid: 1057584, insize: 58, pid: 19680
   unique: 25, error: -2 (No such file or directory), outsize: 16
unique: 26, opcode: LOOKUP (1), nodeid: 1057584, insize: 58, pid: 19680
   unique: 26, error: -2 (No such file or directory), outsize: 16
```

It's almost as if LFS is taking it's time to decide that the user I'm connecting with has access, then ""something happens"" and it starts working. I'm not sure where to go from here.

If I log in through bash and `su - bash2`, then I don't have this issue. I don't have a setup that would allow me to use the old LFS with the new Samba, or vice-versa, so I can't provide data on that. But given the debug output of mfsmount, I'm inclined to think it's an issue there.","> > I'm running 3.13.0-rc1 on Ubuntu Disco Dingo (19.04) but I've also tested it on 3.12. Both work for me.
> 
> I'm running the same 3.13.0-rc1, but on Debian Stretch (9.8). I have another installation that's setup similarly with Samba on Ubuntu 14.04 and LFS 3.9.4 and it works just fine as well.

Does the same test scenario *work for you on Ubuntu 14.0.4 with LFS 3.9.4* and *doesn't work on Debian Stretch (9.8)*? Do you use similar Samba and LizardFS configuration on Ubuntu and on Debian?

> > One more idea that you can try out – add [`ignoregid`][1] to `mfsexports.cfg` as suggested [here][2].
> 
> I'll give this a try once I get a chance and let you know how it goes.

OK, let's give it a try and share the results.

Since I'm not able to debug your problem (because I'm not able to reproduce it), I would suggest you to capture and investigate network traffic that is active within this ~12 seconds delay period – maybe you will notice something suspicious. You can use `wireshark` or `tcpdump` and share with me `.pcap` file unless you have any classified/private unencrypted communication that you don't want to share (you can filter out LizardFS packets). But first give try out [`ignoregid`][1] in `mfsexports.cfg` as suggested [here][2].

[1]: https://docs.lizardfs.com/man/mfsexports.cfg.5.html
[2]: https://github.com/lizardfs/lizardfs/issues/265#issuecomment-91513761",9800119
30,Can I run Mariadb/Mysql on lizardfs?,open,2019-03-24T22:32:40Z,2019-12-26T04:02:53Z,,NONE,"Have been running Mariadb on ZFS for three years with absolutely no problems.  I have ZFS configured to take periodic snapshots of the database and sync them to a copy on remote server that is 3,000 miles away (just in case the main datacenter is destroyed by a meteorite or something).  If the main center goes down or is unreachable for some reason, I can (theoretically) restart Mariadb on the remote copy.

Could I run Mariadb under lizardfs and have it maintain a database copy on a geographically remote server?  Some clustered filesystems like StorageOS say you can use them for databases, while others like Ceph recommend against it.  I have not been able to find any recommendations pro or con regarding lizardfs.","I don't know how valuable this input is, but just in case: My team once ran a low utilization 3 node LizardFS cluster with Mysql using LizardFS for storage and I never ran into Data loss.

The whole LizardFS cluster was running on VMs with SSDs. The applications' responsiveness when using LizardFS for the database *was* noticeably slower compared to using the SSDs directly, but it wasn't unusable. We weren't serving a lot of traffic though so we didn't get a thorough test or anything.",9800119
31,lizardfs-master crash - status=11/SEGV,open,2019-02-20T00:40:12Z,2019-05-06T11:44:05Z,,NONE,"I added a new chunk server today and appear to have crashed the master!

**kern.log**
```
Feb 19 16:11:14 PROD-BackMA-HQ kernel: [521722.605779] do_general_protection: 6 callbacks suppressed
Feb 19 16:11:14 PROD-BackMA-HQ kernel: [521722.605780] traps: mfsmaster[3567] general protection ip:56392a57d663 sp:7ffc3b31fbb0 error:0 in mfsmaster[56392a50e000+1be000]
```
**syslog**

```
Feb 19 16:11:14 mfsmaster mfsmaster: connection with CS(192.168.0.XXX) has been closed by peer
Feb 19 16:11:14 mfsmaster mfsmaster: chunkserver disconnected - ip: 192.168.0.XXX, port: 9XXX, usedspace: 10867363840 (10.12 GiB), totalspace: 92871548928 (86.49 GiB)
Feb 19 16:11:15 mfsmaster systemd[1]: lizardfs-master.service: Main process exited, code=dumped, status=11/SEGV
Feb 19 16:11:15 mfsmaster systemd[1]: lizardfs-master.service: Failed with result 'core-dump'.
```
**Info:**
Ubuntu 18.04
lizardfs-master/bionic,now 3.12.0+dfsg-1 amd64 [installed]
Linux mfsmaster 4.15.0-45-generic #48-Ubuntu SMP Tue Jan 29 16:28:13 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

I had to do a mfsmetarestore -a to get it back online.  :/",Forget about what I wrote about `METARESTORE` symbol – I misinterpreted it (`mfsmetarestore` is built from the same source as `mfsmaster` and thus we need `METARESTORE` condition). I will try to figure out what may be source of the bug.,9800119
32,are ec goals trustworthy? - question,open,2019-02-16T17:37:36Z,2019-06-18T11:30:56Z,,NONE,"Dear all

Considering 3.13.rc and 3.12 versions, would use of ec goals be a good idea?

I seem to spot once upon a time a thread that spurious problems are reported.
I am considering to replace my btrfs raid1 with a lizard with ec target and would not like to regret it.


","@eleaner, I would have chosen MooseFS.
LizardFS is currently unmaintained and I do not feel comfortable recommending unmaintained software...

Just two servers are not enough for EC goals. If you consider server as a unit of failure then your data will remain available with 200% storage requirements for redundancy. That's the same requirement as for as RAID-1 or `ec(2,2)` with one chunkserver per server -- very simple and straightforward replicated setup.

Replicated chunks are best for performance.
With 6 chunkservers you could use `ec(4,2)` which needs 150% of storage but that will be much slower and somewhat more difficult to setup and it won't protect against failure of a server...
",9800119
33,question - snapshot/folder own chunks and referenced usage chunks,open,2019-02-16T17:31:47Z,2019-02-17T02:03:49Z,,NONE,"Dear all,

Is there a way to learn how much disk space I can recover when deleting a snapshot?
I am aware that the unique chunks will be deleted (assuming no trash) and chunks referenced elsewhere will not.

But how to learn this before removing a snapshot/folder?
or even how to search for the snapshot/folder where the gain will be biggest?

","Not sure if it helps, but in [MooseFS](https://github.com/moosefs/moosefs) it is easy to count how big particular (sub)directory is with taking snapshots into consideration. It means, that chunks which belong to more than one file won't be counted twice (trice, ...), but *once*. You can achieve this by using `mfsdirinfo -p` (`-p` switch is crucial as it stands for ""precise mode""). As far as I remember there is no such `-p` switch in LizardFS as it was implemented in MooseFS ""quite recently"", i.e. 3 years ago (see: https://github.com/moosefs/moosefs/commit/d64003d81a423f27df7680874c8cd4eb67f0a762).

Remember, that in such case, in order to count the size you are looking for correctly, snapshot and original file (or dir) must be within the same subtree. Of course you can run `mfsdirinfo -p` for whole filesystem, but then it counts realsize with taking **all snapshots** into consideration, so by running it against whole directory tree (rootdir of MooseFS) you can get answer on ""how much space would I recover if I deleted all the snapshots?"".

For example let's assume you have the following directory structure:

```
/mnt/mfs/something
         └─ mydir
         └─ mydir_snap
```

So in this case running `mfsdirinfo -p` against `/mnt/mfs/something` would return information on realsize with taking into consideration shared chunks between `mydir` and `mydir_snap`. Then you can run it against `/mnt/mfs/something/mydir` and compare results – result of substraction of these two values will be the number you are looking for.

There is also another tool called `mfsmetadirinfo` which actually can give you the same information, but *much faster*. The difference is that `mfsdirinfo -p` can be ran on a mountpoint and `mfsmetadirinfo` needs metadata file to work (e.g. lastly saved `metadata.mfs.back`), so you need to run it on Master or Metalogger. It is also available in MooseFS only (as it was implemented in the latest MooseFS 3.0.103 in Nov '18) and is called `mfsmetadirinfo` (see: https://github.com/moosefs/moosefs/commit/6af88b698e6d3aacad3710efcc4a67e00ea24367).

For more information refer to `man mfsdirinfo` and `man mfsmetadirinfo`.

Hope it helps.

Best,
Piotr
",9800119
34,Question: is it worth it to combine RAID0 with LizardFS?,open,2019-02-15T09:23:20Z,2019-02-15T18:04:43Z,,NONE,"I'm only interested in RAID0 and my setup consists of 4 x 10 TB drives with 32 GB of ECC 32GB memory.

Is it worth it putting these drives in software-raid 0 before mounting LizardFS ontop of it? Or should I keep the 4 drives distinct? I'm concerned with performance only (as evidenced by RAID0). Redundancy does not interest me as the data stored on this setup can be regenerated from public networks quite easily, I just need to be able to read and write to it as fast as possible.

RAID0 writes and reads to multiple drives at the same time, hence the performance increase by using it. Does LizardFS do this also? Is using both better than either etc?","Turns out I forgot to chown the directories for user `lizardfs`, that's an amateur mistake right there.",9800119
35,I/O blocks when CGI/Info page reloads.,open,2019-02-14T05:41:03Z,2019-06-06T04:38:48Z,,MEMBER,"Reloading ""Info"" page in browser blocks I/O...

When I reload ""Info"" page in browser, I/O on another machine (`rsync`) stops/freezes for the duration of page load (few seconds). 

Reproducible every time.",Latest. 3.13.0~rc1 on Debian.,9800119
36,Invalid metadata server status,open,2019-01-31T00:18:13Z,2019-02-09T06:10:30Z,,NONE,"Hi folks

When trying to use the uraft implementation I get the following when running the service:

connecting to Master
Invalid metadata server status
connected to Master
message repeated 3 times: [ Invalid metadata server status.]

Here is the uraft setup:

URAFT_PORT = 9427
URAFT_STATUS_PORT = 9428
URAFT_NODE_ADDRESS = 10.10.0.40
URAFT_NODE_ADDRESS = 10.10.0.41
URAFT_NODE_ADDRESS = 10.10.0.42
URAFT_ID = 0
LOCAL_MASTER_ADDRESS = localhost
LOCAL_MASTER_MATOCL_PORT = 9421
ELECTION_TIMEOUT_MIN = 400
ELECTION_TIMEOUT_MAX = 600
HEARTBEAT_PERIOD = 20
LOCAL_MASTER_CHECK_PERIOD = 250
URAFT_FLOATING_IP      = 10.10.0.50
URAFT_FLOATING_NETMASK = 255.255.255.0
URAFT_FLOATING_IFACE   = br0

I have the same on another 2 servers

All 3 servers are set to PERSONALITY = ha-cluster-managed

Any ideas what could be wrong?

Using keepalived 

",any ideas or help? I'm running this on ubuntu,9800119
37,Chunks Lost When Snapshots Are Removed,open,2019-01-18T11:35:59Z,2019-05-08T07:11:31Z,,NONE,"I use a [cronjob](https://mrw.sh/admin-scripts/backup/src/branch/master/lizardfs) that creates daily/weekly/monthly snapshots and removes old snapshots (you can configure, how many daily/weekly/monthly snapshots to keep). It uses `lizardfs makesnapshot -l` and `lizardfs rremove -l`.

I have a [ssh server](https://github.com/mwaeckerlin/ssh) to store my backups from remote servers. This server's volume is also part of the snapshots. Then two monthes ago, I started to backup a server from the internet, that contains huge amounts of small files (i.e. from amavis virus scanner, postfix and a dokuwiki). The backup plus the snapshots very soon increases the memory usage of my master server from ~26GB to ~100GB within a month (and I had to upgrade from 40GB to now 134GB RAM).

Now I have very huge snapshots of these huge backup. The problem is when the snapshots are removed, and there are many problems:
 1. I had to configure the trash time everywhere to `0`, because otherwise the trash ends up with terabytes of trash files that are never cleaned up and the trash space grows and grows and grows. See also #702. So the issue there still exists.
 2. The snapshots are not properly cleaned up! Even after e.g. `lizardfs rremove -l /srv/backups/snapshots/volumes.daily-2019-01-11-06-25`, the path and some files remain and are not cleaned up. `rm -rf` is extremely slow, but seems to work better, but also `rm -rf` must be run twice (for `lizardfs rremove`, even running it twice or more does not solve the problem). Also this is obviously a bug.
 3. But the worst of all: If I remove snapshots, chunks are lost! It seems that the problem appears more with `lizardfs rremove` than with `rm -rf`, but I am not 100% sure about this.
    - When I loose one chunk, I can easily loose a lot of files (e.g. here: three files, even one from production, the others from the snapshots)
    - I suspect that when I remove a file with shared chunks (chunks that are shared with other snapshots), that these chunks are sometimes deleted (or at least somehow lost in the metadata server), even though they are still in use and should not yet be removed
    - It seems, that when I completely shutdown and restart the master server, the lost chunks may come back (although that's not granted) — unfortunately, I cannot simply restart the master server for testing this, because a shutdown (`systemctl stop lizardfs-master`) lasts between 20-30 minutes and a restart (`systemctl stop lizardfs-master`) between 40-60minutes, plus then I have to wait another ~20 min until all chunks are loaded again, so «restarting the master server» means a downtime of two or more hours for all of my services…

Here a current example:

![grafik](https://user-images.githubusercontent.com/4056726/51382771-f4fe1200-1b17-11e9-9e44-733148c09f9d.png)

```bash
marc@universum:~$ lizardfs-admin list-defective-files universum 9421
Files with error flag = unavailable | undergoal | structure-error
  file 22326392: backups/snapshots/volumes.daily-2019-01-17-06-25/safechat-ch/mysql-bin.000042 - unavailable
  file 51238282: backups/restored/20190119-safechat-ch/mysql-bin.000042 - unavailable
  file 67895520: backups/snapshots/volumes.daily-2019-01-16-06-25/safechat-ch/mysql-bin.000042 - unavailable
  file 245399823: backups/snapshots/volumes.daily-2019-01-18-06-25/safechat-ch/mysql-bin.000042 - unavailable
```
","Next:

```bash
marc@universum:~$ time sudo rm -rf /srv/backups/rm/volumes.daily-2019-01-10-06-25/
rm: das Entfernen von '/srv/backups/rm/volumes.daily-2019-01-10-06-25/[...]' ist nicht möglich: Das Verzeichnis ist nicht leer
rm: das Entfernen von '/srv/backups/rm/volumes.daily-2019-01-10-06-25/[...]' ist nicht möglich: Das Verzeichnis ist nicht leer

real    462m41.437s
user    2m41.480s
sys     24m7.992s
marc@universum:~$ time sudo rm -rf /srv/backups/rm/volumes.daily-2019-01-10-06-25/
[sudo] Passwort für marc: 

real    1m55.673s
user    0m0.628s
sys     0m5.424s
```",9800119
38,chunkservers were not found when fault master node in HA cluster  ,open,2019-01-14T09:49:03Z,2019-07-18T19:02:12Z,,NONE,"I have an HA cluster setup with uRaft running 3.13.0, and it's running really well.
The cluster with 3 machines like this：
URAFT_NODE_ADDRESS = 192.168.0.1            # ip of first node
 URAFT_NODE_ADDRESS = node2                  # hostname of second node
 URAFT_NODE_ADDRESS = node3:99427            # hostname and custom port of third node
 URAFT_ID = 0                                # URAFT_ID for this node
 URAFT_FLOATING_IP = 192.168.0.100           # Shared (floating) ip address for this cluster
 URAFT_FLOATING_NETMASK = 255.255.255.0      # Netmask for the floating ip
 URAFT_FLOATING_IFACE = eth1                 # Network interface for the floating ip on this node

# Configuration for node2:
 URAFT_NODE_ADDRESS = 192.168.0.1            # ip of first node
 URAFT_NODE_ADDRESS = node2                  # hostname of second node
 URAFT_NODE_ADDRESS = node3:99427            # hostname and custom port of third node
 URAFT_ID = 1                                # URAFT_ID for this node
 URAFT_FLOATING_IP = 192.168.0.100           # Shared (floating) ip address for this cluster
 URAFT_FLOATING_NETMASK = 255.255.255.0      # Netmask for the floating ip
 URAFT_FLOATING_IFACE = eth1                 # Network interface for the floating ip on this node

 # Configuration for node3:
 URAFT_NODE_ADDRESS = 192.168.0.1            # ip of first node
 URAFT_NODE_ADDRESS = node2                  # hostname of second node
 URAFT_NODE_ADDRESS = node3:99427            # hostname and custom port of third node
 URAFT_ID = 2                                # URAFT_ID for this node
 URAFT_FLOATING_IP = 192.168.0.100           # Shared (floating) ip address for this cluster
 URAFT_FLOATING_NETMASK = 255.255.255.0      # Netmask for the floating ip
 URAFT_FLOATING_IFACE = eth1                 # Network interface for the floating ip on this node


when I fault the master node(like kill the master processs or reboot the node),  I found the chunkservers were disappear in the graphical web-based monitoring interface, and the IO was stuck.
I want to know if this was normal, 
Only when I manually do the following to restore the master node and the cluster , the cluster will return to normal：
cd /var/lib/mfs/
remove the meta*.lock file
mfschunkserver stop
mfschunkserver start
service lizardfs-uraft restart

Then the chunkservers appeared in the graphical web-based monitoring interface, and the IO returned to normal.

Does a three-node environment not allow a node to fail? 



","> @Blackpaw
> 
> > The floating ip is automatically managed via URAFT using the ""ip addr"" commands. Whatever is the current master (determined by quota voting) gets the floating ip.
> > No need for proxies etc. Just have everything (clients, chunkservers etc.) connect to the floating IP, that way they will always connect to the current master.
> 
> That's probably what I don't understand.
> on Hetzner, the floating IP can be connected to only one of the masters and you switch it via web-API
> So from outside only one of the VPSes is connected to the floating IP at any time.
> URAFT would have to deal with calling web-API as well


Hi, did you figure out a solution for this problem. ",9800119
39,what filesystem for chunkserver (XFS or ZFS?),open,2019-01-12T18:21:15Z,2019-08-21T11:53:43Z,,CONTRIBUTOR,"Hello,

What file system do you recommend for chunkserver for Linux Debian 9?
XFS or ZFS?
What mount options for ZFS are the same as for XFS?","A word of warning I owe you.
Never use non-mirrored ZFS on unreliable medium like usb-attached disk. I developed a ZFS corruption that hit several chunksNN directories on 3 drives. Despite of ec(4,2) parity I lost some data that way.

So now I'm in the process of migration to XFS, since LizardFS utilizes sparse files on XFS pretty well -- see example below:
```
8.0K 65K chunk_ec_1_of_4_2_000000000000002E_00000001.liz
 12K 65K chunk_ec_1_of_4_2_0000000000000198_00000001.liz
 16K 65K chunk_ec_1_of_4_2_0000000000000055_00000001.liz
 28K 65K chunk_ec_1_of_4_2_000000000000010A_00000001.liz
 52K 65K chunk_ec_1_of_4_2_0000000000000121_00000001.liz
 60K 65K chunk_ec_1_of_4_2_0000000000000192_00000001.liz
 68K 65K chunk_ec_1_of_4_2_00000000000001A5_00000001.liz
```
Thus if one copies files around with rsync (like @onlyjob [described above](#issuecomment-453891148) -- this sparseness must have been lost, causing space consumption).",9800119
40,chunkserver: chunk file name to contain inode,open,2018-12-26T05:22:05Z,2018-12-26T05:22:15Z,,MEMBER,"It would be nice to incorporate inode number into chunk files' name.

It will help to find disconnected (dead) chunks (#626) and to reverse map chunk to file.",,9800119
41,client: improve EC write performance,open,2018-12-18T06:19:29Z,2018-12-18T06:21:56Z,,MEMBER,"Performance of EC chunks writing could be improved by only ever writing changes as `K`replicated chunks (as in `$ec(M,K)`) hence letting master and chunkserver to lazily change the goal of modified chunks to _ec_.

That should be easy to implement and it would reduce surface for bugs in client by simplifying the client code (as client won't have to do any processing of saving EC chunks whatsoever).",,9800119
42,local IO limits.,open,2018-12-13T20:23:04Z,2019-04-30T13:57:40Z,,NONE,"HI, i am playing with local IO limits, my configuration file is following:

`limit unclassified 8192`
my application stucks when reading in many threads. I started discover sources and found following in global_io_limiter.cc:

```
uint8_t LimiterProxy::waitForRead(const pid_t pid, const uint64_t size, SteadyTimePoint deadline) {
	std::unique_lock<std::mutex> lock(mutex_);
	uint8_t status;
	do {
		if (!enabled_) {
			return LIZARDFS_STATUS_OK;
		}
...
...
}
```
I am not expert in c++ multithreaded development, but looks like, when enabled, only one thread a time can get IO limit resources and maybe that is a reason of mu reads stuck? Shoul we replace the 
`std::unique_lock<std::mutex> lock(mutex_);` somewhere else (as far as i could understand this lock is needed to get configuration update on the fly, but a can be wrong)
If i am wrong, can anybody, please, explain how can I correctly configure IO limit on my client machine?
Please advise.

","What do you mean by *stuck*? Block? Only one thread at a time can read given *or* any file? How do you know it? What is the behavior of your application when *stuck* happens? Do you modify [`globaliolimits.cfg`][1] or [`iolimits.cfg`][2]? Do you think that I should be able to reproduce it with simple Python script that creates 100 threads that read files in LizardFS share (assuming that I would have configuration file with `limit unclassified 8192`)?

[1]: https://docs.lizardfs.com/man/globaliolimits.cfg.5.html
[2]: https://docs.lizardfs.com/man/iolimits.cfg.5.html",9800119
43,Why the chunk replication speed slows down after a while? ,open,2018-11-26T00:45:32Z,2018-11-29T03:59:43Z,,NONE,"I've notice this behavior in 2 systems I've setup. 
After changing goals for files, LFS starts replication and goes fast and steady... as soon as it starts to get to the end, it starts to slow down until it stays literally CRAWLING to a halt, compared to when it started. 

The most frustrating part of this is, because of this slow down, the very last chunks needed to replicate take weeks, which is very concerning since an HD could fail and we would loose data. 

this is a screenshot of the behavior... a week of replication after a few changes in goal for all files in the pool:
![image](https://user-images.githubusercontent.com/806743/48987204-c2b0d500-f0d1-11e8-8aa9-e3be656bdbb0.png)
I'm feeding grafana with collected data from lizardfs-admin stored in telegraf. This gives me a way better visualization of what LFS is doing in the long term.

as you can see, at the begining of the week, there was a lot of chunk decrease due to replication happening. Most of the replication was being done to a new chunk I've added to the system (11 chunks total).

There was about 250.000 chunks to be replicated at the begining of the week... in about 4 days, this number went down to 62.000 (almost 50.000 a day!)... since then, it's being 3 days and I still have 54.000 to go, and the rate is slowing down exponentially!!! 

this is the graph for the last 24 hours:
![image](https://user-images.githubusercontent.com/806743/48986739-62b82f80-f0cd-11e8-99ae-a82cee199f73.png)

As you can see, since today around 2pm, it's doing almost NOTHING, and I still have 54.000 chunks to go!! 
![image](https://user-images.githubusercontent.com/806743/48986750-87140c00-f0cd-11e8-811a-2d48f7b74d7a.png)

It's actually very frustrating to see this slow down happening so often. I've actually changed the goals last week just to try to force the pool to FINISH replicating, since it's always doing this slow down at the end... This is the 4th time I change goals in the hopes of seeing the replication finishing and finally known that my data is safe! 

but it NEVER ends... It's being 5 months that the pool NEVER finishes replicating and my data is at risk!! 

Today, if I take 1 chunk offline, I always end up with missing chunks... no matter what chunk I take offline! 
The goals are ec(6,2), so in theory, I could take 2 chunks out without loosing anything. But since it never finishes replicating, I have loads of data at risk! 

Yesterday I've set CHUNKS_WRITE_REP_LIMIT=100 and CHUNKS_READ_REP_LIMIT=200, but it had NO EFFECT WHATSOEVER, as you can see on the graphs above... Right now, the replication is almost halting, with those to values!! 

so if any of my chunks really die today, I'll loose data for sure because of this slow down... 

please help!! I need to be able to forcibly end the replication!!!  


PS: as another example, this is another LFS pool that I'm running for more than a year (I think it's almost 2 years actually): 

![image](https://user-images.githubusercontent.com/806743/48986998-ac097e80-f0cf-11e8-9c3e-520cfcb1b09d.png)

Apart from the cp(1) goals, you can see that I have this nexenta_ec6_2  ec(6,2) goal  which still need replicating, and they have being so slow that they replicate at a rate of 1 chunk per DAY!!! 
it's being 4 MONTHS to get from around 5.000 chunks down to 1146 (1 copy replicate), and it's still going.. 

As you can see, I do have missing chunks in this pool because since I started it, I did have failed hard drives, and and indeed lost chunks thanks to this slow down behavior!!! 

I think this is a very serious issue in LizardFS, one that I don't see people giving too much attention. I did see some people complaining, but the answer is always to tweak CHUNKS_* parameters in the master to trade-of replication speed by user access latency... But I REALLY don't think this is the solution! 

Theres something seriously wrong in the way LizardFS goes about the replication tasks, and it really looses momentum during replication of large number of chuncks, which puts the data in risk for months and months... and there's absolutely NOTHING we can do but wait and cross our fingers right now!! 

I'm going back to evaluating other cluster filesystem out there right now because I really need to sleep at night without fearing that I'm going too loose my data. 

very sad indeed... I really like LFS... but after 2 years, I still can't trust it because it never finishes!



","just to keep it registered... It seems the MTU 1400 on the chunkservers made a HUGE difference indeed.
![deepinscreenshot_select-area_20181128195537](https://user-images.githubusercontent.com/806743/49198379-acac4a00-f347-11e8-994d-92f7ea57ba13.png)

Data keeps replicating and re-balancing without stopping for the last 2 days! 
![deepinscreenshot_select-area_20181128195710](https://user-images.githubusercontent.com/806743/49198420-d06f9000-f347-11e8-8dc2-f31aef8e95a0.png)

although, /media and /airport1TB are STILL hanging for 1 chunk!!! LOL
at least it's steady and going... hopefully it will finish soon!!! 
",9800119
44,Cluster Restart Issue/Question,open,2018-11-21T01:24:56Z,2019-06-25T15:50:30Z,,NONE,"When restarting a cluster from a cold state, we noticed that it took an exceedingly long time to start (5+ hours).

I noticed that if I started the chunk servers one at a time it took about 30 seconds each, and then the master would drop in CPU usage.  So, initially I wrote a script that would loop through all of the chunk servers, one at a time, waiting for CPU to drop.  That worked okay and took startup times down to under 2 hours!

Today during a reconfigure test (long story), I noticed that even with that script, it spent a long time at 100% even after the scanning was completed.

So, I made some adjustments to the script to check for any disks in a scanning state rather than CPU usage and let it run.

Everything was great for about 5 minutes...  Then the master server went from slow (normal during startup) to timing out.  This crashed my script and made me go ""Well that's not good!""

Some digging around and I ended up running:
`perf top -p $(pgrep mfsmaster)`

That resulted in this:
```
Samples: 577K of event 'cpu-clock', Event count (approx.): 16029633365
Overhead  Shared Object        Symbol
  80.75%  mfsmaster            [.] _ZN17linear_assignment19auctionOptimizationISt5arrayIS1_IiLm64EELm64EELm64EEEvRT_RS1_IiXT0_EES7_i
   5.33%  mfsmaster            [.] _ZN21ChunkCopiesCalculator8optimizeEv
   2.24%  mfsmaster            [.] 0x000000000014760f
   0.56%  mfsmaster            [.] _ZN21ChunkCopiesCalculator18getFullCopiesCountERK4Goal
   0.55%  mfsmaster            [.] 0x00000000001476fd
....
```

This optimization tasks continued to tank the master for a good 15 minutes or so.

So, my questions:

Is there a way 

1. to disable this optimization function until after a restart has been completed.
2. Should I readjust my script to CPU Load testing.
3. Should it take 2 hours (scripted, 5+ hours not) to restart a cluster with under 50TB of data (1.4GBs of meta)?
4. Am I being dumb and missing something?

Notes:
- Ubuntu 18.04
- libJudy is installed
- The trash is kept empty.
- We do not have HA configured yet.
- We have 6 nodes and about 220 chunk servers (done for erasure coding reasons). 
- And of course the issue is not as bad if we use just 6 chunk servers (one per node).
- We did not think the 220ish chunk servers would be an issue.
","@kaitoan2000 Nope. My report is essentially just a ""plus one"" for the issue. I only have a production cluster, so I can't experiment with it anyways. I jotted down instructions in our internal docs so next time a network outage / maintenance occurs me or my co-workers will know they have to manually restart daemons one by one. The problem might be tied to our relatively low throughput network, 500-1000 MBit/s. But it's a wild guess.",9800119
45,How to use Berkeley?,open,2018-10-27T21:30:01Z,2019-05-30T13:03:14Z,,NONE,"Hi, how do i use Berkeley if i have it already setup using mostly the default settings and it's in prod?

Does it require any extra packages? Does just making USE_BDB_FOR_NAME_STORAGE=1 switch it on and works without doing anything extra?

Does this work in 3.11?",Related issue: #748. I'm trying to reproduce the bug with no luck. Any idea how to reproduce it?,9800119
46,Error in lizardfs when using ipip networking between nodes,open,2018-10-27T08:17:01Z,2018-10-28T04:04:14Z,,NONE,"I have a 6 node lizardfs cluster. 

The address of chunk server is:

```
chunkserver1: 172.18.8.230
chunkserver2: 172.18.9.134
chunkserver3: 172.18.10.122
```

They are interconnected in 3 networks 172.18.9.0/24, 172.18.10.0/24 and 172.18.8.0/24 using ipip networking.

All of their hosts file has entry for proper address:
```
/etc/hosts on lizardfs master
72.18.10.122 chunkserver3
172.18.9.134 chunkserver2
172.18.8.230 chunkserver1
```

Now when I switch on the chunkserver they are registered using 172.18.9.3 and 172.18.10.2 which is the address for ipip tunnel endpoints. I believe there is a bug in the way lizardfs try to work to register chunkserver, metalogger with master.","The chunk servers can be contacted from master, the registered IP looks wrong. Also for some files when accessing it gives input/output error which might be due to not being able to find the chunk stored on the other two chunkserver which is registered with wrong IPIP. 
IPIP networking is pretty standard to create tunnel between two network without the overhead of VPN. Its more a routed network then a broadcast bridge network. 
So probably something wrong with the way master server is trying to get chunkserver ip. Indeed in /etc/hosts on master the chunkserver ip is also there. I believe the code trying to register chunkserver needs to be updated to register ip correctly on the master node.",9800119
47,How to copy the file from chunk servers to another folder,open,2018-10-27T07:42:30Z,2018-11-03T16:33:16Z,,NONE,"Does anyone have an idea how to recover actual files from chunk servers?

I can see all the chunks are there and chunk server running fine. I can cd into directory but cannot copy files.","I doubt you can since I believe the meta data is on the master server. 
That said, I'm curious to know why you want to do this?

If there is a problem with a disk and you want to rescue as many chunks as you can, you can just rsync the chunks to another chunkserver. ",9800119
48,Cannot read or write to lizardfs cluster,open,2018-10-25T00:12:22Z,2019-04-24T06:49:57Z,,NONE,"Created a lizardfs cluster with 3 chunk servers, 1 master, 1 shadowmaster and 1 metalogger servers. The cluster ahs been working for a while. But then chunkserver 2 and 3 went down. So it saved files on chunkserver 1. But when restarted chunk server 2 and 3. None of the files created can be accessed now.
This creates a lot of problem. How to fix it?

There are no documentation help on it.","@vikrantrathore If you use NFS server/client maybe this is NFS problem? Did you look in [`journalctl`][1] or other logs? Use [`mfsmount`][2] with `-d` (debug) option or add `debug` mount option to your `/etc/fstab` and paste your logs here, so that I was able to try to help you.

[1]: https://wiki.archlinux.org/index.php/Systemd/Journal
[2]: https://docs.lizardfs.com/man/mfsmount.1.html",9800119
49,"3.13rc, uRaft, Restart Master",open,2018-10-16T23:40:37Z,2018-10-18T00:57:26Z,,NONE,"I have an HA cluster setup with uRaft running 3.13rc, and it's running really well.

My questions is regarding applying configuration changes to master, such as changes made in `mfsmaster.cfg`.

In previous versions, without uRaft, I would simply execute something like:

`/etc/init.d/lizardfs-master restart`

When I try to do that now, I get this message in syslog:

`This installation is managed by HA cluster, one should manipulate metadata servers only using lizardfs-cluster-manager`

Fair enough.  I can't find any files on my system called lizardfs-cluster-manager (or any files with the word ""cluster"" in the name), so I assume this isn't referring to a literal file name.  I try restarting the uRaft service:

`/etc/init.d/lizard-uraft restart`

Then I'm met with this message:

`can't start: lockfile /var/lib/mfs/.mfsmaster.lock is already locked by another process`

I checked `lizardfs-admin` thinking there might be something in there, and I found the `reload-config` command, so I tried this:

`lizardfs-admin reload-config 172.31.2.13 9419`

But then I receive this message:

`Error: Can't read data from socket: connection reset by peer`

The only thing that's worked for me so far is to stop the uRaft service, `kill -9` the master service (`kill` by itself doesn't work), then start the uRaft service.  It works, but I assume that's not the right way to do it.

Looking for some guidance on this.. thanks!","I am unsure if it is the preferred way but I have found that using this works
`mfsmaster -o ha-cluster-managed reload`",9800119
50,lizardfs security,open,2018-10-07T16:38:26Z,2018-10-25T10:04:50Z,,NONE,"Hi,

I have my lizardfs running over multiple datacentres and I am happy.
However.

1. it looks like it is enough to setup a new chunksever, point to my master and boom. 
no authentication, nothing. I just see more hdd space.
This does not look a bit secure. Am I doing something wrong?

2. The fuse mount is secured by password. This also does not look even a bit secure.
Am I doing something wrong here too?

There is hardly anything about securing lizardfs installation available in the google search. Could someone please point me in the correct direction?","Thank you guys
I just finished setup with openvpn

My data is on encrypted s3ql so I won't bother with encrypting chunk
servers.
But I am curious, does it make sense to encrypt chunk servers at all if I
am using EC targets?




On Sun, 14 Oct 2018 at 22:01, dragon2611 <notifications@github.com> wrote:

> I'd hope that the servers are on a private network.
> If not maybe look into zerotier https://zerotier.com/ LizardFS will work
> over a zerotier vpn, although depending on what your chunkservers are
> running it might be a bit slow (one of mine is an ARM based with a weak CPU
> so struggles to get over 100Mbit on VPN)
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/lizardfs/lizardfs/issues/777#issuecomment-429662216>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AE7gM_h0V3OoqUTFVA2B_gjr-8ZBMe5oks5uk6YdgaJpZM4XL7JC>
> .
>
",9800119
51,How many chunks of each goal on chunkserver?,open,2018-10-02T17:53:17Z,2019-04-24T13:53:40Z,,CONTRIBUTOR,"Is there a way to count not just chunks per chunkserver, but split that into counting how many chunks of each goal there are on a given chunkserver? Say chunkserver 3 has 5000 chunks, how many of them are part of a `_ _` goal, how many of them are part of a `$ec(4,3)` goal. Bonus: For xor or ec goals, how many chunks on a given server are parity vs data?","Currently there is no such a feature – only overall goals' counts are available. For now development is focused on bug fixing and providing stability rather than introducing new features.

If you would like to order this or any other feature, please contact contact@lizardfs.com.",9800119
52,quota on smb share,open,2018-09-29T12:16:27Z,2018-10-08T00:34:55Z,,NONE,"Hi,

I need to set up smb share on one of the lizard subfolders.
I limited the quota on this folder and even mounted it separately.
Still, df and samba share reports the complete size of the filesystem
Do I do something wrong or is it a bug?","The amount of free space shown is the amount of free space across the available drives in the chunkserver.  It can't really show it to you any other way, because it can't predict how goals will be set on yet non-existent files.

However, if you're wanting an SMB client to show a lesser amount of space, you can look into the ""dfree command"" option for Samba in the share declaration section of smb.conf.  Here, you can create a script that returns the information you want Samba to report to the client as total and available space.",9800119
53,main master server module: got invalid message in shadow state,open,2018-09-19T15:00:34Z,2018-10-08T12:47:55Z,,NONE,"Hello, 
What does this error mean:

 mfsmaster[5582]: sessions downloaded 769B/0.000272s (2.827 MB/s)
 mfsmaster[5582]: **main master server module: got invalid message in shadow state (type:1549)**
 mfsmaster[5582]: sessions downloaded 769B/0.000490s (1.569 MB/s)
 mfsmaster[5582]: **main master server module: got invalid message in shadow state (type:1549)**
 mfsmaster[5582]: sessions downloaded 769B/0.000214s (3.593 MB/s)
 mfsmaster[5582]: **main master server module: got invalid message in shadow state (type:1549)**

```
# lizardfs-admin metadataserver-status localhost 9421
     personality: master
   server status: running
metadata version: 48761830
```
```
# lizardfs-admin metadataserver-status localhost 9421
     personality: shadow
   server status: connected
metadata version: 48761830
```","We have 2 servers. One is in master state:
PERSONALITY = master
and the second one is in shadow state:
PERSONALITY = shadow

We are testing keepalived failover.

We use the next steps to switch from master to shadow and .
When master node is an unavailable:
1) Replace string in mfsmaster.cfg on shadow node:
PERSONALITY = shadow
to 
PERSONALITY = master
2) systemctl restart lizardfs-master

When previous master node is available again:
3)   Replace string in mfsmaster.cfg on this node:
PERSONALITY = master
to 
PERSONALITY = shadow
4) systemctl restart lizardfs-master

And this issue appeared during one of this tests.",9800119
54,Almost no logging to stdout/stderr from master (but from shadow!),open,2018-09-17T15:49:17Z,2018-09-17T15:49:17Z,,NONE,"I'm using docker containers and need logging to go to stdout/stderr. Unfortunately this works for mfsmaster only in shadow mode, but not in master mode. In master mode only one line is logged:

```
2018-09-17T00:16:44.460952581Z mfsmaster[1]: Changing SYSLOG_IDENT to mfsmaster
```

while a shadow instance is logging this to stdout/stderr:

```
2018-09-17T15:32:47.419209076Z 09/17/18 15:32:47.418 [info] [1:1] : configuration file /etc/lizardfs/mfsmaster.cfg loaded
2018-09-17T15:32:47.564806027Z mfsmaster[1]: connected to Master
2018-09-17T15:32:56.444331886Z mfsmaster[1]: metadata downloaded 151400992B/8.865805s (17.077 MB/s)
2018-09-17T15:32:56.593916651Z mfsmaster[1]: changelog.mfs.1 downloaded 2936447B/0.127490s (23.033 MB/s)
2018-09-17T15:32:56.924658691Z mfsmaster[1]: changelog.mfs.2 downloaded 5809773B/0.303314s (19.154 MB/s)
2018-09-17T15:32:56.930844161Z mfsmaster[1]: sessions downloaded 5355B/0.001435s (3.732 MB/s)
2018-09-17T15:32:56.957405947Z mfsmaster[1]: opened metadata file /var/lib/lizardfs/metadata.mfs
2018-09-17T15:32:56.957548030Z mfsmaster[1]: loading objects (files,directories,etc.) from the metadata file
2018-09-17T15:32:59.228740778Z mfsmaster[1]: loading names from the metadata file
2018-09-17T15:33:06.566771763Z mfsmaster[1]: loading deletion timestamps from the metadata file
2018-09-17T15:33:07.317505235Z mfsmaster[1]: loading extra attributes (xattr) from the metadata file
2018-09-17T15:33:07.317597151Z mfsmaster[1]: loading access control lists from the metadata file
2018-09-17T15:33:07.317634984Z mfsmaster[1]: loading quota entries from the metadata file
2018-09-17T15:33:07.317671317Z mfsmaster[1]: loading file locks from the metadata file
2018-09-17T15:33:07.317701567Z mfsmaster[1]: loading chunks data from the metadata file
2018-09-17T15:33:08.195694579Z mfsmaster[1]: checking filesystem consistency of the metadata file
2018-09-17T15:33:08.264723906Z mfsmaster[1]: connecting files and chunks
2018-09-17T15:33:14.081901936Z mfsmaster[1]: calculating checksum of the metadata
2018-09-17T15:33:14.752185468Z mfsmaster[1]: metadata file /var/lib/lizardfs/metadata.mfs read (1741842 inodes including 161747 directory inodes and 1545895 file inodes, 306237 chunks)
2018-09-17T15:33:14.752332884Z mfsmaster[1]: AUTO_RECOVERY enabled - applying changelogs from /var/lib/lizardfs
2018-09-17T15:33:15.177900539Z mfsmaster[1]: /var/lib/lizardfs/changelog.mfs.1: 60864 changes applied (662625463 to 662686326), 0 skipped
2018-09-17T15:33:15.178711785Z mfsmaster[1]: /var/lib/lizardfs/changelog.mfs: 12 changes applied (662686327 to 662686338), 277 skipped
2018-09-17T15:33:21.543229934Z mfsmaster[1]: all needed changelogs applied successfully
2018-09-17T15:33:21.543332850Z mfsmaster[1]: synced at version = 662686338
2018-09-17T15:33:21.588187796Z mfsmaster[1]: sessions downloaded 5355B/0.001755s (3.051 MB/s)
2018-09-17T15:34:00.033789407Z mfsmaster[1]: sessions downloaded 5355B/0.000942s (5.685 MB/s)
2018-09-17T15:35:00.024671259Z mfsmaster[1]: sessions downloaded 5355B/0.001241s (4.315 MB/s)
2018-09-17T15:36:00.049862825Z mfsmaster[1]: sessions downloaded 5355B/0.002983s (1.795 MB/s)
...
```

In syslog logging is fine for both modes. 

The thing is that most errors are also not visible in stdout/stderr for master! That might be the reason why I thought my master runs stable, when in reality it was crashing from time to time.
",,9800119
55,Question: ec goal conversion - chunkserver requirements,open,2018-09-14T10:06:49Z,2019-01-10T19:17:02Z,,NONE,"Hi Development team.

Could someone please clarify the requirements for ec goal conversion?
The scenario:
4 chunkservers, ec2,2 
I want to add 5th chunkserver and move to ec3,2

is it enough to add one chunkserver, change the goal and wait?
Or, do I need to add five chunkservers so the ec2,2 can effectively be replicated to another 3,2 copy before it gets deleted. Then only after conversion, I would remove four chunkservers I used previously.

Which one is it?","Thanks for testing 



null",9800119
56,Question: lizardfs on a single machine,open,2018-09-11T10:32:42Z,2019-11-20T23:24:25Z,,NONE,"Hi,

I am running a backup machine at home with a bunch(fifteen) of different HDD. From brand new 10TB to old 1TB. The machine runs only for a few hours a day until backups are done.
I am using BTRFS in so-called RAID1. With BTRFS it guarantees that each file is stored on two physically different drives. BTRFS also handles different sizes of the drives well.

The idea is to recycle drives from other machines. Every old drive that is bigger than the smallest in the pool could increase my storage and be useful for some more time. The one that would drop out of the pool was, a long time ago, meant for bin anyway. 

Recently I started using lizardfs for my network backups and I love it.
It made me think - what if I replace BTRFS on my backup machine with lizardfs? Using EC I could even gain some storage space.
so here are the questions
1. I could run master and chunkserver on the same machine. I could add 15 drives to the chunkserver. Would this work? Or would I have to run 15 virtual chunk servers?
2. how would EC behave with so different disk sizes? What would mean if I set EC on 10,5? Randomly peaking from my pool five drives could have less than 10TB of space or as much as 30TB.","hmmm, yes, I think you are right that it is always safer using the maximum factor available for your drives.  It would depend on whether lizard prioritises by missing chunk ratio or not I guess.",9800119
57,Client hangs when a chunk is unavailable,open,2018-09-10T10:15:45Z,2018-10-25T09:14:11Z,,CONTRIBUTOR,"Problem: on my installation I've had a couple of failing nodes which resulted in files with missing chunks.

Now I do know how to find and remove these files, but in the meantime I'd like access to these chunks to return an immediate error – instead of waiting infinitely long for the chunk to become available again (which won't happen without a time machine).
",@matthiaz it's with a  manual shift. I've found that the problem is because the shadow server was shut down before the master. Consider this problem solved :),9800119
58,Input/output error,open,2018-09-10T08:30:52Z,2018-10-28T10:58:29Z,,NONE,"This weekend my Lizardfs cluster hat again a strange hiccup. The master process was still alive and kicking, but all shadows were down without any error message. I tried to restart the shadows without success. Still no error messages in the logs, neither in the log file from master, nor from the shadows. Here a log file from one shadow instance after an unsuccessful restart:

```
09/09/18 10:20:45.795 [info] [1:1] : configuration file /etc/lizardfs/mfsmaster.cfg loaded
mfsmaster[1]: connected to Master
mfsmaster[1]: metadata downloaded 123378662B/6.851125s (18.009 MB/s)
mfsmaster[1]: changelog.mfs.1 downloaded 73622B/0.002170s (33.927 MB/s)
mfsmaster[1]: changelog.mfs.2 downloaded 0B/0.000001s (0.000 MB/s)
mfsmaster[1]: sessions downloaded 3917B/0.000951s (4.119 MB/s)
mfsmaster[1]: opened metadata file /var/lib/lizardfs/metadata.mfs
mfsmaster[1]: loading objects (files,directories,etc.) from the metadata file
mfsmaster[1]: loading names from the metadata file
mfsmaster[1]: loading deletion timestamps from the metadata file
mfsmaster[1]: loading extra attributes (xattr) from the metadata file
mfsmaster[1]: loading access control lists from the metadata file
mfsmaster[1]: loading quota entries from the metadata file
mfsmaster[1]: loading file locks from the metadata file
mfsmaster[1]: loading chunks data from the metadata file
mfsmaster[1]: checking filesystem consistency of the metadata file
mfsmaster[1]: connecting files and chunks

```
After this the shadow process just died...

During some start/restart experiments I stumbled upon a corrupted (?) directory in my file system. Please have a look, what happened:

```
root@insight:/# ls -l /mnt/lfs/volumes/mygitea-db/postgresql-db/           
total 30
-rw------- 1 70 70     4 Aug 29 21:01 PG_VERSION
drwx------ 6 70 70     0 Aug 29 21:02 base
drwx------ 2 70 70     0 Sep  6 20:08 global
drwx------ 2 70 70     0 Aug 29 21:01 pg_clog
drwx------ 2 70 70     0 Aug 29 21:01 pg_commit_ts
drwx------ 2 70 70     0 Aug 29 21:01 pg_dynshmem
-rw------- 1 70 70  4490 Aug 29 21:02 pg_hba.conf
-rw------- 1 70 70  1636 Aug 29 21:01 pg_ident.conf
drwx------ 4 70 70     0 Aug 29 21:01 pg_logical
drwx------ 4 70 70     0 Aug 29 21:01 pg_multixact
drwx------ 2 70 70     0 Sep  6 20:08 pg_notify
drwx------ 2 70 70     0 Aug 29 21:01 pg_replslot
drwx------ 2 70 70     0 Aug 29 21:01 pg_serial
drwx------ 2 70 70     0 Aug 29 21:01 pg_snapshots
drwx------ 2 70 70     0 Sep  9 11:42 pg_stat
drwx------ 2 70 70     0 Sep  9 11:42 pg_stat_tmp
drwx------ 2 70 70     0 Aug 29 21:01 pg_subtrans
drwx------ 2 70 70     0 Aug 29 21:01 pg_tblspc
drwx------ 2 70 70     0 Aug 29 21:01 pg_twophase
drwx------ 3 70 70     0 Aug 29 21:01 pg_xlog
-rw------- 1 70 70    88 Aug 29 21:01 postgresql.auto.conf
-rw------- 1 70 70 22205 Aug 29 21:01 postgresql.conf
-rw------- 1 70 70    24 Sep  6 20:08 postmaster.opts

root@insight:/# ls -l /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp
ls: reading directory '/mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp': Input/output error
total 35
-rw------- 1 70 70 35833 Sep  8 19:00 db_0.stat
```

Wtf?


```
christoph@insight:~/kubetools$ sudo lizardfs filerepair /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp/db_0.stat
/mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp/db_0.stat:
 chunks not changed:          1
 chunks erased:               0
 chunks repaired:             0

root@insight:/# lizardfs dirinfo /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp           
/mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp:
 inodes:                          4
  directories:                    1
  files:                          3
 chunks:                          3
 length:                      39223
 size:                       211968
 realsize:                   635904

root@insight:/# rm -r /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp
rm: cannot remove '/mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp/db_0.stat': No such file or directory
rm: traversal failed: /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp: Input/output error


root@insight:/# lizardfs checkfile /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp/db_0.stat 
/mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp/db_0.stat: realpath error on (/mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp/db_0.stat): No such file or directory
```

This does not look good!

```
root@insight:/# lizardfs rremove /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp
^C
```
No timeout, I needed to kill it with ctrl-c after some minutes

Ok, good time to restart my master server. Fortunately my shadows were able to start and they stayed alive!! Good so far!

Let's have another look at this strange folder:

```
root@insight:/# rm -rf /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp
rm: cannot remove '/mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp': Directory not empty
```
Not empty!?!? But what about my rm -r there?

```
root@insight:/# ls -l /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp
ls: reading directory '/mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp': Input/output error
total 0
```
Shit

```
root@insight:/# lizardfs dirinfo /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp
/mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp:
 inodes:                          3
  directories:                    1
  files:                          2
 chunks:                          2
 length:                       3390
 size:                       141312
 realsize:                   423936
```
Ok, two file are still there... But they are not visible...!?

```
root@insight:/# lizardfs rremove /mnt/lfs/volumes/mygitea-db/postgresql-db/pg_stat_tmp
^C
```
Still no luck. No timeout, no deleted files. Let's try another mount with debug option!

This is the debug output...

```
root@insight:/# mfsmount /mnt/lfs2 -H 192.168.1.99 -f -d
mfsmaster accepted connection with parameters: read-write,restricted_ip ; root mapped to root:root
09/10/18 09:30:53.845 [info] [30372:30373] : Received IO limits configuration update from master
FUSE library version: 2.9.8
unique: 1, opcode: INIT (26), nodeid: 0, insize: 56, pid: 0
INIT: 7.26
flags=0x001ffffb
max_readahead=0x00020000
   INIT: 7.19
   flags=0x00000041
   max_readahead=0x00020000
   max_write=0x00020000
   max_background=0
   congestion_threshold=0
   unique: 1, success, outsize: 40
unique: 2, opcode: GETATTR (3), nodeid: 1, insize: 56, pid: 30552
   unique: 2, success, outsize: 120
unique: 3, opcode: LOOKUP (1), nodeid: 1, insize: 48, pid: 30552
   unique: 3, success, outsize: 144
unique: 4, opcode: LOOKUP (1), nodeid: 312011, insize: 51, pid: 30552
   unique: 4, success, outsize: 144
unique: 5, opcode: LOOKUP (1), nodeid: 1331362, insize: 54, pid: 30552
   unique: 5, success, outsize: 144
unique: 6, opcode: LOOKUP (1), nodeid: 1559702, insize: 52, pid: 30552
   unique: 6, success, outsize: 144
unique: 7, opcode: GETXATTR (22), nodeid: 1559952, insize: 65, pid: 30552
   unique: 7, error: -61 (No data available), outsize: 16
unique: 8, opcode: GETXATTR (22), nodeid: 1559952, insize: 72, pid: 30552
   unique: 8, error: -61 (No data available), outsize: 16
unique: 9, opcode: GETXATTR (22), nodeid: 1559952, insize: 73, pid: 30552
   unique: 9, error: -61 (No data available), outsize: 16
unique: 10, opcode: OPENDIR (27), nodeid: 1559952, insize: 48, pid: 30552
   unique: 10, success, outsize: 32
unique: 11, opcode: READDIR (28), nodeid: 1559952, insize: 80, pid: 30552
   unique: 11, success, outsize: 224
unique: 12, opcode: READDIR (28), nodeid: 1559952, insize: 80, pid: 30552
   unique: 12, success, outsize: 160
unique: 13, opcode: RELEASEDIR (29), nodeid: 1559952, insize: 64, pid: 0
   unique: 13, success, outsize: 16
```
Of this command:
```
root@insight:/# ls -l /mnt/lfs2/volumes/mygitea-db/postgresql-db/pg_stat_tmp
ls: reading directory '/mnt/lfs2/volumes/mygitea-db/postgresql-db/pg_stat_tmp': Input/output error
total 0 
```

Any idea what to try now? During all of this there were no errors in my master log! Nothing!!

My setup is a little bit esoteric (armhf, all processes in docker container), but lizardfs was stable for ~6 months, until two weeks ago (see #738). This might be related or not, but this is a new directory, so it was not involved in the last incident.
","Could someone help me and try the following command on your master?

```
root@mylizardfs-master:/var/lib/lizardfs# mfsmetadump /var/lib/lizardfs/metadata.mfs | grep --color=NEVER -E ""^E\|p:\s*[0-9]+\|c:\s*[0-9]+\|n:.+/.+""
E|p:    669850|c:    772995|n:container/mymayandb/pg_stat_tmp/db_0.stat
```

Explanation:
I'm still trying to understand why my lizardfs generates metadata with a full path in an edge entry. As far as I can tell the metadata consists of nodes (=files and directories) and edges (=how files and directories correlate to each other). Edges seems to have the following attributes:

- parent inode (p)
- children inode (c)
- name (n)

I have no idea what name is for, because if an edge always links two inodes (with names) together, there should not be the need to have a name on the edge, but that is what mfsmetadump shows me. 

In my case from now and then an edge is created with a full path as name. And exactly this inode is corrupted and not accessible anymore (Input/output error). All other inodes are ok and can be accessed.

I think this happens during snapshotting a bigger folder structure from an already existing snapshot, when this snapshot is currently in use (read+write). 

orig <- snapshot A <- Snapshot B

- orig: No IO currently
- Snapshot A: read and write IO on snapshot ongoing
- Snapshot B: newly created, afterwards corrupted

But as this happens only every some weeks/months it's hard to reproduce :-(

Do you have inode edges in your metadata with a full path? Can somebody from the lizardfs team explain this?
",9800119
59,"""mfschunkserver -c file.cfg restart"" fails silently.",open,2018-09-06T23:28:14Z,2018-09-08T23:19:07Z,,CONTRIBUTOR,"I'm running a chunk server per disk on my testing machine (totalling four), and when I try to restart any chunkserver, the command returns nothing but the exist status is 2, and the chunkserver has not been reloaded. The only information I've found is in syslog where it says `localhost mfschunkserver: main server module: can't listen on socketAddress already in use`

```
$ sudo mfschunkserver -c /etc/mfs/mfschunkserver-1.cfg restart; echo $?
2
```","Basically it is #218. Daemons service management functionality is redundant.
It should be removed in favour of systemd service management which does a better job.",9800119
60,"Contents of written-to files are not immediately reflected on read (git commit --> ""Aborting commit due to empty commit message"")",open,2018-09-03T18:41:17Z,2018-11-09T00:55:12Z,,NONE,"I've been getting a weird issue with a LizardFS mount recently.

1. Enter git repo on LizardFS filesystem
2. `git commit -m ""Overhaul i3 config""`. There's no way this could fail to get a commit message, because it's passed in as an argument
3. Usually (not always), it'll respond `Aborting commit due to empty commit message.`

It's almost as if the write to the commit message file doesn't persist immediately?

I did an strace if that would be of use:

<details><pre><code>
execve(""/home/kevin/.nix-profile/bin/git"", [""git"", ""commit"", ""-m"", ""Overhaul i3 config""], 0x7ffd321d0218 /* 67 vars */) = 0
brk(NULL)                               = 0x1045000
mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f1e0fbc0000
access(""/etc/ld-nix.so.preload"", R_OK)  = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/tls/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/tls/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/tls/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/tls"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane"", {st_mode=S_IFDIR|0555, st_size=20480, ...}) = 0
openat(AT_FDCWD, ""/run/opengl-driver/lib/tls/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver/lib/tls/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver/lib/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver/lib/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/tls/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver/lib/tls"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver/lib/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver/lib/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver/lib/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver/lib"", {st_mode=S_IFDIR|0555, st_size=4096, ...}) = 0
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/tls/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver-32/lib/tls/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver-32/lib/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver-32/lib/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/tls/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver-32/lib/tls"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver-32/lib/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver-32/lib/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver-32/lib/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/run/opengl-driver-32/lib"", {st_mode=S_IFDIR|0555, st_size=4096, ...}) = 0
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/tls/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/tls/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/tls/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/tls"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib"", {st_mode=S_IFDIR|0555, st_size=4096, ...}) = 0
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/tls/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/tls/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/tls/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/tls/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/tls/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/tls"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/x86_64/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/x86_64/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/x86_64/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/x86_64"", 0x7ffda14c9840) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/libpcre2-8.so.0"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0\20%\0\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0555, st_size=546376, ...}) = 0
mmap(NULL, 2626120, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f1e0f71c000
mprotect(0x7f1e0f79d000, 2093056, PROT_NONE) = 0
mmap(0x7f1e0f99c000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x80000) = 0x7f1e0f99c000
close(3)                                = 0
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/libz.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/libz.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/libz.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/libz.so.1"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0`(\0\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0555, st_size=120824, ...}) = 0
mmap(NULL, 2207760, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f1e0f500000
mprotect(0x7f1e0f51b000, 2093056, PROT_NONE) = 0
mmap(0x7f1e0f71a000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1a000) = 0x7f1e0f71a000
close(3)                                = 0
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/tls/x86_64/x86_64/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/tls/x86_64/x86_64"", 0x7ffda14c97e0) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/tls/x86_64/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/tls/x86_64"", 0x7ffda14c97e0) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/tls/x86_64/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/tls/x86_64"", 0x7ffda14c97e0) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/tls/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/tls"", 0x7ffda14c97e0) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/x86_64/x86_64/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/x86_64/x86_64"", 0x7ffda14c97e0) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/x86_64/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/x86_64"", 0x7ffda14c97e0) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/x86_64/librt.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
stat(""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/x86_64"", 0x7ffda14c97e0) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/librt.so.1"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0 \""\0\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0555, st_size=41848, ...}) = 0
mmap(NULL, 2128384, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f1e0f2f8000
mprotect(0x7f1e0f2ff000, 2093056, PROT_NONE) = 0
mmap(0x7f1e0f4fe000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x6000) = 0x7f1e0f4fe000
close(3)                                = 0
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/libgcc_s.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/libgcc_s.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/libgcc_s.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/libgcc_s.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/libgcc_s.so.1"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/libgcc_s.so.1"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0\20+\0\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0444, st_size=100608, ...}) = 0
mmap(NULL, 2185600, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f1e0f0e2000
mprotect(0x7f1e0f0f8000, 2093056, PROT_NONE) = 0
mmap(0x7f1e0f2f7000, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x15000) = 0x7f1e0f2f7000
close(3)                                = 0
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/libpthread.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/libpthread.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/libpthread.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/libpthread.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/libpthread.so.0"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/libpthread.so.0"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0pb\0\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0555, st_size=142424, ...}) = 0
mmap(NULL, 2220624, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f1e0eec3000
mprotect(0x7f1e0eedc000, 2097152, PROT_NONE) = 0
mmap(0x7f1e0f0dc000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x19000) = 0x7f1e0f0dc000
mmap(0x7f1e0f0de000, 12880, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f1e0f0de000
close(3)                                = 0
openat(AT_FDCWD, ""/nix/store/6fpsj3igl3sl19c4c9l2q1ij26mjp4gk-sane-config/lib/sane/libc.so.6"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver/lib/libc.so.6"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/run/opengl-driver-32/lib/libc.so.6"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/bv6znzsv2qkbcwwa251dx7n5dshz3nr3-zlib-1.2.11/lib/libc.so.6"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/apgbaliyj751yjdp2ffrvbb5ckwclfl4-pcre2-10.31/lib/libc.so.6"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/lib/libc.so.6"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0\260\34\2\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0555, st_size=2011232, ...}) = 0
mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f1e0fbbe000
mmap(NULL, 3881376, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f1e0eb0f000
mprotect(0x7f1e0ecb9000, 2097152, PROT_NONE) = 0
mmap(0x7f1e0eeb9000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1aa000) = 0x7f1e0eeb9000
mmap(0x7f1e0eebf000, 14752, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f1e0eebf000
close(3)                                = 0
mmap(NULL, 12288, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f1e0fbbb000
arch_prctl(ARCH_SET_FS, 0x7f1e0fbbb740) = 0
mprotect(0x7f1e0eeb9000, 16384, PROT_READ) = 0
mprotect(0x7f1e0f0dc000, 4096, PROT_READ) = 0
mprotect(0x7f1e0f4fe000, 4096, PROT_READ) = 0
mprotect(0x7f1e0f71a000, 4096, PROT_READ) = 0
mprotect(0x7f1e0f99c000, 4096, PROT_READ) = 0
mprotect(0x834000, 16384, PROT_READ)    = 0
mprotect(0x7f1e0fbc2000, 4096, PROT_READ) = 0
set_tid_address(0x7f1e0fbbba10)         = 5177
set_robust_list(0x7f1e0fbbba20, 24)     = 0
rt_sigaction(SIGRTMIN, {sa_handler=0x7f1e0eec8cf0, sa_mask=[], sa_flags=SA_RESTORER|SA_SIGINFO, sa_restorer=0x7f1e0eed4f10}, NULL, 8) = 0
rt_sigaction(SIGRT_1, {sa_handler=0x7f1e0eec8d80, sa_mask=[], sa_flags=SA_RESTORER|SA_RESTART|SA_SIGINFO, sa_restorer=0x7f1e0eed4f10}, NULL, 8) = 0
rt_sigprocmask(SIG_UNBLOCK, [RTMIN RT_1], NULL, 8) = 0
prlimit64(0, RLIMIT_STACK, NULL, {rlim_cur=8192*1024, rlim_max=RLIM64_INFINITY}) = 0
openat(AT_FDCWD, ""/dev/null"", O_RDWR)   = 3
close(3)                                = 0
brk(NULL)                               = 0x1045000
brk(0x1066000)                          = 0x1066000
stat(""/nix/store/fizihf81dqmlrlgd8fc2lpkv008gl927-git-2.18.0/share/locale"", {st_mode=S_IFDIR|0555, st_size=4096, ...}) = 0
openat(AT_FDCWD, ""/run/current-system/sw/lib/locale/locale-archive"", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0444, st_size=126969360, ...}) = 0
mmap(NULL, 126969360, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f1e071f8000
close(3)                                = 0
rt_sigprocmask(SIG_UNBLOCK, [PIPE], NULL, 8) = 0
rt_sigaction(SIGPIPE, {sa_handler=SIG_DFL, sa_mask=[PIPE], sa_flags=SA_RESTORER|SA_RESTART, sa_restorer=0x7f1e0eb43c50}, {sa_handler=SIG_DFL, sa_mask=[], sa_flags=0}, 8) = 0
getcwd(""/mnt/storage/Kevin/Personal/Code/zero"", 129) = 38
stat(""/mnt/storage/Kevin/Personal/Code/zero"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
stat(""/mnt/storage/Kevin/Personal/Code/zero/.git"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/HEAD"", {st_mode=S_IFREG|0644, st_size=23, ...}) = 0
openat(AT_FDCWD, ""/mnt/storage/Kevin/Personal/Code/zero/.git/HEAD"", O_RDONLY) = 3
read(3, ""ref: refs/heads/master\n"", 255) = 23
read(3, """", 232)                        = 0
close(3)                                = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/commondir"", 0x7ffda14c9bd0) = -1 ENOENT (No such file or directory)
access(""/mnt/storage/Kevin/Personal/Code/zero/.git/objects"", X_OK) = 0
access(""/mnt/storage/Kevin/Personal/Code/zero/.git/refs"", X_OK) = 0
lstat("".git/commondir"", 0x7ffda14c9cd0) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, "".git/config"", O_RDONLY) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=576, ...}) = 0
fstat(3, {st_mode=S_IFREG|0644, st_size=576, ...}) = 0
read(3, ""[core]\n\trepositoryformatversion ""..., 8192) = 576
read(3, """", 8192)                       = 0
close(3)                                = 0
getcwd(""/mnt/storage/Kevin/Personal/Code/zero"", 129) = 38
stat("".git"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat("".git/commondir"", 0x7ffda14c9c50) = -1 ENOENT (No such file or directory)
access(""/etc//gitconfig"", R_OK)         = -1 ENOENT (No such file or directory)
access(""/home/kevin/.config/git/config"", R_OK) = 0
openat(AT_FDCWD, ""/home/kevin/.config/git/config"", O_RDONLY) = 3
fstat(3, {st_mode=S_IFREG|0444, st_size=206, ...}) = 0
fstat(3, {st_mode=S_IFREG|0444, st_size=206, ...}) = 0
read(3, ""[commit]\ngpgSign=true\n\n[gpg]\npro""..., 4096) = 206
read(3, """", 4096)                       = 0
close(3)                                = 0
access(""/home/kevin/.gitconfig"", R_OK)  = -1 ENOENT (No such file or directory)
access("".git/config"", R_OK)             = 0
openat(AT_FDCWD, "".git/config"", O_RDONLY) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=576, ...}) = 0
fstat(3, {st_mode=S_IFREG|0644, st_size=576, ...}) = 0
read(3, ""[core]\n\trepositoryformatversion ""..., 8192) = 576
read(3, """", 8192)                       = 0
close(3)                                = 0
getcwd(""/mnt/storage/Kevin/Personal/Code/zero"", 129) = 38
chdir(""/mnt/storage/Kevin/Personal/Code/zero"") = 0
lstat("".git/commondir"", 0x7ffda14c9180) = -1 ENOENT (No such file or directory)
lstat("".git/HEAD"", {st_mode=S_IFREG|0644, st_size=23, ...}) = 0
openat(AT_FDCWD, "".git/HEAD"", O_RDONLY) = 3
read(3, ""ref: refs/heads/master\n"", 256) = 23
read(3, """", 233)                        = 0
close(3)                                = 0
lstat("".git/refs/heads/master"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
openat(AT_FDCWD, "".git/refs/heads/master"", O_RDONLY) = 3
read(3, ""c37441718740260ccfefa91989ff6958""..., 256) = 41
read(3, """", 215)                        = 0
close(3)                                = 0
access(""/etc//gitconfig"", R_OK)         = -1 ENOENT (No such file or directory)
access(""/home/kevin/.config/git/config"", R_OK) = 0
openat(AT_FDCWD, ""/home/kevin/.config/git/config"", O_RDONLY) = 3
fstat(3, {st_mode=S_IFREG|0444, st_size=206, ...}) = 0
fstat(3, {st_mode=S_IFREG|0444, st_size=206, ...}) = 0
read(3, ""[commit]\ngpgSign=true\n\n[gpg]\npro""..., 4096) = 206
read(3, """", 4096)                       = 0
close(3)                                = 0
access(""/home/kevin/.gitconfig"", R_OK)  = -1 ENOENT (No such file or directory)
access("".git/config"", R_OK)             = 0
openat(AT_FDCWD, "".git/config"", O_RDONLY) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=576, ...}) = 0
fstat(3, {st_mode=S_IFREG|0644, st_size=576, ...}) = 0
read(3, ""[core]\n\trepositoryformatversion ""..., 8192) = 576
read(3, """", 8192)                       = 0
close(3)                                = 0
lstat("".git/MERGE_HEAD"", 0x7ffda14c92a0) = -1 ENOENT (No such file or directory)
lstat("".git/CHERRY_PICK_HEAD"", 0x7ffda14c92a0) = -1 ENOENT (No such file or directory)
lstat("".git/HEAD"", {st_mode=S_IFREG|0644, st_size=23, ...}) = 0
openat(AT_FDCWD, "".git/HEAD"", O_RDONLY) = 3
read(3, ""ref: refs/heads/master\n"", 256) = 23
read(3, """", 233)                        = 0
close(3)                                = 0
lstat("".git/refs/heads/master"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
openat(AT_FDCWD, "".git/refs/heads/master"", O_RDONLY) = 3
read(3, ""c37441718740260ccfefa91989ff6958""..., 256) = 41
read(3, """", 215)                        = 0
close(3)                                = 0
lstat("".git/refs/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, "".git/packed-refs"", O_RDONLY) = -1 ENOENT (No such file or directory)
lstat("".git/refs/tags/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c8c80) = -1 ENOENT (No such file or directory)
lstat("".git/refs/heads/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c8c80) = -1 ENOENT (No such file or directory)
lstat("".git/refs/remotes/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c8c80) = -1 ENOENT (No such file or directory)
lstat("".git/refs/remotes/HEAD/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c8c80) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, "".git/refs/"", O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3
fstat(3, {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
getdents(3, /* 5 entries */, 65536)     = 136
stat("".git/refs/tags"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
stat("".git/refs/stash"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
lstat("".git/refs/stash"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
openat(AT_FDCWD, "".git/refs/stash"", O_RDONLY) = 4
read(4, ""3c49a9226ee65d598791e65b0273d637""..., 256) = 41
read(4, """", 215)                        = 0
close(4)                                = 0
stat("".git/refs/heads"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
getdents(3, /* 0 entries */, 65536)     = 0
close(3)                                = 0
stat("".git/packed-refs"", 0x7ffda14c8fc0) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, "".git/objects/pack"", O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3
fstat(3, {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
getdents(3, /* 6 entries */, 65536)     = 336
access("".git/objects/pack/pack-5a4c9f22a81786aa77ef693e3119207e59de9be9.keep"", F_OK) = -1 ENOENT (No such file or directory)
access("".git/objects/pack/pack-5a4c9f22a81786aa77ef693e3119207e59de9be9.promisor"", F_OK) = -1 ENOENT (No such file or directory)
stat("".git/objects/pack/pack-5a4c9f22a81786aa77ef693e3119207e59de9be9.pack"", {st_mode=S_IFREG|0444, st_size=267685, ...}) = 0
access("".git/objects/pack/pack-5f9147cc7f7cce842ffccadb256ea01361a76dda.keep"", F_OK) = -1 ENOENT (No such file or directory)
access("".git/objects/pack/pack-5f9147cc7f7cce842ffccadb256ea01361a76dda.promisor"", F_OK) = -1 ENOENT (No such file or directory)
stat("".git/objects/pack/pack-5f9147cc7f7cce842ffccadb256ea01361a76dda.pack"", {st_mode=S_IFREG|0444, st_size=589939, ...}) = 0
getdents(3, /* 0 entries */, 65536)     = 0
close(3)                                = 0
openat(AT_FDCWD, "".git/objects/info/alternates"", O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, "".git/objects/pack/pack-5f9147cc7f7cce842ffccadb256ea01361a76dda.idx"", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0444, st_size=45032, ...}) = 0
mmap(NULL, 45032, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f1e0fbb0000
close(3)                                = 0
openat(AT_FDCWD, "".git/objects/pack/pack-5a4c9f22a81786aa77ef693e3119207e59de9be9.idx"", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0444, st_size=19132, ...}) = 0
mmap(NULL, 19132, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f1e0fbab000
close(3)                                = 0
lstat("".git/objects/c3/7441718740260ccfefa91989ff69582cad17ca"", {st_mode=S_IFREG|0444, st_size=824, ...}) = 0
openat(AT_FDCWD, "".git/objects/c3/7441718740260ccfefa91989ff69582cad17ca"", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0444, st_size=824, ...}) = 0
mmap(NULL, 824, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f1e0fbaa000
close(3)                                = 0
munmap(0x7f1e0fbaa000, 824)             = 0
openat(AT_FDCWD, "".git/objects/c3/7441718740260ccfefa91989ff69582cad17ca"", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0444, st_size=824, ...}) = 0
mmap(NULL, 824, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f1e0fbaa000
close(3)                                = 0
munmap(0x7f1e0fbaa000, 824)             = 0
brk(0x1088000)                          = 0x1088000
openat(AT_FDCWD, "".git/info/grafts"", O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, "".git/shallow"", O_RDONLY) = -1 ENOENT (No such file or directory)
mmap(NULL, 524288, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f1e0fb2b000
gettimeofday({tv_sec=1535999687, tv_usec=656375}, NULL) = 0
clock_gettime(CLOCK_MONOTONIC, {tv_sec=192488, tv_nsec=656603404}) = 0
openat(AT_FDCWD, "".git/index"", O_RDONLY) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=24695, ...}) = 0
mmap(NULL, 24695, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f1e0fb24000
close(3)                                = 0
munmap(0x7f1e0fb24000, 24695)           = 0
clock_gettime(CLOCK_MONOTONIC, {tv_sec=192488, tv_nsec=658593061}) = 0
readlink("".git/index"", 0x104b250, 32)   = -1 EINVAL (Invalid argument)
getcwd(""/mnt/storage/Kevin/Personal/Code/zero"", 129) = 38
openat(AT_FDCWD, ""/mnt/storage/Kevin/Personal/Code/zero/.git/index.lock"", O_RDWR|O_CREAT|O_EXCL|O_CLOEXEC, 0666) = 3
rt_sigaction(SIGINT, {sa_handler=0x579990, sa_mask=[INT], sa_flags=SA_RESTORER|SA_RESTART, sa_restorer=0x7f1e0eb43c50}, {sa_handler=SIG_DFL, sa_mask=[], sa_flags=0}, 8) = 0
rt_sigaction(SIGHUP, {sa_handler=0x579990, sa_mask=[HUP], sa_flags=SA_RESTORER|SA_RESTART, sa_restorer=0x7f1e0eb43c50}, {sa_handler=SIG_DFL, sa_mask=[], sa_flags=0}, 8) = 0
rt_sigaction(SIGTERM, {sa_handler=0x579990, sa_mask=[TERM], sa_flags=SA_RESTORER|SA_RESTART, sa_restorer=0x7f1e0eb43c50}, {sa_handler=SIG_DFL, sa_mask=[], sa_flags=0}, 8) = 0
rt_sigaction(SIGQUIT, {sa_handler=0x579990, sa_mask=[QUIT], sa_flags=SA_RESTORER|SA_RESTART, sa_restorer=0x7f1e0eb43c50}, {sa_handler=SIG_DFL, sa_mask=[], sa_flags=0}, 8) = 0
rt_sigaction(SIGPIPE, {sa_handler=0x579990, sa_mask=[PIPE], sa_flags=SA_RESTORER|SA_RESTART, sa_restorer=0x7f1e0eb43c50}, {sa_handler=SIG_DFL, sa_mask=[PIPE], sa_flags=SA_RESTORER|SA_RESTART, sa_restorer=0x7f1e0eb43c50}, 8) = 0
getpid()                                = 5177
clock_gettime(CLOCK_MONOTONIC, {tv_sec=192488, tv_nsec=661476664}) = 0
lstat("".git-crypt"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat("".git-crypt/.gitattributes"", {st_mode=S_IFREG|0644, st_size=152, ...}) = 0
lstat("".git-crypt/keys"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat("".git-crypt/keys/default"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat("".git-crypt/keys/default/0"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat("".git-crypt/keys/default/0/8792E2260F507DA00F0DB58E2160C3EB40A944EC.gpg"", {st_mode=S_IFREG|0644, st_size=725, ...}) = 0
lstat("".gitignore"", {st_mode=S_IFREG|0644, st_size=13, ...}) = 0
lstat("".gitmodules"", {st_mode=S_IFREG|0644, st_size=132, ...}) = 0
lstat("".vscode"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat("".vscode/tasks.json"", {st_mode=S_IFREG|0644, st_size=604, ...}) = 0
lstat(""LICENSE"", {st_mode=S_IFREG|0644, st_size=34451, ...}) = 0
lstat(""kubernetes"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/.gitattributes"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
lstat(""kubernetes/.gitignore"", {st_mode=S_IFREG|0644, st_size=49, ...}) = 0
lstat(""kubernetes/charts"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/gitlab"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
stat(""kubernetes/charts/gitlab/.git"", {st_mode=S_IFREG|0644, st_size=55, ...}) = 0
openat(AT_FDCWD, ""kubernetes/charts/gitlab/.git"", O_RDONLY) = 4
read(4, ""gitdir: ../../../.git/modules/ku""..., 55) = 55
close(4)                                = 0
lstat(""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/HEAD"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
openat(AT_FDCWD, ""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/HEAD"", O_RDONLY) = 4
read(4, ""216b7b9ae8415b8a210a306ee0d6a193""..., 255) = 41
read(4, """", 214)                        = 0
close(4)                                = 0
lstat(""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/commondir"", 0x7ffda14c8d30) = -1 ENOENT (No such file or directory)
access(""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/objects"", X_OK) = 0
access(""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/refs"", X_OK) = 0
getcwd(""/mnt/storage/Kevin/Personal/Code/zero"", 129) = 38
lstat(""/mnt/storage/Kevin/Personal/Code/zero/kubernetes"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/kubernetes/charts"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/kubernetes/charts/gitlab"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
stat(""kubernetes/charts/gitlab/.git"", {st_mode=S_IFREG|0644, st_size=55, ...}) = 0
openat(AT_FDCWD, ""kubernetes/charts/gitlab/.git"", O_RDONLY) = 4
read(4, ""gitdir: ../../../.git/modules/ku""..., 55) = 55
close(4)                                = 0
lstat(""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/HEAD"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
openat(AT_FDCWD, ""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/HEAD"", O_RDONLY) = 4
read(4, ""216b7b9ae8415b8a210a306ee0d6a193""..., 255) = 41
read(4, """", 214)                        = 0
close(4)                                = 0
lstat(""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/commondir"", 0x7ffda14c8d30) = -1 ENOENT (No such file or directory)
access(""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/objects"", X_OK) = 0
access(""kubernetes/charts/gitlab/../../../.git/modules/kubernetes/charts/gitlab/refs"", X_OK) = 0
getcwd(""/mnt/storage/Kevin/Personal/Code/zero"", 129) = 38
lstat(""/mnt/storage/Kevin/Personal/Code/zero/kubernetes"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/kubernetes/charts"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/kubernetes/charts/gitlab"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab/HEAD"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
openat(AT_FDCWD, ""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab/HEAD"", O_RDONLY) = 4
read(4, ""216b7b9ae8415b8a210a306ee0d6a193""..., 255) = 41
read(4, """", 214)                        = 0
close(4)                                = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab/commondir"", 0x7ffda14c8e10) = -1 ENOENT (No such file or directory)
access(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab/objects"", X_OK) = 0
access(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab/refs"", X_OK) = 0
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab/commondir"", 0x7ffda14c8e30) = -1 ENOENT (No such file or directory)
lstat(""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab/HEAD"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
openat(AT_FDCWD, ""/mnt/storage/Kevin/Personal/Code/zero/.git/modules/kubernetes/charts/gitlab/HEAD"", O_RDONLY) = 4
read(4, ""216b7b9ae8415b8a210a306ee0d6a193""..., 256) = 41
read(4, """", 215)                        = 0
close(4)                                = 0
lstat(""kubernetes/charts/kubecfg"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/kubecfg/Chart.yaml"", {st_mode=S_IFREG|0644, st_size=43, ...}) = 0
lstat(""kubernetes/charts/kubecfg/requirements.lock"", {st_mode=S_IFREG|0644, st_size=239, ...}) = 0
lstat(""kubernetes/charts/kubecfg/requirements.yaml"", {st_mode=S_IFREG|0644, st_size=0, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/blog.yaml"", {st_mode=S_IFREG|0644, st_size=1245, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/calibre-web.yaml"", {st_mode=S_IFREG|0644, st_size=1520, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/cups.yaml"", {st_mode=S_IFREG|0644, st_size=1178, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/ddns.secret.yaml"", {st_mode=S_IFREG|0644, st_size=424, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/dl.secret.yaml"", {st_mode=S_IFREG|0644, st_size=159, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/dl.yaml"", {st_mode=S_IFREG|0644, st_size=1844, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/emby.yaml"", {st_mode=S_IFREG|0644, st_size=2104, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/ethminer.yaml"", {st_mode=S_IFREG|0644, st_size=516, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/headers.yaml"", {st_mode=S_IFREG|0644, st_size=123, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/mail.secret.yaml"", {st_mode=S_IFREG|0644, st_size=3209, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/mail.yaml"", {st_mode=S_IFREG|0644, st_size=4155, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/matrix-dimension.yaml"", {st_mode=S_IFREG|0644, st_size=4806, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/matrix.secret.yaml"", {st_mode=S_IFREG|0644, st_size=19334, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/matrix.yaml"", {st_mode=S_IFREG|0644, st_size=4886, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/minecraft.yaml"", {st_mode=S_IFREG|0644, st_size=1778, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/monica.secret.yaml"", {st_mode=S_IFREG|0644, st_size=4894, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/monica.yaml"", {st_mode=S_IFREG|0644, st_size=2192, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/netdata.secret.yaml"", {st_mode=S_IFREG|0644, st_size=169, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/netdata.yaml"", {st_mode=S_IFREG|0644, st_size=1953, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/paperless.secret.yaml"", {st_mode=S_IFREG|0644, st_size=210, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/paperless.yaml"", {st_mode=S_IFREG|0644, st_size=3080, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/parity.yaml"", {st_mode=S_IFREG|0644, st_size=1293, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/red-discord-bot.yaml"", {st_mode=S_IFREG|0644, st_size=961, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/riot.yaml"", {st_mode=S_IFREG|0644, st_size=2308, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/social-410-gone.yaml"", {st_mode=S_IFREG|0644, st_size=1152, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/sonarr.secret.yaml"", {st_mode=S_IFREG|0644, st_size=162, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/sonarr.yaml"", {st_mode=S_IFREG|0644, st_size=3256, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/tor.secret.yaml"", {st_mode=S_IFREG|0644, st_size=420, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/tor.yaml"", {st_mode=S_IFREG|0644, st_size=1172, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/transmission.secret.yaml"", {st_mode=S_IFREG|0644, st_size=428, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/transmission.yaml"", {st_mode=S_IFREG|0644, st_size=2242, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/turnserver.secret.yaml"", {st_mode=S_IFREG|0644, st_size=329, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/turnserver.yaml"", {st_mode=S_IFREG|0644, st_size=669, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/zoneminder.secret.yaml"", {st_mode=S_IFREG|0644, st_size=175, ...}) = 0
lstat(""kubernetes/charts/kubecfg/templates/zoneminder.yaml"", {st_mode=S_IFREG|0644, st_size=2390, ...}) = 0
lstat(""kubernetes/charts/kubecfg/values.yaml"", {st_mode=S_IFREG|0644, st_size=0, ...}) = 0
lstat(""kubernetes/charts/mastodon"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/mastodon/.gitignore"", {st_mode=S_IFREG|0644, st_size=13, ...}) = 0
lstat(""kubernetes/charts/mastodon/Chart.yaml"", {st_mode=S_IFREG|0644, st_size=445, ...}) = 0
lstat(""kubernetes/charts/mastodon/README.md"", {st_mode=S_IFREG|0644, st_size=0, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis"", {st_mode=S_IFDIR|0775, st_size=0, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/.helmignore"", {st_mode=S_IFREG|0664, st_size=333, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/Chart.yaml"", {st_mode=S_IFREG|0664, st_size=163, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/README.md"", {st_mode=S_IFREG|0664, st_size=233, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/templates"", {st_mode=S_IFDIR|0775, st_size=0, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/templates/_helpers.tpl"", {st_mode=S_IFREG|0664, st_size=516, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/templates/deployment.yaml"", {st_mode=S_IFREG|0664, st_size=1510, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/templates/pvc.yaml"", {st_mode=S_IFREG|0664, st_size=741, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/templates/secrets.yaml"", {st_mode=S_IFREG|0664, st_size=444, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/templates/svc.yaml"", {st_mode=S_IFREG|0664, st_size=365, ...}) = 0
lstat(""kubernetes/charts/mastodon/charts/redis/values.yaml"", {st_mode=S_IFREG|0664, st_size=959, ...}) = 0
lstat(""kubernetes/charts/mastodon/requirements.lock"", {st_mode=S_IFREG|0644, st_size=239, ...}) = 0
lstat(""kubernetes/charts/mastodon/requirements.yaml"", {st_mode=S_IFREG|0644, st_size=81, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/NOTES.txt"", {st_mode=S_IFREG|0644, st_size=0, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/_helpers.tpl"", {st_mode=S_IFREG|0644, st_size=989, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/ingress.yaml"", {st_mode=S_IFREG|0644, st_size=1156, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/mastodon-config.yaml"", {st_mode=S_IFREG|0644, st_size=387, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/mastodon-secrets.yaml"", {st_mode=S_IFREG|0644, st_size=273, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/mastodon-sidekiq-deployment.yaml"", {st_mode=S_IFREG|0644, st_size=1214, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/mastodon-streaming-deployment.yaml"", {st_mode=S_IFREG|0644, st_size=995, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/mastodon-streaming-service.yaml"", {st_mode=S_IFREG|0644, st_size=424, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/mastodon-web-deployment.yaml"", {st_mode=S_IFREG|0644, st_size=1821, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/mastodon-web-service.yaml"", {st_mode=S_IFREG|0644, st_size=388, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/scheduled-jobs.yaml"", {st_mode=S_IFREG|0644, st_size=1314, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/system-pvc.yaml"", {st_mode=S_IFREG|0644, st_size=758, ...}) = 0
lstat(""kubernetes/charts/mastodon/templates/upgrade-job.yaml"", {st_mode=S_IFREG|0644, st_size=814, ...}) = 0
lstat(""kubernetes/charts/mastodon/values.yaml"", {st_mode=S_IFREG|0644, st_size=918, ...}) = 0
lstat(""kubernetes/charts/nextcloud"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/nextcloud/.helmignore"", {st_mode=S_IFREG|0644, st_size=333, ...}) = 0
lstat(""kubernetes/charts/nextcloud/Chart.yaml"", {st_mode=S_IFREG|0644, st_size=105, ...}) = 0
lstat(""kubernetes/charts/nextcloud/requirements.lock"", {st_mode=S_IFREG|0644, st_size=328, ...}) = 0
lstat(""kubernetes/charts/nextcloud/requirements.yaml"", {st_mode=S_IFREG|0644, st_size=135, ...}) = 0
lstat(""kubernetes/charts/nextcloud/templates"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/nextcloud/templates/NOTES.txt"", {st_mode=S_IFREG|0644, st_size=0, ...}) = 0
lstat(""kubernetes/charts/nextcloud/templates/_helpers.tpl"", {st_mode=S_IFREG|0644, st_size=1051, ...}) = 0
lstat(""kubernetes/charts/nextcloud/templates/collabora-code.yaml"", {st_mode=S_IFREG|0644, st_size=1265, ...}) = 0
lstat(""kubernetes/charts/nextcloud/templates/cron.yaml"", {st_mode=S_IFREG|0644, st_size=623, ...}) = 0
lstat(""kubernetes/charts/nextcloud/templates/ingress.yaml"", {st_mode=S_IFREG|0644, st_size=1152, ...}) = 0
lstat(""kubernetes/charts/nextcloud/values.yaml"", {st_mode=S_IFREG|0644, st_size=506, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner/.helmignore"", {st_mode=S_IFREG|0644, st_size=333, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner/Chart.yaml"", {st_mode=S_IFREG|0644, st_size=118, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner/templates"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner/templates/NOTES.txt"", {st_mode=S_IFREG|0644, st_size=1439, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner/templates/_helpers.tpl"", {st_mode=S_IFREG|0644, st_size=1069, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner/templates/auth.yaml"", {st_mode=S_IFREG|0644, st_size=1035, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner/templates/nfs.yaml"", {st_mode=S_IFREG|0644, st_size=846, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner/templates/storageclass.yaml"", {st_mode=S_IFREG|0644, st_size=298, ...}) = 0
lstat(""kubernetes/charts/nfs-client-provisioner/values.yaml"", {st_mode=S_IFREG|0644, st_size=1030, ...}) = 0
lstat(""kubernetes/docs"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/docs/Updating-Mastodon.md"", {st_mode=S_IFREG|0644, st_size=711, ...}) = 0
lstat(""kubernetes/helmfile.yaml"", {st_mode=S_IFREG|0644, st_size=1186, ...}) = 0
lstat(""kubernetes/postsetup"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/postsetup/letsencrypt-prod.yaml"", {st_mode=S_IFREG|0644, st_size=339, ...}) = 0
lstat(""kubernetes/presetup"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/presetup/02-gitlab-auth.secret.yaml"", {st_mode=S_IFREG|0644, st_size=9238, ...}) = 0
lstat(""kubernetes/presetup/02-monitoring-auth.secret.yaml"", {st_mode=S_IFREG|0644, st_size=330, ...}) = 0
lstat(""kubernetes/update.sh"", {st_mode=S_IFREG|0755, st_size=101, ...}) = 0
lstat(""kubernetes/values"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""kubernetes/values/cert-manager.yaml"", {st_mode=S_IFREG|0644, st_size=85, ...}) = 0
lstat(""kubernetes/values/gitlab.secret.yaml"", {st_mode=S_IFREG|0644, st_size=70, ...}) = 0
lstat(""kubernetes/values/gitlab.yaml"", {st_mode=S_IFREG|0644, st_size=1333, ...}) = 0
lstat(""kubernetes/values/kubecfg.secret.yaml"", {st_mode=S_IFREG|0644, st_size=636, ...}) = 0
lstat(""kubernetes/values/kubecfg.yaml"", {st_mode=S_IFREG|0644, st_size=0, ...}) = 0
lstat(""kubernetes/values/mastodon.secret.yaml"", {st_mode=S_IFREG|0644, st_size=1438, ...}) = 0
lstat(""kubernetes/values/mastodon.yaml"", {st_mode=S_IFREG|0644, st_size=7241, ...}) = 0
lstat(""kubernetes/values/nextcloud.secret.yaml"", {st_mode=S_IFREG|0644, st_size=140, ...}) = 0
lstat(""kubernetes/values/nextcloud.yaml"", {st_mode=S_IFREG|0644, st_size=603, ...}) = 0
lstat(""kubernetes/values/nginx-ingress.yaml"", {st_mode=S_IFREG|0644, st_size=699, ...}) = 0
lstat(""nixops"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/.gitattributes"", {st_mode=S_IFREG|0644, st_size=43, ...}) = 0
lstat(""nixops/common"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/common/dns.nix"", {st_mode=S_IFREG|0644, st_size=98, ...}) = 0
lstat(""nixops/common/earlyoom.nix"", {st_mode=S_IFREG|0644, st_size=316, ...}) = 0
lstat(""nixops/common/firewall.nix"", {st_mode=S_IFREG|0644, st_size=288, ...}) = 0
lstat(""nixops/common/kernel.nix"", {st_mode=S_IFREG|0644, st_size=84, ...}) = 0
lstat(""nixops/common/nix.nix"", {st_mode=S_IFREG|0644, st_size=69, ...}) = 0
lstat(""nixops/common/ssh.nix"", {st_mode=S_IFREG|0644, st_size=678, ...}) = 0
lstat(""nixops/common/time.nix"", {st_mode=S_IFREG|0644, st_size=155, ...}) = 0
lstat(""nixops/common/users.nix"", {st_mode=S_IFREG|0644, st_size=428, ...}) = 0
lstat(""nixops/default.nix"", {st_mode=S_IFREG|0644, st_size=2783, ...}) = 0
lstat(""nixops/hekate_ctcaer_3.0.bin"", {st_mode=S_IFREG|0644, st_size=125157, ...}) = 0
lstat(""nixops/modules"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/modules/boinc.nix"", {st_mode=S_IFREG|0644, st_size=477, ...}) = 0
lstat(""nixops/modules/desktop.nix"", {st_mode=S_IFREG|0644, st_size=14090, ...}) = 0
lstat(""nixops/modules/desktop"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/modules/desktop/.conkyrc"", {st_mode=S_IFREG|0644, st_size=1579, ...}) = 0
lstat(""nixops/modules/desktop/.spacemacs"", {st_mode=S_IFREG|0644, st_size=31536, ...}) = 0
lstat(""nixops/modules/desktop/conky-bar.sh"", {st_mode=S_IFREG|0644, st_size=296, ...}) = 0
lstat(""nixops/modules/desktop/next-spec-day.el"", {st_mode=S_IFREG|0644, st_size=4072, ...}) = 0
lstat(""nixops/modules/docker.nix"", {st_mode=S_IFREG|0644, st_size=990, ...}) = 0
lstat(""nixops/modules/ipfs.nix"", {st_mode=S_IFREG|0644, st_size=239, ...}) = 0
lstat(""nixops/modules/kdeconnect.nix"", {st_mode=S_IFREG|0644, st_size=240, ...}) = 0
lstat(""nixops/modules/kubernetes-common.nix"", {st_mode=S_IFREG|0644, st_size=3316, ...}) = 0
lstat(""nixops/modules/kubernetes-master.nix"", {st_mode=S_IFREG|0644, st_size=565, ...}) = 0
lstat(""nixops/modules/kubernetes-node.nix"", {st_mode=S_IFREG|0644, st_size=265, ...}) = 0
lstat(""nixops/otto"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/otto/hw.nix"", {st_mode=S_IFREG|0644, st_size=982, ...}) = 0
lstat(""nixops/otto/lizardfs.nix"", {st_mode=S_IFREG|0644, st_size=536, ...}) = 0
lstat(""nixops/puck"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/puck/desktop.nix"", {st_mode=S_IFREG|0644, st_size=789, ...}) = 0
lstat(""nixops/puck/hw.nix"", {st_mode=S_IFREG|0644, st_size=1278, ...}) = 0
lstat(""nixops/puck/nfs-filesystems.nix"", {st_mode=S_IFREG|0644, st_size=390, ...}) = 0
lstat(""nixops/puck/wireguard.nix"", {st_mode=S_IFREG|0644, st_size=1635, ...}) = 0
lstat(""nixops/puck/wireless.nix"", {st_mode=S_IFREG|0644, st_size=354, ...}) = 0
lstat(""nixops/rem"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/rem/backups.nix"", {st_mode=S_IFREG|0644, st_size=6844, ...}) = 0
lstat(""nixops/rem/desktop.nix"", {st_mode=S_IFREG|0644, st_size=694, ...}) = 0
lstat(""nixops/rem/fusee-launcher.nix"", {st_mode=S_IFREG|0644, st_size=471, ...}) = 0
lstat(""nixops/rem/hw.nix"", {st_mode=S_IFREG|0644, st_size=6041, ...}) = 0
lstat(""nixops/rem/ipv6-tunnel.nix"", {st_mode=S_IFREG|0644, st_size=802, ...}) = 0
lstat(""nixops/rem/kindle.nix"", {st_mode=S_IFREG|0644, st_size=968, ...}) = 0
lstat(""nixops/rem/lizardfs.nix"", {st_mode=S_IFREG|0644, st_size=1253, ...}) = 0
lstat(""nixops/rem/locate.nix"", {st_mode=S_IFREG|0644, st_size=378, ...}) = 0
lstat(""nixops/rem/monitoring.nix"", {st_mode=S_IFREG|0644, st_size=566, ...}) = 0
lstat(""nixops/rem/nfs.nix"", {st_mode=S_IFREG|0644, st_size=639, ...}) = 0
lstat(""nixops/rem/nix.nix"", {st_mode=S_IFREG|0644, st_size=452, ...}) = 0
lstat(""nixops/rem/samba.nix"", {st_mode=S_IFREG|0644, st_size=824, ...}) = 0
lstat(""nixops/rem/services.nix"", {st_mode=S_IFREG|0644, st_size=315, ...}) = 0
lstat(""nixops/rem/unbound.nix"", {st_mode=S_IFREG|0644, st_size=2027, ...}) = 0
lstat(""nixops/rem/vfio.nix"", {st_mode=S_IFREG|0644, st_size=1946, ...}) = 0
lstat(""nixops/rem/wireguard.nix"", {st_mode=S_IFREG|0644, st_size=1721, ...}) = 0
lstat(""nixops/scripts"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/scripts/generate-kubes-keys.nix"", {st_mode=S_IFREG|0644, st_size=5528, ...}) = 0
lstat(""nixops/scripts/result"", {st_mode=S_IFLNK|0777, st_size=54, ...}) = 0
lstat(""nixops/scripts/result-2"", {st_mode=S_IFLNK|0777, st_size=53, ...}) = 0
lstat(""nixops/scripts/result-3"", {st_mode=S_IFLNK|0777, st_size=52, ...}) = 0
lstat(""nixops/secrets"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/secrets/boincstats-password.txt"", {st_mode=S_IFREG|0644, st_size=20, ...}) = 0
lstat(""nixops/secrets/emergency-borg-password.txt"", {st_mode=S_IFREG|0644, st_size=20, ...}) = 0
lstat(""nixops/secrets/factorio-password.txt"", {st_mode=S_IFREG|0644, st_size=20, ...}) = 0
lstat(""nixops/secrets/keys"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/secrets/keys/backups.borg-key"", {st_mode=S_IFREG|0644, st_size=553, ...}) = 0
lstat(""nixops/secrets/keys/keyfile-backups.bin"", {st_mode=S_IFREG|0644, st_size=4096, ...}) = 0
lstat(""nixops/secrets/keys/keyfile-data0.bin"", {st_mode=S_IFREG|0644, st_size=4096, ...}) = 0
lstat(""nixops/secrets/keys/keyfile-data1.bin"", {st_mode=S_IFREG|0644, st_size=4096, ...}) = 0
lstat(""nixops/secrets/keys/keyfile-data2.bin"", {st_mode=S_IFREG|0644, st_size=4096, ...}) = 0
lstat(""nixops/secrets/keys/keyfile-data3.bin"", {st_mode=S_IFREG|0644, st_size=4096, ...}) = 0
lstat(""nixops/secrets/keys/keyfile-data4.bin"", {st_mode=S_IFREG|0644, st_size=4096, ...}) = 0
lstat(""nixops/secrets/keys/keyfile-parity0.bin"", {st_mode=S_IFREG|0644, st_size=4096, ...}) = 0
lstat(""nixops/secrets/kubes"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/secrets/kubes/admin"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/secrets/kubes/admin/admin-key.pem"", {st_mode=S_IFREG|0444, st_size=1675, ...}) = 0
lstat(""nixops/secrets/kubes/admin/admin.pem"", {st_mode=S_IFREG|0444, st_size=1265, ...}) = 0
lstat(""nixops/secrets/kubes/admin/ca-config.json"", {st_mode=S_IFREG|0644, st_size=232, ...}) = 0
lstat(""nixops/secrets/kubes/admin/ca-key.pem"", {st_mode=S_IFREG|0444, st_size=1675, ...}) = 0
lstat(""nixops/secrets/kubes/admin/ca.pem"", {st_mode=S_IFREG|0444, st_size=1200, ...}) = 0
lstat(""nixops/secrets/kubes/kubernetes-csr.json"", {st_mode=S_IFREG|0644, st_size=230, ...}) = 0
lstat(""nixops/secrets/kubes/kubernetes.csr"", {st_mode=S_IFREG|0644, st_size=1208, ...}) = 0
lstat(""nixops/secrets/kubes/otto-csr.json"", {st_mode=S_IFREG|0644, st_size=230, ...}) = 0
lstat(""nixops/secrets/kubes/otto.csr"", {st_mode=S_IFREG|0644, st_size=1078, ...}) = 0
lstat(""nixops/secrets/kubes/otto"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/secrets/kubes/otto/apiserver-client-kube-proxy-key.pem"", {st_mode=S_IFREG|0600, st_size=1679, ...}) = 0
lstat(""nixops/secrets/kubes/otto/apiserver-client-kube-proxy.pem"", {st_mode=S_IFREG|0600, st_size=1314, ...}) = 0
lstat(""nixops/secrets/kubes/otto/apiserver-client-kubelet-key.pem"", {st_mode=S_IFREG|0444, st_size=1675, ...}) = 0
lstat(""nixops/secrets/kubes/otto/apiserver-client-kubelet.pem"", {st_mode=S_IFREG|0444, st_size=1277, ...}) = 0
lstat(""nixops/secrets/kubes/otto/ca.pem"", {st_mode=S_IFREG|0600, st_size=1200, ...}) = 0
lstat(""nixops/secrets/kubes/otto/etcd-client-key.pem"", {st_mode=S_IFREG|0600, st_size=1675, ...}) = 0
lstat(""nixops/secrets/kubes/otto/etcd-client.pem"", {st_mode=S_IFREG|0600, st_size=1237, ...}) = 0
lstat(""nixops/secrets/kubes/otto/kubelet-key.pem"", {st_mode=S_IFREG|0600, st_size=1679, ...}) = 0
lstat(""nixops/secrets/kubes/otto/kubelet.pem"", {st_mode=S_IFREG|0644, st_size=1395, ...}) = 0
lstat(""nixops/secrets/kubes/rem-csr.json"", {st_mode=S_IFREG|0644, st_size=229, ...}) = 0
lstat(""nixops/secrets/kubes/rem.csr"", {st_mode=S_IFREG|0644, st_size=1074, ...}) = 0
lstat(""nixops/secrets/kubes/rem"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/secrets/kubes/rem/apiserver-client-kube-controller-manager-key.pem"", {st_mode=S_IFREG|0444, st_size=1679, ...}) = 0
lstat(""nixops/secrets/kubes/rem/apiserver-client-kube-controller-manager.pem"", {st_mode=S_IFREG|0444, st_size=1298, ...}) = 0
lstat(""nixops/secrets/kubes/rem/apiserver-client-kube-proxy-key.pem"", {st_mode=S_IFREG|0600, st_size=1679, ...}) = 0
lstat(""nixops/secrets/kubes/rem/apiserver-client-kube-proxy.pem"", {st_mode=S_IFREG|0600, st_size=1314, ...}) = 0
lstat(""nixops/secrets/kubes/rem/apiserver-client-kube-scheduler-key.pem"", {st_mode=S_IFREG|0444, st_size=1675, ...}) = 0
lstat(""nixops/secrets/kubes/rem/apiserver-client-kube-scheduler.pem"", {st_mode=S_IFREG|0444, st_size=1294, ...}) = 0
lstat(""nixops/secrets/kubes/rem/apiserver-client-kubelet-key.pem"", {st_mode=S_IFREG|0444, st_size=1675, ...}) = 0
lstat(""nixops/secrets/kubes/rem/apiserver-client-kubelet.pem"", {st_mode=S_IFREG|0444, st_size=1273, ...}) = 0
lstat(""nixops/secrets/kubes/rem/ca.pem"", {st_mode=S_IFREG|0444, st_size=1200, ...}) = 0
lstat(""nixops/secrets/kubes/rem/etcd-client-key.pem"", {st_mode=S_IFREG|0600, st_size=1675, ...}) = 0
lstat(""nixops/secrets/kubes/rem/etcd-client.pem"", {st_mode=S_IFREG|0600, st_size=1237, ...}) = 0
lstat(""nixops/secrets/kubes/rem/etcd-key.pem"", {st_mode=S_IFREG|0444, st_size=1675, ...}) = 0
lstat(""nixops/secrets/kubes/rem/etcd.pem"", {st_mode=S_IFREG|0444, st_size=1281, ...}) = 0
lstat(""nixops/secrets/kubes/rem/kube-apiserver-key.pem"", {st_mode=S_IFREG|0600, st_size=1675, ...}) = 0
lstat(""nixops/secrets/kubes/rem/kube-apiserver.pem"", {st_mode=S_IFREG|0644, st_size=1525, ...}) = 0
lstat(""nixops/secrets/kubes/rem/kube-service-accounts-key.pem"", {st_mode=S_IFREG|0444, st_size=1679, ...}) = 0
lstat(""nixops/secrets/kubes/rem/kube-service-accounts.pem"", {st_mode=S_IFREG|0444, st_size=1233, ...}) = 0
lstat(""nixops/secrets/kubes/rem/kubelet-client-key.pem"", {st_mode=S_IFREG|0444, st_size=1679, ...}) = 0
lstat(""nixops/secrets/kubes/rem/kubelet-client.pem"", {st_mode=S_IFREG|0444, st_size=1277, ...}) = 0
lstat(""nixops/secrets/kubes/rem/kubelet-key.pem"", {st_mode=S_IFREG|0600, st_size=1679, ...}) = 0
lstat(""nixops/secrets/kubes/rem/kubelet.pem"", {st_mode=S_IFREG|0644, st_size=1391, ...}) = 0
lstat(""nixops/secrets/password-hash.txt"", {st_mode=S_IFREG|0644, st_size=102, ...}) = 0
lstat(""nixops/secrets/rclone.conf.initial"", {st_mode=S_IFREG|0644, st_size=6435, ...}) = 0
lstat(""nixops/secrets/storage-borg-password.txt"", {st_mode=S_IFREG|0644, st_size=20, ...}) = 0
lstat(""nixops/secrets/wireguard"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
lstat(""nixops/secrets/wireguard/puck-privatekey"", {st_mode=S_IFREG|0644, st_size=45, ...}) = 0
lstat(""nixops/secrets/wireguard/rem-privatekey"", {st_mode=S_IFREG|0644, st_size=45, ...}) = 0
lstat(""nixops/ssh-keys.nix"", {st_mode=S_IFREG|0644, st_size=595, ...}) = 0
lstat(""nixops/wireguard.nix"", {st_mode=S_IFREG|0644, st_size=314, ...}) = 0
lstat("".git/objects/7e/0b60448b3ca69daeef388dce9330b079bcb50a"", {st_mode=S_IFREG|0444, st_size=245, ...}) = 0
lstat("".git/objects/29/21a5a867ca24d70b9aeee52315630c74103892"", {st_mode=S_IFREG|0444, st_size=414, ...}) = 0
lstat("".git/objects/33/8774173f67fc2ba8ca58112fbcbec70d277821"", {st_mode=S_IFREG|0444, st_size=528, ...}) = 0
lstat("".git/objects/1d/d0f8aeccc308fc8997efc5c13aff105f31ca84"", {st_mode=S_IFREG|0444, st_size=84, ...}) = 0
lstat("".git/objects/d0/bbaa53145af8be9c05b322f8ad68361319ba2d"", {st_mode=S_IFREG|0444, st_size=184, ...}) = 0
lstat("".git/objects/51/a415735462a457105e1f8cab2b9ca8a012344f"", {st_mode=S_IFREG|0444, st_size=262, ...}) = 0
lstat("".git/objects/cb/9219298668b9ecbdb6271e2026acbd65762485"", {st_mode=S_IFREG|0444, st_size=312, ...}) = 0
lstat("".git/objects/33/e2aab4573db6cc8b33eca742e473f44b0aaf9d"", {st_mode=S_IFREG|0444, st_size=160, ...}) = 0
prlimit64(0, RLIMIT_NOFILE, NULL, {rlim_cur=1024, rlim_max=4*1024}) = 0
openat(AT_FDCWD, "".git/objects/pack/pack-5a4c9f22a81786aa77ef693e3119207e59de9be9.pack"", O_RDONLY|O_CLOEXEC) = 4
fstat(4, {st_mode=S_IFREG|0444, st_size=267685, ...}) = 0
fcntl(4, F_GETFD)                       = 0x1 (flags FD_CLOEXEC)
fcntl(4, F_SETFD, FD_CLOEXEC)           = 0
read(4, ""PACK\0\0\0\2\0\0\2\205"", 12)   = 12
lseek(4, 267665, SEEK_SET)              = 267665
read(4, ""ZL\237\""\250\27\206\252w\357i>1\31 ~Y\336\233\351"", 20) = 20
lstat("".git/objects/7f/af2900fb9367a282cd0b47d2a6833acef99d52"", {st_mode=S_IFREG|0444, st_size=340, ...}) = 0
lstat("".git/objects/c1/26e4b41d93d283750ab4226c2743b4d31d77bc"", {st_mode=S_IFREG|0444, st_size=265, ...}) = 0
lstat("".git/objects/7b/6bc0c8239bc4ee9d2184a5fbf1242064046c71"", {st_mode=S_IFREG|0444, st_size=285, ...}) = 0
lstat("".git/objects/4b/63154f50c372238ade7dde7b452550dbc47996"", {st_mode=S_IFREG|0444, st_size=651, ...}) = 0
lstat("".git/objects/6e/870e061b3bf09c347f76f4cadf582e9fc1cd61"", {st_mode=S_IFREG|0444, st_size=297, ...}) = 0
lstat("".git/objects/b4/f37c6ae1699d553b281ba4e32c786bc979de40"", {st_mode=S_IFREG|0444, st_size=173, ...}) = 0
lstat("".git/objects/56/2ab61072e658fd866ed9e926c8945b82af5b92"", {st_mode=S_IFREG|0444, st_size=88, ...}) = 0
lstat("".git/objects/60/0a9e2f8eaadf0f5749c29a4c9c469a9f776b6d"", {st_mode=S_IFREG|0444, st_size=55, ...}) = 0
lstat("".git/objects/02/bb51a9a426fd90b201ef5264e00173da323b8d"", {st_mode=S_IFREG|0444, st_size=90, ...}) = 0
lstat("".git/objects/89/51720d6cb111992495eae54b9f500b1fbe26d0"", {st_mode=S_IFREG|0444, st_size=50, ...}) = 0
lstat("".git/objects/5e/9e4ba03ff75f1dddc4e04c7ffdc0919a9b37ec"", {st_mode=S_IFREG|0444, st_size=43, ...}) = 0
lstat("".git/objects/3d/5a6573c9cc717cbcd08b625176790b3b95090e"", {st_mode=S_IFREG|0444, st_size=89, ...}) = 0
lstat("".git/objects/19/bc796359b938ef7a2d0df43f1ec24ae8caed18"", {st_mode=S_IFREG|0444, st_size=315, ...}) = 0
openat(AT_FDCWD, "".git/objects/pack/pack-5f9147cc7f7cce842ffccadb256ea01361a76dda.pack"", O_RDONLY|O_CLOEXEC) = 5
fstat(5, {st_mode=S_IFREG|0444, st_size=589939, ...}) = 0
fcntl(5, F_GETFD)                       = 0x1 (flags FD_CLOEXEC)
fcntl(5, F_SETFD, FD_CLOEXEC)           = 0
read(5, ""PACK\0\0\0\2\0\0\6\"""", 12)     = 12
lseek(5, 589919, SEEK_SET)              = 589919
read(5, ""_\221G\314\177|\316\204/\374\312\333%n\240\23a\247m\332"", 20) = 20
lstat("".git/objects/ad/b83d8e3cbc691e14272d15fc6a1cdfe04fdf3e"", {st_mode=S_IFREG|0444, st_size=193, ...}) = 0
lstat("".git/objects/cf/eac9b0e930be2151c7dde1975936b234920dd0"", {st_mode=S_IFREG|0444, st_size=172, ...}) = 0
lstat("".git/objects/2d/c4c4dcbe08ad28fd17abd9c4c638b4b64d3fb7"", {st_mode=S_IFREG|0444, st_size=1070, ...}) = 0
lstat("".git/objects/86/cc895c65f1adeac7b39ce2c7f30d112b0e23e3"", {st_mode=S_IFREG|0444, st_size=285, ...}) = 0
lstat("".git/objects/7e/36f84dc4332d7c6e47127f4fb53e83f1f7dff3"", {st_mode=S_IFREG|0444, st_size=468, ...}) = 0
lstat("".git/objects/d8/d217f10d17addc47ebe70142226c4b745013cc"", {st_mode=S_IFREG|0444, st_size=225, ...}) = 0
lstat("".git/objects/b9/83561467e1a336ff3500d92c2eae8963e01d97"", {st_mode=S_IFREG|0444, st_size=194, ...}) = 0
lstat("".git/objects/f3/9cb76ba84bb784143e3aea3a4e467c56a6672d"", {st_mode=S_IFREG|0444, st_size=158, ...}) = 0
lstat("".git/objects/5f/7f1078c0bbf447621f6414c73cffe7ae8535ea"", {st_mode=S_IFREG|0444, st_size=187, ...}) = 0
lstat("".git/objects/75/4b875647b4534b3492df818dedd491e39934ba"", {st_mode=S_IFREG|0444, st_size=342, ...}) = 0
lstat("".git/objects/67/c01c0c971cbafae9dc34af783f4955d433ef79"", {st_mode=S_IFREG|0444, st_size=108, ...}) = 0
close(3)                                = 0
unlink(""/mnt/storage/Kevin/Personal/Code/zero/.git/index.lock"") = 0
openat(AT_FDCWD, ""/etc/localtime"", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0444, st_size=3545, ...}) = 0
fstat(3, {st_mode=S_IFREG|0444, st_size=3545, ...}) = 0
read(3, ""TZif2\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\5\0\0\0\5\0\0\0\0""..., 4096) = 3545
lseek(3, -2261, SEEK_CUR)               = 1284
read(3, ""TZif2\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\5\0\0\0\5\0\0\0\0""..., 4096) = 2261
close(3)                                = 0
access("".git/hooks/pre-commit"", X_OK)   = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, "".git/COMMIT_EDITMSG"", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=0, ...}) = 0
lstat("".git/HEAD"", {st_mode=S_IFREG|0644, st_size=23, ...}) = 0
openat(AT_FDCWD, "".git/HEAD"", O_RDONLY) = 6
read(6, ""ref: refs/heads/master\n"", 256) = 23
read(6, """", 233)                        = 0
close(6)                                = 0
lstat("".git/refs/heads/master"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
openat(AT_FDCWD, "".git/refs/heads/master"", O_RDONLY) = 6
read(6, ""c37441718740260ccfefa91989ff6958""..., 256) = 41
read(6, """", 215)                        = 0
close(6)                                = 0
lstat("".git/refs/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c8c80) = -1 ENOENT (No such file or directory)
lstat("".git/refs/tags/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c8c80) = -1 ENOENT (No such file or directory)
lstat("".git/refs/heads/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c8c80) = -1 ENOENT (No such file or directory)
lstat("".git/refs/remotes/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c8c80) = -1 ENOENT (No such file or directory)
lstat("".git/refs/remotes/HEAD/HEAD"", 0x7ffda14c8f40) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c8c80) = -1 ENOENT (No such file or directory)
lstat("".git/HEAD"", {st_mode=S_IFREG|0644, st_size=23, ...}) = 0
openat(AT_FDCWD, "".git/HEAD"", O_RDONLY) = 6
read(6, ""ref: refs/heads/master\n"", 256) = 23
read(6, """", 233)                        = 0
close(6)                                = 0
lstat("".git/refs/heads/master"", {st_mode=S_IFREG|0644, st_size=41, ...}) = 0
openat(AT_FDCWD, "".git/refs/heads/master"", O_RDONLY) = 6
read(6, ""c37441718740260ccfefa91989ff6958""..., 256) = 41
read(6, """", 215)                        = 0
close(6)                                = 0
lstat("".git/refs/HEAD"", 0x7ffda14c8570) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c82b0) = -1 ENOENT (No such file or directory)
lstat("".git/refs/tags/HEAD"", 0x7ffda14c8570) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c82b0) = -1 ENOENT (No such file or directory)
lstat("".git/refs/heads/HEAD"", 0x7ffda14c8570) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c82b0) = -1 ENOENT (No such file or directory)
lstat("".git/refs/remotes/HEAD"", 0x7ffda14c8570) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c82b0) = -1 ENOENT (No such file or directory)
lstat("".git/refs/remotes/HEAD/HEAD"", 0x7ffda14c8570) = -1 ENOENT (No such file or directory)
stat("".git/packed-refs"", 0x7ffda14c82b0) = -1 ENOENT (No such file or directory)
clock_gettime(CLOCK_MONOTONIC, {tv_sec=192488, tv_nsec=830275833}) = 0
openat(AT_FDCWD, "".git/objects/40/3e72c2b8d1bafefb85310963e668e52349172e"", O_RDONLY|O_CLOEXEC) = 6
fstat(6, {st_mode=S_IFREG|0444, st_size=245, ...}) = 0
mmap(NULL, 245, PROT_READ, MAP_PRIVATE, 6, 0) = 0x7f1e0fb2a000
close(6)                                = 0
munmap(0x7f1e0fb2a000, 245)             = 0
openat(AT_FDCWD, "".git/objects/36/638c17a135a074bdba59958c5b8761a89d7640"", O_RDONLY|O_CLOEXEC) = 6
fstat(6, {st_mode=S_IFREG|0444, st_size=415, ...}) = 0
mmap(NULL, 415, PROT_READ, MAP_PRIVATE, 6, 0) = 0x7f1e0fb2a000
close(6)                                = 0
munmap(0x7f1e0fb2a000, 415)             = 0
openat(AT_FDCWD, "".git/objects/25/649d4de6317db6380f1a548bfd06c6afdcd3d7"", O_RDONLY|O_CLOEXEC) = 6
fstat(6, {st_mode=S_IFREG|0444, st_size=311, ...}) = 0
mmap(NULL, 311, PROT_READ, MAP_PRIVATE, 6, 0) = 0x7f1e0fb2a000
close(6)                                = 0
munmap(0x7f1e0fb2a000, 311)             = 0
write(3, ""Overhaul i3 config\n"", 19)    = 19
close(3)                                = 0
access("".git/hooks/pre-commit"", X_OK)   = -1 ENOENT (No such file or directory)
clock_gettime(CLOCK_MONOTONIC, {tv_sec=192488, tv_nsec=838549367}) = 0
lstat("".git/objects/7e/0b60448b3ca69daeef388dce9330b079bcb50a"", {st_mode=S_IFREG|0444, st_size=245, ...}) = 0
access("".git/hooks/prepare-commit-msg"", X_OK) = -1 ENOENT (No such file or directory)
access("".git/hooks/commit-msg"", X_OK)   = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, "".git/COMMIT_EDITMSG"", O_RDONLY) = 3
read(3, """", 8192)                       = 0
close(3)                                = 0
openat(AT_FDCWD, ""/nix/store/fg4yq8i8wd08xg3fy58l6q73cjy8hjr2-glibc-2.27/share/locale/locale.alias"", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0444, st_size=2997, ...}) = 0
read(3, ""# Locale name alias data base.\n#""..., 4096) = 2997
read(3, """", 4096)                       = 0
close(3)                                = 0
openat(AT_FDCWD, ""/nix/store/fizihf81dqmlrlgd8fc2lpkv008gl927-git-2.18.0/share/locale/en_US.UTF-8/LC_MESSAGES/git.mo"", O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fizihf81dqmlrlgd8fc2lpkv008gl927-git-2.18.0/share/locale/en_US.utf8/LC_MESSAGES/git.mo"", O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fizihf81dqmlrlgd8fc2lpkv008gl927-git-2.18.0/share/locale/en_US/LC_MESSAGES/git.mo"", O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fizihf81dqmlrlgd8fc2lpkv008gl927-git-2.18.0/share/locale/en.UTF-8/LC_MESSAGES/git.mo"", O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fizihf81dqmlrlgd8fc2lpkv008gl927-git-2.18.0/share/locale/en.utf8/LC_MESSAGES/git.mo"", O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, ""/nix/store/fizihf81dqmlrlgd8fc2lpkv008gl927-git-2.18.0/share/locale/en/LC_MESSAGES/git.mo"", O_RDONLY) = -1 ENOENT (No such file or directory)
write(2, ""Aborting commit due to empty com""..., 45Aborting commit due to empty commit message.
) = 45
getpid()                                = 5177
exit_group(1)                           = ?
+++ exited with 1 +++
</code></pre></details>

LizardFS mount arguments: `mfsmount -o nodev,noatime,mfsdelayedinit,big_writes,allow_other,nonempty,mfsmaster=192.168.1.5,cacheexpirationtime=500,readaheadmaxwindowsize=4096 /mnt/storage`","I believe the issue is actually something more fundamental: writes don't immediately show up once the writing process completes. This can be replicated by doing:

```
echo x > x; cat x
```

Which often returns nothing when you `cat x`.

This appears to only happen when `DirectIO=false`, which I need for mmaping files (e.g. running wineprefixes). When I toggle `DirectIO=true`, the problem goes away. It also goes away when I restart the mfs mount, but only temporarily. (I'm not sure if the problem would reoccur after a while with `DirectIO=true`; I'll update you.)",9800119
61,lizardfs lacks a test suite to detect regressions.,open,2018-09-03T17:06:17Z,2018-09-09T17:18:40Z,,NONE,"We've seen a number of issues and even disasters due to system level weaknesses such as poor handling of goal change and drive deprecation.  We need a suite of small test systems with built in constraints on resources to quickly test new versions against known difficult situations.
Proposal: provide small virtual machine configurations containing all the chunkservers, master, shadow, etc with strict limits on space for each virtual drive, so that we can automatically test simply by running and upgrading the machine to the current version.

related bugs we might have detected with a robust test suite:  #754 #746 #745 #720 #719 #738 #705 #682",Should we run a bug fix branch?,9800119
62,chunkserver: Add advertise address capability,open,2018-09-03T07:48:36Z,2018-09-03T07:48:36Z,,NONE,Allows setting `CSSERV_ADVERTISE_ADDR` so that the IP address advertised from a chunk server to a master can be different than the IP address the chunk server is listening on. This is mainly needed behind a NAT or when operating a chunk server within a Docker container. This resolves #721.,,9800119
63,3.13.0~rc1/client: read errors; no fail-over to other chunkservers,open,2018-09-03T03:15:24Z,2019-10-10T12:34:20Z,,MEMBER,"I've got the following (on different files) at least dozen times already on 192.168.0.204:

~~~~
mfsmount: read file error, inode: 1488455, index: 0, chunk: 10409019, version: 1 - Status 'No such chunk' sent by chunkserver (server 192.168.0.204:9422) (try counter: 32)
~~~~

Every time read got stuck for 400+ retries and I could only relieve situation by restarting chunkserver on 192.168.0.204.

Note that there are enough chunks to read the file successfully from other chunkservers:

~~~~ fileinfo
std: 2
        chunk 0: 00000000009ED43B_00000001 / (id:10409019 ver:1)
                copy 1: 192.168.0.2:9422:pool part 2/5 of ec(3,2)
                copy 2: 192.168.0.3:9622:pool
                copy 3: 192.168.0.4:9422:wks
                copy 4: 192.168.0.130:9422:wks part 1/5 of ec(3,2)
                copy 5: 192.168.0.204:9422:pool part 4/5 of ec(3,2)
                copy 6: 192.168.0.250:9422:stor part 3/5 of ec(3,2)
                copy 7: 192.168.0.250:9522:stor
~~~~
","`std: 2` is a current goal. I don't remember how the goal was changed. Probably (most certainly) I was changing EC goal to STD (plain replicated goal). Why there are only 4 parts instead of 5? Because goal is partially degraded, probably as result of unfinished change.

Hmm.., maybe error is misleading but the real problem is that algorithm failed to read available data and to make data accessible I had to stop/restart one chunkserver. IMHO a certain problem is exposed by this incident - a problem of selecting chunkserver to read from and a lack of fallback to alternative chunkserver in case of error (for whatever reason).",9800119
64,Directory removal,open,2018-09-01T21:50:14Z,2018-09-04T09:50:17Z,,NONE,"Hello,
I am trying to see why directory removal has not moved any chunks in 3 days. I have added ""*"" in front of directory name in configuration and did a reload of configuration. CGI reports directory as ""marked for removal"". However, I'm not able to find any logs or information showing me when it will start draining the disk of chunks. I know I could just pull out the drive and let lizardfs replicate the chunks, but I want to try using this feature of lizardfs - directory removal. Since then I tried restarting the chunkserver, but that did not help.","Related bugs: #364, #280.",9800119
65,Poor performance on ARM (3.12.0),open,2018-09-01T04:29:21Z,2019-05-13T16:42:01Z,,NONE,"I recently built a new single-node LizardFS cluster on an ARM platform (ODroid HC2) for testing with the intent of expanding it to 4 nodes later if things went well.

Unfortunately I can only seem to get about 10-21MB/s write speed out of it.

I'm wondering if it's because I'm colocating the master on the same node as the chunkserver,  but even so that seems a bit low to me.

I'm using the tuning recommended on an XFS filesystem in the documentation, minus turning off barriers which didn't seem to work.

rw,noexec,nodev,noatime,nodiratime,largeio,inode64

I tried the network tuning which made no tangible difference. I'm currently testing a change of scheduler. (EDIT: which didn't help, if anything made it worse)","There is an option on chunkservers
HDD_PUNCH_HOLES

if enabled then chunkserver detects zero values in chunk data and frees corresponding file blocks (decreasing file system usage). This option works only on Linux and only with file systems supporting punching holes (XFS, ext4, Btrfs, tmpfs)

This option is enabled by default and seriously decreases performance when writing zero-data (as with DD).

Test setting this to 0 on each chunkserver, edit mfschunkserver.cfg and add ""HDD_PUNCH_HOLES = 0""

Did you guys try anything other than DD? Any real world transfer tests?",9800119
66,[Feature] FS notify support (fsnotify/fanotify/inotify/dnotify),open,2018-08-30T07:41:08Z,2018-10-18T10:14:51Z,,NONE,"What would be required to support fsnotify events to reach the client? I can see some commented out definitions in `protocol/MFSCommunication.h` that suggest this may have been started. See https://github.com/libfuse/libfuse/wiki/Fsnotify-and-FUSE for a general idea. I believe that the mfsmount daemon would need to inject the events into the kernel.

Happy to send through a PR with some direction from a developer.","@onlyjob, pinning all processes to one host contradicts the basic idea of a cloud — if I would like to pin everything to one host, then I would not even have need for a LizardFS!

Of course, I could implement a new way of triggering, at least in my containers, by sending signals over tcp/ip, but that requires code changes in all systems and is far away from the elegance of file system notifications.

I strongly suggest to put notifications on the LizardFS-todo-list.

What about file locking? If I lock a file on one host, is it locked on all other hosts too? That would also be a necessary feature for distributed concurrent access and it would allow more types of redundant process distribution. So if not yet supported, I'd suggest to put locking on the todo list too.",9800119
67,3.12.0: Slave crashes if USE_BDB_FOR_NAME_STORAGE = 1,open,2018-08-27T08:18:59Z,2019-05-30T13:02:32Z,,MEMBER,No particular error is logged... It just terminates after loading data from master. `USE_BDB_FOR_NAME_STORAGE = 0` (or commenting that line) fixes the problem.,"The log you got @ccbur is comes from [here][1]. I'm trying to reproduce the bug with no luck. Any idea how to reproduce it?

[1]: https://github.com/lizardfs/lizardfs/blob/master/src/master/hstring_bdbstorage.cc#L72",9800119
68,"3.13.0~rc1: ""Chunk is busy""",open,2018-08-27T05:07:52Z,2018-08-27T17:37:52Z,,MEMBER,"Getting massive amount of `write file error, ... error sent by master server (Chunk is busy)` on replicated files (goal: 3)... 

What does it mean? Files exhibiting the problem are in user's chromium folder, used exclusively by one process. What causes chunks to become busy? Any known remedy?
",Roughly at least 10 times more `Chunk is busy` errors on 3.13.0~rc1 comparing to 3.12.0 so it looks like regression...,9800119
69,"3.13.0~rc1: epic fail? Growing amount of missing chunks; ""replication status: IO error""",open,2018-08-27T00:49:25Z,2019-09-11T18:14:50Z,,MEMBER,"After upgrading chunkservers to 3.13.0~rc1 I'm afraid I'm not getting away without massive data loss: `mfsmaster` logs `replication status: IO error` all the time and as replication progresses, CGI's  _Chunks_ view report __growing__ (!) number of missing chunks in _ec_, and _xor_ goals.

Bloody hell... :( :( :(",LizardFS is now under new management ( see https://github.com/lizardfs/lizardfs/issues/805#issuecomment-528899790 ) so hopefully LizardFS will start to get back on track again.,9800119
70,"chunkserver fails to mark unresponsive HDDs as ""damaged""",open,2018-08-26T23:52:36Z,2018-08-26T23:52:36Z,,MEMBER,"Few days ago I've experienced outage of the whole cluster due to HDD controller error on one server.

All I/O was stuck/suspended cluster-wide:

    mfsmount: write file error, inode: 33313452, index: 0 - Chunk write error (Disconnected)

I've arrived ~60 min. later and noticed that CGI's Disks view responds with following:

~~~~
Traceback (most recent call last):
  File ""/usr/share/mfscgi/mfs.cgi"", line 1415, in
    header = myrecv(s,8)
  File ""/usr/share/mfscgi/mfs.cgi"", line 285, in myrecv
    raise RuntimeError, ""socket connection broken""
RuntimeError: socket connection broken
~~~~

However all other tabs were working properly so at first I could not spot the problem.
Then from logs of other nodes I've found problematic chunkserver which was logging `jobs queue is full !!!` for an hour. System's logs had

~~~~
mpt2sas_cm0: log_info(0x31120303): originator(PL), code(0x12), sub_code(0x0303)
...
scsi_io_completion: 86 callbacks suppressed
~~~~

and mounted partitions were unresponsive.

The incident tells me that chunkservers should have some kind of timeout for marking disks as damaged. In this case no I/O could be completed on all HDDs for an hour yet chunkserver failed to mark HDDs as damaged or remove itself from cluster paralysing everything...
",,9800119
71,3.13.0~rc1: chunkservers report bogus errors,open,2018-08-26T23:36:51Z,2018-08-27T15:59:01Z,,MEMBER,"After upgrade to 3.13.0~rc1 chunkserver report 

~~~~
[warning] [12972:12981] : delete_chunk: file:/var/lib/lizardfs/03/ext4/chunksDB/chunk_ec2_3_of_2_1_00000D9CB5DBB030_00000001.liz - unlink error: No such file or directory
delete_chunk: file:/var/lib/lizardfs/03/ext4/chunksDB/chunk_ec2_3_of_2_1_00000D9CB5DBB030_00000001.liz - unlink error: No such file or directory
~~~~

Errors manifest in CGI's Disks view, on all HDDs of that chunkserver...

I understand that chunks should be converted to new format - is it safe to ignore those ""errors""?
It is uncomforting to see many errors for ""normal"" mode of operation - real errors may be concealed...",,9800119
72,Option to control log level?,open,2018-08-26T03:31:44Z,2018-08-26T23:29:58Z,,MEMBER,"Perhaps it would be great to introduce config option to control log level.

The following nonsense should be removed: https://github.com/lizardfs/lizardfs/blob/master/src/main/main.cc#L200-L204
",,9800119
73,"3.12.0 --> 3.13.0~rc1 master upgrade fails: ""checksum.changing_not_recalculated_chunk""",open,2018-08-26T03:17:50Z,2018-12-19T06:44:14Z,,MEMBER,"I've very disappointed by failure to upgrade Master from 3.12.0 to 3.13.0~rc1. As soon as master/3.13 started it flooded logs with endless `checksum.changing_not_recalculated_chunk`, never to become available and not responding to `SIGTERM`... Rate of logging caused 100% CPU utilisation in _systemd-journald_.","Chunkservers are affected too, they flood system logs with `cs.matocs.replicate`.

The culprit is #743: it is silly to change log verbosity depending on whether service is daemonized.
Systemd-supervised services are running in foreground mode but 3.13.0 sets _debug_ verbosity as per https://github.com/lizardfs/lizardfs/commit/3bde7fe2fd15e1c91e0364e995cd442114b9d1c1.

With that commit undone master appears to work OK...",9800119
74,mfsmaster exits on chunk operation with corrupted files,open,2018-08-20T15:07:54Z,2018-08-21T08:57:46Z,,NONE,"My cluster  setup consists of 3 master/shadow instances, 5 chunkserver and some metaloggers in a keepalived configuration. Everything was fine for some months now until some strange networking problems initiated a lot of master/shadow failovers between my nodes. Normally a master/shadow failover was never a problem, but this time something went wrong.

Result was that my master/shadow instances were no longer able to start. Even with `mfsmaster -d start`, there was only one line on every master/shadow instance in master mode and then the process was gone:

```
08/20/18 11:34:41.212 [info] [1:1] : configuration file /etc/lizardfs/mfsmaster.cfg loaded
```

Ok, I've got metaloggers, so I tried a mfsmetarestore. I delete every meta file from a shadow, recreated the metadata.mfs from the last metadata_ml.mfs / changelog_ml.mfs.x and started the shadow as master. 

Now, this master starts and sometimes I can even mount volumes, but then it exits again after some seconds/minutes:

```
08/20/18 14:23:59.276 [info] [1:1] : configuration file /etc/lizardfs/mfsmaster.cfg loaded
mfsmaster[1]: meta logger wants changes since version: 556139606, but minimal version in storage is: 556139607
mfsmaster[1]: meta logger wants changes since version: 556139606, but minimal version in storage is: 556139607
mfsmaster[1]: meta logger wants changes since version: 556139606, but minimal version in storage is: 556139607
mfsmaster[1]: meta logger wants changes since version: 556139606, but minimal version in storage is: 556139607
mfsmaster[1]: meta logger wants changes since version: 556139606, but minimal version in storage is: 556139607
mfsmaster[1]: chunkserver register begin (packet version: 5) - ip: 192.168.1.10, port: 9561
mfsmaster[1]: chunkserver register end (packet version: 5) - ip: 192.168.1.10, port: 9561, usedspace: 138759323648 (129.23 GiB), totalspace: 233479028736 (217.44 GiB)
mfsmaster[1]: chunkserver register begin (packet version: 5) - ip: 192.168.1.11, port: 9562
mfsmaster[1]: chunkserver register end (packet version: 5) - ip: 192.168.1.11, port: 9562, usedspace: 144290451456 (134.38 GiB), totalspace: 451292184576 (420.30 GiB)
mfsmaster[1]: chunkserver register begin (packet version: 5) - ip: 192.168.1.11, port: 9561
mfsmaster[1]: chunkserver register begin (packet version: 5) - ip: 192.168.1.16, port: 9561
mfsmaster[1]: chunkserver register end (packet version: 5) - ip: 192.168.1.11, port: 9561, usedspace: 128536064000 (119.71 GiB), totalspace: 220554719232 (205.41 GiB)
mfsmaster[1]: chunkserver register end (packet version: 5) - ip: 192.168.1.16, port: 9561, usedspace: 169908752384 (158.24 GiB), totalspace: 467101552640 (435.02 GiB)
mfsmaster[1]: chunk 0000000000232edf has not enough valid parts (0) consider repairing it manually
```

Now i have 140 missing chunks, and maybe I can live with that (and my backup), but because of the crashing mfsmaster, I'm not able identify which files are corrupted. I tried `lizardfs fileinfo` on some files, but my master immediately stops every time. I repaired one big file and now I have 138 missing chunks (and only file with a hole in it). But how to find all files and how to repair this chunk 0000000000232edf? 
 
What to do now? Is there still hope?

Please help...
","Ok, I think I managed the situation. First of all, I forget to mention that I'm using the latest official version 3.12.0 from the debian buster repositories for armhf.

I realized that the mfsmaster process always crashed at the beginning of the chunk operations (300s after start). With 
```
OPERATIONS_DELAY_INIT = 3000
```

I had enough time to find all the corrupted files. This line adapted from @onlyjob was really helpful: 

```
sudo find /mnt/lfs/ -type f -mtime -2 -exec sh -c 'lizardfs fileinfo ""{}"" | grep -q ""no valid copies"" && echo ""{}"";' \;
```

And be aware: Some corrupted files were in my **meta trash**!

In the end I still don't understand why the chunk operations will bring down every mfsmaster process if some files are corrupted. Is this really intended or a bug? At least the normal log could be more helpful!!

Btw. I found a nice setting for mfsmaster.cfg in the sources:

```
#LOG_FLUSH_ON = DEBUG
#MAGIC_DEBUG_LOG = x
```

There you'll get some very useful (magic) log files for your master/shadow servers.
 
I still like lizardfs, but yesterday gave me some headaches...",9800119
75,is fuse needed/synology NAS?,open,2018-08-16T21:47:32Z,2018-08-21T21:25:08Z,,NONE,"I presume fuse is only needed on systems that need to mount the lizardfs filesystem rather than for every component of lizardfs.

I was trying to see if I could get lizardfs (specifically interested in the chunkserver) to compile on an arm based synology nas (I was using an entware-NG bootstrap to install extra packages) but It looks like it struggles to find fuse, I had libfuse in /opt/lib but I think it was missing the dev libraries.

Is there anyway around this, also if compiling from source is there anyway to only compile specific components of lizardFS (e.g only the chunkserver)

Also is compiling it on the NAS itself even the smartest move or so should I be trying to do some kind of cross-compile on a PC.
","Seems to work ok, Mines a bit slow as it's running over zerotier and the nas doesn't have a particularly powerful CPU.

I'd quite like to figure out how to pass a build directory to Cmake properly and also hopefully they'll fix the bzip download URL in it's makefile at somepoint (i did raise an issue for that over at spksrc)

I might look at building the SPK but it looked like it had a fair few steps and I wasn't prepared to try it unless lizard was going to work in the first place",9800119
76,Pointing two chunkservers at the same directory leads to silent data loss,open,2018-08-11T15:11:10Z,2018-08-11T23:08:33Z,,NONE,"If you point two chunkservers at the same directory they both report the same chunks and then the master chooses to delete one as overgoal.  It doesn't notice this and will not notice the chunks are missing until a sweep occurs.
I think a simple file lock in the root of the chunk directory would be sufficient to prevent this (and probably other) cases.","This accident wasn't on my set up, but I also run a chunkserver per drive.  Otherwise it's not practical to use high erasure code factors.",9800119
77,When is the GUI being updated?,open,2018-07-16T00:26:00Z,2018-11-17T05:12:37Z,,NONE,"Hi, when is the GUI being updated, i see https://lizardfs.com/lizardfs-gui-the-first-look/ which looks really really really nice but 3.12 does not include that ui and that blog post was over a year ago.","Hmm, I hope not or it would mean that LizardFS team works on redundant GUI instead of fixing bugs...
Who knows what LizardFS team is doing anyway? There were not a single commit or comment from a team member since July 2018... :(",9800119
78,>=sys-cluster/lizardfs-3.12.0 fails to compile using >=sys-libs/glibc-2.24,open,2018-07-10T15:20:43Z,2018-07-23T11:44:11Z,,NONE,"Hi,

I need help to compile release sys-cluster/lizardfs-3.12.0 on Funtoo/Gentoo Linux:


cd /var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/lizardfs-3.12.0_build/src/master && /usr/bin/x86_64-pc-linux-gnu-g++ -DAPPNAME=mfsmaster -DENABLE_CRC -DLIZARDFS_HAVE_CONFIG_H -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -I/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/lizardfs-3.12.0_build -I/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/mfs-3.12.0/src -I/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/mfs-3.12.0/external/spdlog-0.14.0/include -I/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/mfs-3.12.0/src/master   -DNDEBUG -march=ivybridge -O2 -pipe -pipe -std=c++0x -pthread -Wall -Wextra -fwrapv -pedantic   -o CMakeFiles/master.dir/topology.cc.o -c /var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/mfs-3.12.0/src/master/topology.cc
[ 70%] Linking CXX static library libmaster.a
cd /var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/lizardfs-3.12.0_build/src/master && /usr/bin/cmake -P CMakeFiles/master.dir/cmake_clean_target.cmake
cd /var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/lizardfs-3.12.0_build/src/master && /usr/bin/cmake -E cmake_link_script CMakeFiles/master.dir/link.txt --verbose=1
/usr/bin/x86_64-pc-linux-gnu-ar qc libmaster.a  CMakeFiles/master.dir/acl_storage.cc.o CMakeFiles/master.dir/changelog.cc.o CMakeFiles/master.dir/chartsdata.cc.o CMakeFiles/master.dir/chunk_goal_counters.cc.o CMakeFiles/master.dir/chunks.cc.o CMakeFiles/master.dir/chunkserver_db.cc.o CMakeFiles/master.dir/datacachemgr.cc.o CMakeFiles/master.dir/exports.cc.o CMakeFiles/master.dir/filesystem.cc.o CMakeFiles/master.dir/filesystem_checksum.cc.o CMakeFiles/master.dir/filesystem_checksum_background_updater.cc.o CMakeFiles/master.dir/filesystem_checksum_updater.cc.o CMakeFiles/master.dir/filesystem_dump.cc.o CMakeFiles/master.dir/filesystem_freenode.cc.o CMakeFiles/master.dir/filesystem_node.cc.o CMakeFiles/master.dir/filesystem_operations.cc.o CMakeFiles/master.dir/filesystem_periodic.cc.o CMakeFiles/master.dir/filesystem_quota.cc.o CMakeFiles/master.dir/filesystem_snapshot.cc.o CMakeFiles/master.dir/filesystem_store.cc.o CMakeFiles/master.dir/filesystem_store_acl.cc.o CMakeFiles/master.dir/filesystem_xattr.cc.o CMakeFiles/master.dir/get_servers_for_new_chunk.cc.o CMakeFiles/master.dir/goal_config_loader.cc.o CMakeFiles/master.dir/hstorage_init.cc.o CMakeFiles/master.dir/hstring_memstorage.cc.o CMakeFiles/master.dir/itree.cc.o CMakeFiles/master.dir/locks.cc.o CMakeFiles/master.dir/masterconn.cc.o CMakeFiles/master.dir/matoclserv.cc.o CMakeFiles/master.dir/matocsserv.cc.o CMakeFiles/master.dir/matomlserv.cc.o CMakeFiles/master.dir/matotsserv.cc.o CMakeFiles/master.dir/metadata_dumper.cc.o CMakeFiles/master.dir/personality.cc.o CMakeFiles/master.dir/quota_database.cc.o CMakeFiles/master.dir/recursive_remove_task.cc.o CMakeFiles/master.dir/restore.cc.o CMakeFiles/master.dir/setgoal_task.cc.o CMakeFiles/master.dir/settrashtime_task.cc.o CMakeFiles/master.dir/snapshot_task.cc.o CMakeFiles/master.dir/task_manager.cc.o CMakeFiles/master.dir/topology.cc.o
/usr/bin/x86_64-pc-linux-gnu-ranlib libmaster.a
make[2]: Leaving directory '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/lizardfs-3.12.0_build'
[ 70%] Built target master
make[1]: Leaving directory '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/lizardfs-3.12.0_build'
make: *** [Makefile:130: all] Error 2
 * ERROR: sys-cluster/lizardfs-3.12.0::coffnix-ebuilds failed (compile phase):
 *   emake failed
 * 
 * If you need support, post the output of `emerge --info '=sys-cluster/lizardfs-3.12.0::coffnix-ebuilds'`,
 * the complete build log and the output of `emerge -pqv '=sys-cluster/lizardfs-3.12.0::coffnix-ebuilds'`.
 * The complete build log is located at '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/temp/build.log'.
 * The ebuild environment file is located at '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/temp/environment'.
 * Working directory: '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/lizardfs-3.12.0_build'
 * S: '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/mfs-3.12.0'

>>> Failed to emerge sys-cluster/lizardfs-3.12.0, Log file:

>>>  '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/temp/build.log'
 * Messages for package sys-cluster/lizardfs-3.12.0:
 * ERROR: sys-cluster/lizardfs-3.12.0::coffnix-ebuilds failed (compile phase):
 *   emake failed
 * 
 * If you need support, post the output of `emerge --info '=sys-cluster/lizardfs-3.12.0::coffnix-ebuilds'`,
 * the complete build log and the output of `emerge -pqv '=sys-cluster/lizardfs-3.12.0::coffnix-ebuilds'`.
 * The complete build log is located at '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/temp/build.log'.
 * The ebuild environment file is located at '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/temp/environment'.
 * Working directory: '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/lizardfs-3.12.0_build'
 * S: '/var/tmp/portage/sys-cluster/lizardfs-3.12.0/work/mfs-3.12.0'


#################

. This is my system:

(funtoo) dell_notebook ~ # emerge  --info
Portage 2.3.24 (python 3.4.6-final-0, funtoo/1.0/linux-gnu/arch/x86-64bit, gcc-7.3.1, glibc-2.26-r7, 4.17.4-gentoo x86_64)
=================================================================
System uname: Linux-4.17.4-gentoo-x86_64-Intel-R-_Core-TM-_i7-3632QM_CPU_@_2.20GHz-with-gentoo-2.2.2
KiB Mem:     8021776 total,   2827888 free
KiB Swap:    6291452 total,   6286844 free
sh bash 4.4_p18
ld GNU ld (Gentoo 2.29.1 p3) 2.29.1
app-shells/bash:          4.4_p18::core-kit
dev-java/java-config:     2.2.0-r3::java-kit
dev-lang/perl:            5.26.9999::perl-kit
dev-lang/python:          2.7.14-r2::python-kit, 3.4.6-r1::python-kit
dev-util/cmake:           3.11.4::coffnix-ebuilds
sys-apps/baselayout:      2.2.2::core-kit
sys-apps/openrc:          0.35.0_beta1-r1::core-kit
sys-apps/sandbox:         2.12::core-kit
sys-devel/autoconf:       2.13::core-kit, 2.69-r4::core-kit
sys-devel/automake:       1.11.6-r2::gentoo, 1.15.1-r1::core-kit
sys-devel/binutils:       2.29.1-r1::core-kit
sys-devel/gcc:            7.3.1::core-kit
sys-devel/gcc-config:     1.9.1::core-kit
sys-devel/libtool:        2.4.6-r4::core-kit
sys-devel/make:           4.2.1-r1::core-kit
sys-kernel/linux-headers: 4.14::core-kit (virtual/os-headers)
sys-libs/glibc:           2.26-r7::core-kit
Repositories:

nokit
    location: /var/git/meta-repo/kits/nokit
    masters: core-kit
    priority: -500

coffnix-ebuilds
    location: /var/overlay/local
    masters: core-kit
    priority: 0

core-hw-kit
    location: /var/git/meta-repo/kits/core-hw-kit
    masters: core-kit
    priority: 1

core-kit
    location: /var/git/meta-repo/kits/core-kit
    masters: core-kit
    priority: 1
    aliases: gentoo

desktop-kit
    location: /var/git/meta-repo/kits/desktop-kit
    masters: core-kit
    priority: 1

dev-kit
    location: /var/git/meta-repo/kits/dev-kit
    masters: core-kit
    priority: 1

editors-kit
    location: /var/git/meta-repo/kits/editors-kit
    masters: core-kit
    priority: 1

games-kit
    location: /var/git/meta-repo/kits/games-kit
    masters: core-kit
    priority: 1

gnome-kit
    location: /var/git/meta-repo/kits/gnome-kit
    masters: core-kit
    priority: 1

haskell-kit
    location: /var/git/meta-repo/kits/haskell-kit
    masters: core-kit
    priority: 1

java-kit
    location: /var/git/meta-repo/kits/java-kit
    masters: core-kit
    priority: 1

kde-kit
    location: /var/git/meta-repo/kits/kde-kit
    masters: core-kit
    priority: 1

lang-kit
    location: /var/git/meta-repo/kits/lang-kit
    masters: core-kit
    priority: 1

lisp-scheme-kit
    location: /var/git/meta-repo/kits/lisp-scheme-kit
    masters: core-kit
    priority: 1

llvm-kit
    location: /var/git/meta-repo/kits/llvm-kit
    masters: core-kit
    priority: 1

media-kit
    location: /var/git/meta-repo/kits/media-kit
    masters: core-kit
    priority: 1

ml-lang-kit
    location: /var/git/meta-repo/kits/ml-lang-kit
    masters: core-kit
    priority: 1

net-kit
    location: /var/git/meta-repo/kits/net-kit
    masters: core-kit
    priority: 1

perl-kit
    location: /var/git/meta-repo/kits/perl-kit
    masters: core-kit
    priority: 1

php-kit
    location: /var/git/meta-repo/kits/php-kit
    masters: core-kit
    priority: 1

python-kit
    location: /var/git/meta-repo/kits/python-kit
    masters: core-kit
    priority: 1

python-modules-kit
    location: /var/git/meta-repo/kits/python-modules-kit
    masters: core-kit
    priority: 1

ruby-kit
    location: /var/git/meta-repo/kits/ruby-kit
    masters: core-kit
    priority: 1

science-kit
    location: /var/git/meta-repo/kits/science-kit
    masters: core-kit
    priority: 1

security-kit
    location: /var/git/meta-repo/kits/security-kit
    masters: core-kit
    priority: 1

text-kit
    location: /var/git/meta-repo/kits/text-kit
    masters: core-kit
    priority: 1

xfce-kit
    location: /var/git/meta-repo/kits/xfce-kit
    masters: core-kit
    priority: 1

xorg-kit
    location: /var/git/meta-repo/kits/xorg-kit
    masters: core-kit
    priority: 1

gamerlay
    location: /var/lib/layman/gamerlay
    sync-type: laymansync
    sync-uri: https://anongit.gentoo.org/git/proj/gamerlay.git
    masters: core-kit
    priority: 50

torbrowser
    location: /var/lib/layman/torbrowser
    sync-type: laymansync
    sync-uri: https://github.com/MeisterP/torbrowser-overlay.git
    masters: core-kit
    priority: 50

ACCEPT_KEYWORDS=""amd64 ~amd64""
ACCEPT_LICENSE=""*""
CBUILD=""x86_64-pc-linux-gnu""
CFLAGS=""-march=ivybridge -O2 -pipe""
CHOST=""x86_64-pc-linux-gnu""
CONFIG_PROTECT=""/etc /usr/lib64/libreoffice/program/sofficerc /usr/share/config /usr/share/gnupg/qualified.txt /var/bind""
CONFIG_PROTECT_MASK=""/etc/ca-certificates.conf /etc/dconf /etc/env.d /etc/fonts/fonts.conf /etc/gconf /etc/gentoo-release /etc/php/apache2-php7.1/ext-active/ /etc/php/apache2-php7.3/ext-active/ /etc/php/cgi-php7.1/ext-active/ /etc/php/cgi-php7.3/ext-active/ /etc/php/cli-php7.1/ext-active/ /etc/php/cli-php7.3/ext-active/ /etc/revdep-rebuild /etc/sandbox.d /etc/terminfo""
CXXFLAGS=""-march=ivybridge -O2 -pipe""
DISTDIR=""/var/cache/portage/distfiles""
FEATURES=""assume-digests binpkg-logs buildpkg config-protect-if-modified distlocks ebuild-locks fixlafiles merge-sync metadata-transfer multilib-strict news parallel-fetch preserve-libs sandbox sfperms strict unknown-features-warn unmerge-logs unmerge-orphans userfetch userpriv usersandbox usersync xattr""
FFLAGS=""-march=ivybridge -O2 -pipe""
GENTOO_MIRRORS=""https://fastpull-us.funtoo.org""
INSTALL_MASK=""/etc/systemd/ /usr/lib/systemd/ /lib/systemd/""
LANG=""en_US.UTF-8""
LC_ALL=""en_US.UTF-8""
LDFLAGS=""-Wl,-O1 -Wl,--sort-common -Wl,--as-needed""
LINGUAS=""en_US pt_BR""
MAKEOPTS=""-j7""
PKGDIR=""/storage/portage/packages""
PORTAGE_CONFIGROOT=""/""
PORTAGE_TMPDIR=""/var/tmp""
USE=""X a52 aac acl alsa amd64 apache apng bash-completion berkdb bluetooth bluray bzip2 cairo caps cdda cddb cdio cdr chm clamav clang cluster command-args consolekit cracklib crypt cups curl cxx dbus declarative device-mapper djvu dmx dos dpi dri dtnsec dts dvd dvdr dvdread ebook egl encode evdev exif exiv2 faac faad fam fbcondecor ffmpeg flac fontconfig fuse gallium gdbm geoip gif glamor gles gpm ico iconv icu idn ieee1394 ios ipod ipv6 jpeg jpeg2k kipi kmod kpathsea lame libass libguess libmpeg2 lowpan lua lzma mad matroska mjpeg mmx mobi modules mp3 mpeg mtp mudflap multilib ncurses networkmanager nls nptl nsplugin nsplugins ntp ogg opencl opengl openipmi openmp openssl pam pcre pdf perl plasma png policykit postgres postproc pppd pulseaudio python python_abis_2.6 python_abis_2.7 python_abis_3.4 qml qt3support qt5 quicktime rar readline resolvconf samba sasl semantic-desktop sndfile snmp sse sse2 ssl svg symlink syslog taglib tcl tcpd theora threads tiff tproxy truetype tty-helpers twolame udev unicode urandom v4l vim-syntax vorbis vpx wav wavpack wayland webp widgets wifi win32codecs wmf wps x264 x265 xattr xcomposite xinerama xkb xml xmp xscreensaver xvid zlib"" ABI_X86=""32 64"" ALSA_CARDS=""ali5451 als4000 atiixp atiixp-modem bt87x ca0106 cmipci emu10k1x ens1370 ens1371 es1938 es1968 fm801 hda-intel ice1724 intel8x0 intel8x0m maestro3 trident usb-audio via82xx via82xx-modem ymfpci"" ALSA_PCM_PLUGINS=""adpcm alaw asym copy dmix dshare dsnoop empty extplug file hooks iec958 ioplug ladspa lfloat linear meter mmap_emul mulaw multi null plug rate route share shm softvol"" APACHE2_MODULES=""actions alias auth_basic authn_alias authn_anon authn_dbm authn_default authn_file authz_dbm authz_default authz_groupfile authz_host authz_owner authz_user autoindex cache cgi cgid dav dav_fs dav_lock deflate dir disk_cache env expires ext_filter file_cache filter headers include info log_config logio mem_cache mime mime_magic negotiation rewrite setenvif speling status unique_id userdir usertrack vhost_alias authn_core authz_core socache_shmcb unixd"" CALLIGRA_FEATURES=""kexi words flow plan sheets stage tables krita karbon braindump author"" COLLECTD_PLUGINS=""df interface irq load memory rrdtool swap syslog"" CPU_FLAGS_X86=""mmx sse sse2 sse3 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt mmxext"" ELIBC=""glibc"" GPSD_PROTOCOLS=""ashtech aivdm earthmate evermore fv18 garmin garmintxt gpsclock isync itrax mtk3301 nmea ntrip navcom oceanserver oldstyle oncore rtcm104v2 rtcm104v3 sirf skytraq superstar2 timing tsip tripmate tnt ublox ubx"" GRUB_PLATFORMS=""efi-64 pc"" INPUT_DEVICES=""evdev keyboard mouse synaptics joystick"" KERNEL=""linux"" L10N=""pt-BR"" LCD_DEVICES=""bayrad cfontz cfontz633 glk hd44780 lb216 lcdm001 mtxorb ncurses text"" LIBREOFFICE_EXTENSIONS=""presenter-console presenter-minimizer"" LINGUAS=""en_US pt_BR"" LLVM_TARGETS=""BPF AMDGPU"" OFFICE_IMPLEMENTATION=""libreoffice"" POSTGRES_TARGETS=""postgres9_5"" PYTHON_SINGLE_TARGET=""python3_4"" PYTHON_TARGETS=""python2_7 python3_4"" QEMU_SOFTMMU_TARGETS=""i386 x86_64 arm mips mips64 mips64el mipsel"" QEMU_USER_TARGETS=""i386 x86_64 arm mips mips64 mips64el mipsel"" RUBY_TARGETS=""ruby23 ruby24"" USERLAND=""GNU"" VIDEO_CARDS=""amdgpu radeonsi radeon intel i915 i965"" XTABLES_ADDONS=""quota2 psd pknock lscan length2 ipv4options ipset ipp2p iface geoip fuzzy condition tee tarpit sysrq steal rawnat logmark ipmark dhcpmac delude chaos account""
Unset:  CPPFLAGS, CTARGET, EMERGE_DEFAULT_OPTS, ENV_UNSET, PORTAGE_BUNZIP2_COMMAND, PORTAGE_COMPRESS, PORTAGE_COMPRESS_FLAGS


","Ebuilds fixed:

https://github.com/coffnix/lizardfs-funtoo-overlay/commit/a16a21c845d3bdb361c081fd750499eee4386dc7",9800119
79,lizardfs mksnapshot fails with exit code 141,open,2018-07-02T09:55:40Z,2018-11-07T09:48:17Z,,NONE,"I use lizardfs mksnapshot for daily snapshots in a script in a cronjob. The usage, I described in #670. Now, the last successful daily snapshot was on 2018-06-22, at 06:25 o'clock (gmt). Since then, no snapshots have been taken anymore.

Why?

When I run it manually (as root) and run strace, I get:
```
marc@universum:~$ sudo strace lizardfs makesnapshot /var/volumes/configs /var/volumes/backups/snapshots/configs.daily-2018-07-02-11-45
execve(""/usr/bin/lizardfs"", [""lizardfs"", ""makesnapshot"", ""/var/volumes/configs"", ""/var/volumes/backups/snapshots/c""...], [/* 15 vars */]) = 0
brk(NULL)                               = 0xe1c000
access(""/etc/ld.so.nohwcap"", F_OK)      = -1 ENOENT (No such file or directory)
access(""/etc/ld.so.preload"", R_OK)      = -1 ENOENT (No such file or directory)
open(""/etc/ld.so.cache"", O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=29217, ...}) = 0
mmap(NULL, 29217, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f9981637000
close(3)                                = 0
access(""/etc/ld.so.nohwcap"", F_OK)      = -1 ENOENT (No such file or directory)
open(""/usr/lib/x86_64-linux-gnu/libstdc++.so.6"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0 \235\10\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0644, st_size=1566440, ...}) = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f9981636000
mmap(NULL, 3675136, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f9981098000
mprotect(0x7f998120a000, 2097152, PROT_NONE) = 0
mmap(0x7f998140a000, 49152, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x172000) = 0x7f998140a000
mmap(0x7f9981416000, 13312, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f9981416000
close(3)                                = 0
access(""/etc/ld.so.nohwcap"", F_OK)      = -1 ENOENT (No such file or directory)
open(""/lib/x86_64-linux-gnu/libm.so.6"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0\0V\0\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0644, st_size=1088952, ...}) = 0
mmap(NULL, 3178744, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f9980d8f000
mprotect(0x7f9980e97000, 2093056, PROT_NONE) = 0
mmap(0x7f9981096000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x107000) = 0x7f9981096000
close(3)                                = 0
access(""/etc/ld.so.nohwcap"", F_OK)      = -1 ENOENT (No such file or directory)
open(""/lib/x86_64-linux-gnu/libgcc_s.so.1"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0p*\0\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0644, st_size=89696, ...}) = 0
mmap(NULL, 2185488, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f9980b79000
mprotect(0x7f9980b8f000, 2093056, PROT_NONE) = 0
mmap(0x7f9980d8e000, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x15000) = 0x7f9980d8e000
close(3)                                = 0
access(""/etc/ld.so.nohwcap"", F_OK)      = -1 ENOENT (No such file or directory)
open(""/lib/x86_64-linux-gnu/libpthread.so.0"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0\260`\0\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0755, st_size=138696, ...}) = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f9981635000
mmap(NULL, 2212904, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f998095c000
mprotect(0x7f9980974000, 2093056, PROT_NONE) = 0
mmap(0x7f9980b73000, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x17000) = 0x7f9980b73000
mmap(0x7f9980b75000, 13352, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f9980b75000
close(3)                                = 0
access(""/etc/ld.so.nohwcap"", F_OK)      = -1 ENOENT (No such file or directory)
open(""/lib/x86_64-linux-gnu/libc.so.6"", O_RDONLY|O_CLOEXEC) = 3
read(3, ""\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0>\0\1\0\0\0P\t\2\0\0\0\0\0""..., 832) = 832
fstat(3, {st_mode=S_IFREG|0755, st_size=1868984, ...}) = 0
mmap(NULL, 3971488, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f9980592000
mprotect(0x7f9980752000, 2097152, PROT_NONE) = 0
mmap(0x7f9980952000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1c0000) = 0x7f9980952000
mmap(0x7f9980958000, 14752, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f9980958000
close(3)                                = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f9981634000
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f9981633000
mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f9981631000
arch_prctl(ARCH_SET_FS, 0x7f9981631740) = 0
mprotect(0x7f9980952000, 16384, PROT_READ) = 0
mprotect(0x7f9980b73000, 4096, PROT_READ) = 0
mprotect(0x7f9981096000, 4096, PROT_READ) = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f9981630000
mprotect(0x7f998140a000, 40960, PROT_READ) = 0
mprotect(0x67d000, 4096, PROT_READ)     = 0
mprotect(0x7f998163f000, 4096, PROT_READ) = 0
munmap(0x7f9981637000, 29217)           = 0
set_tid_address(0x7f9981631a10)         = 22560
set_robust_list(0x7f9981631a20, 24)     = 0
rt_sigaction(SIGRTMIN, {0x7f9980961b50, [], SA_RESTORER|SA_SIGINFO, 0x7f998096d390}, NULL, 8) = 0
rt_sigaction(SIGRT_1, {0x7f9980961be0, [], SA_RESTORER|SA_RESTART|SA_SIGINFO, 0x7f998096d390}, NULL, 8) = 0
rt_sigprocmask(SIG_UNBLOCK, [RTMIN RT_1], NULL, 8) = 0
getrlimit(RLIMIT_STACK, {rlim_cur=8192*1024, rlim_max=RLIM64_INFINITY}) = 0
brk(NULL)                               = 0xe1c000
brk(0xe4e000)                           = 0xe4e000
futex(0x7f99814171ac, FUTEX_WAKE_PRIVATE, 2147483647) = 0
futex(0x7f99814171b8, FUTEX_WAKE_PRIVATE, 2147483647) = 0
stat(""/var/volumes/backups/snapshots/configs.daily-2018-07-02-11-45"", 0x7fffeb8bcbf0) = -1 ENOENT (No such file or directory)
lstat(""/var/volumes/configs"", {st_mode=S_IFDIR|0755, st_size=0, ...}) = 0
stat(""/var/volumes/backups/snapshots"", {st_mode=S_IFDIR|0755, st_size=110730, ...}) = 0
lstat(""/var"", {st_mode=S_IFDIR|0755, st_size=174, ...}) = 0
lstat(""/var/volumes"", {st_mode=S_IFDIR|0775, st_size=138211, ...}) = 0
lstat(""/var/volumes/backups"", {st_mode=S_IFDIR|0755, st_size=129333, ...}) = 0
lstat(""/var/volumes/backups/snapshots"", {st_mode=S_IFDIR|0755, st_size=110730, ...}) = 0
rt_sigprocmask(SIG_BLOCK, [HUP INT USR1 TERM], NULL, 8) = 0
lstat(""/var"", {st_mode=S_IFDIR|0755, st_size=174, ...}) = 0
lstat(""/var/volumes"", {st_mode=S_IFDIR|0775, st_size=138211, ...}) = 0
lstat(""/var/volumes/backups"", {st_mode=S_IFDIR|0755, st_size=129333, ...}) = 0
lstat(""/var/volumes/backups/snapshots"", {st_mode=S_IFDIR|0755, st_size=110730, ...}) = 0
statfs(""/var/volumes/backups/snapshots"", {f_type=0x65735546, f_bsize=65536, f_blocks=1493216108, f_bfree=392683710, f_bavail=392683710, f_files=4294967279, f_ffree=4275684066, f_fsid={0, 0}, f_namelen=255, f_frsize=65536, f_flags=1062}) = 0
stat(""/var/volumes/backups/snapshots"", {st_mode=S_IFDIR|0755, st_size=110730, ...}) = 0
stat(""/var/volumes/backups/snapshots"", {st_mode=S_IFDIR|0755, st_size=110730, ...}) = 0
stat(""/var/volumes/backups/snapshots/.masterinfo"", 0x7fffeb8bb760) = -1 ENOENT (No such file or directory)
stat(""/var/volumes/backups"", {st_mode=S_IFDIR|0755, st_size=129333, ...}) = 0
stat(""/var/volumes/backups"", {st_mode=S_IFDIR|0755, st_size=129333, ...}) = 0
stat(""/var/volumes/backups/.masterinfo"", 0x7fffeb8bb760) = -1 ENOENT (No such file or directory)
stat(""/var/volumes"", {st_mode=S_IFDIR|0775, st_size=138211, ...}) = 0
stat(""/var/volumes"", {st_mode=S_IFDIR|0775, st_size=138211, ...}) = 0
stat(""/var/volumes/.masterinfo"", {st_mode=S_IFREG|0444, st_size=14, ...}) = 0
open(""/var/volumes/.masterinfo"", O_RDONLY) = 3
read(3, ""\177\0\0\1\337\324\0\0\1Q\0\3\f\0"", 14) = 14
close(3)                                = 0
socket(PF_INET, SOCK_STREAM, IPPROTO_IP) = 3
fcntl(3, F_GETFL)                       = 0x2 (flags O_RDWR)
fcntl(3, F_SETFL, O_RDWR|O_NONBLOCK)    = 0
connect(3, {sa_family=AF_INET, sin_port=htons(57300), sin_addr=inet_addr(""127.0.0.1"")}, 16) = -1 EINPROGRESS (Operation now in progress)
poll([{fd=3, events=POLLOUT}], 1, 200)  = 1 ([{fd=3, revents=POLLOUT|POLLERR|POLLHUP}])
getsockopt(3, SOL_SOCKET, SO_ERROR, [111], [4]) = 0
poll([{fd=3, events=POLLOUT}], 1, 10000) = 1 ([{fd=3, revents=POLLOUT|POLLHUP}])
sendto(3, ""\0\0\1\220\0\0\0IDjI1GAQDULI5d2YjA26ypc3o""..., 81, 0, NULL, 0) = -1 EPIPE (Broken pipe)
--- SIGPIPE {si_signo=SIGPIPE, si_code=SI_USER, si_pid=22560, si_uid=0} ---
+++ killed by SIGPIPE +++
marc@universum:~$ echo $?
141
```

So: No error message shown, return code is `141`.

What's the problem?","I again have this problem with exit code 141. It used to work until Oct 29 and since then snapshots failed again!

Again: unmounting and remounting the lizardfs filesystem (twice) solved the problem.",9800119
80,Support relative paths in mfschunkserver.cfg,open,2018-06-26T06:10:19Z,2018-06-26T06:10:19Z,,NONE,"I realized today that it would be much nicer for my setup if I just put everything in one place on the data disk.  Given loss of the drive or the metadata makes the data unrecoverable, we would actually reduce dependency compared with the current multidisk arrangement.  It would mean that adding a drive could be a simple untar:
```
root@:/data5/lizcfg# ls -l
drwxr-xr-x 2 mfs  mfs  4096 Jun 25 22:47 data
drwxr-xr-x 2 mfs  mfs  4096 Jun 25 23:04 metadata
-rw-r--r-- 1 root root  318 Jun 25 23:03 mfschunkserver.cfg
-rw-r--r-- 1 root root   52 Jun 25 22:47 mfshdd.cfg
```
then I just set up an entry in systemd to point to the mfschunkserver.cfg which would then use relative paths.  This all works with two exceptions, the path to mfshdd.cfg needs to be absolute, and the paths in mfshdd.cfg need to be absolute.  Ideally I'd be able to set the port and identity via env variables.

Would it be possible to remove the check for absolute paths in these places?  Is there a security reason why this can't work?  If you point me in the general direction I can probably even make and test a patch.",,9800119
81,New LizardFS Docker/Docker Swarm Plugin,open,2018-06-19T18:23:24Z,2018-12-05T22:16:53Z,,NONE,"Hello Everybody, I have recently developed a Docker plugin that allows you to create LizardFS Docker volumes! There are two different versions of the plugin: a Docker managed plugin that works well for individual Docker instances, and a version that can be deployed as a stack on Docker Swarm to create a self-contained storage solution for a Docker cluster.

The Docker plugin has been developed by me and my team at @kadimasolutions to create a distributed storage solution that can be deployed on Docker Swarm and provide shared volumes for the containers in the Docker Swarm cluster. As far as I have found it is **the only** solution that does so.

We will soon mirror the source code for the plugin to GitHub. In the meantime, you can test out the plugin using the Docker images that are on DockerHub. The plugin can be considered in beta and is, as far as I can tell, completely functional, but there may still be bugs or nuances that we have not yet found. Feedback is appreciated. :smiley: Updates to the image on DockerHub may be made without notice, I will try to mention any changes that I make here.

In addition, I will soon be attempting to get the Swarm deployment setup with the very latest highly available LizardFS master from the LizardFS 3.13 release candidate ( thanks to the folks at @lizardfs for getting that to me early ) so that the deployed LizardFS cluster will have automatic failover.

Here are detailed instructions for getting started with both versions of the plugin. If you need any help just comment on this thread and I will try to do what I can when I have the time.

# Docker Managed plugin

The Docker managed plugin can be installed very easily on any Docker host and is great for connecting your Docker containers to an existing LizardFS cluster.

> **Note:** If you don't have a LizardFS cluster, yet you may want to consider using the Swarm deployment instead. You can use the Docker Swarm deployment to create a LizardFS cluster out of your Docker hosts that will supply your Docker containers with shared LizardFS volumes that are distributed across you Docker cluster.

## Usage

### Prerequisites

Before you can use the plugin you must have:

* A running LizardFS cluster that your Docker host can access.
* A directory on the LizardFS filesystem that can be used by the plugin to store Docker volumes. This can be any normal directory. By default the plugin will use `/docker/volumes`, but this can be changed ( see [REMOTE_PATH](#remote-path) ).

Once these conditions are met you are ready to install the plugin.

### Installation

The plugin is simple use and can be installed as a Docker container without having to install any other system dependencies.

    $ docker plugin install --alias lizardfs kadimasolutions/lizardfs-volume-plugin HOST=mfsmaster PORT=9421

Docker will prompt asking if you want to grant the permissions required to run the plugin. Select yes and the plugin will download and install.

> **Note:** We set the plugin alias to `lizardfs`. This is completely optional, but it allows us to refer to the plugin with a much shorter name. Throughout this readme, when reference is made to the `lizardfs` driver, it is referring to this alias.

That's it! You can now see your newly installed Docker plugin by running `docker plugin ls`.

    $ docker plugin ls
    ID                  NAME                 DESCRIPTION                         ENABLED
    4a08a23cf2eb        lizardfs:latest      LizardFS volume plugin for Docker   true

You should now be able to create a Docker volume using our new `lizardfs` driver.

    $ docker volume create --driver lizardfs lizard-vol
    lizard-vol

You can see it by running `docker volume ls`.

    $ docker volume ls
    DRIVER               VOLUME NAME
    lizardfs:latest      lizard-vol

Now that you have created the volume you can mount it into a container using its name. Lets mount it into an alpine container and put some data in it.

```sh
$ docker run -it --rm -v lizard-vol:/data alpine sh
/ $ cd /data # Switch to our volume mountpoint
/data $ cp -R /etc . # Copy the whole container /etc directory to it
/data $ ls # See that the copy was successful
etc
/data $ exit # Exit ( the container will be removed because of the --rm )
```

We should now have a copy of the alpine container's whole `/etc` directory on our `lizard-vol` volume. You can verify this by checking the `/docker/volumes/lizard-vol/` directory on your LizardFS installation. You should see the `etc` folder with all of its files and folders in it. Congratulations! You have successfully mounted your LizardFS filesytem into a docker container and stored data in it!

If you run another container, you can mount the same volume into it and that container will also see the data. Your data will stick around as long as that volume exists. When you are done with it, you can remove the volume by running `docker volume rm lizard-vol`.

### Features

#### Shared Mounts

Any number of containers on any number of hosts can mount the same volume at the same time. The only requirement is that each Docker host have the LizardFS plugin installed on it.

#### Transparent Data Storage ( No Hidden Metadata )

Each LizardFS Docker volume maps 1-to-1 to a directory on the LizardFS filesystem. All directories in the [REMOTE_PATH](#remote-path) on the LizardFS filesystem will be exposed as a Docker volume regardless of whether or not the directory was created by running `docker volume create`. There is no special metadata or any other extra information used by the plugin to keep track of what volumes exist. If there is a directory there, it is a Docker volume and it can be mounted ( and removed ) by the LizardFS plugin. This makes it easy to understand and allows you to manage your Docker volumes directly on the filesystem, if necessary, for things like backup and restore.

#### LizardFS Global Trash Bin

Using LizardFS for your Docker volumes means that you now get the benefit of LizardFS's global trash bin. Removed files and volumes can be restored using LizardFS's [trash bin](https://docs.lizardfs.com/adminguide/advanced_configuration.html?highlight=trash#mounting-the-meta-data) mechanism. Note that the plugin itself has nothing to do with this; it is a native feature of LizardFS.

#### Multiple LizardFS Clusters

It is also possible, if you have multiple LizardFS clusters, to install the plugin multiple times with different settings for the different clusters. For example, if you have two LizardFS clusters, one at `mfsmaster1` and another at `mfsmaster2`, you can install the plugin two times, with different aliases, to allow you to create volumes on both clusters.

    $ docker plugin install --alias lizardfs1 --grant-all-permissions kadimasolutions/lizardfs-volume-plugin HOST=mfsmaster1 PORT=9421
    $ docker plugin install --alias lizardfs2 --grant-all-permissions kadimasolutions/lizardfs-volume-plugin HOST=mfsmaster2 PORT=9421

This gives you the ability to create volumes for both clusters by specifying either `lizardfs1` or `lizardfs2` as the volume driver when creating a volume.

#### Root Mount Option

The plugin has the ability to provide a volume that contains *all* of the LizardFS Docker volumes in it. This is called the Root Volume and is identical to mounting the configured `REMOTE_PATH` on your LizardFS filesystem into your container. This volume does not exist by default. The Root Volume is enabled by setting the `ROOT_VOLUME_NAME` to the name that you want the volume to have. You should pick a name that does not conflict with any other volume. If there is a volume with the same name as the Root Volume, the Root Volume will take precedence over the other volume.

There are a few different uses for the Root Volume. Kadima Solutions designed the Root Volume feature to accommodate for containerized backup solutions. By mounting the Root Volume into a container that manages your Backups, you can backup *all* of your LizardFS Docker volumes without having to manually add a mount to the container every time you create a new volume that needs to be backed up.

The Root Volume also give you the ability to have containers create and remove LizardFS volumes without having to mount the Docker socket and make Docker API calls. Volumes can be added, removed, and otherwise manipulated simply by mounting the Root Volume and making the desired changes.

### Known Issues

#### Hangs on Unresponsive LizardFS Master

In most cases, when the plugin cannot connect to the LizardFS cluster, the plugin will timeout quickly and simply fail to create mounts or listings of volumes. However, when the plugin *has* been able to open a connection with the LizardFS master, and the LizardFS master subsequently fails to respond, a volume list operation will cause the plugin to hang for a period of time. This will cause any Docker operations that request the volume list to freeze while the plugin attempts to connect to the cluster. To fix the issue, the connectivity to the LizardFS master must be restored, otherwise the plugin should be disabled to prevent stalling the Docker daemon.

## Configuration

### Plugin Configuration

You can configure the plugin through plugin variables. You may set these variables at installation time by putting `VARIABLE_NAME=value` after the plugin name, or you can set them after the plugin has been installed using `docker plugin set kadimasolutions/lizardfs-volume-plugin VARIABLE_NAME=value`.

> **Note:** When configuring the plugin after installation, the plugin must first be disabled before you can set variables. There is no danger of accidentally setting variables while the plugin is enabled, though. Docker will simply tell you that it is not possible.

#### HOST

The hostname/ip address that will be used when connecting to the LizardFS master.

> **Note:** The plugin runs in `host` networking mode. This means that even though it is in a container, it shares its network configuration with the host and should resolve all network addresses as the host system would.

**Default:** `mfsmaster`

#### PORT

The port on which to connect to the LizardFS master.

**Default:** `9421`

#### MOUNT_OPTIONS

Options passed to the `mfsmount` command when mounting LizardFS volumes. More information can be found in the [LizardFS documentation](https://docs.lizardfs.com/man/mfsmount.1.html).

**Default:** empty string

#### REMOTE_PATH

The path on the LizardFS filesystem that Docker volumes will be stored in. This path will be mounted for volume storage by the plugin and must exist on the LizardFS filesystem. The plugin fail to connect to the master server if the path does not exist.

**Default:** `/docker/volumes`

#### ROOT_VOLUME_NAME

The name of the Root Volume. If specified, a special volume will be created of the given name will be created that will contain all of the LizardFS volumes. It is equivalent to mounting the `REMOTE_PATH` on the LizardFS filesystem. See [Root Mount Option](#root-mount-option).

**Default:** empty string

#### CONNECT_TIMEOUT

The timeout for LizardFS mount commands. If a mount takes longer than the `CONNECT_TIMEOUT` in milliseconds, it will be terminated and the volume will not be mounted. This is to keep Docker operations from hanging in the event of an unresponsive master.

**Default:** `10000`

#### LOG_LEVEL

Plugin logging level. Set to `DEBUG` to get more verbose log messages. Logs from Docker plugins can be found in the Docker log and will be suffixed with the plugin ID.

**Default:** `INFO`

### Volume Options

Volume options are options that can be passed to Docker when creating a Docker volume. Volume options are set per volume, therefore setting an option for one volume does not set that option for any other volume.

Volume options can be passed in on the command line by
adding `-o OptionName=value` after the volume name. For example:

    $ docker volume create -d lizardfs my-volume -o ReplicationGoal=3

#### ReplicationGoal

The replication goal option can be used to set the LizardFS replication goal on a newly created volume. The goal can be any valid goal name or number that exists on the LizardFS master. See the LizardFS [documentation](https://docs.lizardfs.com/adminguide/replication.html) for more information.

Note that even after a volume has been created and a goal has been set, it is still possible to manually change the goal of the volume directory on the LizardFS filesystem manually. For example, assuming you have mounted the LizardFS filesystem manually ( not using a docker volume ):

    lizardfs setgoal goal_name /mnt/mfs/docker/volumes/volume_name

Also, if you want to set a default goal for all of your Docker volumes, you can manually set the goal of the directory containing your docker volumes on the LizardFS filesystem ( `/docker/volumes` by default, see [REMOTE_PATH](#remote-path) ).

**Default:** empty string

# Swarm Deployment

Docker Swarm is where the LizardFS plugin shows its full potential. You can deploy an entire LizardFS cluster *and* the Docker volume plugin as a single stack on you Docker Swarm. This lets you create a shared storage cluster out of any Docker Swarm. There are a few steps to prepare your hosts before launching the stack.

## Usage

### Setup Master

One node in your Swarm cluster needs to have the label `lizardfs.master-personality=master`. This is the node that the LizardFS master will be deployed on.

The master server is also expected to have a directory /lizardfs/mfsmaster on the host that will be used to store the master data. In production this should be the mountpoint for an XFS or ZFS filesystem.
Setup Chunkservers

Every node in the Swarm cluster gets a Chunkserver deployed to it. All servers are expected to have a `/lizardfs/chunkserver` directory that will be used for storing chunks. Like the master storage directory, `/lizardfs/chunkserver` should be formatted XFS or ZFS for production installations.

### ( Optional ) Setup Shadow Masters

You can optionally add the lizardfs.master-personality=shadow label to any nodes in the cluster that you want to run shadow masters on. Shadow master servers should have a /lizardfs/mfsmaster-shadow directory that is mounted to an XFS or ZFS filesystem for storage.
Deploy The LizardFS Stack

> Note: Before you deploy the stack you should make sure that you have disabled the Docker managed version of the LizardFS plugin if it is installed.

After you have provided for the storage for your LizardFS cluster, you can deploy the LizardFS stack to your Swarm cluster by downloading the attached lizardfs.yml and using docker stack deploy -c lizardfs.yml lizardfs. The particular yaml I gave you requires that the name of the stack be lizardfs.

### Deploy the Stack

After you have setup the storage directories for you Swarm cluster you deploy the stack with the following yaml.

    $ docker stack deploy -c docker-stack.yml lizardfs

> **Note:** The stack **must** be named `lizardfs` for this yaml. It is because the `docker-run-d` container has the network name `lizardfs_lizardfs` hard-codded into the yaml. Reading the ""Swarm Service Privileges Workaround"" explanation below will help explain the `docker-run-d` container.

**docker-stack.yml**
```yaml
version: '3.6'
services:
  mfsmaster:
    image: kadimasolutions/lizardfs:latest
    command: master
    environment:
      MFSMASTER_AUTO_RECOVERY: 1
    networks:
      - lizardfs
    volumes:
      - /lizardfs/mfsmaster:/var/lib/mfs
    deploy:
      mode: global
      placement:
        constraints:
          - node.labels.lizardfs.master-personality==master
  mfsmaster-shadow:
    image: kadimasolutions/lizardfs:latest
    command: master
    networks:
      - lizardfs
    environment:
      MFSMASTER_PERSONALITY: shadow
    volumes:
      - /lizardfs/mfsmaster:/var/lib/mfs
    deploy:
      mode: global
      placement:
        constraints:
          - node.labels.lizardfs.master-personality==shadow
  chunkserver:
    image: kadimasolutions/lizardfs:latest
    command: chunkserver
    networks:
      - lizardfs
    environment:
      # This lets you run the chunkserver with less available disk space
      MFSCHUNKSERVER_HDD_LEAVE_SPACE_DEFAULT: 400Mi # 4Gi is the default
      MFSHDD_1: /mnt/mfshdd
    volumes:
      - /lizardfs/chunkserver:/mnt/mfshdd
    deploy:
      mode: global
  cgiserver:
    image: kadimasolutions/lizardfs:latest
    command: cgiserver
    networks:
      - lizardfs
    restart: on-failure
    ports:
      - 8080:80
    deploy:
      replicas: 0
  docker-plugin:
    image: kadimasolutions/docker-run-d:latest
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command:
      - ""--restart=always -v /var/lib/docker/plugins/lizardfs/propagated-mount:/mnt/docker-volumes/:rshared -v /run/docker/plugins/lizardfs:/run/docker/plugins/ --net lizardfs_lizardfs --cap-add SYS_ADMIN --device=/dev/fuse:/dev/fuse --security-opt=apparmor:unconfined -e ROOT_VOLUME_NAME=lizardfs -e LOG_LEVEL=debug -e REMOTE_PATH=/docker/volumes -e LOCAL_PATH=/var/lib/docker/plugins/lizardfs/propagated-mount -e MOUNT_OPTIONS='-o big_writes -o cacheexpirationtime=500 -o readaheadmaxwindowsize=1024' kadimasolutions/lizardfs-volume-driver""
    environment:
      CONTAINER_NAME: lizardfs-plugin
    deploy:
      mode: global
  lizardfs-client:
    image: kadimasolutions/docker-run-d:latest
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command:
      - ""--restart=always --net lizardfs_lizardfs --cap-add SYS_ADMIN --device=/dev/fuse:/dev/fuse --security-opt=apparmor:unconfined kadimasolutions/lizardfs client""
    environment:
      CONTAINER_NAME: lizardfs-client
    deploy:
      mode: global

networks:
  lizardfs:
    attachable: true
```

This will deploy the Docker plugin, the LizardFS chunkserver, and a LizardFS client container on *every* host in your cluster. If you have different goals, you may want to update the scheduling rules to match your particular use case.

The stack uses @kadimasolutions's LizardFS Docker image to create the LizardFS cluster. You can modify the environment variables for the mfsmaster, mfsmaster-shadow, and chunkserver containers to completely configure your LizardFS cluster. Documetnation for the `kadimasolutions/lizardfs` docker image can be found in the [git repo](https://github.com/kadimasolutions/docker_lizardfs).

### Create Volume Storage Directory

The last step after deploying the stack is to create the `/docker/volumes` directory on the LizardFS cluster. This is the directory that the Docker volumes will be stored under.

On any host in your cluster you should be able to run the following command to create the directory:

    $ docker exec lizardfs-client mkdir -p /mnt/mfs/docker/volumes

That's it! You should now be able to create docker volumes with the `lizardfs` driver that will be shared across your Swarm cluster. Tell me if you have any success and/or failure with it! :smiley:

### Things You Should Know

Here are some things that you should know about the setup.

#### Different Container Image

The new container for deploying the plugin is actually the same software as the Docker managed plugin, but it is under a different repo on DockerHub. The plugin that you install with docker plugin install is under the kadimasolutions/lizardfs-volume-plugin repository. The plugin that you run as a standard Docker container on Swarm is under the kadimasolutions/lizardfs-volume-driver repository ( these may or may not be the final names for either ). The only difference between the two are how they are installed, otherwise they are running the same code.

#### Swarm Service Privileges Workaround

There is a limitation imposed by the Docker daemon on Swarm services that prevents them from running with admin privileges on the host. This is an issue for the LizardFS plugin container because it needs to have the SYS_ADMIN capability along with the FUSE device. In order to work around this I created a very simple container ( kadimasolutions/docker-run-d ) that uses the Docker CLI to run a container that does have privileges. This container can be deployed as a Swarm service to allow you to run privileged swarm containers. This is how the docker-plugin and lizardfs-client services are deployed in the attached yaml.

#### lizardfs-client Convenience Container

As a convenience, the stack will deploy a container named lizardfs-client on every host in your Swarm. This container mounts the root of the LizardFS filesystem to /mnt/mfs and provides the LizardFS CLI tools to allow you to manage your LizardFS filesystem. To access the tools you exec into the lizardfs-client container on any host in your cluster. For example:

    $ docker exec -it lizardfs-client bash
    root@containerid $ lizardfs setgoal 3 /mnt/mfs/docker/volumes
    root@containerid $ exit

This removes the need to install any LizardFS tools on your hosts.

### Known Issues

#### Docker Restart Issue

> **Note:** This is only a concern when using the Swarm deployment. It is not a problem when using the Docker managed version of the plguin.

When the Docker daemon is started it checks to make sure that all of your LizardFS volumes exist and it tries to connect to the LizardFS Docker plugin. Because I am running the plugin in a Docker container, the Docker daemon cannot connect to the plugin as the daemon is still starting up and the plugin container has not been started yet. Unfortunately, Docker will spend about 15 seconds timing out for each lizardfs volume before it finishes starting up. This can push your Docker daemon startup time up by several minutes if you have a lot of LizardFS volumes. After it finishes timing out for each volume, the Docker daemon starts up and everything works as you would expect.

This doesn’t cause any critical issues it just takes longer to start Docker because of all of the timeouts. Another option that I’ve speculated is to run two Docker daemons on each host in the cluster and create a dedicated Swarm cluster just for LizardFS. This would be more of a specialized setup, but I think it would still work. In the end I think that the method of deployment will depend on the individual user’s needs. Eventually I may try to test and document more deployment methods
","@3eka I'm still a little confused as to what is going on in your explanation. Couple questions:

1. Did it work in the end. :smiley:
2. What do you mean mounting has to be done on OS level?
3. I can't tell for sure, but does your last example indicate that the plugin is appending `/docker` to the `REMOTE_PATH` when it shouln't? It looks like you set the remote path to `/` and it created a volume at `/docker/guests`. That wouldn't be right. If the remote path is `/` it should create volumes right in `/[volume name]` and not in `/docker/[volume name]`.",9800119
82,Very slow write speeds,open,2018-06-18T15:55:26Z,2020-02-18T14:07:57Z,,NONE,"I am working on testing out Lizardfs. I have a setup with 8 machines that have at least 128GB of ram and 30TB of storage. Setting up the system was straight forward. However, if i write to the storage i get only about 30MB/s write speeds. I looked into the network (10GBE) and it works just fine. I used NFS over the same network and i get 400-500MB write speeds. So i thought of removing all bottlenecks and trying to think of the simplest setup to test.

I tried running everything on one machine and storing all data and metadata in a ramdisk. So the lizardfs-master, lizardfs-chunkserver are set up to store data/metadata completely in a ramdisk. Since everything is connected through localhost there should not be any networking issue as well. I can start the setup and i can use the lizardfs mount normally, but i still get only 30MB/s write speeds. I have not changed any settings beyond path of the data directories to point to a ramdisk. So it is a vanilla CentOS using EPEL repositories for lizardfs 3.12. I use a single chunkserver with a single 4GB file that i am copying from a ramdisk...  I should get speeds of several hundred MB/s, but i am getting only 30MB/s (the same as over the network). I thought there might be throttling in play, but at that does not seem to be activated. So it seems there is some thread (it looks like in the client) that slows down the write speed of a single stream dramatically.
","Ask and ye shall receive! 

This is actually {20,4}, but with plans to to some granular testing with { 18-22 , 4-7 }

===   1 file series   ===
1 file, 1 thread, seq 1M writes, simple: 256 write iops (141% HDD)
1 file, 1 thread, rnd 16k writes, simple: 72 write iops (10% HDD)
1 file, 1 thread, rnd 16k writes, simple, take 2: 186 write iops (28% HDD)
1 file, 16 threads, rnd 4k writes, posixaio: 191 write iops (54% HDD)
1 file, 16 threads, rnd 8k writes, posixaio: 221 write iops (52% HDD)
1 file, 16 threads, rnd 16k writes, posixaio: 219 write iops (48% HDD)
1 file, 16 threads, rnd 16k writes, posixaio, take 2: 216 write iops (42% HDD)
===  16 file series   ===
16 files, 1 thread each, seq 1M writes, simple: 373 write iops (248% HDD)
16 files, 1 thread each, rnd 16k writes, simple: 638 write iops (97% HDD)
16 files, 1 thread each, rnd 16k writes, simple, take 2: 650 write iops (103% HDD)
16 files, 1 thread each, rnd 16k writes, posixaio: 655 write iops (140% HDD)
16 files, 16 threads each, rnd 16k writes, posixaio: 718 write iops (133% HDD)
16 files, 16 threads each, rnd 16k writes, posixaio, take 2: 816 write iops (162% HDD)
===   O_SYNC series   ===
1 file, 1 thread, rnd 16k writes, simple, o_sync: 180 write iops (116% HDD)
1 file, 16 threads, rnd 16k writes, posixaio, o_sync: 178 write iops (73% HDD)
16 files, 1 thread each, rnd 16k writes, simple, o_sync: 643 write iops (127% HDD)
16 files, 16 threads each, rnd 16k writes, posixaio, o_sync: 644 write iops (126% HDD)
===    read series    ===
1 file, 1 thread, seq 1M reads, simple: 242 read iops (6% HDD)
1 file, 16 threads, rnd 16k reads, posixaio: 690 read iops (403% HDD)
16 files, 1 thread each, seq 1M reads, simple: 3504 read iops (86% HDD)
16 files, 1 thread each, rnd 16k reads, posixaio: 9810 read iops (2087% HDD)
16 files, 16 threads each, rnd 16k reads, posixaio: 9580 read iops (1931% HDD)
=== native aio series ===
1 file, 16 threads, rnd 16k writes, native aio: 188 write iops (30% HDD)
16 files, 16 threads each, rnd 16k writes, native aio: 627 write iops (125% HDD)
1 file, 16 threads, rnd 16k reads, native aio: 685 read iops (522% HDD)
16 files, 16 threads each, rnd 16k reads, native aio: 8602 read iops (1741% HDD)
--> Looks like these were less impacted by caching, which is good as they're likely more accurate.



I've realized that there is a bottleneck somewhere single threaded, for sure.
Single threaded / single file transfer is still hard pressed to exceed 300MB on a good run usually right around 200MB/s, however if I line up a whole bunch of single transfers (Naively using tmux) I saw iotop report over 4GB/s at peak about half way through the test of ~16 6GB files.

Not sure how to debug a slow single file transfer with LizardFS though..

 ",9800119
83,FreeBSD port,open,2018-06-16T17:01:04Z,2019-09-12T18:09:28Z,,NONE,is some work in progress to make a FreeBSD Port besides some benchmarks using ZFS?,,9800119
84,Rebalance/replication caused to fill all the disks,open,2018-05-30T06:39:27Z,2018-06-02T21:30:25Z,,NONE,"Hi again :)

My cluster contains 2 goals (ssd and hdd, both with replication):
![obraz](https://user-images.githubusercontent.com/27781296/40703092-6790784e-63e4-11e8-83fc-5a4ac6584775.png)

Few days ago, I've removed and recreated one of the chunkservers and in the same time - added one additional disk to each chunkserver. Before that, whole cluster had around 77% of HDD goal/disks usage.

The removal/recreation caused rebalance to make all the endangered chunks save, here is what I saw todays morning:

![obraz](https://user-images.githubusercontent.com/27781296/40703191-ab621ffa-63e4-11e8-98f6-0f0a2d131f32.png)
![obraz](https://user-images.githubusercontent.com/27781296/40703207-be31641a-63e4-11e8-81ce-4edc5ae6aae6.png)

As you see, there is a huge difference between chunks amount on particular servers (usually 30 million per 2TB disk). The recreated CS has 10 million and is almost full. Why? :(

There is still a lot of chunks to make safe/replicate:
![obraz](https://user-images.githubusercontent.com/27781296/40703247-d9cffe8e-63e4-11e8-9e31-8741d1aa3661.png)
![obraz](https://user-images.githubusercontent.com/27781296/40703257-e4fb97aa-63e4-11e8-8e33-f570718db07e.png)

I'm not sure what just happened.. :(

In the master server logs I see a lot of that entries:

> May 28 14:13:43 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 00000000003B2F43 replication status: Unknown LizardFS error
> May 28 14:13:43 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 0000000001D03C4D replication status: Unknown LizardFS error
> May 28 14:13:43 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 0000000001E408DF replication status: Unknown LizardFS error
> May 28 14:13:43 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 00000000040BBAA0 replication status: Unknown LizardFS error
> May 28 14:13:52 lfs-master03 mfsmaster[26330]: chunk hasn't been deleted since previous loop - retry
> May 28 14:13:53 lfs-master03 mfsmaster[26330]: (10.125.6.19:9422) chunk: 0000000001837246 deletion status: No such chunk
> May 28 20:47:10 lfs-master03 mfsmaster[26330]: chunk hasn't been deleted since previous loop - retry
> May 28 20:47:15 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 0000000002CF78E6 replication status: Unknown LizardFS error
> May 28 20:47:15 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 00000000002AC325 replication status: Unknown LizardFS error
> May 28 20:47:15 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 00000000032F78E6 replication status: Unknown LizardFS error
> May 28 20:47:15 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 0000000003E6C206 replication status: Unknown LizardFS error
> May 28 20:47:15 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 0000000004628B5F replication status: Unknown LizardFS error
> May 28 20:47:26 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 000000000291DDF8 replication status: Unknown LizardFS error
> May 28 20:47:27 lfs-master03 mfsmaster[26330]: (10.125.6.18:9422) chunk: 00000000039F8EF1 deletion status: No such chunk
> May 28 20:47:43 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 0000000002F4159C replication status: Unknown LizardFS error
> May 28 20:55:34 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 0000000002C9BAEC replication status: Unknown LizardFS error
> May 28 20:55:34 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 0000000002E9BAEC replication status: Unknown LizardFS error
> May 28 20:55:34 lfs-master03 mfsmaster[26330]: (10.125.6.21:9422) chunk: 000000000055F047 replication status: Unknown LizardFS error

Should I set the goal of all files to some single one (without replication) and then back to dual?

Thanks in advance for any help.
Regards","In case of additional NFS on single host I will loose the ""distributed"" feature of this FS and make a SPOF :( But for now it's the only way. 

Did anyone used GFS or OCFS with Lizard? ",9800119
85,Pause/cancel rebalance during some maintenance,open,2018-05-24T19:54:20Z,2018-05-29T19:15:08Z,,NONE,"Hi,
for a 10TB installation with 5 chunkservers I need to schedule maintenance to migrate some VMs. For this time, two chunkservers (one at a time) will be powered off.

All my resources are set to goal of two (like RAID1).

When LFS Master detect missing chunkserver - it will trigger rebalance to make endangered chunks safe - I don't want that for two reasons: there is no enough space on the chunkservers to handle additional data from the missing chunkserver, and it will cost some IO.

I don't want to set single goal for all resources as it will mark replicas to be deleted.

How can I achieve this?
Thanks inadvance!

Robert","I'm pretty sure the first one does. The second, I don't know. We need
someone with better knowledge of the internal workings to answer that.

2018-05-29 19:56 GMT+02:00 robson1 <notifications@github.com>:

> Thanks!
>
> AFAIK, those vars will also slow down regular chunk operations during
> normal operation:
> CHUNKS_LOOP_PERIOD
> CHUNKS_WRITE_REP_LIMIT
>
> Am I right?
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/lizardfs/lizardfs/issues/704#issuecomment-392875318>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXnE3Fg5IXpl-f5ZMpKgfjG01mGgPmwks5t3YvSgaJpZM4UM4ig>
> .
>
",9800119
86,Single Threaded Write Performance Question/Issue,open,2018-05-21T18:15:06Z,2018-06-01T12:27:31Z,,NONE,"So, there have been a few posts about performance in the past, and I have done my best to go through them and try out as many options in as many combinations that I could think of.  However, I am still seeing unusually low write performance.

I have a small cluster with 7 chunk servers, 6 of those on a single server (one per disk).  The smaller node with a single chunk server is an i7 with 32GBs of RAM, the larger with six of the chunk servers is a dual quad core Xeon with 96GBs of RAM.

Performance on disks when maxed out @ 64KB random is about 6MBps on the slowest disk and around 20MBs on the fastest.

The slower node has four disks in a single chunk server and is connected at 1Gbit.  And before I go any further, that node was added for capacity and the issue pre-dates its addition.

All disks are formated with XFS @ 4KB starting at the 2MB mark.  I originally had it formated to the defaults using the the root of the device.  I migrated each one thinking that would help (and it did, a tiny bit).

What I see is good replication and rebalanced performance, averaging around the 5-7MBps mark per disk on 7200 RPM disks.  This is what I would expect for random @ 64KB.

However, when I do writes, I am drop down to about 800KBps during writes per disk and about **4MBps for the transfer**. I have D2P1 erasure, so 4MBps / 2D * 3P / 7C = 850KBps.  All adds up so far, just slow...

Reads are FAST, even during a heavy rebalanced I can average 26MBps read, but I have seen as high as 260MBps.  

Here is the odd part, I can stack writes.  Upwards of 6 write threads and performance increases almost linearly up to about 22MBps write... Why?  What am I missing?

I have tried different goals, even a goal of 1.  By far, performance seems to increase the most as I use erasure and increase the total width.

I have experimented with several dozen mount option combos, and this is what I am using as of now:
big_writes,nodev,noatime,mfsdelayedinit,nonempty,cacheexpirationtime=1000,mfsattrcacheto=30,mfswritecachesize=2048,mfswriteworkers=128,mfswritewindowsize=15

Thoughts?  Suggestions?  Sorry, I know the “why is it slow” questions are annoying.  Not sure if I am doing something wrong, or if this is expected for some reason.

I am adding a screen shot of the bytes written performance, in the first part is a single threaded file copy, all large files (200MBs – 2GBs).  The last portion us showing a goal change (from D2P1 to D3P2).  As you can see, the goal change dwarfs the write operation in performance.  The first node is the small one over 1GB.

![screenshot_2018-05-21_10-41-36](https://user-images.githubusercontent.com/3953102/40322624-c94be75c-5ce7-11e8-9e23-5ceeb91004a4.png)
","Added ""Single file seq read direct"" to the script and released it on Github: https://github.com/Korkman/storage-tuner-benchmark",9800119
87,metadata reliability,open,2018-05-17T19:33:51Z,2018-05-18T15:54:48Z,,NONE,"Some questions about metadata reliability.

AFAIK, all metadata are kept in RAM, for performance, on master server.
Then we have one ore more shadow servers, that are synced in realtime with the master (something like MySQL master-slave replication).

So, in case of master failure, a shadow should be able to became a new master with no data loss.
By using multiple shadows, we should be able to reduce the risk of data loss (we have to loose the master and multiple shadows at the same time)

Other than this, master will hourly dump the whole metadata archive. This archive could be restored in case of disaster (obviously, we loose everything that was changed between the last dump and the crash)

Then , we also have metaloggers, that will fetch metadata from master server once per day.

To summarize, data loss (and with ""data loss"" I also mean a restore with an outdated dump) could happen if and only if: all master and shadows are lost.

If we have to restore from metalogger, data loss should be assured, because metalogger are not updated in realtime. (exactly like restoring a database from an `.sql` dump). The same should be when restoring from master automatic dump, made once per hours, right ?",Shrugs. I stand corrected. Anyway..,9800119
88,Leader election,open,2018-05-17T13:38:41Z,2019-09-12T18:10:22Z,,NONE,"During a shadow promotion, we should check which shadow has updated metadata version, avoiding to promote an outdated shadow when a newer one is available. In example, if master node is at version 1001, shadow1 is at 999 and shadow2 is at 1001, we must promote shadow2 or data loss will occur.

This is true for promotion, but what about the first run, when all master are started as shadow waiting for a leader election ? Is it safe to promote the newer shadow that we have ?",,9800119
89,livelock acquiring write cache blocks,open,2018-05-11T15:22:00Z,2018-12-17T22:54:15Z,,CONTRIBUTOR,"`dataChainSize` needs to be updated after the condition variable is signalled at https://github.com/lizardfs/lizardfs/blob/3.12/src/mount/writedata.cc#L251, otherwise it can livelock the thread.",,9800119
90,"Add ""soft"" goal",open,2018-05-11T11:14:00Z,2019-05-31T08:23:11Z,,CONTRIBUTOR,"I have a lizardfs network where some chunk servers are off-site. This is great for redundancy but not so cool for write performance, esp. when initially copying the files onto the cluster (which is my primary use case).

My ideal setup would be a marker in mfsgoals.cfg which says that new chunks only *need* to be created on servers responsible for the labels to the left of the marker; copying to the other chunk servers can then happen in the background.","On 17.05.2018 23:41, BloodBlight wrote:
>
> I am hoping that some sort of multi-level goals will be part of that.
> So you could do something like this:
>
> mfsgoals.cfg:
> 30 Site1_3D1P : $ec(3,1) {Site1 Site1 Site1 Site1}
> 31 Site2_3D1P : $ec(3,1) {Site2 Site2 Site2 Site2}
> 32 MultiSite_Site1Master : $h(Site1_3D1P) $s(Site2_3D1P)
>
Well, in this case that's basically equivalent to ""$ec(3,5) { Site1
Site1 Site1 Site1 Site2 Site2 Site2 Site2 }"" except with less
redundancy. :-P

In general though I agree, as it'd be useful to be able to store, for
example, one direct copy at site1 and one ec(5,2) copy at site2.

This ties in with the ""dynamic goals"" idea, as it would make sense to
have a staging system at site2 (so that I only need to transmit the
actual data to site2 but not the EC vectors).

Thus a dynamic goal could look like ""store one copy at site1. When
that's done (i.e. the file is closed), switch to a goal that also stores
a non-ECC copy at site2. When *that* is done, also create an EC copy.
*Then* delete the non-EC copy."" We could accomplish this by activating
the timer to switch goals when the file is closed / the previous goal is
actually reached.

-- 
-- Matthias Urlichs

",9800119
91,Kick-out disks with errors,open,2018-05-09T21:22:22Z,2018-05-18T09:05:46Z,,NONE,"MooseFS has a nice hdd check, after a X number or I/O errors in Y seconds, disks is flagged as bad and marked for removal, triggering a replication.

This is very very cool as you don't have to wait for a total failure to migrate data from. Assured that Lizard works great without any RAID, prevent catastrophic disk failure could save your day.

It shouldn't be that hard to backport from MooseFS (for a c++ programmer):
https://github.com/moosefs/moosefs/blob/5c518e96571d285b3155665152b08b68d8101354/mfschunkserver/hddspacemgr.c#L1767

it's just a matter of a simple check on a counter.","I've seen that this is already partially supported. The counter is already there: https://github.com/lizardfs/lizardfs/blob/master/src/chunkserver/hddspacemgr.cc#L980

Main difference is that Lizard is setting the folder as damaged:
https://github.com/lizardfs/lizardfs/blob/master/src/chunkserver/hddspacemgr.cc#L983

while MooseFS is setting the folder as to be removed:
https://github.com/moosefs/moosefs/blob/master/mfschunkserver/hddspacemgr.c#L1771

Anyone knows what ""folder"" is in Lizard and the meaning of `todelete`, `toremove` properties ?",9800119
92,Master transition,open,2018-05-09T20:21:23Z,2018-05-15T13:37:42Z,,NONE,"What happens by swapping the IP address between the current master and the current slave (and obviously, change the personality in config)

The slave become the new master, the old master become the new slave, but the newly-set slave (the ""old master""), wuold be able to automatically connect to the new master and sync from that ?","Yes, that works. Use case is power cuts which we have quite frequently.
I found existing free HA solutions cumbersome or requiring more than two machines and wrote a BASH script that runs on both Master and Shadow (1 Master, 1 Shadow, both identical) and it works like this:

If Shadow detects that Master is not reachable it changes its personality to Master and assigns itself the Master IP. When the former Master comes back, it detects that there's already a Master running and makes itself Shadow and assigns itself the Shadow IP, issues `mfsmetarestore -a` if necessary. 
Testing was done simply by randomly switching off power to either machine, often several times in a row. Gross latency (until client r/w resumes) is usually a few seconds up to a minute, mostly depending on how long it takes until all relevant chunkservers recognize the change (also see `MASTER_RECONNECTION_DELAY` and `MASTER_TIMEOUT`. Setting the latter to something lower than 1 caused instability even though ping is much lower than that.)
I tried to reduce this client latency by issuing arpings and similar methods to clear arp caches but couldn't reliably reproduce any improvements. It's probalby better left to the OS to handle this anyway.
Old Master machine, now a Shadow, then syncs with new Master. No problems there. Running a live VM off LFS while switching off Master is not recommended. It has worked in the past (with a very low load VM guest) but today, for the first time, it didn't and the VM crashed. 

Another more frequent use case is Master updates that require reboots. Procedure is logging in on Shadow first, making all necessary updates, rebooting it to see if there are any problems with the updates. If not, tell Shadow to reboot in say 15 minutes (or whenever, some time greater than it takes for Master updates, plus extra for boot time, etc.), then logging in Master, doing the updates and rebooting Master right after. This triggers the script that constantly checks for an available Master, switches Master and Shadow and 15 minutes later switches back again (not necessary since the machines are ""identical"" but I prefer it because of SSH key logins).

No lost chunks, done this dozens of times now either when updating or during testing phase with ""pulling the plug"" but I can't give any guarantees and would recommend doing this only when there is low load and no endangered chunks and when metadata versions of Master and Shadow are both the same.

Ideas for improvement much appreciated.",9800119
93,3.12 nfs-ganesha-acl incomplete save,open,2018-05-07T20:52:04Z,2018-05-10T07:31:54Z,,NONE,"I have a machine which exports with nfs-ganesha the lizardfs filesystem hosted on an other machine. My goal is to mount this exported filesystem with nfsv4 with kerberos authentication.

I successfully mount the filesystem on a third machine with the krb5 authentication but when I try to use the `nfs4_acl` something goes wrong.

I can use the command `nfs4_setfacl` on the file but the ACL is not correctly saved. In particular, the output of the command `nfs4_getfacl` is like the following:

```bash
$ nfs4_setfacl -a A::bar@EXAMPLE.COM:rwx foo
$ nfs4_getfacl foo
A::bar@EXAMPLE.COM:
A::OWNER@:
D::GROUP@:
A::EVERYONE@:
```

Obviously I specified some permission, but from the prevoius output it seems that the permissions are not correctly saved. I tried doing some stuff to the file but I actually have the previous permissions on that file: none.

Am I missing anything?

The nfs-ganesha server runs on CentOS Linux release 7.4.1708 with nfs-ganesha 2.5.2. The lizardfs-master server runs on Debian 9.4 with lizardfs 3.12.0

Here is the configuration of the ganesha server.

```
EXPORT
{
        ## Export Id (mandatory, each EXPORT must have a unique Export_Id)
        Export_Id = 7;

        ## Exported path (mandatory)
        Path = ""/"";

        ## Pseudo Path (required for NFSv4 or if mount_path_pseudo = true)
        Pseudo = ""/mount_point"";

        ## Restrict the protocols that may use this export.  This cannot allow
        ## access that is denied in NFS_CORE_PARAM.
        Protocols = NFSv4;

        ## Access type for clients.  Default is None, so some access must be
        ## given. It can be here, in the EXPORT_DEFAULTS, or in a CLIENT block
        Access_Type = RW;

        ## Whether to squash various users.
        # Squash = root_squash;
        Squash = no_root_squash;

        ## Allowed security types for this export
        Sectype = krb5, krb5i, krb5p;

        ## Exporting FSAL
        FSAL {
                Name = LizardFS;
                hostname = ""192.168.0.1"";
                port = ""9421"";
                io_retries = 5;
        }
               
        ## Enable ACLs
        Disable_ACL = FALSE;
}

LizardFS {
        PNFS_MDS = true;
        PNFS_DS = false;
}

NFSV4 {
        DomainName = ""EXAMPLE.COM"";
        Lease_Lifetime = 90;

NFS_KRB5 {
        PrincipalName = ""nfs/hostname.example.com"";
        KeytabPath = ""/etc/krb5.keytab"";
        Active_krb5 = true;
}
```

The mount on the client is done with the following command

```bash
mount -t nfs -o proto=tcp,port=2049,nfsvers=4 example.com:/mount_point /mnt/mount_point
```",I am curious about the outcome of this issue.,9800119
94,3.12.0 - samba ping_pong I/O coherence data does not increment,open,2018-05-03T23:55:02Z,2018-05-08T22:49:51Z,,NONE,"I'm currently testing lizardfs 3.12.0 from epel repository as a storage backend for a samba cluster on CentOS 7.

The file system has been mounted with `mfsmount -o enablefilelocks=1 /mnt/lizardfs/` on both clients.

I've been running the following command simultaneously on two clients:

`ping_pong -rw /mnt/lizardfs/test.lock 3`

This results in the following output:

`data increment = 1`
`762 locks/sec`

locks/sec decreases but data increment never changes to 2. I've also tested this with version 3.11.3 but the results were the same.

Is lizardfs currently supposed to pass the I/O coherence ping_pong test?","I've just tested this with the suggested tweak `echo DirectIO=true > /mnt/lizardfs/.lizardfs_tweaks` from #689  and data now increments but locks/sec goes really low:

`data increment = 2`
`1 locks/sec`",9800119
95,3.10.4 - Slow Write Performance,open,2018-04-23T02:45:49Z,2018-06-17T20:24:49Z,,NONE,"I have a cluster set up with 3.10.4 and slow write performance.

First, I'm running Debian Jessie, and 3.10.4 is the newest version in the repos, including in the backports, making upgrading a challenge.  I'm open to upgrading to a newer version, but would rather do it to solve an known problem (i.e. a known ""bug"" in 3.10.4 that is fixed in a newer version) rather than ""try upgrading and see if that works"", since upgrading is non-trivial.

Here's the setup:

1 server as master and chunkserver
1 server as shadow and chunkserver

Both servers have 2 120GB SSDs in RAID 1 as root and where metadata is stored.  They also each have 3 8TB HDDs dedicated to chunkservers.  Individually, I can write to the drives at >200MB/sec consistently.  This has been tested with dd by creating files of different sizes (1GB, 5GB, and 10GB), and rsyncing a file from root also of each of those sizes.  However, writing to LFS root using either of those methods gives me 25-40MB/sec.  Reading from LFS is very fast, >300MB/sec.

The network is gigabit, and the two servers are connected to the same switch.  When writing to LFS, iftop shows that the network link isn't anywhere near saturated.  However, in an attempt to rule out the network, I created a directory on the master/chunkserver with a goal set to 1, then I stopped the chunkserver services on the second server so that I could make sure that all writes to LFS were local.  The results were the same.  I've also fsck'd all drives to make sure the underlying filesystem is good.  I'm running EXT4 (thinking of moving to XFS for a little speed improvement, though from what I've read that probably isn't the underlying issue).

I've tried a few options with mfsmount (like big_writes and noatime) with no changes.  I've read through [this lengthy thread](https://github.com/lizardfs/lizardfs/issues/659) and tried a few things there with no impact.  The OP seems to have solved his issue (getting ~100MB/sec for writes, I'd be super happy with that), and I tried the configs that he posted, but still to no avail.

I have another setup that's basically identical, except that I'm using 1TB SSDs and running 3.9.4.  Of course it runs faster because it's SSD, but the key here is that writing to LFS and writing to the SSDs directly provides almost identical performance.. it's close to 1:1, vs. the 1:8 or so that I'm getting on my HDD cluster.

During these tests, top doesn't show any particularly high load on the CPU, RAM usage, or I/O Wait.  There's no other load on the servers while testing.  So I'm having difficulty narrowing down why this slowness exists.",@guestisp see https://github.com/Korkman/storage-tuner-benchmark,9800119
96,"Add ""choose one of"" goal target",open,2018-04-22T18:43:25Z,2018-04-22T19:55:23Z,,CONTRIBUTOR,"It would be wonderful if, in each goal target, there could be a way to specify multiple labels, only one of which is chosen to store the data.

**Use case**: I have two servers (A, B) with not too much storage.
For downtime reasons (they don't have the same functionality) I already store some data on only one server (with label respectively A and B).

However I should now store some data which is not critical (and may be stored on one or the other server). The total capacity of each server is below the size of such data, but their total capacity well accomodates the load. I would like to have a goal such as *either store this on A or on B* (`oneof: [A B]`).

**Proposed semantic**:
```
# Goal 1: Two copy on two different servers of first group
1 first_group: [A B C] [A B C]

# Goal 2: Two copy on two different servers of second group
2 second_group: [D E F] [D E F]

# Goal 3: One copy on first group, another on second group
3 mixed: [A B C] [D E F]
```

**Possible Problems**: choose an assignment of chunkservers matching the constraints if they exists
e.g. in case `$ec(4, 1) { [A D] [B E] [C F] [A B C] [D E F] }` it is not straightforward to extract which possible assignments are allowed, if (for example) each label only has one chunkserver.
Obviously the master could just enumerate over all possible choices and select the ones applicable.

Thank you!",Self note. Find and link related issues.,9800119
97,Distantly client from master - different data centers,open,2018-04-17T12:41:12Z,2018-05-01T10:10:49Z,,NONE,"Hi,

I'm running test setup with LizardFS in two data centers (A and B), the typical latency between them is ~10ms. The setup is the same in both DC: master (primary in DC A, shadow in DC B), chunkserver, client. I know that I can configure topology to force clinet B to talk to chunkserver B to improve performance. But what about communication between master and client? Because right now client B is talking to master in DC A and it dramatically slows down communication.

My questions:
- is there any way to improve this setup?
- does client B can talk to shadow server which is in DC B?

After tests, I wanna go with a similar setup to large scale but first I have to solve the geo-communication problem, so any input / concept will be appreciated.","@borkd to be honest, I thought there are enough details in this thread already to start a meaningful conversation (despite the LFS version I'm running of course).
But in regards to your questestions:
* I have no idea if this is a bug or an issue with the code, so basically I'm starting with the question, if there might something be wrong with my setup.
* I read the documentation and tried different settings to remedy the problem, but nothing helped (chunk-sync and prefer local chunks to be precise).
* My expectation is/was, that LFS should have no problem in regards to using the master cross-location. Especially if the latency is in the range of 0,3ms-1ms. And as this latency was no problem, when using NFS with DRBD, and the client mounting the storage cross-location.
* Currently I'm running 3.10.4 from jessie backports (for all parts involved); I didn't see any relevant change in the releases after that version. And as said someone I know, running MooseFS in a very similar setup does not experience these issues.
* As said, my expectation was, that it should be irrelevant, if I mount LFS cross-location. At least the performance might be impacted, but not with a factor of 4x or even worse.
* Also as said, I tried:
  * configuring the master to prefer a local chunk (I mainly run 4 nodes, each having a master and a chunk, but also in a setup of separated masters/chunks the problem exists)
  * configuring topology, so that the master would respond with a local chunk, so a client from DC-A would get chunks from DC-A. Afterwards the client only had connections to local chunks, but the performance issue was the same, leading me to the conclusion that the client/master traffic being cross location is the problem.
  * configuring the chunks not to perform fsync, didn't change anything in regards to my issue.

I hope I could add more valuable information, to find out if this is an issue in my setup, something that can be fixed with configuration or an actual bug.
If I can do something to help debug it, just tell me what you need me to do.",9800119
98,liz_setxattr reference undefined in liblizardfs-client.a (but there is its mangled name),open,2018-04-14T13:40:00Z,2018-04-14T13:40:00Z,,CONTRIBUTOR,"__Problem__: When trying to link with `liblizardfs-client.a` (v3.12.0) a source file using the function `liz_setxattr` exported from `src/mount/client/lizardfs_c_api.h` the linking fails with error `reference not defined`

__Some Data__:
* I'm using the package `lizardfs-lib-client` from lizardfs repos version 3.12.0
* Running on Debian Stretch with standard GNU compiler c++ 7.3.0

__Some Thoughts__: What follows is a little analysis of the problem:
```
$ nm -g /usr/lib/liblizardfs-client.a | grep xattr
0000000000006320 T _ZN8lizardfs6Client11removexattrERKN12LizardClient7ContextEjRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
0000000000005810 T _ZN8lizardfs6Client11removexattrERKN12LizardClient7ContextEjRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERSt10error_code
0000000000005ae0 T _ZN8lizardfs6Client8getxattrERKN12LizardClient7ContextEjRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
0000000000005900 T _ZN8lizardfs6Client8getxattrERKN12LizardClient7ContextEjRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERSt10error_code
0000000000005550 T _ZN8lizardfs6Client8setxattrERKN12LizardClient7ContextEjRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKSt6vectorIhSaIhEEi
0000000000005430 T _ZN8lizardfs6Client8setxattrERKN12LizardClient7ContextEjRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKSt6vectorIhSaIhEEiRSt10error_code
0000000000005ff0 T _ZN8lizardfs6Client9listxattrERKN12LizardClient7ContextEj
0000000000005e10 T _ZN8lizardfs6Client9listxattrERKN12LizardClient7ContextEjRSt10error_code
0000000000002550 T liz_getxattr
0000000000002680 T liz_listxattr
0000000000002760 T liz_removexattr
00000000000023d0 T _Z12liz_setxattrP3lizP11liz_contextjPKcPKhmi
                 U _ZN8lizardfs6Client11removexattrERKN12LizardClient7ContextEjRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERSt10error_code
                 U _ZN8lizardfs6Client8getxattrERKN12LizardClient7ContextEjRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERSt10error_code
                 U _ZN8lizardfs6Client8setxattrERKN12LizardClient7ContextEjRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKSt6vectorIhSaIhEEiRSt10error_code
                 U _ZN8lizardfs6Client9listxattrERKN12LizardClient7ContextEjRSt10error_code
```
As you can see, while the symbols `liz_getxattr`, `liz_listxattr` and `liz_removexattr` appear with their C names, `liz_setxattr` appear with its mangled C++ name.

Looking at source code, I can see that in `src/mount/client/lizardfs_c_api.cc` the function `liz_setxattr` implementation is:
https://github.com/lizardfs/lizardfs/blob/3bde7fe2fd15e1c91e0364e995cd442114b9d1c1/src/mount/client/lizardfs_c_api.cc#L715-L716
while in `src/mount/client/lizardfs_c_api.h` the function declaration is:
https://github.com/lizardfs/lizardfs/blob/3bde7fe2fd15e1c91e0364e995cd442114b9d1c1/src/mount/client/lizardfs_c_api.h#L672-L673
Please notice that their last argument is of different type.
I think this induces the C++ compiler to export said function in C++ mangled style instead of plain C export as declared in the header file.

__Suggested Fix__:
1. Change the last argument type of one of the two functions (I don't know which one is the ""right"" one).
2. (Optional) Add a test that tries to compile `src/data/liblizardfs-client-example.c` (and add in the source code some example calls to `setxattr`, `getxattr`, `listxattr` and `removexattr`).

Thank you for your time and the good work.
Hope it will be fixed soon, given the amount of time it requires :)",,9800119
99,XOR on a full cluster writes empty files,open,2018-04-10T13:23:45Z,2018-04-10T13:23:45Z,,NONE,"This is the answer to the question I raised on issue  #680 ""what happens on a real cluster when all chunk servers are full but one?"" 
I've opened a new ticket because this is, in my opinion, a more serious problem.

I have 3 chunk servers, one with 2 disks and two with one disk, all the same size. I've added a directory TestXOR with goal xor2 (16 default_xor2 : $xor2 ) and write to it until it was full.

```
[root@lizardfs-master mfs]# ls -l
drwxr-xr-x. 2 root root 14889 Apr 10 13:41 TestXOR
[root@lizardfs-master mfs]# lizardfs getgoal *
TestXOR: default_xor2
```

dd failed when writing the last file with:
```
dd: error writing ‘file16’: Invalid argument
dd: closing output file ‘file16’: Invalid argument
```
And keep on failing on new write attempts.

But further writes did not fail and empty files are written as it happen in issue #680. 
```
[root@lizardfs-master mfs]# echo testXOR > TestXOR/testXOR
[root@lizardfs-master mfs]# cat TestXOR/testXOR
[root@lizardfs-master mfs]# 
```
Creating a text file with a editor also created a empty file.

",,9800119
100,master server: distributed metadata setup,open,2018-04-02T15:38:14Z,2018-04-03T08:45:57Z,,NONE,"In this FOSDEM [PDF/slide show](https://fosdem.org/2018/schedule/event/lizardfs/attachments/paper/2458/export/events/attachments/lizardfs/paper/2458/FOSDEM_SDS_Devroom_Presentation_2018.pdf), on slide # 47, it mentions:

> new metadata server: New fully consistent/coherent write algorithms allowing for a distributed metadata setup across multiple servers.

Is this referring to scaling out the master server so that the master is no longer restricted to the RAM in one server? If so, when might we see this released?
","Hello!
Yes, this is the basic idea - with the latest hi speed local storage solutions like NVMe, etc. it seems to be feasible to cache the most accessed metadata and keep the rest outside. The other consideration are remote nodes of LizardFS with (potentially) unstable internet connection, this kind of a setup seems to gain popularity. 

We were planning this solution for the current calendar year, but it seems like we are at the moment quite understaffed for the whole rewrite plan, so this plan will need to be revised in Q3. 
Please do not hold your breath for now ;)",9800119
101,New shadow server join into cluster，but load medata failed；,open,2018-03-28T07:36:46Z,2019-10-08T07:46:30Z,,NONE,"when add a new shadow server to cluster，master process failed with the fllowing error：

“
[ OK ] configuration file /etc/mfs/mfsmaster.cfg loaded
[ OK ] changed working directory to: /MFS
[ OK ] lockfile /dev/shm/.mfsmaster.lock created and locked
[WARN] sessions file /dev/shm/sessions.mfs not found; if it is not a fresh installation you have to restart all active mounts
[ OK ] initialized exports from file /etc/mfs/mfsexports.cfg
[ OK ] initialized topology from file /etc/mfs/mfstopology.cfg
[ OK ] initialized goal definitions from file /etc/mfs/mfsgoals.cfg
[WARN] no charts data file /dev/shm/stats.mfs - initializing empty charts
[....] connecting to Master
[ OK ] master <-> metaloggers module: listen on *:9419
[ OK ] master <-> chunkservers module: listen on *:9420
[ OK ] master <-> tapeservers module: listen on (*:9424)
[ OK ] main master server module: listen on *:9421
[ OK ] open files limit: 60000
[ OK ] mfsmaster daemon initialized properly
mfsmaster[25827]: connected to Master
mfsmaster[25827]: metadata downloaded 40518681926B/107.510277s (376.882 MB/s)
mfsmaster[25827]: changelog.mfs.1 downloaded 6289B/0.001188s (5.294 MB/s)
mfsmaster[25827]: changelog.mfs.2 downloaded 15863B/0.001205s (13.164 MB/s)
mfsmaster[25827]: sessions downloaded 717B/0.001170s (0.613 MB/s)
mfsmaster[25827]: opened metadata file /MFS/metadata.mfs
mfsmaster[25827]: loading objects (files,directories,etc.) from the metadata file


mfsmaster[25827]: loading names from the metadata file
**Profiling timer expired**

”","> Oct  8 14:26:04 hostname mfsmaster[7221]: calculating checksum of the metadata
Oct  8 14:26:25 hostname mfsmaster[7221]: metadata file /var/lib/mfs/metadata.mfs read (149492818 inodes including 3398851 directory inodes and 143303556 file inodes, 151507041 chunks)
Oct  8 14:26:25 hostname mfsmaster[7221]: running in shadow mode - applying changelogs from /var/lib/mfs
Oct  8 14:26:28 hostname mfsmaster[7221]: /var/lib/mfs/changelog.mfs.1: 1460881 changes applied (40274216674 to 40275677554), 0 skipped
Oct  8 14:26:28 hostname mfsmaster[7221]: /var/lib/mfs/changelog.mfs: 23911 changes applied (40275677555 to 40275701465), 624880 skipped
Oct  8 14:27:32 hostname mfsmetalogger[40260]: sessions downloaded 4392B/0.000825s (5.324 MB/s)
Oct  8 14:28:14 hostname mfsmaster: 10/08/19 14:28:14.487 [warning] [7190:7190] : can't find process to terminate
Oct  8 14:28:14 hostname mfsmaster: can't find process to terminate

service was crashed when try to load metadata to enable shadow mode.
",9800119
102,XOR on a single chunk server writes empty file,open,2018-03-22T11:52:16Z,2018-05-04T07:49:14Z,,NONE,"I'm testing a single master with a single chunk server.
When I set a directory to XOR2 or XOR3, impossible to fulfil with one chunk,  I have no error on write but the file is empty. Shouldn’t this fail or accept the write in degrade mode. What am I missing and what happens on a real cluster when all chunk servers are full but one?

[root@lizardfs-master mfs]# ls -l
total 0
drwxr-xr-x. 2 root root 0 Mar 22 09:56 Test1
drwxr-xr-x. 2 root root 0 Mar 21 16:50 Test2
drwxr-xr-x. 2 root root 0 Mar 22 11:18 TestXOR3

[root@lizardfs-master mfs]# lizardfs getgoal *
Test1: 1
Test2: 2
TestXOR3: default_xor3

[root@lizardfs-master mfs]# echo test > Test1/test1
[root@lizardfs-master mfs]# cat Test1/test1
test
[root@lizardfs-master mfs]# echo test2 > Test2/test2
[root@lizardfs-master mfs]# cat Test2/test2
test2
[root@lizardfs-master mfs]# echo testXOR > TestXOR3/testXOR
[root@lizardfs-master mfs]# cat TestXOR3/testXOR
[root@lizardfs-master mfs]# (empty)


","Hi,

It is not strictly a bug in bash, but a design choice of Unix command
interpreters to NOT check fclose() for errors, I understand not to hamper
possible further jobs that might follow.
It ""works"" the same way for all filesystems, but for a distributed FS it is
more likely to mess with the files due to the lower overall speed and more
possible issues with network, etc. Or like in your example - improper
settings.

A workaround I used in the past was a simple executable that was reading
stdin and saving to a file with proper error handling.

On 4 May 2018 at 00:16, Korkman <notifications@github.com> wrote:

> Setting a goal > chunkservers causes writes to be lost, with mfsmount
> logging smth. like this:
>
> mfsmount: write file error, inode: 840779, index: 0 - error sent by master server (No space left) (try counter: 1)
> mfsmount: error writing file number 840779: No space left
>
> In my case, I set the file to $ec(27,3), with only 11 servers available.
> Interestingly, bash will exit 0 for a simple ""echo"" redirection, but
> writing with dd will properly fail when closing the file:
>
> # dd if=/dev/urandom of=/data/lizardfs/test/massiveundergoal/testfile bs=100 count=1
> dd: closing output file '/data/lizardfs/test/massiveundergoal/testfile': Invalid argument
>
> So this seems to be a bug in bash (also tested: dash) not checking
> fclose() for clean exit?
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/lizardfs/lizardfs/issues/680#issuecomment-386453736>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AV8z7CcQgJ8R-SfYvtc9e1lX-z0yc8Gyks5tu4HFgaJpZM4S26T0>
> .
>
",9800119
103,qemu native integration,open,2018-03-21T08:00:29Z,2019-09-12T18:06:54Z,,NONE,"Any update about qemu native integration ti totally by-pass fuse?

Is something expected this year or no?","Hello!
This is something we would really like to push further, but no manpower to do this at the very moment. Right now, with the separate library (circumventing FUSE entirely) and C bindings it should be much easier than before and we would be extremely happy to see the community looking at this, manageable, enhancement.",9800119
104,Question: How are small files stored?,open,2018-03-14T23:01:34Z,2018-08-06T18:20:33Z,,NONE,"Searched but didn't find any answer:
How are small files (i.e. smaller than the chunk sizes which is 64MiB) stored? Will small files be combined together and saved into one chunk files?

We are storing hundreds of millions of small files (from 100 Bytes to 30KB) in a NFS and heavily accessing most of them. We are suffering the ""lots of small files"" problem right now. Is Lizardfs working well in this case?","Where is the read ahead code?  One of the problems I have with commodity disks is the read ahead hardware cache of the disks is small.  For example WD Red 3/4T commodity only have 64 MB cache in the first place --- i.e., one chunk.  So the software cache code in Lizard is still relevant where we clearly want to cache multiple chunks at least for read purposes.

Predictive techniques are interesting with the chunk abstraction, rather than the disk at the block layer.  Clearly with size(f) > size(chunk) we are trying to reference multiple chunks in parallel and caching may not provide much help unless parallelism is less than the number of necessary chunks.  In any event...

is hdd_readahead.(cc|h) in chunkserver the right location?

Oh yes just for information, I have great success with Arm chunkservers -- including georeplication.  Arm devices (server class in the data center) and Arm (not RPi but those which can keep up with sata III like odroid XU4 or M1 or a well designed RK3399 based board).  I'm looking forward to a SAS controller on Arm now that PCI bus is supported on newer chip sets.  For us, using Lizard has been a BIG STEP UP from Ceph in terms of stability and maintainability.
",9800119
105,mfsmount's stderr is thrown away,open,2018-03-14T10:14:32Z,2019-05-31T08:05:16Z,,CONTRIBUTOR,"Redirecting stderr to /dev/null is not a good idea.

Could we default to forking a logger which forwards it to syslog, when mfsmount runs in the background?","Redirecting stderr to `/dev/null` is normal if it comes to implementing daemons.

See:
- [`daemon(7)`][3] manual
  Quote:
  > Close all open file descriptors except standard input, output, and error (i.e. the first three file descriptors 0, 1, 2). (...) In the daemon process, **connect `/dev/null` to standard input, output, and error**.
- [*Creating a daemon in Linux*][1] (my SO answer)
- [*Daemonizing Processes and System Log*][2]

Note that handling error messages from the daemon process [is handled][4] [by][5] [`spdlog`][6] wrappers (i.e. [`lzfs_pretty_syslog`][7]). Those wrapper functions don't use `stderr` because using `stderr` is always closed in any daemon.

[1]: https://stackoverflow.com/a/38818264/1321680
[2]: https://fabiobaltieri.com/2011/09/30/daemonizing-processes-and-system-log/
[3]: http://man7.org/linux/man-pages/man7/daemon.7.html
[4]: https://github.com/lizardfs/lizardfs/blob/master/src/mount/fuse/main.cc#L192
[5]: https://github.com/lizardfs/lizardfs/blob/master/src/mount/fuse/main.cc#L221
[6]: https://github.com/gabime/spdlog
[7]: https://github.com/lizardfs/lizardfs/blob/master/src/common/slogger.h#L121",9800119
106,mfsmount crashed (possibly ACL related),open,2018-03-14T10:13:15Z,2019-04-17T09:54:53Z,,CONTRIBUTOR,"Trying to copy a file to lizardfs via Nautilus. The process hangs after the first file, requiring me to `killall -9 mfsmount`.
strace-ing `mfsmount`reveals:
```
[pid 93486] write(2, ""terminate called after throwing an instance of '"", 48) = 48
[pid 93486] write(2, ""richAclConverter::ExtractionException"", 37) = 37
[pid 93486] write(2, ""'\n"", 2)          = 2
[pid 93486] write(2, ""  what():  "", 11) = 11
[pid 93486] write(2, ""Buffer too short for ACL header"", 31) = 31
[pid 93486] write(2, ""\n"", 1)           = 1
```
","@smurfix I know that you reported this bug year ago, but do you remember:
- Which version of LizardFS did you use at that time?
- Were you copying the file:
    - within LizardFS share?
    - from client to mounted share?
    - from mounted share to client?
- I assume that the file you were moving had some ACL. Any particular type of the ACL?
- If you are still using LizardFS (which version?) can you reproduce the bug?

I was not able to reproduce the bug on `3.13.0~rc1`.",9800119
107,3.12.0: data corruption in apt-cacher-ng,open,2018-03-10T05:10:47Z,2018-12-17T22:37:48Z,,MEMBER,"Ever since upgrade to 3.12.0 my apt-cacher-ng is malfunctioning: installing or upgrading packages is no longer possible because downloaded (.deb) files fail integrity check. Changing cache location from LizardFS mount to local file system fixed the problem. 3.11.3 (and all previous releases back to 2.6) never had this problem.

I don't have synthetic reproducer for this problem yet and I am very worried about integrity of the data. I've tested integrity of some old files and found no problems so corruption must be happening on write.

Please investigate this as a highest priority.","Thank you, Pawel, for the summary of acng issues. It helped us to resolve similar problem with corrupted files (despite we do not use LizardFS). We haven't found any proofs but our impression about acng is that they prioritize serving files and may not complete caching. There is a maintenance cron job (in Ubuntu 16.04 daily) ""acngtool maint"" that fixes incomplete/corrupted files in cache. Therefore, we execute /etc/cron.daily/apt-cacher-ng for data integrity.",9800119
108,mfsmount high cpu usage,open,2018-03-09T12:24:57Z,2018-03-26T15:59:01Z,,NONE,"Hi,

Thanks for reading. First, let me give you some data:

Environment (client):

```
KVM OpenStack
2 vCore(s) Intel Haswell or better
2,4 GHz
8 GB RAM
5Gbit network
```

LizardFS share mounted with:

`mfsmount -o big_writes,nosuid,nodev,noatime /mnt/lizardfs`

LIzardFS daemons using default config and EC 2+1 goal.

Command that reproduces the issue:

`fio --filename=/mnt/lizardfs/random --direct=1 --rw=randrw --refill_buffers --norandommap --randrepeat=0 --ioengine=libaio --bs=8k --rwmixread=100 --iodepth=16 --numjobs=16 --runtime=600 --group_reporting --name=8ktest`

I'm experiencing a very high CPU usage (165% mfsmount, almost 0% idle) on clients that make them almost unresponsive during the above test. The same test performed over local non-root disk consumes almost 0% CPU.

Below are the fio test results. I don't think they are astonishing, so such CPU usage is justified.

What I'm doing wrong?

Thank you very much.

```
8ktest: (g=0): rw=randrw, bs=8K-8K/8K-8K/8K-8K, ioengine=libaio, iodepth=16
...
fio-2.2.10
Starting 16 processes
Jobs: 16 (f=16): [r(16)] [100.0% done] [40935KB/0KB/0KB /s] [5116/0/0 iops] [eta 00m:00s]
8ktest: (groupid=0, jobs=16): err= 0: pid=27055: Fri Mar  9 12:22:17 2018
  read : io=1198.6MB, bw=40897KB/s, iops=5112, runt= 30010msec
    slat (usec): min=39, max=142403, avg=3095.11, stdev=3392.13
    clat (usec): min=3, max=494763, avg=46800.06, stdev=18254.76
     lat (msec): min=1, max=544, avg=49.90, stdev=19.03
    clat percentiles (msec):
     |  1.00th=[   25],  5.00th=[   29], 10.00th=[   31], 20.00th=[   35],
     | 30.00th=[   38], 40.00th=[   40], 50.00th=[   43], 60.00th=[   46],
     | 70.00th=[   50], 80.00th=[   56], 90.00th=[   67], 95.00th=[   79],
     | 99.00th=[  116], 99.50th=[  135], 99.90th=[  190], 99.95th=[  210],
     | 99.99th=[  334]
    bw (KB  /s): min=   14, max= 3760, per=6.25%, avg=2556.38, stdev=408.67
    lat (usec) : 4=0.01%, 10=0.01%, 50=0.01%
    lat (msec) : 2=0.01%, 4=0.01%, 10=0.02%, 20=0.06%, 50=70.24%
    lat (msec) : 100=27.84%, 250=1.79%, 500=0.02%
  cpu          : usr=0.24%, sys=0.48%, ctx=243430, majf=0, minf=151
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=99.8%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=153413/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=16

Run status group 0 (all jobs):
   READ: io=1198.6MB, aggrb=40896KB/s, minb=40896KB/s, maxb=40896KB/s, mint=30010msec, maxt=30010msec
```

",Any help or hint would be greatly appreciated.,9800119
109,restore ability to read inode in parallel,open,2018-03-02T13:46:18Z,2018-03-02T13:46:18Z,,CONTRIBUTOR,"If I understood correctly, following https://github.com/lizardfs/lizardfs/commit/149b9dc89fcd28e5782742d30dcf02ee1848aca6 each mount allows only one reader of an inode.

It would be great to be able to read one inode in parallel again, since in our use (HPC) its very common to have parallel readers of a large file, that are reading different parts of the file.",,9800119
110,Questions about data resiliency (Can I be unblocked by guestit please),open,2018-02-25T23:24:02Z,2018-03-01T03:17:25Z,,NONE,"Would love to reply but I think https://github.com/guestisp GUEST.it still has be blocked. ;)

Per https://github.com/lizardfs/lizardfs/issues/534#issuecomment-368339873 @asbai asked:

> I've some questions about this thread:
> 1. The replication between master and shadow is asynchronized, right? Is there a way to change it to a full synchronized form? For example: use some external sync tool like DRBD to replicate the master's disk to a salve node, and auto start the ""Master"" daemon on the slave node when the master node is down?
So, the real question is: Will the master flush related updates to the disk before it return the ACK to client? If yes, then we could use some external tools to achieve the full sync semantic.

No, properly implemented LizardFS would never deliver stale data from old chunks. The master know the chunk revisions and the chunkservers have checksums I believe. You will never get ""obsolete data"".

Now can someone design a multi-master cluster to split brain or regress in time, sure. Can one recover from such a thing, perhaps.. In my experience you have to work hard to break LFS data consistence or do something else wrong to self induce a problem... ;)

I believe LFS is ACID (Atomicity, Consistency, Isolation, Durability) compliant? Like any good filesystem must be.

Sort of wish there was an 'old_chunk_retention and lfstool function for the metadata...

All Shadow-Masters follow the acting masters, as seen in the changlog.mfs files.  A metalogger service does only the reception of periodic metadata.mfs binary memory dumps a follows the changelog.mfs which are streamed to disk in realtime like any other log.  This is the sequential changeset for the filesystem metadata.  When dealing with shadow masters and HA the acting master might only dump and flush it's metadata to disk just once per day or less.  Since it's copy of the metadata will become invalid upon it's demotion when another shadow master is promoted.  By default a master only dumps metadata to disk hourly and is continuously writing a changelog.mfs between dumps.

> 2. like @psarna said: ""Yes, ACK from chunkserver is sent after all chunkservers stored the chunk."". If I've a goal=3, and one of the chunck server node is down (by a network partition or hardware failure. i.e. there are only two replicas of three can be reachable at that given time), then what will happen when the client try to read or write this chunk?
> What will happen if only one of the three replicas can be reached? Is there a quorum algorithm to ensure only the majority part can continue to access the data when a network partition occurred?

There is no quorum, there is only ever one master.  Old chunk files are no longer valid, but it would be neat to be able to leverage them via a metadata tool like the trash.  Unless, per https://github.com/lizardfs/lizardfs/blob/master/src/data/mfsmaster.cfg.in#L266
> Minimum number of required redundant chunk parts that can be lost before
> chunk becomes endangered ## (Default: 0)
> REDUNDANCY_LEVEL = 0

You can be more strict and require more than one chunk be required to ACK a write, but I think you only need one to be successful by default.  You only need one though because that one copy will be the only valid one and any other old copies will not be offered to clients.


> 3. Like the question 2, what if I'm using EC replica? For example: If I'm using a 8+4 EC scheme, then:
> 3.1. Could I let the fragments uniformly distributed to three different data center (4+4+4), so data still accessible even if any one DC fails?
> 3.2. Like the question 2, Will the EC replicas using a quorum algorithm to achieve the strong consistency and partition tolerance with care of the high availability?

I do not use EC yet, so I can not speak from experience.  But it should be comparable to normal goals.  You can tolerate faults up to however much standard goal or EC parity you have as redundancy.  If by data centers you mean distant high latency replicas then EC is perhaps over complicating an already difficult setup.  What is a ""DC"", I do not believe LizardFS has any such thing.  What quorum algorithm, where do you think LFS leverages quorum exactly, I do not believe that there is any such native mechanic?
",Here is a comment with a list of links to other HA related discussions that might be helpful https://github.com/lizardfs/lizardfs/issues/598#issuecomment-334202503,9800119
111,3.12.0: replication error 'No such chunk',open,2018-02-13T12:55:15Z,2018-03-23T13:17:44Z,,MEMBER,"One chunkserver's log is flooded with 

~~~~
mfschunkserver: Received invalid response for chunk get block
mfschunkserver: replication error: Status 'No such chunk' sent by chunkserver (server 192.168.0.250:9422)
~~~~

logged every few seconds on fully replicated cluster...","Hello @onlyjob,

What is the situation now? Did it repeat?

Thanks,

Pawel Kalinowski",9800119
112,Is there a way to limit the amount of memory used by a chunkserver? ,open,2018-02-09T18:46:10Z,2018-02-09T18:50:15Z,,NONE,"I'm trying to replace a Linux software raid's running on an old duo core computer, with just 4GB of RAM. 

Since in our studio, with 50TB of data mfsmaster is using less than 3GB of ram, I thought 4GB would suffice for my home server. 

The problem I'm having is that I'm using Raspberry Pi's to run the chunkservers with USB disks attached to then (keep in mind... it's a home server, so USB2 data transfer is fine!)

Since a rpi have only 512mb (I'm using the old ones I have lying around) of RAM, the chunkserver's can grow the memory usage way above that, depending of the disk it's attached to. 

I've tried to limit the memory for the process using ulimit, but that just makes the chunkserver process crash once it gets to the limit. 

I've notice that those 2 options are directly connected to the amount of memory a chunkserver uses: 
NR_OF_NETWORK_WORKERS = 1
NR_OF_HDD_WORKERS_PER_NETWORK_WORKER = 1

If I increase either of then, it does allocate much more memory. But even having booth at 1, it's not enough to keep the memory usage lower.

So, is there some specific configuration for the chunkserver to, either limit the maximum amount of memory used, or at least decrease it the most possible?



cheers...
-H


",,9800119
113,when will the lizardfs-uraft release,open,2018-02-08T14:20:55Z,2018-04-01T08:13:21Z,,NONE,"#634 states never. But I was on FOSDEM 2018 and there was a mention about releasing it:
https://video.fosdem.org/2018/H.2213/lizardfs.webm at 37.04

Also in the presentation for 2018 Roadmap : https://fosdem.org/2018/schedule/event/lizardfs/attachments/paper/2458/export/events/attachments/lizardfs/paper/2458/FOSDEM_SDS_Devroom_Presentation_2018.pdf

@cypromis are those plan still valid? Where to look for the source code?",@pkali It's great to know that you are going to release version 3.13 by mid April along with uRaft.. Cannot wait to get it for testing. I think it will be a great step to make LizardFS the ultimate SDS in current open source SDS arena. ,9800119
114,"lizardfs becomes slow, unstable when growing and with replication",open,2018-02-05T11:55:02Z,2018-03-13T19:59:14Z,,NONE,"My setup is described [in my blog](https://marc.wäckerlin.ch/computer/evaluation-of-distributed-filesystems-for-docker-swarm#Setup).

Everything was fast and stable at the beginning. I then successfully migrated from GlusterFS to LizadFS. But when I started to copy more and more data to LizardFS, everything became more and more slow, fuzzy, unstable. CPUs are not stressed, nor is RAM full. So probably the problem is I/O or Network. Currently, used disk space is 5,4TB.

Entry in `/etc/fstab`:
```
mfsmount /var/volumes fuse rw,mfsmaster=universum,mfsdelayedinit,mfschunkserverwriteto=20000,mfsioretries=120 0 0
```

A week ago, I've seen, that default goal was 1, so I set goal for everything to 2, and since then it is much worse. This weekend, I changed th goal so that everything must explicitely be on the two hosts `universum`  (the strongest machine) and `urknall` since `raum` is in another location and `pulsar` is the slowest. Also I upgraded fom Ubuntu' s lizardsfs version 3.7 to 3.12.

It still does not work.

In `/var/log/syslog`, I still see many problems:

Host `universum` (Master and Chunk):
```
Feb  5 11:39:17 universum mfschunkserver[7368]: Did not manage to receive packet header                                                                                                      
Feb  5 11:39:23 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 4)               
Feb  5 11:39:27 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)               
Feb  5 11:39:34 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 5)               
Feb  5 11:39:38 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)           
Feb  5 11:39:39 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)               
Feb  5 11:39:40 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)           
Feb  5 11:39:42 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)           
Feb  5 11:39:44 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)           
Feb  5 11:39:45 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 6)               
Feb  5 11:39:46 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)           
Feb  5 11:39:48 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)           
Feb  5 11:39:50 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)           
Feb  5 11:39:51 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 3)               
Feb  5 11:39:52 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)           
Feb  5 11:39:54 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)           
Feb  5 11:39:56 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 7)               
Feb  5 11:39:56 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 10)          
Feb  5 11:39:58 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 11)          
Feb  5 11:40:01 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 12)          
Feb  5 11:40:02 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 4)               
Feb  5 11:40:05 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 13)          
Feb  5 11:40:07 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 8)               
Feb  5 11:40:12 universum mfschunkserver[7368]: replication error: Chunkserver communication timed out: 192.168.99.3:9422                                                                    
Feb  5 11:40:12 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007BF51 replication status: Unknown LizardFS error                                                          
Feb  5 11:40:13 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 5)               
Feb  5 11:40:13 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 14)          
Feb  5 11:40:14 universum mfschunkserver[7368]: Did not manage to receive packet header                                                                                                      
Feb  5 11:40:17 universum mfschunkserver[7368]: replication error: Chunkserver communication timed out: 192.168.99.3:9422                                                                    
Feb  5 11:40:17 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007463B replication status: Unknown LizardFS error
Feb  5 11:40:18 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 9)
Feb  5 11:40:19 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:40:23 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 15)
Feb  5 11:40:24 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 6)
Feb  5 11:40:29 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 10)
Feb  5 11:40:33 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 16)
Feb  5 11:40:35 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 7)
Feb  5 11:40:40 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 11)
Feb  5 11:40:43 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 17)
Feb  5 11:40:46 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 8)
Feb  5 11:40:52 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 12)
Feb  5 11:40:53 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 18)
Feb  5 11:40:58 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 9)
Feb  5 11:41:03 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 19)
Feb  5 11:41:05 universum mfsmount: write file error, inode: 415346, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 13)
Feb  5 11:41:10 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 10)
Feb  5 11:41:13 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 20)
Feb  5 11:41:14 universum mfschunkserver[7368]: replication error: Chunkserver communication timed out: 192.168.99.3:9422
Feb  5 11:41:14 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000078E36 replication status: Unknown LizardFS error
Feb  5 11:41:19 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:41:19 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007BAE3 replication status: Unknown LizardFS error
Feb  5 11:41:19 universum dockerd[1458]: time=""2018-02-05T11:41:19.943977779+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:41:19 universum dockerd[1458]: time=""2018-02-05T11:41:19.944442530+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:41:35 universum mfsmount: read file error, inode: 141719, index: 15, chunk: 506898, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 1)
Feb  5 11:41:37 universum mfsmount: read file error, inode: 141719, index: 15, chunk: 506898, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 2)
Feb  5 11:41:39 universum mfsmount: read file error, inode: 141719, index: 15, chunk: 506898, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 3)
Feb  5 11:41:41 universum mfsmount: read file error, inode: 141719, index: 15, chunk: 506898, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 4)
Feb  5 11:41:43 universum mfsmount: read file error, inode: 141719, index: 15, chunk: 506898, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 5)
Feb  5 11:41:43 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:41:47 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:41:49 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:41:51 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:41:53 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:41:55 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:41:57 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 11:41:59 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 11:42:01 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)
Feb  5 11:42:03 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)
Feb  5 11:42:05 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 10)
Feb  5 11:42:07 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 11)
Feb  5 11:42:09 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 12)
Feb  5 11:42:13 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 13)
Feb  5 11:42:14 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:42:16 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007E398 replication status: Unknown LizardFS error
Feb  5 11:42:17 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:42:17 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007CA23 replication status: Unknown LizardFS error
Feb  5 11:42:19 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:42:23 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:42:25 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:42:27 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:42:29 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:42:31 universum mfsmount: read file error, inode: 141719, index: 14, chunk: 506777, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:42:53 universum mfsmount: read file error, inode: 141719, index: 16, chunk: 506985, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 1)
Feb  5 11:42:55 universum mfsmount: read file error, inode: 141719, index: 16, chunk: 506985, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 2)
Feb  5 11:43:17 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:43:17 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:43:17 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007AB54 replication status: Unknown LizardFS error
Feb  5 11:43:17 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000082933 replication status: Unknown LizardFS error
Feb  5 11:43:27 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:43:29 universum dockerd[1458]: time=""2018-02-05T11:43:29.434596908+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:43:29 universum dockerd[1458]: time=""2018-02-05T11:43:29.434656283+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:43:29 universum dockerd[1458]: time=""2018-02-05T11:43:29.434684659+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:43:29 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:43:31 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:43:33 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:43:35 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 11:43:37 universum dockerd[1458]: time=""2018-02-05T11:43:37.234326051+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:43:37 universum dockerd[1458]: time=""2018-02-05T11:43:37.234407024+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:43:37 universum dockerd[1458]: time=""2018-02-05T11:43:37.234444493+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:43:37 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 6)
Feb  5 11:43:38 universum dhclient[1339]: DHCPREQUEST of 192.168.99.137 on eno1 to 192.168.99.1 port 67 (xid=0x5da66249)
Feb  5 11:43:38 universum dhclient[1339]: DHCPACK of 192.168.99.137 from 192.168.99.1
Feb  5 11:43:38 universum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:43:39 universum dhclient[1339]: bound to 192.168.99.137 -- renewal in 237 seconds.
Feb  5 11:43:39 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 7)
Feb  5 11:43:41 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 8)
Feb  5 11:43:43 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 9)
Feb  5 11:43:45 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 10)
Feb  5 11:43:47 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 11)
Feb  5 11:43:49 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:43:49 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 12)
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306076028+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306146867+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:npq26i6igr6n6fn1idntes46c leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306185869+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306204829+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:ksx36pb1e18i5cywyl0itazk1 leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306238582+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306255195+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:o5j79tmzry4nxic6p0uymd9qr leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306271580+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:sv1iurfiu8hvvlkql0955wr3x leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306288554+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:fbh4j37soxs97qxvjj36b2eis leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306322590+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306339247+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:4vpfofq2v8hubh30a3td4xwgx leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306373078+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306407266+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306440676+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:48 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306457046+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:jer1lv709js55yyve3pudm9sq leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:43:51 universum dockerd[1458]: time=""2018-02-05T11:43:51.306473376+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:azbnydw3jzhx3cp6dy38fmldk leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:44:04 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:44:06 universum mfsmount: read file error, inode: 141719, index: 16, chunk: 506985, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 1)
Feb  5 11:44:08 universum mfsmount: read file error, inode: 141719, index: 16, chunk: 506985, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 2)
Feb  5 11:44:09 universum mfsmaster[8676]: chunk 000000000000f127 has not enough valid parts (2) consider repairing it manually
Feb  5 11:44:09 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.137 - ver:0000000e)
Feb  5 11:44:09 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.3 - ver:0000000e)
Feb  5 11:44:09 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:44:13 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:44:15 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:44:17 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:44:19 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:44:21 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:44:44 universum mfsmount: write file error, inode: 414011, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:44:44 universum mfsmount: write file error, inode: 140664, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:44:49 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:44:49 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:44:49 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007BF51 replication status: Unknown LizardFS error
Feb  5 11:44:49 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000078253 replication status: Unknown LizardFS error
Feb  5 11:44:51 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:44:54 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:44:55 universum mfsmount: write file error, inode: 414011, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:44:56 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:44:58 universum mfsmount: write file error, inode: 140664, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 2)
Feb  5 11:44:58 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:44:59 universum kernel: [237839.493938] IPVS: __ip_vs_del_service: enter
Feb  5 11:45:00 universum dockerd[1458]: time=""2018-02-05T11:45:00.240829158+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:45:00 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:45:10 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:45:11 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:45:11 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 00000000000799CB replication status: Unknown LizardFS error
Feb  5 11:45:13 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:45:25 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:45:27 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:45:29 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:45:31 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:45:33 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 11:45:34 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:45:35 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 6)
Feb  5 11:45:37 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 7)
Feb  5 11:45:43 universum dockerd[1458]: sync duration of 7.384164527s, expected less than 1s
Feb  5 11:45:45 universum dockerd[1458]: sync duration of 2.717259987s, expected less than 1s
Feb  5 11:45:54 universum mfsmount: read file error, inode: 141719, index: 17, chunk: 507041, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:45:55 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:45:57 universum mfsmount: write file error, inode: 140664, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:46:14 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:46:14 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007C942 replication status: Unknown LizardFS error
Feb  5 11:46:22 universum dockerd[1458]: sync duration of 5.620471903s, expected less than 1s
Feb  5 11:46:28 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:46:33 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:46:41 universum mfsmount: read file error, inode: 141719, index: 21, chunk: 507309, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:46:58 universum mfsmount: read file error, inode: 141719, index: 20, chunk: 507244, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 1)
Feb  5 11:47:10 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:47:10 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007BCB7 replication status: Unknown LizardFS error
Feb  5 11:47:10 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:47:10 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 00000000000791F2 replication status: Unknown LizardFS error
Feb  5 11:47:11 universum kernel: [237970.684192] IPVS: __ip_vs_del_service: enter
Feb  5 11:47:14 universum mfsmount: read file error, inode: 141719, index: 21, chunk: 507309, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:47:15 universum dockerd[1458]: time=""2018-02-05T11:47:15.834626167+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:47:16 universum mfsmount: read file error, inode: 141719, index: 21, chunk: 507309, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:47:18 universum mfsmount: read file error, inode: 141719, index: 21, chunk: 507309, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:47:20 universum mfsmount: read file error, inode: 141719, index: 21, chunk: 507309, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:47:22 universum mfsmount: read file error, inode: 141719, index: 21, chunk: 507309, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 11:47:35 universum dhclient[1339]: DHCPREQUEST of 192.168.99.137 on eno1 to 192.168.99.1 port 67 (xid=0x5da66249)
Feb  5 11:47:35 universum dhclient[1339]: DHCPACK of 192.168.99.137 from 192.168.99.1
Feb  5 11:47:35 universum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:47:35 universum dhclient[1339]: bound to 192.168.99.137 -- renewal in 260 seconds.
Feb  5 11:47:36 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:47:36 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:47:36 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000079821 replication status: Unknown LizardFS error
Feb  5 11:47:36 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000079A0D replication status: Unknown LizardFS error
Feb  5 11:47:37 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:48:09 universum mfschunkserver[7368]: message repeated 3 times: [ Did not manage to receive packet header]
Feb  5 11:48:30 universum mfsmount: write file error, inode: 400037, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:48:40 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:48:42 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:48:44 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:48:46 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:48:48 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:48:50 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 11:48:51 universum mfsmount: write file error, inode: 400037, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 2)
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506102982+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506171356+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:sv1iurfiu8hvvlkql0955wr3x leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506208850+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506242855+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506260007+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:jer1lv709js55yyve3pudm9sq leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506292716+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506324478+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506356380+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506389027+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:46 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506405958+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:4vpfofq2v8hubh30a3td4xwgx leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506422194+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:azbnydw3jzhx3cp6dy38fmldk leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506438579+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:ksx36pb1e18i5cywyl0itazk1 leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506455455+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:fbh4j37soxs97qxvjj36b2eis leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506472739+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:npq26i6igr6n6fn1idntes46c leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:51 universum dockerd[1458]: time=""2018-02-05T11:48:51.506488714+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:o5j79tmzry4nxic6p0uymd9qr leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:48:52 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 11:48:54 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)
Feb  5 11:48:56 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)
Feb  5 11:48:57 universum mfsmount: write file error, inode: 413757, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:48:58 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 10)
Feb  5 11:49:00 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 11)
Feb  5 11:49:02 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 12)
Feb  5 11:49:02 universum mfschunkserver[7368]: replication error: Chunkserver communication timed out: 192.168.99.3:9422
Feb  5 11:49:02 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007AB45 replication status: Unknown LizardFS error
Feb  5 11:49:05 universum mfsmount: write file error, inode: 400037, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 3)
Feb  5 11:49:05 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:49:05 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 00000000000760C9 replication status: Unknown LizardFS error
Feb  5 11:49:06 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 13)
Feb  5 11:49:14 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 14)
Feb  5 11:49:15 universum mfsmaster[8676]: chunk 000000000000f127 has not enough valid parts (2) consider repairing it manually
Feb  5 11:49:15 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.137 - ver:0000000e)
Feb  5 11:49:15 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.3 - ver:0000000e)
Feb  5 11:49:15 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:49:22 universum mfsmount: write file error, inode: 400037, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 4)
Feb  5 11:49:22 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:49:22 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007B638 replication status: Unknown LizardFS error
Feb  5 11:49:24 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 15)
Feb  5 11:49:34 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:49:34 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 16)
Feb  5 11:49:44 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 17)
Feb  5 11:49:46 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:49:48 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:49:48 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:49:48 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007356D replication status: Unknown LizardFS error
Feb  5 11:49:48 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000089882 replication status: Unknown LizardFS error
Feb  5 11:49:54 universum mfsmount: read file error, inode: 141719, index: 21, chunk: 507309, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:50:02 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:50:04 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:50:06 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:50:08 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:50:09 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:50:10 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:50:12 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 11:50:14 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 11:50:20 universum mfsmount: read file error, inode: 141719, index: 23, chunk: 507500, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 1)
Feb  5 11:50:22 universum mfsmount: read file error, inode: 141719, index: 23, chunk: 507500, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 2)
Feb  5 11:50:24 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:50:27 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:50:29 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:50:31 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:50:33 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:50:35 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:50:36 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:50:37 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 11:50:39 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 11:50:41 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)
Feb  5 11:50:43 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)
Feb  5 11:50:44 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:50:44 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007E71B replication status: Unknown LizardFS error
Feb  5 11:50:45 universum mfsmount: read file error, inode: 141719, index: 22, chunk: 507401, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 10)
Feb  5 11:50:53 universum dockerd[1458]: time=""2018-02-05T11:50:53.034604217+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:50:53 universum dockerd[1458]: time=""2018-02-05T11:50:53.034685478+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:50:53 universum dockerd[1458]: time=""2018-02-05T11:50:53.034723577+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:50:53 universum dockerd[1458]: sync duration of 1.707560792s, expected less than 1s
Feb  5 11:50:53 universum mfsmount: read file error, inode: 141719, index: 23, chunk: 507500, version: 1 - Chunkserver communication timed out: 192.168.99.137:9422 (try counter: 1)
Feb  5 11:51:09 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:51:11 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:51:13 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:51:15 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:51:31 universum mfsmount: write file error, inode: 400037, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:51:33 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:51:33 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007ADE2 replication status: Unknown LizardFS error
Feb  5 11:51:33 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:51:35 universum dockerd[1458]: time=""2018-02-05T11:51:35.034564344+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:51:35 universum dockerd[1458]: time=""2018-02-05T11:51:35.034620249+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:51:35 universum dockerd[1458]: time=""2018-02-05T11:51:35.034650220+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:51:35 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:51:37 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:51:39 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:51:41 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 11:51:43 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 6)
Feb  5 11:51:46 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:51:49 universum dockerd[1458]: time=""2018-02-05T11:51:49.634597600+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:51:55 universum dhclient[1339]: DHCPREQUEST of 192.168.99.137 on eno1 to 192.168.99.1 port 67 (xid=0x5da66249)
Feb  5 11:51:55 universum dhclient[1339]: DHCPACK of 192.168.99.137 from 192.168.99.1
Feb  5 11:51:55 universum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:51:55 universum dhclient[1339]: bound to 192.168.99.137 -- renewal in 274 seconds.
Feb  5 11:52:13 universum dockerd[1458]: time=""2018-02-05T11:52:13.634581686+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:13 universum kernel: [238273.275100] IPVS: __ip_vs_del_service: enter
Feb  5 11:52:20 universum dockerd[1458]: time=""2018-02-05T11:52:20.234426928+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:20 universum dockerd[1458]: time=""2018-02-05T11:52:20.234480613+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:20 universum dockerd[1458]: time=""2018-02-05T11:52:20.234507939+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:20 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:52:34 universum dockerd[1458]: sync duration of 2.323483652s, expected less than 1s
Feb  5 11:52:35 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:52:37 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:52:39 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:52:41 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:52:43 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 11:52:45 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 6)
Feb  5 11:52:47 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 7)
Feb  5 11:52:49 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 8)
Feb  5 11:52:51 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 9)
Feb  5 11:53:01 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:53:03 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:53:05 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:53:07 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:53:09 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 11:53:11 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 6)
Feb  5 11:53:13 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 7)
Feb  5 11:53:15 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 8)
Feb  5 11:53:17 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 9)
Feb  5 11:53:19 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 10)
Feb  5 11:53:21 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 11)
Feb  5 11:53:23 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 12)
Feb  5 11:53:30 universum mfsmount: write file error, inode: 400037, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:53:30 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:53:32 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:53:34 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:53:36 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:53:44 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:53:46 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:53:48 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:53:48 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:53:50 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706060747+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:ksx36pb1e18i5cywyl0itazk1 leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706139321+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:sv1iurfiu8hvvlkql0955wr3x leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706159755+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:jer1lv709js55yyve3pudm9sq leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706177144+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:azbnydw3jzhx3cp6dy38fmldk leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706216333+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706250724+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706284148+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:46 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706317549+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706334981+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:4vpfofq2v8hubh30a3td4xwgx leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706367985+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706385282+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:npq26i6igr6n6fn1idntes46c leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706418506+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706451868+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706468341+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:o5j79tmzry4nxic6p0uymd9qr leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:51 universum dockerd[1458]: time=""2018-02-05T11:53:51.706507309+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:fbh4j37soxs97qxvjj36b2eis leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:53:52 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 11:53:54 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 6)
Feb  5 11:53:56 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 7)
Feb  5 11:53:58 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 8)
Feb  5 11:54:00 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 9)
Feb  5 11:54:01 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:54:01 universum mfsmaster[8676]: (192.168.99.3:9422) chunk: 0000000000079799 replication status: Unknown LizardFS error
Feb  5 11:54:02 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 10)
Feb  5 11:54:04 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 11)
Feb  5 11:54:06 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 12)
Feb  5 11:54:14 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:54:16 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:54:18 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:54:20 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:54:20 universum mfsmaster[8676]: chunk 000000000000f127 has not enough valid parts (2) consider repairing it manually
Feb  5 11:54:20 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.137 - ver:0000000e)
Feb  5 11:54:20 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.3 - ver:0000000e)
Feb  5 11:54:27 universum dockerd[1458]: time=""2018-02-05T11:54:27.234498316+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:54:27 universum dockerd[1458]: time=""2018-02-05T11:54:27.234557320+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:54:27 universum dockerd[1458]: time=""2018-02-05T11:54:27.234585824+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:54:30 universum kernel: [238409.871518] IPVS: __ip_vs_del_service: enter
Feb  5 11:54:30 universum dockerd[1458]: time=""2018-02-05T11:54:30.435126660+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:54:44 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:54:46 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:54:48 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:54:50 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:54:52 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 11:54:54 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 6)
Feb  5 11:54:56 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 7)
Feb  5 11:55:02 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:55:07 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:55:18 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:55:20 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:55:29 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:55:31 universum mfsmount: write file error, inode: 400037, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:55:31 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 11:55:33 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 11:55:35 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 11:55:37 universum mfsmount: read file error, inode: 141719, index: 24, chunk: 507591, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 11:55:39 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:55:52 universum mfsmount: write file error, inode: 400037, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 2)
Feb  5 11:55:59 universum mfsmount: write file error, inode: 63712, index: 0 - Timeout after 56320 ms (Timeout) (try counter: 1)
Feb  5 11:55:59 universum mfsmount: write file error, inode: 413360, index: 0 - Timeout after 48907 ms (Timeout) (try counter: 1)
Feb  5 11:56:07 universum mfschunkserver[7368]: replication error: Chunkserver communication timed out: 192.168.99.3:9422
Feb  5 11:56:07 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007BA75 replication status: Unknown LizardFS error
Feb  5 11:56:11 universum mfsmount: write file error, inode: 413360, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:56:11 universum mfsmount: write file error, inode: 63712, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:56:13 universum mfsmount: write file error, inode: 400037, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 3)
Feb  5 11:56:16 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:56:17 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:56:17 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007BAE3 replication status: Unknown LizardFS error
Feb  5 11:56:18 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:56:25 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:56:27 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:56:29 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:56:29 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:56:29 universum dhclient[1339]: DHCPREQUEST of 192.168.99.137 on eno1 to 192.168.99.1 port 67 (xid=0x5da66249)
Feb  5 11:56:29 universum dhclient[1339]: DHCPACK of 192.168.99.137 from 192.168.99.1
Feb  5 11:56:30 universum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:56:31 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:56:32 universum dhclient[1339]: bound to 192.168.99.137 -- renewal in 278 seconds.
Feb  5 11:56:33 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:56:35 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 11:56:37 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 11:56:39 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)
Feb  5 11:56:41 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:56:41 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)
Feb  5 11:56:43 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 10)
Feb  5 11:56:45 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 11)
Feb  5 11:56:47 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 12)
Feb  5 11:56:52 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 13)
Feb  5 11:56:52 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 3)
Feb  5 11:57:00 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 14)
Feb  5 11:57:04 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 4)
Feb  5 11:57:05 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:57:05 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000072F7E replication status: Unknown LizardFS error
Feb  5 11:57:05 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:57:05 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000081245 replication status: Unknown LizardFS error
Feb  5 11:57:06 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:57:06 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:57:10 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 15)
Feb  5 11:57:16 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:57:20 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 16)
Feb  5 11:57:27 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:57:30 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 17)
Feb  5 11:57:38 universum mfsmount: write file error, inode: 400427, index: 0 - Timeout after 10023 ms (Timeout) (try counter: 3)
Feb  5 11:57:39 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:57:39 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:57:39 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007A1C4 replication status: Unknown LizardFS error
Feb  5 11:57:39 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007A0CE replication status: Unknown LizardFS error
Feb  5 11:57:40 universum mfsmount: read file error, inode: 141719, index: 25, chunk: 507645, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 18)
Feb  5 11:57:49 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:57:51 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:57:53 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:57:54 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:57:55 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:57:58 universum mfsmount: write file error, inode: 63715, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:57:58 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:57:58 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007A9DF replication status: Unknown LizardFS error
Feb  5 11:58:10 universum mfsmount: write file error, inode: 63715, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:58:15 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:58:17 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:58:21 universum mfsmount: write file error, inode: 63715, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 3)
Feb  5 11:58:24 universum mfsmount: write file error, inode: 400037, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:58:24 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:58:24 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007C4C4 replication status: Unknown LizardFS error
Feb  5 11:58:26 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:58:28 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:58:30 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:58:32 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:58:34 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:58:36 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 11:58:38 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 11:58:40 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)
Feb  5 11:58:41 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:58:42 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)
Feb  5 11:58:44 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 10)
Feb  5 11:58:46 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 11)
Feb  5 11:58:48 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 12)
Feb  5 11:58:49 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906059197+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:4vpfofq2v8hubh30a3td4xwgx leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906166318+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906203962+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906221413+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:npq26i6igr6n6fn1idntes46c leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906237681+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:o5j79tmzry4nxic6p0uymd9qr leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906256065+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:fbh4j37soxs97qxvjj36b2eis leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906289815+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906306563+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:jer1lv709js55yyve3pudm9sq leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906323036+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:sv1iurfiu8hvvlkql0955wr3x leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906342618+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:azbnydw3jzhx3cp6dy38fmldk leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906375582+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906407643+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906440244+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906457171+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:ksx36pb1e18i5cywyl0itazk1 leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:51 universum dockerd[1458]: time=""2018-02-05T11:58:51.906500505+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:46 Queue qLen:0 netMsg/s:0""
Feb  5 11:58:52 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:58:53 universum dockerd[1458]: time=""2018-02-05T11:58:53.353304585+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:58:53 universum dockerd[1458]: time=""2018-02-05T11:58:53.353378407+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:58:53 universum dockerd[1458]: time=""2018-02-05T11:58:53.365181437+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:58:53 universum dockerd[1458]: time=""2018-02-05T11:58:53.365243192+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:58:53 universum dockerd[1458]: time=""2018-02-05T11:58:53.365278410+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:58:55 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 11:58:57 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:58:58 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:58:59 universum mfsmount: read file error, inode: 141719, index: 26, chunk: 507728, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:59:03 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 3)
Feb  5 11:59:04 universum mfsmount: write file error, inode: 358923, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 12)
Feb  5 11:59:04 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:59:04 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007CB68 replication status: Unknown LizardFS error
Feb  5 11:59:06 universum dockerd[1458]: time=""2018-02-05T11:59:06.341072066+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:59:06 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:59:06 universum kernel: [238686.463527] IPVS: __ip_vs_del_service: enter
Feb  5 11:59:15 universum mfsmount: write file error, inode: 413342, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:59:17 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 4)
Feb  5 11:59:25 universum mfsmount: write file error, inode: 358923, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 13)
Feb  5 11:59:25 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:59:25 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007C2BB replication status: Unknown LizardFS error
Feb  5 11:59:25 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:59:25 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007B736 replication status: Unknown LizardFS error
Feb  5 11:59:26 universum mfsmount: write file error, inode: 413342, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 11:59:26 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:59:26 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:59:26 universum mfsmaster[8676]: chunk 000000000000f127 has not enough valid parts (2) consider repairing it manually
Feb  5 11:59:26 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.137 - ver:0000000e)
Feb  5 11:59:26 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.3 - ver:0000000e)
Feb  5 11:59:28 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 5)
Feb  5 11:59:28 universum dockerd[1458]: time=""2018-02-05T11:59:28.634475035+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:59:30 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:59:38 universum dockerd[1458]: sync duration of 1.294411485s, expected less than 1s
Feb  5 11:59:41 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:59:41 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 11:59:50 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:59:53 universum mfsmount: write file error, inode: 40710, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 11:59:55 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:59:55 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 11:59:55 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000079508 replication status: Unknown LizardFS error
Feb  5 11:59:55 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007948D replication status: Unknown LizardFS error
Feb  5 12:00:00 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 12:00:02 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 12:00:04 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 12:00:06 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 12:00:26 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 12:00:29 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 12:00:31 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 12:00:33 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 12:00:35 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 12:00:36 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:00:37 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 12:00:44 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 12:00:46 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 12:00:48 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 12:00:50 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 12:00:50 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:00:52 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:00:52 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007E71B replication status: Unknown LizardFS error
Feb  5 12:01:01 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 12:01:10 universum dhclient[1339]: DHCPREQUEST of 192.168.99.137 on eno1 to 192.168.99.1 port 67 (xid=0x5da66249)
Feb  5 12:01:10 universum dhclient[1339]: DHCPACK of 192.168.99.137 from 192.168.99.1
Feb  5 12:01:10 universum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:01:10 universum dhclient[1339]: bound to 192.168.99.137 -- renewal in 254 seconds.
Feb  5 12:01:12 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 3)
Feb  5 12:01:15 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:01:15 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007CB37 replication status: Unknown LizardFS error
Feb  5 12:01:17 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:01:20 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 12:01:22 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 12:01:30 universum dockerd[1458]: time=""2018-02-05T12:01:30.234696422+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:01:30 universum dockerd[1458]: time=""2018-02-05T12:01:30.234755160+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:01:30 universum dockerd[1458]: time=""2018-02-05T12:01:30.234781904+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:01:37 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:01:43 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 12:01:45 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 12:01:47 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 12:01:49 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 12:01:51 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 5)
Feb  5 12:01:53 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 6)
Feb  5 12:01:55 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 7)
Feb  5 12:01:57 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 8)
Feb  5 12:01:59 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 9)
Feb  5 12:02:01 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 10)
Feb  5 12:02:03 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 11)
Feb  5 12:02:05 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 12)
Feb  5 12:02:09 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:02:09 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:02:30 universum mfsmount: write file error, inode: 400037, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 12:02:40 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:02:40 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:02:40 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007E5FF replication status: Unknown LizardFS error
Feb  5 12:02:40 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007E398 replication status: Unknown LizardFS error
Feb  5 12:03:11 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:03:22 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:03:22 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 00000000000733A5 replication status: Unknown LizardFS error
Feb  5 12:03:22 universum mfsmount: write file error, inode: 400037, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 1)
Feb  5 12:03:22 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:03:22 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007AB54 replication status: Unknown LizardFS error
Feb  5 12:03:27 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:03:35 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 12:03:35 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.2:9422)
Feb  5 12:03:35 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 00000000000747CE replication status: Unknown LizardFS error
Feb  5 12:03:47 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 12:03:47 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:03:47 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 00000000000760C9 replication status: Unknown LizardFS error
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106085774+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:jer1lv709js55yyve3pudm9sq leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106194180+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106215985+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:npq26i6igr6n6fn1idntes46c leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106235080+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:4vpfofq2v8hubh30a3td4xwgx leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106272510+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106324843+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106358124+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106395805+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:48 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106413597+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:o5j79tmzry4nxic6p0uymd9qr leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106430222+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:sv1iurfiu8hvvlkql0955wr3x leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106446484+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:ksx36pb1e18i5cywyl0itazk1 leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106479591+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106495912+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:fbh4j37soxs97qxvjj36b2eis leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106513661+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:azbnydw3jzhx3cp6dy38fmldk leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:52 universum dockerd[1458]: time=""2018-02-05T12:03:52.106551088+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 12:03:58 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 3)
Feb  5 12:04:10 universum mfsmount: write file error, inode: 399491, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 4)
Feb  5 12:04:23 universum dockerd[1458]: time=""2018-02-05T12:04:23.435646989+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:04:23 universum dockerd[1458]: time=""2018-02-05T12:04:23.435720109+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:04:23 universum dockerd[1458]: time=""2018-02-05T12:04:23.435754698+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:04:32 universum mfsmaster[8676]: chunk 000000000000f127 has not enough valid parts (2) consider repairing it manually
Feb  5 12:04:32 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.137 - ver:0000000e)
Feb  5 12:04:32 universum mfsmaster[8676]: chunk 000000000000f127_0000000f - invalid part on (192.168.99.3 - ver:0000000e)
Feb  5 12:04:38 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:04:43 universum mfsmount: write file error, inode: 140664, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 12:05:02 universum mfsmount: write file error, inode: 140664, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 2)
Feb  5 12:05:02 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:05:02 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007BA0A replication status: Unknown LizardFS error
Feb  5 12:05:02 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:05:02 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007B638 replication status: Unknown LizardFS error
Feb  5 12:05:03 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:05:23 universum mfsmount: write file error, inode: 140664, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 3)
Feb  5 12:05:24 universum dhclient[1339]: DHCPREQUEST of 192.168.99.137 on eno1 to 192.168.99.1 port 67 (xid=0x5da66249)
Feb  5 12:05:24 universum dhclient[1339]: DHCPACK of 192.168.99.137 from 192.168.99.1
Feb  5 12:05:24 universum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:05:24 universum dhclient[1339]: bound to 192.168.99.137 -- renewal in 255 seconds.
Feb  5 12:05:33 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:05:33 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:05:33 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007B799 replication status: Unknown LizardFS error
Feb  5 12:05:51 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 12:06:02 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 12:06:10 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:06:10 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:06:10 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000079E90 replication status: Unknown LizardFS error
Feb  5 12:06:10 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007DB96 replication status: Unknown LizardFS error
Feb  5 12:06:13 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 3)
Feb  5 12:06:13 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:06:16 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:06:24 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 4)
Feb  5 12:06:28 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:06:28 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:06:28 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 00000000000782A2 replication status: Unknown LizardFS error
Feb  5 12:06:28 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 000000000007C305 replication status: Unknown LizardFS error
Feb  5 12:06:33 universum dockerd[1458]: time=""2018-02-05T12:06:33.634990609+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:06:33 universum kernel: [239133.251368] IPVS: __ip_vs_del_service: enter
Feb  5 12:06:39 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 12:07:11 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:07:11 universum mfschunkserver[7368]: replication error: Read from chunkserver error: connection reset by peer (server 192.168.99.3:9422)
Feb  5 12:07:11 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 00000000000812C0 replication status: Unknown LizardFS error
Feb  5 12:07:11 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000081245 replication status: Unknown LizardFS error
Feb  5 12:07:12 universum mfsmount: write file error, inode: 48513, index: 0 - Timeout after 37108 ms (Timeout) (try counter: 1)
Feb  5 12:07:13 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:07:13 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:07:34 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 12:07:45 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 12:07:57 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 3)
Feb  5 12:08:09 universum mfsmount: write file error, inode: 400427, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 4)
Feb  5 12:08:13 universum mfschunkserver[7368]: replication error: Chunkserver communication timed out: 192.168.99.3:9422
Feb  5 12:08:13 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000079B0C replication status: Unknown LizardFS error
Feb  5 12:08:13 universum mfschunkserver[7368]: replication error: Chunkserver communication timed out: 192.168.99.3:9422
Feb  5 12:08:13 universum mfsmaster[8676]: (192.168.99.137:9422) chunk: 0000000000079CF8 replication status: Unknown LizardFS error
Feb  5 12:08:16 universum mfsmount: write file error, inode: 409072, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
Feb  5 12:08:27 universum mfsmount: write file error, inode: 409072, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 2)
Feb  5 12:08:34 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 12:08:36 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 12:08:36 universum mfsmount: write file error, inode: 140664, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 12:08:38 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 12:08:39 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:08:40 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 4)
Feb  5 12:08:43 universum dockerd[1458]: time=""2018-02-05T12:08:43.434724346+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:08:43 universum kernel: [239263.047510] IPVS: __ip_vs_del_service: enter
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306033047+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:4vpfofq2v8hubh30a3td4xwgx leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306101830+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306122038+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:o5j79tmzry4nxic6p0uymd9qr leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306139536+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:jer1lv709js55yyve3pudm9sq leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306174622+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306191648+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:azbnydw3jzhx3cp6dy38fmldk leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306225132+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306242263+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:ksx36pb1e18i5cywyl0itazk1 leaving:false netPeers:1 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306258691+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:npq26i6igr6n6fn1idntes46c leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306291511+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:47 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306325107+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306342317+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:sv1iurfiu8hvvlkql0955wr3x leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306375309+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306412309+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:52 universum dockerd[1458]: time=""2018-02-05T12:08:52.306430023+01:00"" level=info msg=""NetworkDB stats universum(8c31e37f1156) - netID:fbh4j37soxs97qxvjj36b2eis leaving:false netPeers:1 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:08:53 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 1)
Feb  5 12:08:55 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 2)
Feb  5 12:08:57 universum mfsmount: read file error, inode: 141719, index: 28, chunk: 507912, version: 1 - Chunkserver communication timed out: 192.168.99.7:9422 (try counter: 3)
Feb  5 12:09:00 universum mfschunkserver[7368]: Did not manage to receive packet header
Feb  5 12:09:02 universum dockerd[1458]: time=""2018-02-05T12:09:02.539874916+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:09:02 universum dockerd[1458]: time=""2018-02-05T12:09:02.539957196+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:09:03 universum mfsmount: write file error, inode: 48506, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.137:9422) (try counter: 1)
```

Host `urknall` (Chunk and Metalogger)
```
Feb  5 11:44:59 urknall kernel: [439533.674510] device veth8fbb4a7 left promiscuous mode
Feb  5 11:44:59 urknall kernel: [439533.674521] docker_gwbridge: port 21(veth8fbb4a7) entered disabled state
Feb  5 11:44:59 urknall dockerd[1676]: time=""2018-02-05T11:44:59+01:00"" level=info msg=""shim docker-containerd-shim started"" address=""/containerd-shim/moby/b56bb316fe60faf95cea28cc55fa544f854ba19096fa340393e7f297c6be6cfa/shim.sock"" debug=false module=""containerd/tasks"" pid=11659
Feb  5 11:44:59 urknall kernel: [439533.707232] br0: port 5(veth1042) entered disabled state
Feb  5 11:44:59 urknall kernel: [439533.707839] veth7b0561a: renamed from eth0
Feb  5 11:44:59 urknall kernel: [439533.794199] IPVS: Creating netns size=2192 id=1956
Feb  5 11:44:59 urknall kernel: [439533.853586] IPVS: __ip_vs_del_service: enter
Feb  5 11:45:00 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.003541s (0.210 MB/s)
Feb  5 11:45:00 urknall kernel: [439534.918243] eth0: renamed from veth7df8d6d
Feb  5 11:45:00 urknall kernel: [439534.938086] br0: port 4(veth1041) entered forwarding state
Feb  5 11:45:00 urknall kernel: [439534.938115] br0: port 4(veth1041) entered forwarding state
Feb  5 11:45:08 urknall dockerd[1676]: time=""2018-02-05T11:45:08.634000848+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:45:08 urknall dockerd[1676]: time=""2018-02-05T11:45:08.634166121+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:45:08 urknall dockerd[1676]: time=""2018-02-05T11:45:08.634222758+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:dpu8s5w8nx7vtqyiqai8rflqm leaving:false netPeers:1 entries:12 Queue qLen:0 netMsg/s:0""
Feb  5 11:45:08 urknall dockerd[1676]: time=""2018-02-05T11:45:08.634297624+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:49 Queue qLen:0 netMsg/s:0""
Feb  5 11:45:08 urknall dockerd[1676]: time=""2018-02-05T11:45:08.634370376+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:45:08 urknall dockerd[1676]: time=""2018-02-05T11:45:08.634422287+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:gliwcqzlhjzjulhs3xdbxuqrv leaving:false netPeers:1 entries:7 Queue qLen:0 netMsg/s:0""
Feb  5 11:45:08 urknall dockerd[1676]: time=""2018-02-05T11:45:08.634493086+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 11:45:08 urknall dockerd[1676]: time=""2018-02-05T11:45:08.634564372+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:45:08 urknall dockerd[1676]: time=""2018-02-05T11:45:08.634639279+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:45:13 urknall mfsmount: write file error, inode: 67783, index: 0 - Timeout after 12008 ms (Timeout) (try counter: 29)
Feb  5 11:45:15 urknall mfschunkserver[8794]: Did not manage to receive packet header
Feb  5 11:45:15 urknall kernel: [439549.728988] br0: port 5(veth1042) entered disabled state
Feb  5 11:45:15 urknall kernel: [439549.736377] device veth1042 left promiscuous mode
Feb  5 11:45:15 urknall kernel: [439549.736386] br0: port 5(veth1042) entered disabled state
Feb  5 11:45:15 urknall kernel: [439549.948972] br0: port 4(veth1041) entered forwarding state
Feb  5 11:45:17 urknall kernel: [439551.477655] br0: port 2(veth0) entered disabled state
Feb  5 11:45:17 urknall kernel: [439551.482445] device veth0 left promiscuous mode
Feb  5 11:45:17 urknall kernel: [439551.482471] br0: port 2(veth0) entered disabled state
Feb  5 11:45:18 urknall kernel: [439552.232287] br0: port 3(veth5) entered disabled state
Feb  5 11:45:18 urknall kernel: [439552.232316] br0: port 1(vxlan0) entered disabled state
Feb  5 11:45:18 urknall kernel: [439552.237236] ov-00100d-dpu8s: renamed from br0
Feb  5 11:45:18 urknall kernel: [439552.254267] device veth5 left promiscuous mode
Feb  5 11:45:18 urknall kernel: [439552.254318] ov-00100d-dpu8s: port 3(veth5) entered disabled state
Feb  5 11:45:18 urknall kernel: [439552.273546] device vxlan0 left promiscuous mode
Feb  5 11:45:18 urknall kernel: [439552.273586] ov-00100d-dpu8s: port 1(vxlan0) entered disabled state
Feb  5 11:45:18 urknall kernel: [439552.303426] eth1: renamed from veth2040f5e
Feb  5 11:45:18 urknall kernel: [439552.324078] vx-00100d-dpu8s: renamed from vxlan0
Feb  5 11:45:18 urknall kernel: [439552.338467] IPVS: __ip_vs_del_service: enter
Feb  5 11:45:18 urknall kernel: [439552.355298] docker_gwbridge: port 40(vetha782956) entered forwarding state
Feb  5 11:45:18 urknall kernel: [439552.355373] docker_gwbridge: port 40(vetha782956) entered forwarding state
Feb  5 11:45:18 urknall kernel: [439552.371458] vethd110afc: renamed from veth5
Feb  5 11:45:18 urknall kernel: [439552.441719] vethed4e8de: renamed from eth2
Feb  5 11:45:15 urknall mfschunkserver[8794]: Did not manage to receive packet header
Feb  5 11:45:19 urknall dockerd[1676]: time=""2018-02-05T11:45:19.072637973+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/delete type=""*events.ContainerDelete""
Feb  5 11:45:19 urknall kernel: [439553.181765] eth2: renamed from veth6eac6ab
Feb  5 11:45:19 urknall kernel: [439553.198533] br0: port 2(veth0) entered forwarding state
Feb  5 11:45:19 urknall kernel: [439553.198554] br0: port 2(veth0) entered forwarding state
Feb  5 11:45:28 urknall kernel: [439562.694571] docker_gwbridge: port 37(veth0b18fd5) entered disabled state
Feb  5 11:45:28 urknall kernel: [439562.694864] veth204cabe: renamed from eth1
Feb  5 11:45:33 urknall kernel: [439567.359237] docker_gwbridge: port 40(vetha782956) entered forwarding state
Feb  5 11:45:34 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunk write error (Disconnected) (try counter: 30)
Feb  5 11:45:34 urknall kernel: [439568.255447] br0: port 2(veth0) entered forwarding state
Feb  5 11:45:34 urknall dockerd[1676]: time=""2018-02-05T11:45:34.513176377+01:00"" level=error msg=""fatal task error"" error=""task: non-zero exit (3)"" module=node/agent/taskmanager node.id=4z5kkc15en2q29wt1crkxomc8 service.id=g6trfvrh61m8pdp2l1ttht8m8 task.id=n7oevpc7g1s9tnya7ma8nhrp1
Feb  5 11:45:35 urknall dockerd[1676]: time=""2018-02-05T11:45:35.754305089+01:00"" level=warning msg=""unknown container"" container=b56bb316fe60faf95cea28cc55fa544f854ba19096fa340393e7f297c6be6cfa module=libcontainerd namespace=plugins.moby
Feb  5 11:45:35 urknall dockerd[1676]: time=""2018-02-05T11:45:35.844875898+01:00"" level=warning msg=""unknown container"" container=b56bb316fe60faf95cea28cc55fa544f854ba19096fa340393e7f297c6be6cfa module=libcontainerd namespace=plugins.moby
Feb  5 11:45:37 urknall mfschunkserver[8794]: Did not manage to receive packet header
Feb  5 11:45:45 urknall dhclient[1979]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x1f01ed1a)
Feb  5 11:45:45 urknall dhclient[1979]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 11:45:45 urknall marc: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:45:50 urknall mfsmount: read file error, inode: 387293, index: 0, chunk: 618667, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:45:52 urknall mfsmount: read file error, inode: 387293, index: 0, chunk: 618667, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:45:54 urknall mfsmount: read file error, inode: 387293, index: 0, chunk: 618667, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:45:56 urknall mfsmount: read file error, inode: 387293, index: 0, chunk: 618667, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:45:57 urknall dhclient[1979]: bound to 192.168.99.3 -- renewal in 299 seconds.
Feb  5 11:45:57 urknall mfsmount: write file error, inode: 196764, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 21)
Feb  5 11:45:57 urknall mfsmount: write file error, inode: 67783, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 31)
Feb  5 11:45:57 urknall kernel: [439591.782830] docker_gwbridge: port 37(veth0b18fd5) entered disabled state
Feb  5 11:45:57 urknall kernel: [439591.795431] device veth0b18fd5 left promiscuous mode
Feb  5 11:45:57 urknall kernel: [439591.795440] docker_gwbridge: port 37(veth0b18fd5) entered disabled state
Feb  5 11:46:00 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000618s (1.201 MB/s)
Feb  5 11:46:04 urknall mfsmount: read file error, inode: 413912, index: 0, chunk: 622939, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:46:06 urknall mfsmount: read file error, inode: 413912, index: 0, chunk: 622939, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:46:08 urknall mfsmount: read file error, inode: 413912, index: 0, chunk: 622939, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:46:10 urknall mfsmount: read file error, inode: 413912, index: 0, chunk: 622939, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:46:12 urknall mfsmount: read file error, inode: 413912, index: 0, chunk: 622939, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:46:15 urknall kernel: [439609.724889] IPVS: __ip_vs_del_service: enter
Feb  5 11:46:17 urknall mfschunkserver[8794]: Did not manage to receive packet header
Feb  5 11:46:17 urknall mfschunkserver[8794]: Did not manage to receive packet header
Feb  5 11:46:17 urknall dockerd[1676]: time=""2018-02-05T11:46:17.894089436+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/delete type=""*events.ContainerDelete""
Feb  5 11:46:26 urknall mfsmount: read file error, inode: 413912, index: 0, chunk: 622939, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:46:28 urknall mfsmount: read file error, inode: 413912, index: 0, chunk: 622939, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:46:30 urknall mfsmount: read file error, inode: 413912, index: 0, chunk: 622939, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:46:32 urknall mfsmount: read file error, inode: 413912, index: 0, chunk: 622939, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:46:33 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:46:33 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 37)
Feb  5 11:46:35 urknall dockerd[1676]: time=""2018-02-05T11:46:35.343356885+01:00"" level=error msg=""fatal task error"" error=""task: non-zero exit (1)"" module=node/agent/taskmanager node.id=4z5kkc15en2q29wt1crkxomc8 service.id=gq2beoh3n3f0y9ix02ri5kiyl task.id=onf2yrslt1rihpxg6jf750a98
Feb  5 11:46:36 urknall mfschunkserver[8794]: Did not manage to receive packet header
Feb  5 11:46:47 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:46:49 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:46:51 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:46:53 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:46:55 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:46:57 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 11:46:59 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 11:47:01 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)
Feb  5 11:47:02 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 32)
Feb  5 11:47:03 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)
Feb  5 11:47:03 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 38)
Feb  5 11:47:05 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 10)
Feb  5 11:47:07 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 11)
Feb  5 11:47:09 urknall mfsmount: read file error, inode: 73950, index: 0, chunk: 67081, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 12)
Feb  5 11:47:10 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000647s (1.147 MB/s)
Feb  5 11:47:10 urknall dockerd[1676]: time=""2018-02-05T11:47:10.756025073+01:00"" level=warning msg=""unknown container"" container=b56bb316fe60faf95cea28cc55fa544f854ba19096fa340393e7f297c6be6cfa module=libcontainerd namespace=plugins.moby
Feb  5 11:47:10 urknall dockerd[1676]: time=""2018-02-05T11:47:10+01:00"" level=info msg=""shim reaped"" id=b56bb316fe60faf95cea28cc55fa544f854ba19096fa340393e7f297c6be6cfa module=""containerd/tasks""
Feb  5 11:47:10 urknall dockerd[1676]: time=""2018-02-05T11:47:10.909541156+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=plugins.moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 11:47:10 urknall dockerd[1676]: time=""2018-02-05T11:47:10.909708254+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 11:47:10 urknall kernel: [439665.026807] br0: port 4(veth1041) entered disabled state
Feb  5 11:47:10 urknall kernel: [439665.036044] veth7df8d6d: renamed from eth0
Feb  5 11:47:10 urknall kernel: [439665.090674] IPVS: __ip_vs_del_service: enter
Feb  5 11:47:21 urknall kernel: [439675.386698] br0: port 4(veth1041) entered disabled state
Feb  5 11:47:21 urknall kernel: [439675.398582] device veth1041 left promiscuous mode
Feb  5 11:47:21 urknall kernel: [439675.398592] br0: port 4(veth1041) entered disabled state
Feb  5 11:47:21 urknall kernel: [439675.437724] br0: port 2(veth0) entered disabled state
Feb  5 11:47:21 urknall kernel: [439675.437785] br0: port 1(vxlan0) entered disabled state
Feb  5 11:47:21 urknall kernel: [439675.441846] ov-00100b-gliwc: renamed from br0
Feb  5 11:47:21 urknall kernel: [439675.465408] device veth0 left promiscuous mode
Feb  5 11:47:21 urknall kernel: [439675.465443] ov-00100b-gliwc: port 2(veth0) entered disabled state
Feb  5 11:47:21 urknall kernel: [439675.481183] device vxlan0 left promiscuous mode
Feb  5 11:47:21 urknall kernel: [439675.481220] ov-00100b-gliwc: port 1(vxlan0) entered disabled state
Feb  5 11:47:21 urknall kernel: [439675.545315] vx-00100b-gliwc: renamed from vxlan0
Feb  5 11:47:21 urknall kernel: [439675.596712] veth54eaca9: renamed from veth0
Feb  5 11:47:21 urknall kernel: [439675.680849] veth6eac6ab: renamed from eth2
Feb  5 11:47:22 urknall kernel: [439676.605356] docker_gwbridge: port 40(vetha782956) entered disabled state
Feb  5 11:47:22 urknall kernel: [439676.605501] veth2040f5e: renamed from eth1
Feb  5 11:47:24 urknall kernel: [439678.586334] docker_gwbridge: port 40(vetha782956) entered disabled state
Feb  5 11:47:24 urknall kernel: [439678.604355] device vetha782956 left promiscuous mode
Feb  5 11:47:24 urknall kernel: [439678.604365] docker_gwbridge: port 40(vetha782956) entered disabled state
Feb  5 11:47:24 urknall dockerd[1676]: time=""2018-02-05T11:47:24.537105545+01:00"" level=warning msg=""Peer operation failed:Unable to find the peerDB for nid:dpu8s5w8nx7vtqyiqai8rflqm op:&{3 dpu8s5w8nx7vtqyiqai8rflqm  [] [] [] [] false false false DeleteNetwork}""
Feb  5 11:47:35 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 39)
Feb  5 11:47:50 urknall mfsmount: write file error, inode: 67783, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 33)
Feb  5 11:47:51 urknall kernel: [439705.200793] IPVS: __ip_vs_del_service: enter
Feb  5 11:48:11 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 2)
Feb  5 11:48:11 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 40)
Feb  5 11:48:23 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 3)
Feb  5 11:48:30 urknall dockerd[1676]: time=""2018-02-05T11:48:30.021923199+01:00"" level=error msg=""792a0ffc1c14e4fac6e81ae9c79de4c67148e60e3581ee9ba032aaa7ee78b14b cleanup: failed to delete container from containerd: no such container""
Feb  5 11:48:32 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 41)
Feb  5 11:48:34 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 4)
Feb  5 11:48:52 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 42)
Feb  5 11:48:52 urknall dhclient[1206]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x65d559d6)
Feb  5 11:48:52 urknall dhclient[1206]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 11:48:54 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 5)
Feb  5 11:49:04 urknall root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:49:05 urknall mfsmetalogger[8545]: sessions downloaded 742B/30.022665s (0.000 MB/s)
Feb  5 11:49:22 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000352s (2.108 MB/s)
Feb  5 11:49:22 urknall dhclient[1206]: bound to 192.168.99.3 -- renewal in 224 seconds.
Feb  5 11:49:22 urknall dockerd[1676]: time=""2018-02-05T11:49:22.197179631+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/delete type=""*events.ContainerDelete""
Feb  5 11:49:22 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 43)
Feb  5 11:49:22 urknall mfsmount: write file error, inode: 67783, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 34)
Feb  5 11:49:48 urknall mfsmount: write file error, inode: 67783, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 35)
Feb  5 11:49:48 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 44)
Feb  5 11:49:54 urknall dockerd[1676]: time=""2018-02-05T11:49:54.512666154+01:00"" level=error msg=""fatal task error"" error=""task: non-zero exit (255)"" module=node/agent/taskmanager node.id=4z5kkc15en2q29wt1crkxomc8 service.id=dun7e29phoubjq10posm5f24q task.id=008ucp94pkosrhsv7zasm51hh
Feb  5 11:49:54 urknall kernel: [439828.651560] IPVS: Creating netns size=2192 id=1957
Feb  5 11:49:55 urknall kernel: [439829.169906] br0: renamed from ov-00100d-dpu8s
Feb  5 11:49:55 urknall dockerd[1676]: time=""2018-02-05T11:49:55.063937041+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: invalid argument""
Feb  5 11:49:55 urknall kernel: [439829.218927] vxlan0: renamed from vx-00100d-dpu8s
Feb  5 11:49:55 urknall dockerd[1676]: time=""2018-02-05T11:49:55.106285802+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:49:55 urknall kernel: [439829.233267] device vxlan0 entered promiscuous mode
Feb  5 11:49:55 urknall kernel: [439829.236216] br0: port 1(vxlan0) entered forwarding state
Feb  5 11:49:55 urknall kernel: [439829.236236] br0: port 1(vxlan0) entered forwarding state
Feb  5 11:49:55 urknall dockerd[1676]: time=""2018-02-05T11:49:55.120645305+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:49:56 urknall kernel: [439830.365261] veth0: renamed from vethe550f6a
Feb  5 11:49:56 urknall dockerd[1676]: time=""2018-02-05T11:49:56.258586112+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:49:56 urknall kernel: [439830.385256] device veth0 entered promiscuous mode
Feb  5 11:49:58 urknall kernel: [439832.505545] device vethe467f7c entered promiscuous mode
Feb  5 11:50:00 urknall kernel: [439834.527886] veth1: renamed from veth9f98ea4
Feb  5 11:50:00 urknall dockerd[1676]: time=""2018-02-05T11:50:00.413372939+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:50:00 urknall kernel: [439834.541936] device veth1 entered promiscuous mode
Feb  5 11:50:00 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000438s (1.694 MB/s)
Feb  5 11:50:08 urknall dockerd[1676]: time=""2018-02-05T11:50:08.833933025+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:dpu8s5w8nx7vtqyiqai8rflqm leaving:false netPeers:1 entries:12 Queue qLen:0 netMsg/s:0""
Feb  5 11:50:08 urknall dockerd[1676]: time=""2018-02-05T11:50:08.835035985+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:50:08 urknall dockerd[1676]: time=""2018-02-05T11:50:08.835216525+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:50:08 urknall dockerd[1676]: time=""2018-02-05T11:50:08.835369189+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:50:08 urknall dockerd[1676]: time=""2018-02-05T11:50:08.835448181+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:50:08 urknall dockerd[1676]: time=""2018-02-05T11:50:08.835503566+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:gliwcqzlhjzjulhs3xdbxuqrv leaving:false netPeers:1 entries:6 Queue qLen:0 netMsg/s:0""
Feb  5 11:50:08 urknall dockerd[1676]: time=""2018-02-05T11:50:08.835580191+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:46 Queue qLen:0 netMsg/s:0""
Feb  5 11:50:08 urknall dockerd[1676]: time=""2018-02-05T11:50:08.835654156+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:50:08 urknall dockerd[1676]: time=""2018-02-05T11:50:08.835727869+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 11:50:10 urknall kernel: [439844.258584] br0: port 1(vxlan0) entered forwarding state
Feb  5 11:50:18 urknall kernel: [439852.280876] device vethc7329c3 entered promiscuous mode
Feb  5 11:50:44 urknall mfsmount: write file error, inode: 67783, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 36)
Feb  5 11:50:44 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:50:44 urknall mfsmount: write file error, inode: 67787, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 6)
Feb  5 11:50:44 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 45)
Feb  5 11:50:47 urknall dockerd[1676]: time=""2018-02-05T11:50:47.666860168+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/create type=""*events.ContainerCreate""
Feb  5 11:50:50 urknall dockerd[1676]: time=""2018-02-05T11:50:50+01:00"" level=info msg=""shim docker-containerd-shim started"" address=""/containerd-shim/moby/792a0ffc1c14e4fac6e81ae9c79de4c67148e60e3581ee9ba032aaa7ee78b14b/shim.sock"" debug=false module=""containerd/tasks"" pid=12285
Feb  5 11:50:50 urknall kernel: [439884.962145] IPVS: Creating netns size=2192 id=1958
Feb  5 11:50:51 urknall kernel: [439885.234381] eth0: renamed from veth4e7c042
Feb  5 11:50:51 urknall kernel: [439885.248521] br0: port 2(veth0) entered forwarding state
Feb  5 11:50:51 urknall kernel: [439885.248543] br0: port 2(veth0) entered forwarding state
Feb  5 11:50:51 urknall kernel: [439885.924293] eth1: renamed from veth6a35d9b
Feb  5 11:50:51 urknall kernel: [439885.948585] docker_gwbridge: port 21(vethe467f7c) entered forwarding state
Feb  5 11:50:51 urknall kernel: [439885.948635] docker_gwbridge: port 21(vethe467f7c) entered forwarding state
Feb  5 11:50:52 urknall kernel: [439886.701906] veth1043: renamed from vethecb6e62
Feb  5 11:50:52 urknall dockerd[1676]: time=""2018-02-05T11:50:52.585252199+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:50:52 urknall kernel: [439886.720517] device veth1043 entered promiscuous mode
Feb  5 11:50:53 urknall dockerd[1676]: time=""2018-02-05T11:50:53.107459203+01:00"" level=warning msg=""unknown container"" container=792a0ffc1c14e4fac6e81ae9c79de4c67148e60e3581ee9ba032aaa7ee78b14b module=libcontainerd namespace=plugins.moby
Feb  5 11:50:53 urknall dockerd[1676]: time=""2018-02-05T11:50:53.296299525+01:00"" level=warning msg=""unknown container"" container=792a0ffc1c14e4fac6e81ae9c79de4c67148e60e3581ee9ba032aaa7ee78b14b module=libcontainerd namespace=plugins.moby
Feb  5 11:50:56 urknall dhclient[1979]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x1f01ed1a)
Feb  5 11:50:56 urknall dhclient[1979]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 11:51:06 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 46)
Feb  5 11:51:06 urknall kernel: [439900.265651] br0: port 2(veth0) entered forwarding state
Feb  5 11:51:06 urknall kernel: [439900.969761] docker_gwbridge: port 21(vethe467f7c) entered forwarding state
Feb  5 11:51:16 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 2)
Feb  5 11:51:23 urknall marc: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:51:27 urknall mfsmount: read file error, inode: 67936, index: 0, chunk: 61732, version: 11 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:51:29 urknall mfsmount: read file error, inode: 67936, index: 0, chunk: 61732, version: 11 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:51:31 urknall mfsmount: read file error, inode: 67936, index: 0, chunk: 61732, version: 11 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:51:33 urknall mfsmount: read file error, inode: 67936, index: 0, chunk: 61732, version: 11 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:51:33 urknall dhclient[1979]: bound to 192.168.99.3 -- renewal in 250 seconds.
Feb  5 11:51:33 urknall mfsmetalogger[8545]: sessions downloaded 742B/23.100761s (0.000 MB/s)
Feb  5 11:51:33 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 47)
Feb  5 11:51:33 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 3)
Feb  5 11:51:34 urknall dockerd[1676]: time=""2018-02-05T11:51:34.611059330+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/create type=""*events.ContainerCreate""
Feb  5 11:51:34 urknall kernel: [439929.122562] veth1044: renamed from vethae5b2af
Feb  5 11:51:35 urknall dockerd[1676]: time=""2018-02-05T11:51:35.001296156+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:51:35 urknall kernel: [439929.141973] device veth1044 entered promiscuous mode
Feb  5 11:51:35 urknall kernel: [439929.147985] device veth997d831 entered promiscuous mode
Feb  5 11:51:35 urknall kernel: [439929.148202] docker_gwbridge: port 40(veth997d831) entered forwarding state
Feb  5 11:51:35 urknall kernel: [439929.148235] docker_gwbridge: port 40(veth997d831) entered forwarding state
Feb  5 11:51:35 urknall kernel: [439929.261455] docker_gwbridge: port 40(veth997d831) entered disabled state
Feb  5 11:51:35 urknall dockerd[1676]: time=""2018-02-05T11:51:35+01:00"" level=info msg=""shim docker-containerd-shim started"" address=""/containerd-shim/moby/e9ead84691f5fa24a4fea7d2ca2f603ab729d792f4e1a355d56b24e7cff3e14c/shim.sock"" debug=false module=""containerd/tasks"" pid=12502
Feb  5 11:51:35 urknall kernel: [439929.556275] IPVS: Creating netns size=2192 id=1959
Feb  5 11:51:35 urknall kernel: [439929.988796] eth0: renamed from veth4da4725
Feb  5 11:51:35 urknall kernel: [439930.014132] br0: port 4(veth1043) entered forwarding state
Feb  5 11:51:35 urknall kernel: [439930.014154] br0: port 4(veth1043) entered forwarding state
Feb  5 11:51:36 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 1)
Feb  5 11:51:36 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 2)
Feb  5 11:51:36 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 3)
Feb  5 11:51:36 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 4)
Feb  5 11:51:36 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 5)
Feb  5 11:51:36 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 6)
Feb  5 11:51:36 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 7)
Feb  5 11:51:36 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 8)
Feb  5 11:51:36 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 9)
Feb  5 11:51:37 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 10)
Feb  5 11:51:37 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 11)
Feb  5 11:51:37 urknall kernel: [439932.031842] eth1: renamed from veth4add636
Feb  5 11:51:37 urknall kernel: [439932.042748] docker_gwbridge: port 37(vethc7329c3) entered forwarding state
Feb  5 11:51:37 urknall kernel: [439932.042798] docker_gwbridge: port 37(vethc7329c3) entered forwarding state
Feb  5 11:51:38 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 12)
Feb  5 11:51:39 urknall kernel: [439933.315776] eth2: renamed from veth4d297c1
Feb  5 11:51:39 urknall kernel: [439933.330799] br0: port 3(veth1) entered forwarding state
Feb  5 11:51:39 urknall kernel: [439933.330821] br0: port 3(veth1) entered forwarding state
Feb  5 11:51:40 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 13)
Feb  5 11:51:44 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 14)
Feb  5 11:51:49 urknall dockerd[1676]: time=""2018-02-05T11:51:49.455392320+01:00"" level=warning msg=""deleteServiceInfoFromCluster NetworkDB DeleteEntry failed for e181b2e5fc881c505a15e78c0f923613581f1bddfafd17992bbdcf783e85d8ab 2vg30sgcyr0omhvavdcptdrgv err:cannot delete entry as the entry in table endpoint_table with network id 2vg30sgcyr0omhvavdcptdrgv and key e181b2e5fc881c505a15e78c0f923613581f1bddfafd17992bbdcf783e85d8ab does not exist""
Feb  5 11:51:49 urknall dockerd[1676]: time=""2018-02-05T11:51:49.456691795+01:00"" level=warning msg=""rmServiceBinding deleteServiceInfoFromCluster ldap-mrw-world_ldap e181b2e5fc881c505a15e78c0f923613581f1bddfafd17992bbdcf783e85d8ab aborted c.serviceBindings[skey] !ok""
Feb  5 11:51:49 urknall dockerd[1676]: time=""2018-02-05T11:51:49.502960504+01:00"" level=error msg=""Failed to receive from netlink: interrupted system call ""
Feb  5 11:51:49 urknall dockerd[1676]: time=""2018-02-05T11:51:49.771461372+01:00"" level=warning msg=""unknown container"" container=e9ead84691f5fa24a4fea7d2ca2f603ab729d792f4e1a355d56b24e7cff3e14c module=libcontainerd namespace=plugins.moby
Feb  5 11:51:49 urknall dockerd[1676]: time=""2018-02-05T11:51:49.875547166+01:00"" level=warning msg=""unknown container"" container=e9ead84691f5fa24a4fea7d2ca2f603ab729d792f4e1a355d56b24e7cff3e14c module=libcontainerd namespace=plugins.moby
Feb  5 11:51:50 urknall kernel: [439945.071333] br0: port 4(veth1043) entered forwarding state
Feb  5 11:51:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 15)
Feb  5 11:51:52 urknall kernel: [439947.055535] docker_gwbridge: port 37(vethc7329c3) entered forwarding state
Feb  5 11:51:54 urknall kernel: [439948.335691] br0: port 3(veth1) entered forwarding state
Feb  5 11:52:00 urknall dockerd[1676]: time=""2018-02-05T11:52:00.321543525+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:00 urknall dockerd[1676]: time=""2018-02-05T11:52:00.322494535+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:00 urknall dockerd[1676]: time=""2018-02-05T11:52:00.323020592+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:00 urknall dockerd[1676]: time=""2018-02-05T11:52:00.514016079+01:00"" level=warning msg=""unknown container"" container=e9ead84691f5fa24a4fea7d2ca2f603ab729d792f4e1a355d56b24e7cff3e14c module=libcontainerd namespace=plugins.moby
Feb  5 11:52:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 16)
Feb  5 11:52:07 urknall dockerd[1676]: time=""2018-02-05T11:52:07+01:00"" level=info msg=""shim reaped"" id=e9ead84691f5fa24a4fea7d2ca2f603ab729d792f4e1a355d56b24e7cff3e14c module=""containerd/tasks""
Feb  5 11:52:07 urknall dockerd[1676]: time=""2018-02-05T11:52:07.185962068+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=plugins.moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 11:52:07 urknall dockerd[1676]: time=""2018-02-05T11:52:07.186140401+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 11:52:07 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000550s (1.349 MB/s)
Feb  5 11:52:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 17)
Feb  5 11:52:13 urknall kernel: [439967.363943] docker_gwbridge: port 40(veth997d831) entered disabled state
Feb  5 11:52:13 urknall kernel: [439967.381794] device veth997d831 left promiscuous mode
Feb  5 11:52:13 urknall kernel: [439967.381806] docker_gwbridge: port 40(veth997d831) entered disabled state
Feb  5 11:52:13 urknall kernel: [439967.400329] br0: port 4(veth1043) entered disabled state
Feb  5 11:52:13 urknall kernel: [439967.400469] veth4da4725: renamed from eth0
Feb  5 11:52:13 urknall kernel: [439967.680379] IPVS: __ip_vs_del_service: enter
Feb  5 11:52:14 urknall kernel: [439969.006841] br0: port 4(veth1043) entered disabled state
Feb  5 11:52:14 urknall kernel: [439969.013139] device veth1043 left promiscuous mode
Feb  5 11:52:14 urknall kernel: [439969.013172] br0: port 4(veth1043) entered disabled state
Feb  5 11:52:14 urknall kernel: [439969.039110] br0: port 3(veth1) entered disabled state
Feb  5 11:52:14 urknall kernel: [439969.039291] veth4d297c1: renamed from eth2
Feb  5 11:52:14 urknall kernel: [439969.070944] IPVS: __ip_vs_del_service: enter
Feb  5 11:52:15 urknall kernel: [439969.472410] br0: port 5(veth1044) entered disabled state
Feb  5 11:52:15 urknall kernel: [439969.483568] device veth1044 left promiscuous mode
Feb  5 11:52:15 urknall kernel: [439969.483578] br0: port 5(veth1044) entered disabled state
Feb  5 11:52:15 urknall dockerd[1676]: time=""2018-02-05T11:52:15.911033925+01:00"" level=error msg=""d6e570d2de914967b7ec3c3b7d86d5235719df625ca0b2e320f2510e9125bdd6 cleanup: failed to delete container from containerd: no such container""
Feb  5 11:52:16 urknall kernel: [439970.760350] docker_gwbridge: port 37(vethc7329c3) entered disabled state
Feb  5 11:52:16 urknall kernel: [439970.760572] veth4add636: renamed from eth1
Feb  5 11:52:17 urknall kernel: [439971.955689] docker_gwbridge: port 37(vethc7329c3) entered disabled state
Feb  5 11:52:17 urknall kernel: [439971.961985] device vethc7329c3 left promiscuous mode
Feb  5 11:52:17 urknall kernel: [439971.961995] docker_gwbridge: port 37(vethc7329c3) entered disabled state
Feb  5 11:52:20 urknall kernel: [439974.339791] veth1045: renamed from veth94c16b8
Feb  5 11:52:20 urknall dockerd[1676]: time=""2018-02-05T11:52:20.209147926+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:20 urknall kernel: [439974.355563] device veth1045 entered promiscuous mode
Feb  5 11:52:20 urknall kernel: [439974.894622] device veth15fcb08 entered promiscuous mode
Feb  5 11:52:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 18)
Feb  5 11:52:23 urknall kernel: [439977.197639] br0: port 3(veth1) entered disabled state
Feb  5 11:52:23 urknall kernel: [439977.208461] device veth1 left promiscuous mode
Feb  5 11:52:23 urknall kernel: [439977.208472] br0: port 3(veth1) entered disabled state
Feb  5 11:52:23 urknall kernel: [439977.295579] IPVS: __ip_vs_del_service: enter
Feb  5 11:52:23 urknall kernel: [439977.295591] IPVS: __ip_vs_del_service: enter
Feb  5 11:52:25 urknall dockerd[1676]: time=""2018-02-05T11:52:25.221572872+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/delete type=""*events.ContainerDelete""
Feb  5 11:52:31 urknall dockerd[1676]: time=""2018-02-05T11:52:31.827055756+01:00"" level=error msg=""fatal task error"" error=""task: non-zero exit (1)"" module=node/agent/taskmanager node.id=4z5kkc15en2q29wt1crkxomc8 service.id=gq2beoh3n3f0y9ix02ri5kiyl task.id=w2pc86ezvwi62zyvli54xg5g0
Feb  5 11:52:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 19)
Feb  5 11:52:34 urknall kernel: [439988.202974] IPVS: Creating netns size=2192 id=1960
Feb  5 11:52:34 urknall dockerd[1676]: time=""2018-02-05T11:52:34.189823883+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: invalid argument""
Feb  5 11:52:34 urknall kernel: [439988.338652] br0: renamed from ov-00100b-gliwc
Feb  5 11:52:34 urknall kernel: [439988.390437] vxlan0: renamed from vx-00100b-gliwc
Feb  5 11:52:34 urknall kernel: [439988.405652] device vxlan0 entered promiscuous mode
Feb  5 11:52:34 urknall dockerd[1676]: time=""2018-02-05T11:52:34.265524840+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:34 urknall dockerd[1676]: time=""2018-02-05T11:52:34.268166234+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:34 urknall kernel: [439988.416995] br0: port 1(vxlan0) entered forwarding state
Feb  5 11:52:34 urknall kernel: [439988.417019] br0: port 1(vxlan0) entered forwarding state
Feb  5 11:52:35 urknall kernel: [439989.974775] veth0: renamed from veth3fef976
Feb  5 11:52:35 urknall dockerd[1676]: time=""2018-02-05T11:52:35.841191421+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:52:35 urknall kernel: [439989.989610] device veth0 entered promiscuous mode
Feb  5 11:52:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 20)
Feb  5 11:52:49 urknall kernel: [440003.446684] br0: port 1(vxlan0) entered forwarding state
Feb  5 11:52:49 urknall dockerd[1676]: time=""2018-02-05T11:52:49.437228331+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/create type=""*events.ContainerCreate""
Feb  5 11:52:50 urknall dockerd[1676]: time=""2018-02-05T11:52:50+01:00"" level=info msg=""shim docker-containerd-shim started"" address=""/containerd-shim/moby/d6e570d2de914967b7ec3c3b7d86d5235719df625ca0b2e320f2510e9125bdd6/shim.sock"" debug=false module=""containerd/tasks"" pid=12914
Feb  5 11:52:50 urknall kernel: [440004.988381] IPVS: Creating netns size=2192 id=1961
Feb  5 11:52:51 urknall kernel: [440005.757851] eth0: renamed from veth2f79259
Feb  5 11:52:51 urknall kernel: [440005.767899] br0: port 4(veth1045) entered forwarding state
Feb  5 11:52:51 urknall kernel: [440005.767922] br0: port 4(veth1045) entered forwarding state
Feb  5 11:52:52 urknall kernel: [440006.165240] eth1: renamed from veth4ae28bc
Feb  5 11:52:52 urknall kernel: [440006.183817] docker_gwbridge: port 37(veth15fcb08) entered forwarding state
Feb  5 11:52:52 urknall kernel: [440006.183871] docker_gwbridge: port 37(veth15fcb08) entered forwarding state
Feb  5 11:52:52 urknall kernel: [440006.609355] eth2: renamed from veth6aa3f55
Feb  5 11:52:52 urknall kernel: [440006.627949] br0: port 2(veth0) entered forwarding state
Feb  5 11:52:52 urknall kernel: [440006.627974] br0: port 2(veth0) entered forwarding state
Feb  5 11:52:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 21)
Feb  5 11:52:54 urknall dockerd[1676]: time=""2018-02-05T11:52:54.781558470+01:00"" level=warning msg=""unknown container"" container=d6e570d2de914967b7ec3c3b7d86d5235719df625ca0b2e320f2510e9125bdd6 module=libcontainerd namespace=plugins.moby
Feb  5 11:52:55 urknall dockerd[1676]: time=""2018-02-05T11:52:55.157119449+01:00"" level=warning msg=""unknown container"" container=d6e570d2de914967b7ec3c3b7d86d5235719df625ca0b2e320f2510e9125bdd6 module=libcontainerd namespace=plugins.moby
Feb  5 11:53:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 22)
Feb  5 11:53:05 urknall dhclient[1206]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x65d559d6)
Feb  5 11:53:05 urknall dhclient[1206]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 11:53:06 urknall kernel: [440020.793028] br0: port 4(veth1045) entered forwarding state
Feb  5 11:53:07 urknall kernel: [440021.244973] docker_gwbridge: port 37(veth15fcb08) entered forwarding state
Feb  5 11:53:07 urknall kernel: [440021.657050] br0: port 2(veth0) entered forwarding state
Feb  5 11:53:12 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 48)
Feb  5 11:53:12 urknall mfsmount: write file error, inode: 413364, index: 0 - Chunk write error (Disconnected) (try counter: 1)
Feb  5 11:53:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 23)
Feb  5 11:53:22 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:53:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 24)
Feb  5 11:53:24 urknall mfsmount: write file error, inode: 413364, index: 0 - Chunk write error (Disconnected) (try counter: 2)
Feb  5 11:53:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 25)
Feb  5 11:53:33 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 49)
Feb  5 11:53:34 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 2)
Feb  5 11:53:35 urknall root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:53:35 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000619s (1.199 MB/s)
Feb  5 11:53:36 urknall mfsmount: write file error, inode: 413364, index: 0 - Chunk write error (Disconnected) (try counter: 3)
Feb  5 11:53:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 26)
Feb  5 11:53:47 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 3)
Feb  5 11:53:50 urknall dhclient[1206]: bound to 192.168.99.3 -- renewal in 267 seconds.
Feb  5 11:53:50 urknall mfsmount: write file error, inode: 413364, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 4)
Feb  5 11:53:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 27)
Feb  5 11:53:54 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 50)
Feb  5 11:54:01 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 4)
Feb  5 11:54:01 urknall mfschunkserver[8794]: replication error: Chunkserver communication timed out: 192.168.99.7:9422
Feb  5 11:54:02 urknall mfsmount: write file error, inode: 413364, index: 0 - Chunk write error (Disconnected) (try counter: 5)
Feb  5 11:54:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 28)
Feb  5 11:54:03 urknall mfsmetalogger[8545]: sessions downloaded 742B/1.775205s (0.000 MB/s)
Feb  5 11:54:07 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:54:09 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:54:11 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:54:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 29)
Feb  5 11:54:13 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:54:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 30)
Feb  5 11:54:27 urknall kernel: [440101.242413] veth1046: renamed from veth53fe535
Feb  5 11:54:27 urknall kernel: [440101.260214] device veth1046 entered promiscuous mode
Feb  5 11:54:27 urknall dockerd[1676]: time=""2018-02-05T11:54:27.100613684+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:54:27 urknall kernel: [440101.288595] device vethf4cc4fc entered promiscuous mode
Feb  5 11:54:27 urknall kernel: [440101.288876] docker_gwbridge: port 40(vethf4cc4fc) entered forwarding state
Feb  5 11:54:27 urknall kernel: [440101.288904] docker_gwbridge: port 40(vethf4cc4fc) entered forwarding state
Feb  5 11:54:27 urknall kernel: [440101.507321] docker_gwbridge: port 40(vethf4cc4fc) entered disabled state
Feb  5 11:54:28 urknall dockerd[1676]: time=""2018-02-05T11:54:28.496588535+01:00"" level=warning msg=""unknown container"" container=d6e570d2de914967b7ec3c3b7d86d5235719df625ca0b2e320f2510e9125bdd6 module=libcontainerd namespace=plugins.moby
Feb  5 11:54:28 urknall dockerd[1676]: time=""2018-02-05T11:54:28+01:00"" level=info msg=""shim reaped"" id=d6e570d2de914967b7ec3c3b7d86d5235719df625ca0b2e320f2510e9125bdd6 module=""containerd/tasks""
Feb  5 11:54:28 urknall dockerd[1676]: time=""2018-02-05T11:54:28.597322211+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 11:54:28 urknall dockerd[1676]: time=""2018-02-05T11:54:28.597143313+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=plugins.moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 11:54:30 urknall kernel: [440104.337282] br0: port 4(veth1045) entered disabled state
Feb  5 11:54:30 urknall kernel: [440104.337416] veth2f79259: renamed from eth0
Feb  5 11:54:30 urknall kernel: [440104.373433] IPVS: __ip_vs_del_service: enter
Feb  5 11:54:30 urknall kernel: [440105.124455] br0: port 4(veth1045) entered disabled state
Feb  5 11:54:30 urknall kernel: [440105.133302] device veth1045 left promiscuous mode
Feb  5 11:54:30 urknall kernel: [440105.133312] br0: port 4(veth1045) entered disabled state
Feb  5 11:54:31 urknall kernel: [440105.164525] br0: port 2(veth0) entered disabled state
Feb  5 11:54:31 urknall kernel: [440105.164586] br0: port 1(vxlan0) entered disabled state
Feb  5 11:54:31 urknall kernel: [440105.170421] ov-00100b-gliwc: renamed from br0
Feb  5 11:54:31 urknall kernel: [440105.188133] device veth0 left promiscuous mode
Feb  5 11:54:31 urknall kernel: [440105.188179] ov-00100b-gliwc: port 2(veth0) entered disabled state
Feb  5 11:54:31 urknall kernel: [440105.200077] device vxlan0 left promiscuous mode
Feb  5 11:54:31 urknall kernel: [440105.200115] ov-00100b-gliwc: port 1(vxlan0) entered disabled state
Feb  5 11:54:31 urknall kernel: [440105.253747] vx-00100b-gliwc: renamed from vxlan0
Feb  5 11:54:31 urknall kernel: [440105.295739] veth3fef976: renamed from veth0
Feb  5 11:54:31 urknall kernel: [440105.364172] veth6aa3f55: renamed from eth2
Feb  5 11:54:31 urknall kernel: [440106.023498] veth2: renamed from veth0f94f9f
Feb  5 11:54:31 urknall dockerd[1676]: time=""2018-02-05T11:54:31.877422051+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:54:31 urknall kernel: [440106.040510] device veth2 entered promiscuous mode
Feb  5 11:54:31 urknall kernel: [440106.040830] br0: port 3(veth2) entered forwarding state
Feb  5 11:54:31 urknall kernel: [440106.040844] br0: port 3(veth2) entered forwarding state
Feb  5 11:54:32 urknall kernel: [440106.335845] br0: port 3(veth2) entered disabled state
Feb  5 11:54:32 urknall kernel: [440106.800846] docker_gwbridge: port 37(veth15fcb08) entered disabled state
Feb  5 11:54:32 urknall kernel: [440106.801099] veth4ae28bc: renamed from eth1
Feb  5 11:54:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 31)
Feb  5 11:54:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 32)
Feb  5 11:54:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 33)
Feb  5 11:55:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 34)
Feb  5 11:55:09 urknall dockerd[1676]: time=""2018-02-05T11:55:09.035431938+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:48 Queue qLen:0 netMsg/s:0""
Feb  5 11:55:09 urknall dockerd[1676]: time=""2018-02-05T11:55:09.035652147+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:55:09 urknall dockerd[1676]: time=""2018-02-05T11:55:09.035731265+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 11:55:09 urknall dockerd[1676]: time=""2018-02-05T11:55:09.035996584+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:dpu8s5w8nx7vtqyiqai8rflqm leaving:false netPeers:1 entries:13 Queue qLen:0 netMsg/s:0""
Feb  5 11:55:09 urknall dockerd[1676]: time=""2018-02-05T11:55:09.036052077+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:gliwcqzlhjzjulhs3xdbxuqrv leaving:false netPeers:1 entries:6 Queue qLen:0 netMsg/s:0""
Feb  5 11:55:09 urknall dockerd[1676]: time=""2018-02-05T11:55:09.036130815+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:55:09 urknall dockerd[1676]: time=""2018-02-05T11:55:09.036204016+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 11:55:09 urknall dockerd[1676]: time=""2018-02-05T11:55:09.036276625+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:55:09 urknall dockerd[1676]: time=""2018-02-05T11:55:09.036407726+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 11:55:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 35)
Feb  5 11:55:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 36)
Feb  5 11:55:25 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:55:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 37)
Feb  5 11:55:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 38)
Feb  5 11:55:43 urknall dhclient[1979]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x1f01ed1a)
Feb  5 11:55:43 urknall dhclient[1979]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 11:55:46 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 2)
Feb  5 11:55:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 39)
Feb  5 11:55:58 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 3)
Feb  5 11:56:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 40)
Feb  5 11:56:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 41)
Feb  5 11:56:17 urknall marc: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:56:17 urknall dhclient[1979]: bound to 192.168.99.3 -- renewal in 238 seconds.
Feb  5 11:56:17 urknall mfsmetalogger[8545]: sessions downloaded 742B/55.797584s (0.000 MB/s)
Feb  5 11:56:17 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 4)
Feb  5 11:56:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 42)
Feb  5 11:56:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 43)
Feb  5 11:56:36 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.001396s (0.532 MB/s)
Feb  5 11:56:38 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 22)
Feb  5 11:56:38 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 5)
Feb  5 11:56:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 44)
Feb  5 11:56:50 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 6)
Feb  5 11:56:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 45)
Feb  5 11:56:53 urknall mfsmount: write file error, inode: 413353, index: 0 - Chunk write error (Disconnected) (try counter: 1)
Feb  5 11:57:02 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 7)
Feb  5 11:57:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 46)
Feb  5 11:57:05 urknall mfsmount: write file error, inode: 196764, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 23)
Feb  5 11:57:05 urknall mfsmount: write file error, inode: 413353, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 2)
Feb  5 11:57:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 47)
Feb  5 11:57:16 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunk write error (Disconnected) (try counter: 37)
Feb  5 11:57:17 urknall mfsmount: write file error, inode: 413353, index: 0 - Chunk write error (Disconnected) (try counter: 3)
Feb  5 11:57:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 48)
Feb  5 11:57:24 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000974s (0.762 MB/s)
Feb  5 11:57:24 urknall kernel: [440278.434954] docker_gwbridge: port 37(veth15fcb08) entered disabled state
Feb  5 11:57:24 urknall kernel: [440278.441134] device veth15fcb08 left promiscuous mode
Feb  5 11:57:24 urknall kernel: [440278.441143] docker_gwbridge: port 37(veth15fcb08) entered disabled state
Feb  5 11:57:24 urknall dockerd[1676]: time=""2018-02-05T11:57:24.258181805+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/create type=""*events.ContainerCreate""
Feb  5 11:57:29 urknall mfsmount: write file error, inode: 413353, index: 0 - Chunk write error (Disconnected) (try counter: 4)
Feb  5 11:57:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 49)
Feb  5 11:57:35 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 24)
Feb  5 11:57:37 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunk write error (Disconnected) (try counter: 38)
Feb  5 11:57:40 urknall dockerd[1676]: time=""2018-02-05T11:57:40+01:00"" level=info msg=""shim docker-containerd-shim started"" address=""/containerd-shim/moby/4e9dc2919f07e3b5c419533ffdd0831950a4c1139915b497036137d913c48601/shim.sock"" debug=false module=""containerd/tasks"" pid=13329
Feb  5 11:57:40 urknall kernel: [440294.839686] IPVS: Creating netns size=2192 id=1962
Feb  5 11:57:40 urknall kernel: [440295.132327] eth0: renamed from veth29c0558
Feb  5 11:57:40 urknall kernel: [440295.171823] br0: port 5(veth1046) entered forwarding state
Feb  5 11:57:40 urknall kernel: [440295.171844] br0: port 5(veth1046) entered forwarding state
Feb  5 11:57:42 urknall kernel: [440296.450930] eth1: renamed from vethdc6891f
Feb  5 11:57:42 urknall kernel: [440296.474116] docker_gwbridge: port 40(vethf4cc4fc) entered forwarding state
Feb  5 11:57:42 urknall kernel: [440296.474176] docker_gwbridge: port 40(vethf4cc4fc) entered forwarding state
Feb  5 11:57:42 urknall kernel: [440296.484187] IPVS: __ip_vs_del_service: enter
Feb  5 11:57:42 urknall dockerd[1676]: time=""2018-02-05T11:57:42.642311406+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/delete type=""*events.ContainerDelete""
Feb  5 11:57:42 urknall kernel: [440296.846810] eth2: renamed from vethc48ad91
Feb  5 11:57:42 urknall kernel: [440296.861075] br0: port 3(veth2) entered forwarding state
Feb  5 11:57:42 urknall kernel: [440296.861097] br0: port 3(veth2) entered forwarding state
Feb  5 11:57:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 50)
Feb  5 11:57:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 51)
Feb  5 11:57:56 urknall kernel: [440310.209720] br0: port 5(veth1046) entered forwarding state
Feb  5 11:57:57 urknall kernel: [440311.518045] docker_gwbridge: port 40(vethf4cc4fc) entered forwarding state
Feb  5 11:57:57 urknall kernel: [440311.901930] br0: port 3(veth2) entered forwarding state
Feb  5 11:58:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 52)
Feb  5 11:58:03 urknall dockerd[1676]: time=""2018-02-05T11:58:03.383062223+01:00"" level=error msg=""fatal task error"" error=""task: non-zero exit (255)"" module=node/agent/taskmanager node.id=4z5kkc15en2q29wt1crkxomc8 service.id=dun7e29phoubjq10posm5f24q task.id=ln2d322kranyv8xu8xm9138ei
Feb  5 11:58:03 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 8)
Feb  5 11:58:03 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 51)
Feb  5 11:58:03 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000733s (1.012 MB/s)
Feb  5 11:58:03 urknall dockerd[1676]: time=""2018-02-05T11:58:03.595713851+01:00"" level=warning msg=""unknown container"" container=4e9dc2919f07e3b5c419533ffdd0831950a4c1139915b497036137d913c48601 module=libcontainerd namespace=plugins.moby
Feb  5 11:58:03 urknall dockerd[1676]: time=""2018-02-05T11:58:03.800291983+01:00"" level=warning msg=""unknown container"" container=4e9dc2919f07e3b5c419533ffdd0831950a4c1139915b497036137d913c48601 module=libcontainerd namespace=plugins.moby
Feb  5 11:58:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 53)
Feb  5 11:58:15 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 9)
Feb  5 11:58:16 urknall dhclient[1206]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x65d559d6)
Feb  5 11:58:16 urknall dhclient[1206]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 11:58:16 urknall root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:58:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 54)
Feb  5 11:58:23 urknall mfsmount: read file error, inode: 27960, index: 0, chunk: 542619, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:58:24 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 52)
Feb  5 11:58:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 55)
Feb  5 11:58:33 urknall dockerd[1676]: time=""2018-02-05T11:58:33.426478867+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:58:33 urknall dockerd[1676]: time=""2018-02-05T11:58:33.427930972+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:58:33 urknall dockerd[1676]: time=""2018-02-05T11:58:33.428072352+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 11:58:33 urknall dockerd[1676]: time=""2018-02-05T11:58:33.701412653+01:00"" level=warning msg=""unknown container"" container=4e9dc2919f07e3b5c419533ffdd0831950a4c1139915b497036137d913c48601 module=libcontainerd namespace=plugins.moby
Feb  5 11:58:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 56)
Feb  5 11:58:49 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 39)
Feb  5 11:58:49 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 25)
Feb  5 11:58:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 57)
Feb  5 11:58:54 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 53)
Feb  5 11:59:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 58)
Feb  5 11:59:03 urknall dhclient[1206]: bound to 192.168.99.3 -- renewal in 280 seconds.
Feb  5 11:59:04 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000484s (1.533 MB/s)
Feb  5 11:59:04 urknall dockerd[1676]: time=""2018-02-05T11:59:04+01:00"" level=info msg=""shim reaped"" id=4e9dc2919f07e3b5c419533ffdd0831950a4c1139915b497036137d913c48601 module=""containerd/tasks""
Feb  5 11:59:04 urknall dockerd[1676]: time=""2018-02-05T11:59:04.435883342+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=plugins.moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 11:59:04 urknall dockerd[1676]: time=""2018-02-05T11:59:04.436053942+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 11:59:04 urknall kernel: [440378.645159] br0: port 5(veth1046) entered disabled state
Feb  5 11:59:04 urknall kernel: [440378.662653] veth29c0558: renamed from eth0
Feb  5 11:59:06 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 11:59:06 urknall kernel: [440380.867797] IPVS: __ip_vs_del_service: enter
Feb  5 11:59:08 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 11:59:10 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunk write error (Disconnected) (try counter: 26)
Feb  5 11:59:10 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunk write error (Disconnected) (try counter: 40)
Feb  5 11:59:10 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 11:59:12 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 11:59:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 59)
Feb  5 11:59:14 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 11:59:16 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 11:59:18 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 11:59:20 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)
Feb  5 11:59:22 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)
Feb  5 11:59:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 60)
Feb  5 11:59:24 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 10)
Feb  5 11:59:24 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 54)
Feb  5 11:59:26 urknall mfsmount: read file error, inode: 370135, index: 0, chunk: 616311, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 11)
Feb  5 11:59:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 61)
Feb  5 11:59:34 urknall kernel: [440408.844171] br0: port 5(veth1046) entered disabled state
Feb  5 11:59:34 urknall kernel: [440408.852282] device veth1046 left promiscuous mode
Feb  5 11:59:34 urknall kernel: [440408.852291] br0: port 5(veth1046) entered disabled state
Feb  5 11:59:34 urknall kernel: [440408.887228] br0: port 3(veth2) entered disabled state
Feb  5 11:59:34 urknall kernel: [440408.887362] vethc48ad91: renamed from eth2
Feb  5 11:59:34 urknall kernel: [440408.920116] IPVS: __ip_vs_del_service: enter
Feb  5 11:59:35 urknall kernel: [440409.307652] docker_gwbridge: port 40(vethf4cc4fc) entered disabled state
Feb  5 11:59:35 urknall kernel: [440409.307934] vethdc6891f: renamed from eth1
Feb  5 11:59:35 urknall kernel: [440410.053143] docker_gwbridge: port 40(vethf4cc4fc) entered disabled state
Feb  5 11:59:35 urknall kernel: [440410.073836] device vethf4cc4fc left promiscuous mode
Feb  5 11:59:35 urknall kernel: [440410.073845] docker_gwbridge: port 40(vethf4cc4fc) entered disabled state
Feb  5 11:59:37 urknall kernel: [440411.519059] br0: port 3(veth2) entered disabled state
Feb  5 11:59:37 urknall kernel: [440411.541079] device veth2 left promiscuous mode
Feb  5 11:59:37 urknall kernel: [440411.541090] br0: port 3(veth2) entered disabled state
Feb  5 11:59:37 urknall kernel: [440411.646714] IPVS: __ip_vs_del_service: enter
Feb  5 11:59:37 urknall kernel: [440411.646726] IPVS: __ip_vs_del_service: enter
Feb  5 11:59:43 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 62)
Feb  5 11:59:49 urknall mfsmount: write file error, inode: 3791, index: 0 - Timeout after 10041 ms (Timeout) (try counter: 55)
Feb  5 11:59:51 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 10)
Feb  5 11:59:51 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunk write error (Disconnected) (try counter: 41)
Feb  5 11:59:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 63)
Feb  5 11:59:55 urknall mfsmount: write file error, inode: 67786, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 1)
Feb  5 11:59:55 urknall dockerd[1676]: time=""2018-02-05T11:59:55.351173147+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/delete type=""*events.ContainerDelete""
Feb  5 12:00:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 64)
Feb  5 12:00:09 urknall dockerd[1676]: time=""2018-02-05T12:00:09.234567161+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:dpu8s5w8nx7vtqyiqai8rflqm leaving:false netPeers:1 entries:12 Queue qLen:0 netMsg/s:0""
Feb  5 12:00:09 urknall dockerd[1676]: time=""2018-02-05T12:00:09.236714307+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:47 Queue qLen:0 netMsg/s:0""
Feb  5 12:00:09 urknall dockerd[1676]: time=""2018-02-05T12:00:09.238627818+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:00:09 urknall dockerd[1676]: time=""2018-02-05T12:00:09.240044566+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:00:09 urknall dockerd[1676]: time=""2018-02-05T12:00:09.241408979+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:00:09 urknall dockerd[1676]: time=""2018-02-05T12:00:09.242163874+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:00:09 urknall dockerd[1676]: time=""2018-02-05T12:00:09.242584012+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:00:09 urknall dockerd[1676]: time=""2018-02-05T12:00:09.243010542+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:gliwcqzlhjzjulhs3xdbxuqrv leaving:false netPeers:1 entries:6 Queue qLen:0 netMsg/s:0""
Feb  5 12:00:09 urknall dockerd[1676]: time=""2018-02-05T12:00:09.243935699+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 12:00:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 65)
Feb  5 12:00:13 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.001120s (0.662 MB/s)
Feb  5 12:00:13 urknall dockerd[1676]: time=""2018-02-05T12:00:13.619278525+01:00"" level=error msg=""fatal task error"" error=""task: non-zero exit (1)"" module=node/agent/taskmanager node.id=4z5kkc15en2q29wt1crkxomc8 service.id=gq2beoh3n3f0y9ix02ri5kiyl task.id=zdiqqm853yjfol0skyhfe2sza
Feb  5 12:00:15 urknall dhclient[1979]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x1f01ed1a)
Feb  5 12:00:15 urknall dhclient[1979]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 12:00:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 66)
Feb  5 12:00:23 urknall mfsmount: write file error, inode: 3791, index: 0 - Timeout after 10025 ms (Timeout) (try counter: 56)
Feb  5 12:00:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 67)
Feb  5 12:00:33 urknall mfsmount: write file error, inode: 66435, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 12:00:33 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 27)
Feb  5 12:00:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 68)
Feb  5 12:00:52 urknall marc: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:00:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 69)
Feb  5 12:00:56 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 57)
Feb  5 12:00:59 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 42)
Feb  5 12:01:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 70)
Feb  5 12:01:03 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 28)
Feb  5 12:01:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 71)
Feb  5 12:01:15 urknall dhclient[1979]: bound to 192.168.99.3 -- renewal in 190 seconds.
Feb  5 12:01:21 urknall mfsmetalogger[8545]: sessions downloaded 742B/17.311260s (0.000 MB/s)
Feb  5 12:01:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 72)
Feb  5 12:01:30 urknall kernel: [440524.298418] veth1047: renamed from vethb05a062
Feb  5 12:01:30 urknall dockerd[1676]: time=""2018-02-05T12:01:30.099781278+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:01:30 urknall kernel: [440524.313521] device veth1047 entered promiscuous mode
Feb  5 12:01:30 urknall kernel: [440524.328893] device veth886ee36 entered promiscuous mode
Feb  5 12:01:30 urknall kernel: [440524.329592] docker_gwbridge: port 37(veth886ee36) entered forwarding state
Feb  5 12:01:30 urknall kernel: [440524.329620] docker_gwbridge: port 37(veth886ee36) entered forwarding state
Feb  5 12:01:30 urknall kernel: [440524.993196] docker_gwbridge: port 37(veth886ee36) entered disabled state
Feb  5 12:01:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 73)
Feb  5 12:01:39 urknall kernel: [440533.831872] IPVS: Creating netns size=2192 id=1963
Feb  5 12:01:40 urknall kernel: [440534.239771] br0: renamed from ov-00100b-gliwc
Feb  5 12:01:40 urknall dockerd[1676]: time=""2018-02-05T12:01:40.049308941+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: invalid argument""
Feb  5 12:01:40 urknall kernel: [440534.304748] vxlan0: renamed from vx-00100b-gliwc
Feb  5 12:01:40 urknall dockerd[1676]: time=""2018-02-05T12:01:40.110911929+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:01:40 urknall dockerd[1676]: time=""2018-02-05T12:01:40.112023572+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:01:40 urknall kernel: [440534.328044] device vxlan0 entered promiscuous mode
Feb  5 12:01:40 urknall kernel: [440534.331001] br0: port 1(vxlan0) entered forwarding state
Feb  5 12:01:40 urknall kernel: [440534.331024] br0: port 1(vxlan0) entered forwarding state
Feb  5 12:01:40 urknall kernel: [440534.985662] veth0: renamed from veth3162d51
Feb  5 12:01:40 urknall kernel: [440535.007002] device veth0 entered promiscuous mode
Feb  5 12:01:40 urknall kernel: [440535.007320] br0: port 2(veth0) entered forwarding state
Feb  5 12:01:40 urknall kernel: [440535.007332] br0: port 2(veth0) entered forwarding state
Feb  5 12:01:40 urknall dockerd[1676]: time=""2018-02-05T12:01:40.794124810+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:01:41 urknall kernel: [440535.327216] br0: port 2(veth0) entered disabled state
Feb  5 12:01:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 74)
Feb  5 12:01:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 75)
Feb  5 12:01:55 urknall kernel: [440549.376159] br0: port 1(vxlan0) entered forwarding state
Feb  5 12:02:00 urknall mfsmount: write file error, inode: 67787, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 7)
Feb  5 12:02:00 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 1)
Feb  5 12:02:00 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 58)
Feb  5 12:02:00 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000655s (1.133 MB/s)
Feb  5 12:02:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 76)
Feb  5 12:02:11 urknall mfsmount: write file error, inode: 67783, index: 0 - Timeout after 10005 ms (Timeout) (try counter: 43)
Feb  5 12:02:12 urknall mfsmount: write file error, inode: 67786, index: 0 - Chunk write error (Disconnected) (try counter: 2)
Feb  5 12:02:12 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 2)
Feb  5 12:02:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 77)
Feb  5 12:02:21 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 59)
Feb  5 12:02:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 78)
Feb  5 12:02:32 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunk write error (Disconnected) (try counter: 44)
Feb  5 12:02:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 79)
Feb  5 12:02:32 urknall mfsmount: write file error, inode: 67786, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 3)
Feb  5 12:02:32 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 3)
Feb  5 12:02:41 urknall dockerd[1676]: time=""2018-02-05T12:02:41.038454337+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/create type=""*events.ContainerCreate""
Feb  5 12:02:41 urknall dockerd[1676]: time=""2018-02-05T12:02:41+01:00"" level=info msg=""shim docker-containerd-shim started"" address=""/containerd-shim/moby/fb1d30cb50dcd52e996d0837d78afdcdd68ab0f8e51a6fed6619ac53040cc0c5/shim.sock"" debug=false module=""containerd/tasks"" pid=13841
Feb  5 12:02:41 urknall kernel: [440595.995097] IPVS: Creating netns size=2192 id=1964
Feb  5 12:02:42 urknall kernel: [440596.250518] eth0: renamed from vethd926fc2
Feb  5 12:02:42 urknall kernel: [440596.267050] br0: port 4(veth1047) entered forwarding state
Feb  5 12:02:42 urknall kernel: [440596.267075] br0: port 4(veth1047) entered forwarding state
Feb  5 12:02:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 80)
Feb  5 12:02:42 urknall kernel: [440597.216963] eth1: renamed from veth769d6bf
Feb  5 12:02:43 urknall kernel: [440597.231884] docker_gwbridge: port 37(veth886ee36) entered forwarding state
Feb  5 12:02:43 urknall kernel: [440597.231940] docker_gwbridge: port 37(veth886ee36) entered forwarding state
Feb  5 12:02:43 urknall kernel: [440598.042829] eth2: renamed from vethe98a694
Feb  5 12:02:43 urknall kernel: [440598.064148] br0: port 2(veth0) entered forwarding state
Feb  5 12:02:43 urknall kernel: [440598.064170] br0: port 2(veth0) entered forwarding state
Feb  5 12:02:45 urknall dockerd[1676]: time=""2018-02-05T12:02:45.015428623+01:00"" level=warning msg=""unknown container"" container=fb1d30cb50dcd52e996d0837d78afdcdd68ab0f8e51a6fed6619ac53040cc0c5 module=libcontainerd namespace=plugins.moby
Feb  5 12:02:45 urknall dockerd[1676]: time=""2018-02-05T12:02:45.126432307+01:00"" level=warning msg=""unknown container"" container=fb1d30cb50dcd52e996d0837d78afdcdd68ab0f8e51a6fed6619ac53040cc0c5 module=libcontainerd namespace=plugins.moby
Feb  5 12:02:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 81)
Feb  5 12:02:57 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 60)
Feb  5 12:02:57 urknall kernel: [440611.300122] br0: port 4(veth1047) entered forwarding state
Feb  5 12:02:58 urknall kernel: [440612.260226] docker_gwbridge: port 37(veth886ee36) entered forwarding state
Feb  5 12:02:58 urknall kernel: [440613.092320] br0: port 2(veth0) entered forwarding state
Feb  5 12:03:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 82)
Feb  5 12:03:07 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 4)
Feb  5 12:03:07 urknall mfsmount: write file error, inode: 67787, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 8)
Feb  5 12:03:07 urknall mfsmount: write file error, inode: 67786, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 4)
Feb  5 12:03:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 83)
Feb  5 12:03:22 urknall mfsmount: write file error, inode: 67786, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 5)
Feb  5 12:03:22 urknall mfsmount: write file error, inode: 67787, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 9)
Feb  5 12:03:22 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 5)
Feb  5 12:03:22 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 61)
Feb  5 12:03:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 84)
Feb  5 12:03:27 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 12:03:29 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 12:03:31 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 12:03:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 85)
Feb  5 12:03:33 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 12:03:34 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 6)
Feb  5 12:03:35 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 12:03:37 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 12:03:39 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 12:03:41 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)
Feb  5 12:03:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 86)
Feb  5 12:03:43 urknall dhclient[1206]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x65d559d6)
Feb  5 12:03:43 urknall dhclient[1206]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 12:03:43 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)
Feb  5 12:03:45 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 10)
Feb  5 12:03:47 urknall root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:03:47 urknall mfsmetalogger[8545]: sessions downloaded 742B/3.401270s (0.000 MB/s)
Feb  5 12:03:47 urknall dhclient[1206]: bound to 192.168.99.3 -- renewal in 263 seconds.
Feb  5 12:03:47 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 7)
Feb  5 12:03:47 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 62)
Feb  5 12:03:47 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 11)
Feb  5 12:03:50 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 12)
Feb  5 12:03:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 87)
Feb  5 12:03:54 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 13)
Feb  5 12:04:02 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 14)
Feb  5 12:04:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 88)
Feb  5 12:04:08 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 63)
Feb  5 12:04:08 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 8)
Feb  5 12:04:12 urknall mfsmount: read file error, inode: 413364, index: 0, chunk: 622947, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 15)
Feb  5 12:04:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 89)
Feb  5 12:04:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 90)
Feb  5 12:04:23 urknall kernel: [440697.581733] veth1048: renamed from veth979e383
Feb  5 12:04:23 urknall dockerd[1676]: time=""2018-02-05T12:04:23.359400712+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:04:23 urknall kernel: [440697.595692] device veth1048 entered promiscuous mode
Feb  5 12:04:23 urknall kernel: [440697.632090] device veth6663354 entered promiscuous mode
Feb  5 12:04:23 urknall mfsmetalogger[8545]: sessions downloaded 742B/3.950299s (0.000 MB/s)
Feb  5 12:04:25 urknall dhclient[1979]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x1f01ed1a)
Feb  5 12:04:25 urknall dhclient[1979]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 12:04:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 91)
Feb  5 12:04:34 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 9)
Feb  5 12:04:34 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 64)
Feb  5 12:04:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 92)
Feb  5 12:04:45 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 10)
Feb  5 12:04:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 93)
Feb  5 12:04:55 urknall marc: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:04:57 urknall mfsmount: write file error, inode: 411287, index: 0 - Timeout after 30040 ms (Timeout) (try counter: 1)
Feb  5 12:05:02 urknall dhclient[1979]: bound to 192.168.99.3 -- renewal in 209 seconds.
Feb  5 12:05:02 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000602s (1.233 MB/s)
Feb  5 12:05:02 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 11)
Feb  5 12:05:02 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 65)
Feb  5 12:05:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 94)
Feb  5 12:05:09 urknall dockerd[1676]: time=""2018-02-05T12:05:09.433889982+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:gliwcqzlhjzjulhs3xdbxuqrv leaving:false netPeers:1 entries:5 Queue qLen:0 netMsg/s:0""
Feb  5 12:05:09 urknall dockerd[1676]: time=""2018-02-05T12:05:09.434060410+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 12:05:09 urknall dockerd[1676]: time=""2018-02-05T12:05:09.434142880+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:05:09 urknall dockerd[1676]: time=""2018-02-05T12:05:09.434196186+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:dpu8s5w8nx7vtqyiqai8rflqm leaving:false netPeers:1 entries:10 Queue qLen:0 netMsg/s:0""
Feb  5 12:05:09 urknall dockerd[1676]: time=""2018-02-05T12:05:09.434272947+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:05:09 urknall dockerd[1676]: time=""2018-02-05T12:05:09.434346553+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:05:09 urknall dockerd[1676]: time=""2018-02-05T12:05:09.434421311+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:05:09 urknall dockerd[1676]: time=""2018-02-05T12:05:09.434494570+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:05:09 urknall dockerd[1676]: time=""2018-02-05T12:05:09.434571762+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:46 Queue qLen:0 netMsg/s:0""
Feb  5 12:05:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 95)
Feb  5 12:05:13 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunk write error (Disconnected) (try counter: 29)
Feb  5 12:05:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 96)
Feb  5 12:05:23 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 66)
Feb  5 12:05:23 urknall mfsmount: write file error, inode: 409250, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 1)
Feb  5 12:05:24 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 12)
Feb  5 12:05:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 97)
Feb  5 12:05:33 urknall mfsmount: write file error, inode: 196764, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 30)
Feb  5 12:05:39 urknall kernel: [440774.075370] veth3: renamed from veth3967052
Feb  5 12:05:39 urknall dockerd[1676]: time=""2018-02-05T12:05:39.841308580+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:05:39 urknall kernel: [440774.089477] device veth3 entered promiscuous mode
Feb  5 12:05:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 98)
Feb  5 12:05:44 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 12:05:46 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 2)
Feb  5 12:05:48 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 3)
Feb  5 12:05:50 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 4)
Feb  5 12:05:51 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 1)
Feb  5 12:05:52 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 67)
Feb  5 12:05:52 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 5)
Feb  5 12:05:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 99)
Feb  5 12:05:54 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 6)
Feb  5 12:05:55 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunk write error (Disconnected) (try counter: 31)
Feb  5 12:05:55 urknall mfsmount: write file error, inode: 409249, index: 0 - Chunk write error (Disconnected) (try counter: 1)
Feb  5 12:05:55 urknall mfsmount: write file error, inode: 409250, index: 0 - Chunk write error (Disconnected) (try counter: 1)
Feb  5 12:05:56 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 7)
Feb  5 12:05:58 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 8)
Feb  5 12:06:00 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 9)
Feb  5 12:06:02 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 2)
Feb  5 12:06:02 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 10)
Feb  5 12:06:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 100)
Feb  5 12:06:04 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 11)
Feb  5 12:06:06 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 12)
Feb  5 12:06:10 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 13)
Feb  5 12:06:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 101)
Feb  5 12:06:15 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunk write error (Disconnected) (try counter: 32)
Feb  5 12:06:15 urknall mfsmount: write file error, inode: 409250, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 2)
Feb  5 12:06:15 urknall mfsmount: write file error, inode: 409249, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 2)
Feb  5 12:06:18 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 14)
Feb  5 12:06:22 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 68)
Feb  5 12:06:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 102)
Feb  5 12:06:22 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 3)
Feb  5 12:06:27 urknall mfsmount: write file error, inode: 409249, index: 0 - Chunk write error (Disconnected) (try counter: 3)
Feb  5 12:06:27 urknall mfsmount: write file error, inode: 409250, index: 0 - Chunk write error (Disconnected) (try counter: 3)
Feb  5 12:06:28 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000734s (1.011 MB/s)
Feb  5 12:06:28 urknall mfsmount: read file error, inode: 409247, index: 0, chunk: 622965, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 15)
Feb  5 12:06:29 urknall dockerd[1676]: time=""2018-02-05T12:06:29.265668536+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/create type=""*events.ContainerCreate""
Feb  5 12:06:30 urknall dockerd[1676]: time=""2018-02-05T12:06:30+01:00"" level=info msg=""shim docker-containerd-shim started"" address=""/containerd-shim/moby/e17345c4539366316e5a760f71f72b626a4008443a9a58c94a5b758b462b9f9c/shim.sock"" debug=false module=""containerd/tasks"" pid=14157
Feb  5 12:06:30 urknall kernel: [440824.498238] IPVS: Creating netns size=2192 id=1965
Feb  5 12:06:30 urknall kernel: [440824.732599] eth0: renamed from veth691b387
Feb  5 12:06:30 urknall kernel: [440824.752197] br0: port 5(veth1048) entered forwarding state
Feb  5 12:06:30 urknall kernel: [440824.752219] br0: port 5(veth1048) entered forwarding state
Feb  5 12:06:31 urknall kernel: [440825.861131] eth1: renamed from vetha200f2b
Feb  5 12:06:31 urknall kernel: [440825.876139] docker_gwbridge: port 40(veth6663354) entered forwarding state
Feb  5 12:06:31 urknall kernel: [440825.876199] docker_gwbridge: port 40(veth6663354) entered forwarding state
Feb  5 12:06:32 urknall kernel: [440826.653244] eth2: renamed from vethf9d4e18
Feb  5 12:06:32 urknall kernel: [440826.668970] br0: port 3(veth3) entered forwarding state
Feb  5 12:06:32 urknall kernel: [440826.668991] br0: port 3(veth3) entered forwarding state
Feb  5 12:06:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 103)
Feb  5 12:06:33 urknall dockerd[1676]: time=""2018-02-05T12:06:33.329723925+01:00"" level=warning msg=""unknown container"" container=fb1d30cb50dcd52e996d0837d78afdcdd68ab0f8e51a6fed6619ac53040cc0c5 module=libcontainerd namespace=plugins.moby
Feb  5 12:06:33 urknall dockerd[1676]: time=""2018-02-05T12:06:33+01:00"" level=info msg=""shim reaped"" id=fb1d30cb50dcd52e996d0837d78afdcdd68ab0f8e51a6fed6619ac53040cc0c5 module=""containerd/tasks""
Feb  5 12:06:33 urknall dockerd[1676]: time=""2018-02-05T12:06:33.419368524+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=plugins.moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 12:06:33 urknall dockerd[1676]: time=""2018-02-05T12:06:33.419527406+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 12:06:33 urknall dockerd[1676]: time=""2018-02-05T12:06:33.426277217+01:00"" level=warning msg=""unknown container"" container=e17345c4539366316e5a760f71f72b626a4008443a9a58c94a5b758b462b9f9c module=libcontainerd namespace=plugins.moby
Feb  5 12:06:33 urknall kernel: [440827.680276] br0: port 4(veth1047) entered disabled state
Feb  5 12:06:33 urknall kernel: [440827.680456] vethd926fc2: renamed from eth0
Feb  5 12:06:33 urknall kernel: [440827.719467] IPVS: __ip_vs_del_service: enter
Feb  5 12:06:33 urknall dockerd[1676]: time=""2018-02-05T12:06:33.680225489+01:00"" level=warning msg=""unknown container"" container=e17345c4539366316e5a760f71f72b626a4008443a9a58c94a5b758b462b9f9c module=libcontainerd namespace=plugins.moby
Feb  5 12:06:34 urknall kernel: [440828.618377] br0: port 4(veth1047) entered disabled state
Feb  5 12:06:34 urknall kernel: [440828.624045] device veth1047 left promiscuous mode
Feb  5 12:06:34 urknall kernel: [440828.624053] br0: port 4(veth1047) entered disabled state
Feb  5 12:06:34 urknall kernel: [440828.667723] br0: port 2(veth0) entered disabled state
Feb  5 12:06:34 urknall kernel: [440828.667776] br0: port 1(vxlan0) entered disabled state
Feb  5 12:06:34 urknall kernel: [440828.672279] ov-00100b-gliwc: renamed from br0
Feb  5 12:06:34 urknall kernel: [440828.692134] device veth0 left promiscuous mode
Feb  5 12:06:34 urknall kernel: [440828.692170] ov-00100b-gliwc: port 2(veth0) entered disabled state
Feb  5 12:06:34 urknall kernel: [440828.724087] device vxlan0 left promiscuous mode
Feb  5 12:06:34 urknall kernel: [440828.724167] ov-00100b-gliwc: port 1(vxlan0) entered disabled state
Feb  5 12:06:34 urknall kernel: [440828.806527] vx-00100b-gliwc: renamed from vxlan0
Feb  5 12:06:34 urknall kernel: [440828.888277] veth3162d51: renamed from veth0
Feb  5 12:06:34 urknall kernel: [440829.005902] vethe98a694: renamed from eth2
Feb  5 12:06:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 104)
Feb  5 12:06:45 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 69)
Feb  5 12:06:45 urknall kernel: [440839.777251] br0: port 5(veth1048) entered forwarding state
Feb  5 12:06:46 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunk write error (Disconnected) (try counter: 45)
Feb  5 12:06:46 urknall mfsmount: write file error, inode: 67787, index: 0 - Chunk write error (Disconnected) (try counter: 10)
Feb  5 12:06:46 urknall kernel: [440840.929364] docker_gwbridge: port 40(veth6663354) entered forwarding state
Feb  5 12:06:47 urknall kernel: [440841.697451] br0: port 3(veth3) entered forwarding state
Feb  5 12:06:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 105)
Feb  5 12:07:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 106)
Feb  5 12:07:06 urknall mfsmount: write file error, inode: 67783, index: 0 - Chunk write error (Disconnected) (try counter: 46)
Feb  5 12:07:07 urknall mfsmount: write file error, inode: 67787, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 11)
Feb  5 12:07:11 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 70)
Feb  5 12:07:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 107)
Feb  5 12:07:21 urknall mfsmount: read file error, inode: 27960, index: 0, chunk: 542619, version: 1 - Chunkserver communication timed out: 192.168.99.3:9422 (try counter: 1)
Feb  5 12:07:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 108)
Feb  5 12:07:23 urknall mfsmetalogger[8545]: sessions downloaded 742B/12.324977s (0.000 MB/s)
Feb  5 12:07:23 urknall kernel: [440877.366045] docker_gwbridge: port 37(veth886ee36) entered disabled state
Feb  5 12:07:23 urknall kernel: [440877.366324] veth769d6bf: renamed from eth1
Feb  5 12:07:24 urknall dockerd[1676]: time=""2018-02-05T12:07:24.698969135+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:07:24 urknall dockerd[1676]: time=""2018-02-05T12:07:24.699129256+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:07:24 urknall dockerd[1676]: time=""2018-02-05T12:07:24.699216071+01:00"" level=error msg=""Failed to deserialize netlink ndmsg: Link not found""
Feb  5 12:07:24 urknall dockerd[1676]: time=""2018-02-05T12:07:24.873453119+01:00"" level=warning msg=""unknown container"" container=e17345c4539366316e5a760f71f72b626a4008443a9a58c94a5b758b462b9f9c module=libcontainerd namespace=plugins.moby
Feb  5 12:07:24 urknall dockerd[1676]: time=""2018-02-05T12:07:24+01:00"" level=info msg=""shim reaped"" id=e17345c4539366316e5a760f71f72b626a4008443a9a58c94a5b758b462b9f9c module=""containerd/tasks""
Feb  5 12:07:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 109)
Feb  5 12:07:34 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunk write error (Disconnected) (try counter: 33)
Feb  5 12:07:34 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 4)
Feb  5 12:07:34 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 71)
Feb  5 12:07:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 110)
Feb  5 12:07:45 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 5)
Feb  5 12:07:52 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 111)
Feb  5 12:07:54 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunk write error (Disconnected) (try counter: 34)
Feb  5 12:07:54 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 72)
Feb  5 12:07:57 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 6)
Feb  5 12:08:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 112)
Feb  5 12:08:03 urknall dockerd[1676]: time=""2018-02-05T12:08:03.903462468+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=plugins.moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 12:08:03 urknall dockerd[1676]: time=""2018-02-05T12:08:03.903637495+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/tasks/delete type=""*events.TaskDelete""
Feb  5 12:08:10 urknall mfsmount: write file error, inode: 409239, index: 0 - Timeout after 30012 ms (Timeout) (try counter: 1)
Feb  5 12:08:11 urknall dhclient[1206]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x65d559d6)
Feb  5 12:08:11 urknall dhclient[1206]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 12:08:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 113)
Feb  5 12:08:16 urknall root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:08:16 urknall mfsmetalogger[8545]: sessions downloaded 742B/3.925297s (0.000 MB/s)
Feb  5 12:08:16 urknall mfsmount: write file error, inode: 3791, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 73)
Feb  5 12:08:16 urknall mfsmount: write file error, inode: 196764, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 35)
Feb  5 12:08:16 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 7)
Feb  5 12:08:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 114)
Feb  5 12:08:28 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 8)
Feb  5 12:08:31 urknall dhclient[1979]: DHCPREQUEST of 192.168.99.3 on em1 to 192.168.99.1 port 67 (xid=0x1f01ed1a)
Feb  5 12:08:31 urknall dhclient[1979]: DHCPACK of 192.168.99.3 from 192.168.99.1
Feb  5 12:08:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 115)
Feb  5 12:08:37 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunk write error (Disconnected) (try counter: 74)
Feb  5 12:08:37 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunk write error (Disconnected) (try counter: 36)
Feb  5 12:08:37 urknall dhclient[1206]: bound to 192.168.99.3 -- renewal in 229 seconds.
Feb  5 12:08:37 urknall marc: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:08:42 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 116)
Feb  5 12:08:43 urknall dhclient[1979]: bound to 192.168.99.3 -- renewal in 285 seconds.
Feb  5 12:08:43 urknall kernel: [440957.348909] docker_gwbridge: port 37(veth886ee36) entered disabled state
Feb  5 12:08:43 urknall kernel: [440957.373837] device veth886ee36 left promiscuous mode
Feb  5 12:08:43 urknall kernel: [440957.373849] docker_gwbridge: port 37(veth886ee36) entered disabled state
Feb  5 12:08:43 urknall kernel: [440957.398167] br0: port 5(veth1048) entered disabled state
Feb  5 12:08:43 urknall kernel: [440957.398339] veth691b387: renamed from eth0
Feb  5 12:08:43 urknall kernel: [440957.549936] IPVS: __ip_vs_del_service: enter
Feb  5 12:08:44 urknall kernel: [440958.979267] br0: port 5(veth1048) entered disabled state
Feb  5 12:08:44 urknall kernel: [440958.996825] device veth1048 left promiscuous mode
Feb  5 12:08:44 urknall kernel: [440958.996836] br0: port 5(veth1048) entered disabled state
Feb  5 12:08:45 urknall kernel: [440960.017558] br0: port 3(veth3) entered disabled state
Feb  5 12:08:45 urknall kernel: [440960.017685] vethf9d4e18: renamed from eth2
Feb  5 12:08:45 urknall kernel: [440960.057124] IPVS: __ip_vs_del_service: enter
Feb  5 12:08:47 urknall kernel: [440962.185054] IPVS: __ip_vs_del_service: enter
Feb  5 12:08:48 urknall kernel: [440962.759301] vetha200f2b: renamed from eth1
Feb  5 12:08:48 urknall kernel: [440962.789643] docker_gwbridge: port 40(veth6663354) entered disabled state
Feb  5 12:08:49 urknall dockerd[1676]: time=""2018-02-05T12:08:49.566163881+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/delete type=""*events.ContainerDelete""
Feb  5 12:08:53 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 117)
Feb  5 12:09:01 urknall mfsmount: write file error, inode: 67786, index: 0 - Timeout after 10026 ms (Timeout) (try counter: 1)
Feb  5 12:09:01 urknall mfsmount: write file error, inode: 67788, index: 0 - Timeout after 10038 ms (Timeout) (try counter: 9)
Feb  5 12:09:01 urknall mfsmount: write file error, inode: 67787, index: 0 - Timeout after 10022 ms (Timeout) (try counter: 12)
Feb  5 12:09:02 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 118)
Feb  5 12:09:10 urknall mfsmount: write file error, inode: 3791, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 75)
Feb  5 12:09:12 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 119)
Feb  5 12:09:13 urknall mfsmount: write file error, inode: 67786, index: 0 - Chunk write error (Disconnected) (try counter: 2)
Feb  5 12:09:13 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 10)
Feb  5 12:09:15 urknall mfsmount: write file error, inode: 67787, index: 0 - Chunk write error (Disconnected) (try counter: 13)
Feb  5 12:09:22 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 120)
Feb  5 12:09:29 urknall mfsmount: write file error, inode: 67787, index: 0 - Chunk write error (Disconnected) (try counter: 14)
Feb  5 12:09:29 urknall kernel: [441003.935899] docker_gwbridge: port 40(veth6663354) entered disabled state
Feb  5 12:09:29 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 11)
Feb  5 12:09:29 urknall mfsmount: write file error, inode: 67786, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 3)
Feb  5 12:09:29 urknall kernel: [441003.944068] device veth6663354 left promiscuous mode
Feb  5 12:09:29 urknall kernel: [441003.944077] docker_gwbridge: port 40(veth6663354) entered disabled state
Feb  5 12:09:30 urknall CRON[14618]: (root) CMD (  [ -x /usr/lib/php5/maxlifetime ] && [ -x /usr/lib/php5/sessionclean ] && [ -d /var/lib/php5 ] && /usr/lib/php5/sessionclean /var/lib/php5 $(/usr/lib/php5/maxlifetime))
Feb  5 12:09:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 121)
Feb  5 12:09:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 1)
Feb  5 12:09:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 2)
Feb  5 12:09:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 3)
Feb  5 12:09:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 4)
Feb  5 12:09:32 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 5)
Feb  5 12:09:33 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 6)
Feb  5 12:09:33 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 7)
Feb  5 12:09:33 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 8)
Feb  5 12:09:33 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 9)
Feb  5 12:09:33 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 10)
Feb  5 12:09:34 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 11)
Feb  5 12:09:35 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 12)
Feb  5 12:09:37 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 13)
Feb  5 12:09:41 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 14)
Feb  5 12:09:47 urknall dockerd[1676]: time=""2018-02-05T12:09:47.990461848+01:00"" level=error msg=""fatal task error"" error=""task: non-zero exit (255)"" module=node/agent/taskmanager node.id=4z5kkc15en2q29wt1crkxomc8 service.id=dun7e29phoubjq10posm5f24q task.id=f76ijutsskh8j8uvps8dsqhr4
Feb  5 12:09:48 urknall mfsmetalogger[8545]: sessions downloaded 742B/23.364226s (0.000 MB/s)
Feb  5 12:09:49 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 15)
Feb  5 12:09:50 urknall mfsmount: write file error, inode: 67786, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 4)
Feb  5 12:09:50 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 37)
Feb  5 12:09:51 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 12)
Feb  5 12:09:54 urknall mfsmount: write file error, inode: 67787, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 15)
Feb  5 12:09:57 urknall kernel: [441031.340176] br0: port 3(veth3) entered disabled state
Feb  5 12:09:57 urknall kernel: [441031.348857] device veth3 left promiscuous mode
Feb  5 12:09:57 urknall kernel: [441031.348869] br0: port 3(veth3) entered disabled state
Feb  5 12:09:57 urknall kernel: [441031.441712] IPVS: __ip_vs_del_service: enter
Feb  5 12:09:57 urknall kernel: [441031.441724] IPVS: __ip_vs_del_service: enter
Feb  5 12:09:57 urknall dockerd[1676]: time=""2018-02-05T12:09:57.999295732+01:00"" level=info msg=""ignoring event"" module=libcontainerd namespace=moby topic=/containers/delete type=""*events.ContainerDelete""
Feb  5 12:09:59 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 16)
Feb  5 12:10:09 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 17)
Feb  5 12:10:09 urknall dockerd[1676]: time=""2018-02-05T12:10:09.634083440+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mh26wxh3legx189b8ummizzvg leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:10:09 urknall dockerd[1676]: time=""2018-02-05T12:10:09.634290442+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:mwursuov7sc87fxhzkmpdtyes leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:10:09 urknall dockerd[1676]: time=""2018-02-05T12:10:09.634347558+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:gliwcqzlhjzjulhs3xdbxuqrv leaving:false netPeers:1 entries:6 Queue qLen:0 netMsg/s:0""
Feb  5 12:10:09 urknall dockerd[1676]: time=""2018-02-05T12:10:09.634429591+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:xx0m50f4cog3dik8xwqxd4g0q leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:10:09 urknall dockerd[1676]: time=""2018-02-05T12:10:09.634480657+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:dpu8s5w8nx7vtqyiqai8rflqm leaving:false netPeers:1 entries:12 Queue qLen:0 netMsg/s:0""
Feb  5 12:10:09 urknall dockerd[1676]: time=""2018-02-05T12:10:09.634553602+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2vg30sgcyr0omhvavdcptdrgv leaving:false netPeers:2 entries:47 Queue qLen:0 netMsg/s:0""
Feb  5 12:10:09 urknall dockerd[1676]: time=""2018-02-05T12:10:09.634624971+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:hixi0z35nd69dxzd621dtjl0e leaving:false netPeers:2 entries:2 Queue qLen:0 netMsg/s:0""
Feb  5 12:10:09 urknall dockerd[1676]: time=""2018-02-05T12:10:09.634696822+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:87vvi5rhozzl8qws2ufe7ltl2 leaving:false netPeers:2 entries:4 Queue qLen:0 netMsg/s:0""
Feb  5 12:10:09 urknall dockerd[1676]: time=""2018-02-05T12:10:09.634772000+01:00"" level=info msg=""NetworkDB stats urknall(f1266e563c51) - netID:2ntxgh39t5rk8b55tapu1l8gh leaving:false netPeers:2 entries:8 Queue qLen:0 netMsg/s:0""
Feb  5 12:10:10 urknall mfsmount: write file error, inode: 3791, index: 0 - Timeout after 10037 ms (Timeout) (try counter: 76)
Feb  5 12:10:12 urknall mfsmount: write file error, inode: 67788, index: 0 - Chunk write error (Disconnected) (try counter: 13)
Feb  5 12:10:17 urknall mfsmount: write file error, inode: 67787, index: 0 - Chunk write error (Disconnected) (try counter: 16)
Feb  5 12:10:19 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 18)
Feb  5 12:10:20 urknall mfsmount: write file error, inode: 196764, index: 0 - Chunkserver timed out (server 192.168.99.3:9422) (try counter: 38)
Feb  5 12:10:21 urknall mfsmetalogger[8545]: sessions downloaded 742B/0.000566s (1.311 MB/s)
Feb  5 12:10:22 urknall mfschunkserver[8794]: Did not manage to receive packet header
Feb  5 12:10:22 urknall dockerd[1676]: time=""2018-02-05T12:10:22.782034139+01:00"" level=error msg=""fatal task error"" error=""task: non-zero exit (1)"" module=node/agent/taskmanager node.id=4z5kkc15en2q29wt1crkxomc8 service.id=gq2beoh3n3f0y9ix02ri5kiyl task.id=w4urw8nvv1629qikt3f4d6apj
Feb  5 12:10:29 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 19)
Feb  5 12:10:39 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 20)
Feb  5 12:10:46 urknall mfsmount: write file error, inode: 67783, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 47)
Feb  5 12:10:46 urknall mfsmount: write file error, inode: 196764, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 39)
Feb  5 12:10:46 urknall mfsmount: write file error, inode: 67788, index: 0 - Read from chunkserver: connection closed by peer (server 192.168.99.3:9422) (try counter: 14)
Feb  5 12:10:49 urknall mfsmount: read file error, inode: 67938, index: 0, chunk: 61735, version: 15 - no valid copies (try counter: 21)
```

Host `pulsar` (Chubk and Metalogger):
```
Feb  5 11:41:13 pulsar dhclient[1004]: DHCPREQUEST of 192.168.99.2 on em1 to 192.168.99.1 port 67 (xid=0x45d74c8a)                                                                           
Feb  5 11:41:13 pulsar dhclient[1004]: DHCPACK of 192.168.99.2 from 192.168.99.1                                                                                                             
Feb  5 11:41:24 pulsar mfsmetalogger[6534]: connection was reset by Master                                                                                                                   
Feb  5 11:41:25 pulsar mfsmetalogger[6534]: connecting to Master                                                                                                                             
Feb  5 11:41:25 pulsar mfsmetalogger[6534]: connected to Master                                                                                                                              
Feb  5 11:42:07 pulsar root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                          
Feb  5 11:42:07 pulsar dhclient[1004]: bound to 192.168.99.2 -- renewal in 221 seconds.                                                                                                      
Feb  5 11:45:01 pulsar CRON[22885]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)                                                                                         
Feb  5 11:45:49 pulsar dhclient[1004]: DHCPREQUEST of 192.168.99.2 on em1 to 192.168.99.1 port 67 (xid=0x45d74c8a)                                                                           
Feb  5 11:45:49 pulsar dhclient[1004]: DHCPACK of 192.168.99.2 from 192.168.99.1                                                                                                             
Feb  5 11:46:18 pulsar mfsmetalogger[6534]: connecting to Master                                                                                                                             
Feb  5 11:46:18 pulsar mfsmetalogger[6534]: connected to Master                                                                                                                              
Feb  5 11:46:22 pulsar root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                          
Feb  5 11:46:22 pulsar dhclient[1004]: bound to 192.168.99.2 -- renewal in 257 seconds.                                                                                                      
Feb  5 11:50:39 pulsar dhclient[1004]: DHCPREQUEST of 192.168.99.2 on em1 to 192.168.99.1 port 67 (xid=0x45d74c8a)                                                                           
Feb  5 11:50:39 pulsar dhclient[1004]: DHCPACK of 192.168.99.2 from 192.168.99.1                                                                                                             
Feb  5 11:50:41 pulsar mfsmetalogger[6534]: connecting to Master                                                                                                                             
Feb  5 11:50:41 pulsar mfsmetalogger[6534]: connected to Master                                                                                                                              
Feb  5 11:50:45 pulsar root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                          
Feb  5 11:50:45 pulsar dhclient[1004]: bound to 192.168.99.2 -- renewal in 238 seconds.                                                                                                      
Feb  5 11:54:43 pulsar dhclient[1004]: DHCPREQUEST of 192.168.99.2 on em1 to 192.168.99.1 port 67 (xid=0x45d74c8a)                                                                           
Feb  5 11:54:43 pulsar dhclient[1004]: DHCPACK of 192.168.99.2 from 192.168.99.1                                                                                                             
Feb  5 11:55:01 pulsar CRON[23597]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)                                                                                         
Feb  5 11:55:11 pulsar mfsmetalogger[6534]: connecting to Master                                                                                                                             
Feb  5 11:55:11 pulsar mfsmetalogger[6534]: connected to Master                                                                                                                              
Feb  5 11:55:14 pulsar root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                          
Feb  5 11:55:14 pulsar dhclient[1004]: bound to 192.168.99.2 -- renewal in 204 seconds.                                                                                                      
Feb  5 11:57:26 pulsar mfsmetalogger[6534]: connecting to Master                                                                                                                             
Feb  5 11:57:26 pulsar mfsmetalogger[6534]: connected to Master                                                                                                                              
Feb  5 11:58:38 pulsar dhclient[1004]: DHCPREQUEST of 192.168.99.2 on em1 to 192.168.99.1 port 67 (xid=0x45d74c8a)                                                                           
Feb  5 11:58:38 pulsar dhclient[1004]: DHCPACK of 192.168.99.2 from 192.168.99.1
Feb  5 11:59:35 pulsar root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 11:59:35 pulsar dhclient[1004]: bound to 192.168.99.2 -- renewal in 220 seconds.
Feb  5 12:03:15 pulsar dhclient[1004]: DHCPREQUEST of 192.168.99.2 on em1 to 192.168.99.1 port 67 (xid=0x45d74c8a)
Feb  5 12:03:15 pulsar dhclient[1004]: DHCPACK of 192.168.99.2 from 192.168.99.1
Feb  5 12:04:26 pulsar root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:04:26 pulsar dhclient[1004]: bound to 192.168.99.2 -- renewal in 173 seconds.
Feb  5 12:05:01 pulsar CRON[24237]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)
Feb  5 12:07:18 pulsar mfsmetalogger[6534]: connecting to Master
Feb  5 12:07:18 pulsar mfsmetalogger[6534]: connected to Master
Feb  5 12:07:20 pulsar dhclient[1004]: DHCPREQUEST of 192.168.99.2 on em1 to 192.168.99.1 port 67 (xid=0x45d74c8a)
Feb  5 12:07:20 pulsar dhclient[1004]: DHCPACK of 192.168.99.2 from 192.168.99.1
Feb  5 12:07:21 pulsar root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:07:21 pulsar dhclient[1004]: bound to 192.168.99.2 -- renewal in 236 seconds.
Feb  5 12:09:03 pulsar kernel: [443248.752180] audit_printk_skb: 66 callbacks suppressed
Feb  5 12:09:03 pulsar kernel: [443248.752188] audit: type=1400 audit(1517828943.975:5728): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:09:03 pulsar kernel: [443248.752303] audit: type=1400 audit(1517828943.975:5729): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:09:03 pulsar kernel: [443248.752338] audit: type=1400 audit(1517828943.975:5730): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:09:03 pulsar kernel: [443248.752409] audit: type=1400 audit(1517828943.975:5731): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:09:03 pulsar kernel: [443248.752441] audit: type=1400 audit(1517828943.975:5732): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:09:03 pulsar kernel: [443248.752509] audit: type=1400 audit(1517828943.975:5733): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:09:03 pulsar kernel: [443248.752542] audit: type=1400 audit(1517828943.975:5734): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:09:03 pulsar kernel: [443248.752606] audit: type=1400 audit(1517828943.975:5735): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:09:03 pulsar kernel: [443248.752639] audit: type=1400 audit(1517828943.975:5736): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:09:03 pulsar kernel: [443248.752708] audit: type=1400 audit(1517828943.975:5737): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=24418 comm=""pidof"" requested_mask=""trace"" denied_mask=""trace"" peer=""docker-default""
Feb  5 12:11:18 pulsar dhclient[1004]: DHCPREQUEST of 192.168.99.2 on em1 to 192.168.99.1 port 67 (xid=0x45d74c8a)
Feb  5 12:11:18 pulsar dhclient[1004]: DHCPACK of 192.168.99.2 from 192.168.99.1
Feb  5 12:11:18 pulsar root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:11:18 pulsar dhclient[1004]: bound to 192.168.99.2 -- renewal in 299 seconds
```

Host `raum` (Chunk and CGI-Server):
```
Feb  5 11:42:16 raum dhclient[1051]: DHCPREQUEST of 192.168.99.7 on em1 to 192.168.99.1 port 67 (xid=0x67ed8381)                                                                             
Feb  5 11:42:16 raum dhclient[1051]: DHCPACK of 192.168.99.7 from 192.168.99.1                                                                                                               
Feb  5 11:42:16 raum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                            
Feb  5 11:42:16 raum dhclient[1051]: bound to 192.168.99.7 -- renewal in 275 seconds.                                                                                                        
Feb  5 11:43:03 raum apcupsd[955]: Communications with UPS lost.                                                                                                                             
Feb  5 11:45:01 raum CRON[46398]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)                                                                                           
Feb  5 11:46:51 raum dhclient[1051]: DHCPREQUEST of 192.168.99.7 on em1 to 192.168.99.1 port 67 (xid=0x67ed8381)                                                                             
Feb  5 11:46:51 raum dhclient[1051]: DHCPACK of 192.168.99.7 from 192.168.99.1                                                                                                               
Feb  5 11:46:51 raum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                            
Feb  5 11:46:51 raum dhclient[1051]: bound to 192.168.99.7 -- renewal in 244 seconds.                                                                                                        
Feb  5 11:50:56 raum dhclient[1051]: DHCPREQUEST of 192.168.99.7 on em1 to 192.168.99.1 port 67 (xid=0x67ed8381)                                                                             
Feb  5 11:50:56 raum dhclient[1051]: DHCPACK of 192.168.99.7 from 192.168.99.1                                                                                                               
Feb  5 11:50:56 raum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                            
Feb  5 11:50:56 raum dhclient[1051]: bound to 192.168.99.7 -- renewal in 239 seconds.                                                                                                        
Feb  5 11:53:03 raum apcupsd[955]: Communications with UPS lost.                                                                                                                             
Feb  5 11:54:55 raum dhclient[1051]: DHCPREQUEST of 192.168.99.7 on em1 to 192.168.99.1 port 67 (xid=0x67ed8381)                                                                             
Feb  5 11:54:55 raum dhclient[1051]: DHCPACK of 192.168.99.7 from 192.168.99.1                                                                                                               
Feb  5 11:54:55 raum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                            
Feb  5 11:54:55 raum dhclient[1051]: bound to 192.168.99.7 -- renewal in 277 seconds.                                                                                                        
Feb  5 11:55:01 raum CRON[54920]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)                                                                                           
Feb  5 11:59:32 raum dhclient[1051]: DHCPREQUEST of 192.168.99.7 on em1 to 192.168.99.1 port 67 (xid=0x67ed8381)                                                                             
Feb  5 11:59:32 raum dhclient[1051]: DHCPACK of 192.168.99.7 from 192.168.99.1                                                                                                               
Feb  5 11:59:33 raum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                            
Feb  5 11:59:33 raum dhclient[1051]: bound to 192.168.99.7 -- renewal in 282 seconds.                                                                                                        
Feb  5 12:03:03 raum apcupsd[955]: Communications with UPS lost.                                                                                                                             
Feb  5 12:04:15 raum dhclient[1051]: DHCPREQUEST of 192.168.99.7 on em1 to 192.168.99.1 port 67 (xid=0x67ed8381)                                                                             
Feb  5 12:04:15 raum dhclient[1051]: DHCPACK of 192.168.99.7 from 192.168.99.1                                                                                                               
Feb  5 12:04:16 raum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1                                                                                            
Feb  5 12:04:16 raum dhclient[1051]: bound to 192.168.99.7 -- renewal in 249 seconds.                                                                                                        
Feb  5 12:05:01 raum CRON[54024]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)                                                                                           
Feb  5 12:08:25 raum dhclient[1051]: DHCPREQUEST of 192.168.99.7 on em1 to 192.168.99.1 port 67 (xid=0x67ed8381)
Feb  5 12:08:25 raum dhclient[1051]: DHCPACK of 192.168.99.7 from 192.168.99.1
Feb  5 12:08:25 raum root: /etc/dhcp/dhclient-enter-hooks.d/samba returned non-zero exit status 1
Feb  5 12:08:26 raum dhclient[1051]: bound to 192.168.99.7 -- renewal in 289 seconds.
Feb  5 12:09:01 raum CRON[64865]: (root) CMD (  [ -x /usr/lib/php5/maxlifetime ] && [ -x /usr/lib/php5/sessionclean ] && [ -d /var/lib/php5 ] && /usr/lib/php5/sessionclean /var/lib/php5 $(/usr/lib/php5/maxlifetime))
```","If moving the machine to the same network to complete the drain in a couple of hours is out of question, try setting REPLICATION_BANDWIDTH_LIMIT_KBPS on well connected chunkservers, CHUNKS_READ_REP_LIMIT=1 on master and wait it out.",9800119
115,nfs-ganesha performance with pNFS 4.1 and NFS4 ,open,2018-01-31T13:31:39Z,2018-02-23T08:03:29Z,,NONE,"Thank you so much for the wonderful 3.12 release! I've been experimenting a bit with NFS-ganesha, originally to improve performance, but then I also added the write caching patch (still pending), which does miracles to the normal write performance. I figured you might be interested in some performance feedback.

Our lizardfs setup has master+slave metadata servers that also host SSD chunkservers, and then another ~10 large chunkservers with magnetic disk, all on 10Gb ethernet, using one ZFS volume per disk.

The bandwidth gets pretty close to the network speed with large files, so I've mainly been testing writing or reading tarballs with lots of small files.

Using just the plain mfsmount client on linux, version 3.12 with the added patches now gives us performance that is within 10% of NFS to a single NFS server not under load, which is simply awesome. The new cache patch also means the reading is way better than it used to be, sometimes even outperforming NFS.

When using the partial write goals patch with one copy to SSD and two to HDDs, I improve write performance by another ~30%.

When I enable NFS-ganesha I put the active master as NFS-ganesha metadata server, and another 6 fast chunkservers as NFS dataservers. Here the performance was a bit more disappointing; using plain NFS 4.0 I get performance that is ~60% slower than for the mfsmount client, and when I enable pNFS4.1 the performance is horribly bad - like a factor 10 lower.

The good news is of course that NFS-ganesha works, including the parallel option. Is this the type of performance to be expected for now? In theory it should be a huge advantage not having to go through the kernel context switches on the client, but maybe the effective mfsmount caching hides this?




 ","> ...The new cache patch also means the reading is way better than it used to be, sometimes even outperforming NFS...

May I ask, what patch is that? Really wanna try it! :)",9800119
116,Partial write goal order depends on label names,open,2018-01-27T18:09:28Z,2018-02-01T12:05:25Z,,NONE,"We've tested http://cr.skytechnology.pl/#/c/2984/ in combination with version 3.12, and in general it works great. It's particularly nice to be able to improve performance through goals with 1 ssd + 2 hdd copies, and have the hdd writes happen asynchronously in the background.

However, there might be one bug: I noticed that in the GUI each goal has the disks listed in alphabetical order, not the order they are specified in mfsgoals.cfg. This didn't matter while they were all equivalent, but if we set a goal with labels ""ssd hdd hdd"" and a partial "":1"" goal, lizardfs will rearrange the labels and first write to the ""hdd"" one (confirmed with lizardfs fileinfo).

For now we've worked around it by simply renaming ssd to ""fastssd"" so it comes first in the alphabet, but the proper solution seems to be to keep the labels in the order they were listed.

","Thanks, noted. This patch indeed needs a rewritten MediaLabelManager to work as expected.",9800119
117,chunks lost during draining of disks,open,2018-01-24T08:38:59Z,2018-01-27T09:24:49Z,,NONE,"Dear LizardFS Team, 

yesterday I started to drain a larger chunk-server by appending a `*` to the corresponding disk line in the `mfshdd.cfg` file. This resulted immediately in five lost chunks. The service was not restarted, only the configuration was read again by doing 

    systemctl reload lizardfs-chunkserver

During the night the number of lost chunks increased to 10. What's wrong there? 

I can not tell for sure for all affected files, but at least some of them were open for writing when I reloaded the CS-service. One of the affected files is a virtual machine disk image, which is continuously open for writing. The chunk-server in question has about 13M chunks. How many of them will I loose when I continue to drain it? For now, I stopped the draining process. ","I just realized that the problem is much more serious than expected. Even chunks that are reported by the system to exist are missing. Example:

    $ lizardfs fileinfo file_with_missing_chunk
    file_with_missing_chunk:
    	chunk 0: 000000000D7ADF8A_00000001 / (id:226156426 ver:1)
		copy 1: ip-of-cs-server:9561:HA

Any attempts to read this file are blocked. Login in to the CS-server and executing

    find /path/to/chunks -name '*000000000D7ADF8A_00000001*'

reveals that the chunk is definitively gone. The file in question was created long before the attempt to drain the CS and was not accessed during draining. ",9800119
118,Add versioning to .so libraries,open,2018-01-18T07:47:17Z,2018-02-15T09:04:54Z,,CONTRIBUTOR,"Can you please add a version to liblizardfsmount_shared.so and liblizardfs-client.so?  This will allow downstream programs to continue linking to the libraries when security fixes are issues, while making it clear that there's a backwards-incompatible change when the API changes.

.so versioning is a requirement for all packages in Fedora, which is why I'm asking:
https://fedoraproject.org/wiki/Packaging:Guidelines#Downstream_.so_name_versioning","This is also necessary to comply with Debian policy. 

See https://www.netfort.gr.jp/~dancer/column/libpkg-guide/libpkg-guide.html for details, especially in regards to naming. This is a blocker for introducing library to Debian.

Please note that `lizardfs-lib-client` package name is wrong and `-dev` package will be needed to provide headers. Once again it reminds me of #456 so my recommendation would be not to make any (binary) vendor packages for Debian and to work with Debian to coordinate packaging.",9800119
119,Slow restart of lizardfs services in large setups,open,2018-01-05T10:36:03Z,2018-01-23T11:45:20Z,,NONE,"We have a rather large installation of LizardFS in use (about 200M files). We always have a problem when installing updates. Restarting the services takes too long. This applies in particular to the master and the chunk servers. An update results in a downtime of at least one hour. 

Is there any way to speed up the restart? For example, not to execute checks directly at the start of the services, but asynchronously later? Is something like that conceivable?

The number of files in the system will increase rather than decrease in the future, so the problem will unfortunately get worse. ","Yes. CS scan all disks in parallel.  Sorry for the complex answer before. All good counterpoints.

So, having multiple zfs 'disk' may help. I would guess.",9800119
120,Request: Send email to warn about running out of critical disk space,open,2018-01-03T13:36:37Z,2018-05-24T03:25:17Z,,NONE,"In my experience with Lizardfs and communication with the support team, the problem of lacking critical disk space is a serious threat:
Meta data corruption
Master nodes fail to start (I experienced this)

This disk usage is not the same as disk space for chunks. And even space for chunks also cause troubles as I saw here an there in the list of issues here at this git repo. 

So a rather proactive prevention is to check periodically and send email to administrators to warn about disk space problem. 

A poor man solution would be to have a bash script run by a cron job that checks disk space and send email with send mail. But if this can be built into LizardFS itself, it will be more convenient and make Lizardfs a more thoughtful solution, which can take care of some ugly and likely situations. 

  ",">Monitoring available disk space is a generic task that would be best to leave to more specialised tools like Zabbix, etc.

Agree.",9800119
121,Terrible performance when LizardFS client runs on virtualized machine,open,2017-12-03T12:08:40Z,2018-01-16T00:50:50Z,,NONE,"Dear LizardFS community & dev team,

I've set up LFS infra on 2 physical hosts and 4 VMs:

- 2x metadata server
- 2x chunkserver
all on ESXi 6.5 and CentOS 7, hosts connected with 1GbE, VMs within one host with 10GbE vSwitch.

Then, connected one test client (CentOS 7 VM) with settings suggested by LFS docs (eg. big_writes which helped so much with getting more than 40MB/s). 

Unfortunately, the performance on the client is terrible:
- approx. 200 IOPS
- average bandwidth about 60-70MB/s, starts with 270-200MB/s and then slows down to 60-70

Some examples: https://pastebin.com/JLWwxFXq and https://pastebin.com/BsagFTMt

As you see, first few seconds looks OK, but then slowing down to ~70MB/s which does not satisfy me at all!

Then, I set up similar config on my home lab, and started a Ubuntu Live-CD on bare-metal, installed lizardfs-client, mounted and volia! Full gigabit speed and around 800 IOPS.

Unfortunately, the mfsmount process does not show anything in strace so I don't know how to debug it properly. Do you have any ideas. How can I tweak the FUSE client or strace it?

I will appreciate any help!
Thanks in advance.
Robert","Hi @pkali ... 

regarding this:
> a. hardware related issues like faulty NICs or NIC drivers, router misconfig, failing/bad/slow HDD, faulty links, etc. Even when synthetic tests like iperf show ideal results, real LizardFS performance may suffer due to some driver/buffer/hardware misalignments.

Do you have any tips on how to diagnose hardware related issues like you mentioned? 

I have a DELL R610 server and about 3 other machines (xeon/amd mixed) as chunks. (no VM in my case!)

We're seeing a very bad performance with LFS, and I'm starting to consider this scenario, where something related to driver/hardware is not performing correct, but I can't find a way to diagnose. 

I'm actually starting to wonder if our newtwork switches are OK, due to the poor and instable connection between clients<>chunks and even chunks<>chunks. 

Any information would be greatly appreciated!! 

cheers..
-H


",9800119
122,How to use REPLICATION_BANDWIDTH_LIMIT_KBPS ?,open,2017-11-23T19:49:30Z,2017-11-28T12:43:42Z,,NONE,"I don't want to use current chunks loop for replication (mostly because is still unclear how it works).
What I would like is to set our cluster to replicate as much chunks as possible up to `REPLICATION_BANDWIDTH_LIMIT_KBPS`

What should I have to set and how ? I think that both `REPLICATION_BANDWIDTH_LIMIT_KBPS` and chunks loop limits are honored, so I have to set chunk loop to very high value ?",,9800119
123,Chunk locked -- what does it mean and why is this happening?,open,2017-11-23T03:29:55Z,2018-10-15T06:03:09Z,,MEMBER,"Once in a while (every week) user have to reboot her computer because of

    write file error, inode: 58132317, index: 0 - error sent by master server (Chunk locked)

Unfortunately after reboot I can not find the file but most certainly it is one of Chromium files.
Workstation where it happens only uses desktop apps: Thunderbird, LibreOffice, Chromium, etc.

It always happens on file(s) in user's home and as far as I can tell there are no undergoal chunks, all chunkservers are up and cluster is healthy.

Why is this happening? What could be causing locked chunks? What is the best course of action to recover?

Thanks.
","On 3.13.0 I've experienced outrageous case when several chunks remained locked for more than an hour (444 retries) resulting _IO error_. I've tracked affected files to Firefox profile -- therefore there were only one client (Firefox locks the profile to prevent concurrent use by more than one process).
Problem was apparently triggered by restart of chunkserver.
",9800119
124,lfs_console_headsup teaser screenshot,open,2017-11-17T05:49:34Z,2017-11-22T09:19:11Z,,NONE,"Just for kicks I thought I would share one of my crazy scripts.  This is a cron task that paints the main local console with status for our Corosync and LizardFS Master (masters and shadows) so to know their states at a glance...  Unfortunately I can not share the script until I sanitize it. Enjoy..

![lfs_console_headsup](https://user-images.githubusercontent.com/10667324/32931232-ffa555f2-cb17-11e7-8827-74c3bc264084.png)
","We originally monitored MooseFS masters by emulating a CS connection with our nagios, but I think that disrupts various disconnect delay timers.  I will need to rewrite those using `lizardfs-admin` or a metalogger service call or something else that will not cause trouble with the master timers and shadows and safeties and such.",9800119
125,"""Error: Can't read data from socket: timeout"" in lizardfs-admin metadataserver-status",open,2017-11-15T14:41:21Z,2018-02-28T14:57:52Z,,NONE,"Trying to run `lizardfs-admin metadataserver-status` during a shadow with sync in progress, I often get `Error: Can't read data from socket: timeout`

Immediatly after the sync phase, output is returned properly.

Probably, we need a way to increase the default timeout","@psarna is there anything one can do, to disable the apparent internal timeout of the lizardfs-admin cli tool?",9800119
126,Bypass trash for empty files?,open,2017-11-15T08:57:36Z,2017-11-15T09:58:46Z,,MEMBER,"Examining Trash revealed many (temporary) files with size == 0. Sending zero size files to Trash is not very useful so perhaps we could reduce some overhead by permanently deleting empty files without sending them to Trash?

Of course it could be a configurable option but I've found that some apps touch a certain file (flag?) then delete it (and repeat that over and over again quite often) so Trash is getting filled with rubbish allowing me to pick a file to undelete from large collection of identical empty files that were deleted in the past... :)","If you can identify locations where such null files are touched, you can set that parent folders trash time to 0 so that they will not be kept... There could be situations where the mere existence of an empty file (inode only), and simply it's name, holds some value, and so should still follow the standard trash retention rules.  So if added, this should probably be a tunable option such as:

`IGNORE_TRASH_FOR_NULL_FILES = [ 0 , 1 ] # Default to current behaviour of 0`

I always use `find --name` to look through the trash, a traditional `ls` may take many many minutes to return output, but find always returns much more quickly, given that you know what you are looking for...  With that in mind you could use find for a particular known name of file with is touched and removed excessively, and simply have find delete those from the trash volume as needed?",9800119
127,Chunks that should have been deleted are left behind,open,2017-11-15T06:49:25Z,2017-11-22T02:24:48Z,,MEMBER,"I'm still recovering from disastrous goal change (#612) and one of the problems is space leak from leftover chunks that should have been deleted after goal change. It's been a week since all goal changes, replication and re-balancing has finished yet there are significant number of chunks `chunk_ec_N_of_4` and `chunk_xor_N_of_2` are left behind. 

Some of those chunks are leftovers from goal change to `1` ($std on one server) and some of the files were deleted before goal change was completed. (Some _xor2_ files could have been deleted without goal change). ""_Chunks which need deletion_"" view shows thousands of chunks in _1..5 copies to delete_ and numbers are not changing.

There are no _reserved_ files or files in _trash_. A week ago there were much more chunks to delete in those goals but now those obsolete chunks are just left behind. I tried renaming label of one chunkserver so once all data has been moved away only those obsolete chunks are left. Marking that chunkserver for removal changes nothing (as expected) and there are no undergoal or endangered chunks even if I remove chunkserver. Since all goal changes has been completed I'm quite confident that those chunks should have been deleted.

I don't have clean reproducer. It might be that chunks are forgotten if goal is changed (more than once) after previous goal change is completed or it might be that deleting files during goal change leaves chunks behind... All I can tell is that not all obsolete chunks are removed...

Another thing to note is that I have no files with goal ec41 any more yet _some_ of `chunk_ec_N_of_4` chink files are still around.

There are no massive undergoing deletions for a while so deletions limit is not a problem.
","unfortunately I do not have any processes/scripts for this sort of issue.  Of course, best case, it should be taking care of this itself, but apparently it is not for some reason.  So, to manually address it, perhaps:

Use `mfsmetadump `to dump a human readable version of the metadata:
* Figure out the values which represent chunks, perhaps convert those to chunk file name formats.

You could also walk all file with `mfsfileinfo `and collect all valid chunk names that way.
* For every valid chunk file do: ` touch /Brick_Name_*/??/each_valid_chunk_file`
** You may not know which Brick/Disk it lives on.
** The ?? sub folder is deterministic so you can narrow the wildcard expansion to # of disks.
* Now that all valid chunks have a modify time of $now-ish you can: 
** I do not believe chunk files mod times have any meaningful value normally, so this should be safe.
* Use `find -mtime -3` # to identify all older than 3 days old chunks, for example, these are your orphaned chunks.

Now just do whatever you want, find -delete them, find -exec move them somewhere, etc..

ps. I almost never use the ""marked for removal"" because it does not really do what one might expect.  It basically makes the Brick/Disk Read-Only, but does not actively attempt to move chunks away AFIKT.  So I just remove the Brick from the hdd.cfg entirely if I do not want it to be used any longer.  You could use this method, just remove each disk one at a time to make sure it will not lose any disks, then purge all chunks from it and let it re-absorb the good chunks...",9800119
128,Add support for scrubbing all chunks on a chunkserver for CRC errors,open,2017-11-10T13:18:04Z,2018-05-11T19:31:27Z,,NONE,"By sending a SIGUSR2 to a chunkserver process, a scrub loop is started (or aborted).
Scrub will check all chunks for any inconsistencies (crc errors) and will mark any failed chunks as damaged, forcing LizardFS to fix them","Ok, still trying to implement this feature by resuing `hdd_tester_thread` here: https://github.com/lizardfs/lizardfs/blob/master/src/chunkserver/hddspacemgr.cc#L2847

If I understood properly, this is a huge infinite loop
https://github.com/lizardfs/lizardfs/blob/master/src/chunkserver/hddspacemgr.cc#L2859
that will cycle all chunks here https://github.com/lizardfs/lizardfs/blob/master/src/chunkserver/hddspacemgr.cc#L2897 and wait up to the defined interval: https://github.com/lizardfs/lizardfs/blob/master/src/chunkserver/hddspacemgr.cc#L2906

Is this true ?

Now, what I'm unable to understand is what the following code does:
https://github.com/lizardfs/lizardfs/blob/master/src/chunkserver/hddspacemgr.cc#L2876-L2893

variables are too ""short"" and not self-explaining

Another question: is that thread self-updating when chunks are added or removed ? How can I ""fix"" that ? I don't want to create an infinite loop for the scrub. Scrubbing should start, scrub what already exist at the time of start, and then finish.",9800119
129,Attempted shadow->master transition without metadata - aborting,open,2017-11-09T15:35:44Z,2017-11-20T08:45:12Z,,NONE,"When promoting a (new) shadow to master, i'm hitting the error:

```
Attempted shadow->master transition without metadata - aborting
```

isn't possible to promote a brand new server as master?
Running it directly as master works, but If I start as shadow and then trying to became a master, it fails.","The corosync metadataserver bash script calls the lizardfs-admin to poll and prompt the master service.
At any moment (at 1 sec intervals for the master, and 2 second intervals for shadows) the script is called, and then itself calls lizardfs-admin to collect states and revisions.  I do not know specifically what is collected and parsed and used to decide things... That is why I spent a month hacking the closest thing to a core HA solution, the original corosync wizard that never worked for me.  Each master is aware of it's rev, and the master records it to corosync ( the cluster, all members, I also did not write corosync or ask for them to fix stuff ).  I just tweaked it until it worked reliably enough for me to trust it with my data and job.  It is not perfect and faults happen but I have not lost anything yet, praise to the bit gods.

I use lizardfs-admin to do many but not all of the things that lizardfs-admin is capable of doing.  It is what enables corosync and my hacked up sloppy script to pull off HA, homebrew style.  Others have had success with things like CARP, which I also used to use.  Or other HA solutions, or you can buy it from the team.  Or you can reference direct lines in my script and I will answer specific questions.  As for lizardfs-admin, it is simply an extension to the master service to allow for interesting, dangerous, beautiful, and awesome interactions.

The shadow/syncing is just something that masters and shadows do.  I'm not even sure you can use those ha-managed-personalities, without the lizardfs-admin tool though...

capisci?",9800119
130,Improve speed of HDD removal,open,2017-11-09T02:39:00Z,2017-11-09T08:25:43Z,,MEMBER,"Marking HDD for removal sends data only to other chunkservers but not to other HDDs on the same chunkserver. That makes recovery from HDD failure faster than marking HDD for removal because missing chunks are replicated to all chunkservers simultaneously when HDD is failed.

I suggest to move data out of HDD(s) marked for removal not only to other chunkservers but also to other HDDs on the same chunkserver.

Currently marking HDD for removal can overflow other chunkservers and trigger rebalancing that can send data back to the chunkserver where HDD is marked for removal...
",I've asked the same some weeks ago...,9800119
131,loss of data when HDD is 100% full,open,2017-11-06T02:40:30Z,2020-03-11T08:10:25Z,,MEMBER,"LizardFS is not safe when it comes to handling out of space situations. Consider the following scenario: there are three chunkservers with several HDDs each and a file with a replicated goal `3` (three copies).
All HDDs have plenty of free space but _one_ HDD on one chunkserver is suddenly runs out of free space. In my case I run command `tune2fs -m 50` on one HDD (utilised around 60%) out of four.
A half an hour later there are two missing chunks:

~~~~
mfsmount[1795]: truncate file 77499928 to length 1572864: Requested operation not completed (try 1/333)
mfsmount[1795]: truncate file 77499928 to length 1572864: Chunk lost (try 2/333)
~~~~

~~~~
mfschunkserver[10840]: write_block_to_chunk: file:/var/lib/lizardfs/5/ext4/chunksDE/chunk_00000D9CA9DE6F7D_00000001.liz - write error: ESUCCESS (Success)
mfschunkserver[10840]: mfschunkserver[10840]: write_block_to_chunk: file:/var/lib/lizardfs/5/ext4/chunksDF/chunk_00000D9CA9DF4D9F_00000001.liz - crc write error: ENOSPC (No space left on device)
mfschunkserver[10840]: write_block_to_chunk: file:/var/lib/lizardfs/5/ext4/chunksDF/chunk_00000D9CA9DF4D9F_00000001.liz - crc write error: ENOSPC (No space left on device)
~~~~

Looks like it happened because chunkserver tried to save chunk to full disk and failed without falling back to other HDDs before replication had a chance to make a copy...

Obviously chunkserver should

  * not try to write to full disks
  * handle out-of-space error gracefully

Though handling of re-writes on full disks might be somewhat more difficult...

Also this example shows how important it is to __honour goal settings #227__ (still unfixed)... :(",">  Free space threshold to set volume as 100% utilized when there is less than given amount of free
> space left. This number is always added to the used disk space reported by chunkserver.
> (Default: 4GiB)

I get 100% disk usage but it more than 4GiB by default? Why is't reported as no space left?
",9800119
132,"`fileinfo` throws ""master query: receive error"" on large files",open,2017-11-05T23:22:34Z,2018-09-26T13:18:08Z,,MEMBER,I'm running `lizardfs fileinfo {{large_file_20_GiB}}` and after printing partial/incomplete information about first 200+ chunks it throws `master query: receive error`... Sometime this command succeeds.,"Please do not implement timeout. The correct way to fix that would be to remove all timeouts and make `-l` a no-op.

`-l` option should have never been implemented because such approach makes commands unreliable depending on network latency and how much data should be processed.

There is no need to incorporate functionality of [timeout(1)](https://manpages.debian.org/stretch/coreutils/timeout.1.en.html) command into utilities.",9800119
133,Shadow Metadata is corrupted when Master crashes,open,2017-10-29T19:09:21Z,2017-10-31T09:14:25Z,,NONE,"I have a failover set up with uCarp between a metadata master and shadow server.  When I initiate a shutdown of the master server, uCarp runs some scripts that reconfigures the shadow as the new master, then restarts the service.  This all appears to work properly.

However, when the master server crashes, uCarp does its thing, but the former-shadow server will not restart as a master.  It complains that the metadata is corrupted and that I need to run mfsmetarestore (which then indicates that it's not able to resolve the issue).  Bringing the original master back up, then running mfsmetarestore brings LFS back up.

But my question is, why would my shadow metadata become corrupted when the master crashes?  This seems to happen consistently.  Am I misunderstanding how this is supposed to work?  That is, is the shadow supposed to be able to take over for a crashed master?  Or is it only supposed to work if I schedule a shutdown of master?

I'm currently running 3.9.4, but am planning for an upgrade in the near future.  I'm curious if this is:

1) A misunderstanding on my part of how it should work.
2) Something that's broken in 3.9.4 but has been fixed in a newer version.
3) Potentially something with my specific configuration (I'm trying to rule out #1 and #2 before spinning my wheels on my specific setup).","If you are using a floating ip that is now assigned to the new master, then yes shadow2 will reconnect, along with all the chunkserevrs, mounts etc.",9800119
134,Docs improvement about replication settings,open,2017-10-28T08:31:46Z,2017-11-28T12:44:12Z,,NONE,"Lizard is very powerful and has a bounch of settings to fine tune replication and failover

These settings should be deeply described in docs because for new users could be a little bit hard to understand how they works

I know there is a chunk loop that parse all chunks and does some action

A better documentation would describe how this loop works, what happens by changing the length and frequency of this loop and how to speed up/slow down chunks replication

It's almost one month that I'm trying to understand lizard with no success, I'm forced to change settings in a random way to see what's happening

Official docs should clarify all of these settings",,9800119
135,lizardfs centos7 boot start,open,2017-10-28T05:30:37Z,2017-11-13T08:18:04Z,,NONE,"service(like chunkserver, metalogger, etc) start with boot does not working. so i reger to MooseFS configure to solve it! In my system, it works well.
PS: (some commit is my own change, i do not know how to remove it)",,9800119
136,Transition a Master to an available Shadow,open,2017-10-20T09:33:24Z,2020-02-24T13:00:58Z,,NONE,"I built a HA-solution based on etcd (with test-and-set), which basically works great and as expected.

I'm struggling right now with, that there does not seem to be an option to convert a running master to a available shadow.

I tried to use the personality 'ha-cluster-managed', which allows me to do:
`lizardfs-admin promote-shadow localhost 9421`

But if I want to ""demote"" a master, I only see the options:
* Changing the config to shadow and start
* running `mfsmaster -o ha-cluster-managed -o initial-personality=shadow -c /path/to/config`

In both cases the shadow comes up, but has a metadataversion of 0 and therefore is not usable as it never had a connection to a master before.

Isn't there an option to make a master to a available shadow or did I miss something?","hi @guestisp, sent you my contact information over the contact form at https://www.guest.it/contatti (hope it reaches you).
Contact me if you want and when you are back in business :).",9800119
137,Suggestion about network configuration,open,2017-10-13T15:58:12Z,2017-10-31T09:04:27Z,,NONE,"I'm still trying to reach, more or less, gigabit speed with LizardFS.

Our test setup is simple: 3 chunkserver, 1 master server, goal=3, 2 gigabit switches and all nic bonded togheter (2 ports each) with bond mode `balance-rr`

Running `dbench -s -S -t 30 12` i can't go over 30MB/s

As lizard replicates in daisy-chain, client should be able to saturate at least 1 gigabit connection, as it writes to just a single chunkserver (opposite than gluster where each client must be able to write to all replica concurrently, thus available speed would be NIC speed/3)

Any advice on what should I check ? each disks used as chunk storage is able to reach about 130-140MB/s, iperf made from all servers to all servers is able to reach 1.98MB/s (thus, saturanting both gigabit connection on each host)","Bump
By resetting values for NR_OF_NETWORK_WORKERS and CHUNKS_* as suggested by @psarna now I'm able to reach about 100MB/s when writing but replication is way too slow, current speed is unacceptable, 4 days to replicate about 250/300GB

Any suggestions for speed up replication without killing write speed?

I think my biggest issue is with chunks loop, I'm still trying to understand how it works, docs is unclear about that and standard settings is way too slow for production use.
4 days to replicate 300GB is unacceptable, in 4 days I'm able to fully resync an 8-10TB RAID6",9800119
138,loads of chunkserver timeouts making mfsmount use loads of memory! ,open,2017-10-12T18:12:52Z,2018-01-15T00:36:48Z,,NONE,"I'm seeing a weird behavior in my setup. 

Our network infrastructure is far from perfect, but we do have a fairly good traffic statiscs between servers and clients, with just a couple of clients having around 1% of packet loss and sporadic ping timing variations from 0.1 to 0.5ms with machines on the same switcher, and 0.3 to 5ms between machines in different switchers.

But I'm seeing a lot of  chunkserver timeout happening in mfsmount log from all clients and servers.

And the worst has being the HUGE memory allocation mfsmount is doing when those timeouts happen a lot.

One thing that made this problem more evident was by using the mfschunkserverwriteto mount options. Because I was getting a lot of 10-30 secs hanging when rsyncing a bunch of files (and the timeout messages show up), I've tried to decrease it so the chunkserver would timeout faster! 

using mfschunkserverwriteto=300, the hanging stopped, and I was blowed away to see rsync reporting 150mb/sec on transfers, where before I could never get more than 20-40mb/sec, since I started testing LFS, (I'm gessing now the timeout is responsible for my poor speed)

UNFORTUNATELLY, my happiness didnt last long, since mfsmount went to use 16GB OF RAM, eventually swapping everything else and making my machine unresponsive! 

SO... there is something going on with the chunkserver timeout problems... Until LFS, I was sure that our network was OK since we allways got steady 90mb/sec transfer using ZRAID and NFS. 

But this timeouts are making me worry that maybe there's something going on with our network. 

what puzzles  me more is that the timeout is not isolate to/from a specific chunkserver, or two... its with ALL chunkservers, including a chunkserver running on the same machine as the rsync happened! (so maybe its not network related at all!) 

HA... when mfsmount starts to eat memory, I have also noticed I cant unmount it... it gives an error saying it cant be unmounted, and I keep receiving timeout messages. I suppose this happens because mfsmount is trying to commit the data, but keep getting timeout from every chunkserver. 


## some more aditional info about my setup: 

the rsync was being done on a folder with goal ec(2,1). which I reckon mfsmount needs to write to 3 chunkservers in parallel, at least.
 
we have a total of 6 chunkservers, where:

-  3 chunkserver are running on one DELL R610 server with 36GB of RAM (just so we can test with more chunkservers for goals like ec(4,2) )
- 1 chunk server on a vmware arch linux VM, running under windows 2008 server, which has a 20TB RAID formated as NFS.
- 1 chunkserver running on a commodity 4 cores XEON with 4GB ram and a 10TB  lsi megaraid raid5 disk running xfs
- 1 chunkserver on a 8 cores Xeon with 16GB ram, using a ZRAID with 8TB and also a single disk of 2TB. (btw, this is the machine running rsync from the start of the message)


So, I would definetely expect to see timeouts from the vmware machine, but Im actually getting timeouts from ALL of then, even from the chunkserver that runs on the same machine as the rsync. 

Actually, the chunkserver that gives the most timeouts is the one running in the 4cores XEON machine (with ls megaraid raid5), which is in the same switcher as the 8 cores Xeon machine, which was doing the rsync. 

At the end of the day, for me it seems my main issue here seems to be the chunkservers  timeouts, which explain LFS low speed overal since I started using it. Decreasing the timeout made a HUGE improvement on transfer speed and responsiveness of LZF, which is pretty exciting!  

Althought the mfsmount is using too much memory when mfschunkserverwriteto is low, (which kinda makes sense since its trying to get at least 4 chunksevers to write the ec(2,2) data, so it keeps holding the data in memory) it was only caused because the chunkservers are being ""lazy"" and not getting the data through... 

so... not sure what to go from here!! 

if this is a network issue, I don't have the expertise to diagnose properly. Any help would be greatly appreciated here!!

If its a disk speed issue causing the chunkservers to be unresponsive, what can I do? (although when the rsync machine gets the timeouts, other machines can talk to the same chunkservers without issue)

If it's a real code bug, what can I do to help fix it? 

any thoghts? 

cheers... 
-H","Sorry the delay... I decided to test it for a longer time before continuing bothering you guys!

I did find out what was causing the huge memory usage by mfsmount. It was these options:
```
mfswritecachesize=16384,readaheadmaxwindowsize=65536
```
plus the default chunkserver timeout time. By reducing the chunk timeout time to 5, and removing these options from mfsmount, the high memory usage stopped.

So now, it's being a few months, and I tried a lot of different things... from the defaults, to tweaking master config, chunk config and mount config.

The timeouts are still there, causing huge slow downs to data streaming (read/write) when they happen!

I did find out that tweaking this parameters in fsmaster.cfg makes a HUGE difference on LFS read/write speed and stability on clients, though: 
```
CHUNKS_LOOP_MAX_CPS
CHUNKS_LOOP_MIN_TIME
CHUNKS_LOOP_PERIOD
CHUNKS_WRITE_REP_LIMIT 
CHUNKS_READ_REP_LIMIT 
```
So, currently, I have 2 mfsmaster.cfg files which I switch back and forward based on business hours, one for production and one for maintenance. (production for faster client response, and maintenance to speed up replication!) 

this is my main production mfsmaster (only what differs from default, as requested):
```
# current production mfsmaster setup different from default
LOCK_MEMORY = 1                                                          
PREFER_LOCAL_CHUNKSERVER = 0                                             
NO_ATIME = 1                                                             
OPERATIONS_DELAY_INIT = 0   # any reason we should have a delay? 
CHUNKS_LOOP_MAX_CPS = 1000  #  increasing this DOES make more timeouts, but decreasing doesn't stop then!
CHUNKS_WRITE_REP_LIMIT = 8 # theres a sweet spot here. the default takes to long to replicate when a disk is replaced, too high and no-one can access LFS. 8 seems to work well for us, not increasing timeouts, and replicating faster (from weeks to a few days to replicate)
CHUNKS_READ_REP_LIMIT  = 50   #  didn't notice much difference with this one. but since we increased the write equivalent, we increased this one! 
ACCEPTABLE_DIFFERENCE=0.05  # balancing is not actually happening at all... so we decreased the difference to see if it would help... no difference!                  
CHUNKS_REBALANCING_BETWEEN_LABELS = 1 # we do want to re-balance between labels... but no re-balance at all so far!
REDUNDANCY_LEVEL=1  # we want to be warned if we have less than 3 safe copies - not sure how this is applied to ec(2,1) and ec(3,1) though.
```

the best chunkservers cfg I got is: (after trying the defaults and lots of tweakings)
```
LOCK_MEMORY = 1 
NR_OF_NETWORK_WORKERS = 10 # this helps disk scanning to go faster, but doesn't help on timeouts
HDD_PUNCH_HOLES = 1 # since we use xfs and ext4, it's 1
READ_AHEAD_KB = 0 # no difference 
BGJOBSCNT_PER_NETWORK_WORKER = 200000 # this doesnt seem to make any difference!
NR_OF_HDD_WORKERS_PER_NETWORK_WORKER = 50  # this helps disk scanning to go faster
CSSERV_TIMEOUT = 5  # it seems to make the retrying on clients to go faster... 
HDD_ADVISE_NO_CACHE = 1 # can't notice any difference
HDD_LEAVE_SPACE_DEFAULT = 20GiB
PERFORM_FSYNC = 1 # 0 does make it a bit faster to write, but doesn't helps with timeouts!!
```

and the best mfsmount I got is:
```
allow_other,big_writes,nodev,noatime,mfsdelayedinit,nonempty,cacheexpirationtime=1000,mfsattrcacheto=30
```

our goals are basically:
```
ec(2,1)
ec(3,1)
ec(2,2)
cp(4)
```

When timeouts don't happen, data streaming goes relatively fast (around 30mb/sec with 4 chunk servers). But as soon a timeout occurs, the stream hangs, while mfsmount keeps retrying on that same chunk server. Most of the time, it keeps going after 1-8 retries... sometimes we do get write errors after 30 retries.

This hanging  due to timeouts is a huge issue for us, specially when we need to rely on file sequence streaming.

I've tried to monitor a chunkserver during these retry sessions, and I NEVER saw CPU usage going over 20%, while `iostat` iowait  rarelly goes over 2%. 

I've also used ext4 filesystem on some chunks, and xfs on others. It doesn't actually seem to matter, since the timeouts happens on booth! 

So, I can't pinpoint exactly the reason for the timeouts! I've tried to set CHUNKS_LOOP_PERIOD to 90000 so to decrease the amount o replication (which works), but even without much duplication happening, the timeouts still happen. 

After a few months, I really can't figure out WHAT is the cause of the timeouts!!! 

> **It would be VERY helpful if there was MORE debug information, at least on the chunkserver log... It's really frustrating to see mfsmount complaining about a chunk timeout, and in the chunk server theres NOTHING happening... no cpu usage, no disk usage sometimes, and NO messages in the log!** 


------8<-----------------------------------8<-----------------------------------8<-----------------------------------8<----------------
## Some extra information about LFS usage in our studio: 
### (maybe helpful to someone else which relies on search paths like us...)

One thing that I ended up doing to make the workstations respond faster, was layer an local NFS mount on top of the LFS mount! (basically running an nfs-server on every client, sharing LFS locally to itself)

![image](https://user-images.githubusercontent.com/806743/34922304-ef1252fe-f942-11e7-8e65-f1012af057b2.png)

Since our system makes heavy usage of python scripts and search paths (which was awfully slow over LFS), encapsulating LFS under an localized NFS mount made it actually FASTER than actually mounting a server ZFS storage directly over NFS. 

for example (python instalation is on the LFS storage or ZFS storage):
```
first run after a reboot:
   python -c 'import os'  => takes 21.2 secs average when PYTHONATH=LFS mount point
   python -c 'import os'  => takes 20.8 secs average when PYTHONATH=NFS(LFS)
   python -c 'import os'  => takes 0.15 secs average when PYTHONATH=NFS of a ZFS storage

subsequent runs:
   python -c 'import os'  => takes 21.1 secs average when PYTHONATH=LFS mount point
   python -c 'import os'  => takes 0.3 secs average when PYTHONATH=NFS(LFS)
   python -c 'import os'  => takes 0.8 secs average when PYTHONATH=NFS of a ZFS storage
```

As you can see, the only problem is after a reboot, the very FIRST access when NFS  needs to access LFS for the first time, it behaves pretty much like using LFS directly.
But after that very first access, things are really fast, faster than usual NFS of a ZFS storage!!

So, using NFS(LFS) is a win for us, and made our studio able to actually use LFS in production, for applications, pipeline and home folders. 

BUT, we're still struggling to use it as our main storage for actual work files, since the performance of reading/writing is still extremely unstable, due to the ""chunk timeout"" errors. 






",9800119
139,a way to find how much REAL disk space a file is using.,open,2017-10-05T18:29:10Z,2017-10-20T10:35:54Z,,NONE,"During my tests, I start to wonder how much REAL disk space a file would take, according to its goal. 

Regarding standard replication, lets say 3 times,  I'm guessing  a 100MB file would take +-100MB from 3 chunkservers. (+- to account for the 64kb chunk size alignment)

This means, if the LizardFS filesystem has 1000M free, after writing the file it goes down to +-700MB, correct? 

Now, what about in a ec(2,1) goal? how much disk space a 100MB will occupy on each of the 3  chunkservers?

I've tried to look for a way to get the REAL disk space a file is using regarding its goal, but couldn't find any.

I think it would be really nice to have mfsfileinfo to return this information for us, specially for testing and defining the best goal for the data regarding to disk space. It also would be REALLY useful to make real predictions of future needs of disk space against available free disk space!

For example, in our studio, we have projects that generate a large number of files, sometimes files with  10GB or even 1TB.  Having a 1TB file  with goal 3 means we need at least 3TB free.

make sense?


","Just a side note - actually, a 64kB alignment exists in LizardFS as well. Every regular chunk consists of ""up to 1024 blocks, each one 64kB in size"". For ec(X, Y) chunks, it's ""up to 1024/X blocks, each one 64kB in size"".",9800119
140,Strange errors during replications,open,2017-10-04T20:53:07Z,2017-10-04T20:53:07Z,,NONE,"During a small replication, i'm getting the following errors on master and chunkservers.

What I did was:
1) removed a disk from chunkserver1's `mfshdd.cfg` and adding a brand new one in the same file. This has triggered a replication

2) removed a disk, in the same way, from chunkserver2. Also this triggered a new replication.

I have goal=3, thus both chunkserver were able to replicate from the only chunkserver ""survived""
cgi page properly show that all chunks are healty and fully replicated, but some error message are happened in log files. Are that normal ?

`Unknown LizardFS error` and `Status 'Wrong chunk version' sent by chunkserver` made me worried.

MASTER:
```
Oct  4 22:00:09 pve01 mfsmount[17742]: write file error, inode: 27, index: 6 - Timeout after 188094 ms (Timeout) (try counter: 1)
Oct  4 22:04:39 pve01 mfsmount[17742]: write file error, inode: 27, index: 3 - Timeout after 227209 ms (Timeout) (try counter: 1)
Oct  4 22:09:21 pve01 mfsmaster[30061]: (172.16.0.14:9422) chunk: 0000000000001652 replication status: Unknown LizardFS error
Oct  4 22:11:38 pve01 mfsmount[17742]: write file error, inode: 27, index: 1888 - Timeout after 159377 ms (Timeout) (try counter: 1)
Oct  4 22:11:39 pve01 mfsmaster[30061]: (172.16.0.14:9422) chunk: 0000000000001652 replication status: Unknown LizardFS error
Oct  4 22:11:40 pve01 mfsmaster[30061]: (172.16.0.14:9422) chunk: 0000000000001851 replication status: Unknown LizardFS error
Oct  4 22:12:26 pve01 mfsmaster[30061]: (172.16.0.14:9422) chunk: 000000000000184D replication status: Unknown LizardFS error
Oct  4 22:13:24 pve01 mfsmount[17742]: write file error, inode: 27, index: 24 - Timeout after 74349 ms (Timeout) (try counter: 1)
```


CHUNKSERVER1:
```
Oct  4 21:23:35 osd12 mfschunkserver[18784]: Did not manage to receive packet header
Oct  4 21:23:35 osd12 mfschunkserver[18784]: Did not manage to receive packet header
Oct  4 21:23:36 osd12 mfschunkserver[18784]: Did not manage to receive packet header
Oct  4 21:23:36 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:36 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:37 osd12 mfschunkserver[18784]: Did not manage to receive packet header
Oct  4 21:23:37 osd12 mfschunkserver[18784]: Did not manage to receive packet header
Oct  4 21:23:37 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:37 osd12 mfschunkserver[18784]: Did not manage to receive packet header
Oct  4 21:23:37 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.13:9422)
Oct  4 21:23:37 osd12 mfschunkserver[18784]: Did not manage to receive packet header
Oct  4 21:23:37 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.13:9422)
Oct  4 21:23:37 osd12 mfschunkserver[18784]: Did not manage to receive packet header
Oct  4 21:23:38 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:38 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:38 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:38 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:38 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:38 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:38 osd12 mfschunkserver[18784]: Connection error: ETIMEDOUT (Operation timed out) (server 172.16.0.14:9422)
Oct  4 21:23:39 osd12 mfschunkserver[18784]: replication error: Can't connect to 172.16.0.13:9422
Oct  4 21:23:39 osd12 mfschunkserver[18784]: replication error: Can't connect to 172.16.0.13:9422
Oct  4 21:23:39 osd12 mfschunkserver[18784]: replication error: Can't connect to 172.16.0.13:9422
Oct  4 21:23:39 osd12 mfschunkserver[18784]: replication error: Can't connect to 172.16.0.13:9422
Oct  4 21:23:39 osd12 mfschunkserver[18784]: replication error: Can't connect to 172.16.0.13:9422
Oct  4 21:23:39 osd12 mfschunkserver[18784]: replication error: Can't connect to 172.16.0.13:9422
Oct  4 21:23:39 osd12 mfschunkserver[18784]: replication error: Can't connect to 172.16.0.13:9422
Oct  4 21:23:39 osd12 mfschunkserver[18784]: replication error: Can't connect to 172.16.0.13:9422
Oct  4 21:23:43 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:23:43 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:23:43 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:23:48 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:23:48 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:23:48 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:23:56 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:23:56 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:23:56 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:24:04 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:04 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:04 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:24:04 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:04 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:04 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:24:13 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:13 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:13 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:24:13 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:13 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:13 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:24:20 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:20 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:20 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:24:28 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:28 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:28 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:24:30 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:30 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:30 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:24:43 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:43 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:24:43 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:25:25 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:25:25 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:25:25 osd12 mfschunkserver[18784]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: Received invalid response for chunk get block
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
Oct  4 21:29:35 osd12 mfschunkserver[18784]: replication error: Status 'No such chunk' sent by chunkserver (server 172.16.0.13:9422)
```

CHUNKSERVER3:
```
Oct  4 21:55:59 osd14 mfschunkserver[5074]: Did not manage to receive packet header
Oct  4 22:11:34 osd14 mfschunkserver[5074]: Received invalid response for chunk get block
Oct  4 22:11:34 osd14 mfschunkserver[5074]: Received invalid response for chunk get block
Oct  4 22:11:34 osd14 mfschunkserver[5074]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.12:9422)
Oct  4 22:13:52 osd14 mfschunkserver[5074]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.12:9422)
Oct  4 22:13:53 osd14 mfschunkserver[5074]: Received invalid response for chunk get block
Oct  4 22:13:53 osd14 mfschunkserver[5074]: Received invalid response for chunk get block
Oct  4 22:13:53 osd14 mfschunkserver[5074]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.12:9422)
Oct  4 22:14:38 osd14 mfschunkserver[5074]: Received invalid response for chunk get block
Oct  4 22:14:38 osd14 mfschunkserver[5074]: Received invalid response for chunk get block
Oct  4 22:14:38 osd14 mfschunkserver[5074]: replication error: Status 'Wrong chunk version' sent by chunkserver (server 172.16.0.12:9422)
```",,9800119
141,To Balance or not to Balance? (and how),open,2017-10-04T02:24:10Z,2017-10-08T08:28:52Z,,NONE,"> That is the question.
> whether to suffer the latencies of outrageous IOPS,
> or rsync against a sea of empty disk space

_Apologies to Hamlet, and humanity in general :)_

Am facing I guess a common issue - migrating data from our current DFS to lizardfs. Due tp node, disc and uptime requirements I've separated all the ZFS mirrors on the existing nodes so I could run them in parallel (I also added two extra nodes).

So far I've migrated half the VM's and results have been excellent, so ready to move the remainder and repurpose the remaining disks for lizardfs.

Trouble is, that will leave a terribly lopsided distribution on the chunkservers. Right now each chunkserver has 2 HD in RAIDZ0, there is sufficient space to store all our VM's on these, replica 3. Once that is done I'll be adding another 2 HD in RAIDZ0 as a 2nd HDD entry for the chunkservers.

As far as storage goes, probably doesn't matter - I imagine new writes will be biased to the new HDD and over time they will fill up.

IOPS however are an issue :) You can never have to many of them. I'm assuming random read/writes to existing VM's would benefit from being spread over 2 HDD entries rather than one.

So my questions are:

- Would my VM's benefit being rebalanced from a 100/0% distribution on each chunkserver to a 50/50%  spread.

If so, I see the various methods for doing so as:
1. rsync
Just rsync all the chunks from HDD1 to HDD2, then add HDD2 to the chunkserver. Hopefully it will delete the duplicate chunks in a even manner

2. replication (undergoal)
Current goal is R3, so I could just delete all data from the chunkserver and allow it to rebuild. This would involve being under goal (R2) for a extended period, which I find stressfull :)

3. replication (overgoal)
I could increase the Goal to 4, then delete the chunkserver data and let it replicate. This would take twice as long but we would never drop under R3.

4. Just add the disks to the existing RAIDZ0 array. This has the advantage of being really easy :) and new writes would striped. Data would gradually rebalance as it was written. However if one disk dies then the whole node has to be resync'd


1 seems a bit hinky to me, 2 & 3 go through appropriate channels with 3 being the safest (but slowest). Fortunately I am not in any hurry, I can afford to spread this over a few weekends and the volume of data (3TB) is not huge these days. Once I'm finished we will have a ridiculous amount of space available, so performance is much more important than any space wastage.

I'd be interested in peoples thoughts - is this worth doing? are there better ways of doing it?

Thanks.","Yes, it's true, chunkservers test one chunk per `HDD_TEST_FREQ` seconds. After chunkserver detects a damaged chunk, it reports it immediately to master. Master acknowledges it and will order a replication during the next chunk loop.

Also, if this particular chunk becomes endangered after losing this copy and `ENDANGERED_CHUNKS_PRIORITY` is set in master config, its replication will most likely happen faster.",9800119
142,New eject and abandon disk statuses similar to marked for removal.,open,2017-10-03T19:42:16Z,2017-11-10T12:39:49Z,,NONE,"I think these could perhaps be trivial to add and may provide for more advanced fault features in the future.

A new feature similar to 'marked' could be 'eject', it could be an isolated CS action internal to a discrete CS service to quickly move chunk away from the 'ejected' disk. An additional hdd.cfg line prefix of + perhaps indication you want to be +1 disk in your hand asap, would still take some time but be reflected in the cgi and cli tools.

Another 'abandon' status could be auto chosen if logic can be predicted, or you could issue to the master such a status change for a disk.  This would cause the rapid abandonment of the disk by replicating away as if the impacted chunks did not exist and caused undergoals. The master would still consider their existence and direct CS to CS replication to utilize them if needed.  The true disk chunk counts should still reflected in the cgi and cli, but perhaps the goals would reflect as if the chunks were already missing.",See also #604,9800119
143,How does LizardFS write when using simple goals?,open,2017-10-03T17:54:03Z,2017-10-03T17:54:03Z,,NONE,"According to https://docs.lizardfs.com/adminguide/replication.html
> In the simplest one, the simple goal setup, you specify how many copies of every chunk of a file or directory will be copied to how many and optionally also “which” chunk servers.
> Note that the write modus here is: client writes chunk to ONE chunk server and this chunk server replicates this chunk to the other chunk servers.

And this https://www.researchgate.net/publication/271464202 from 2012 has an image that describes ""chained writes"" also in MooseFS also, But I think this predates the 1.x series of MooseFS.

Anyway, I COULD SWEAR that with MooseFS and LizardFS <= 2.6.0 the client write to all goal copies directly and does not write to ONE/GOALS copies and then replicates to others...  Can one of the devs please clarify?  Did this behaviour change in LFS during the 3.2 or 3.2 series?  Have I just been wrong this whole time?  If it is chained, would I not see replication values in charts that roughly match the write values?  If replication is in fact a part of the normal clients write mechanics then I would expect to see that in replication charts...",,9800119
144,replication error: failed to create chunk (Chunk already exists),open,2017-10-03T16:18:07Z,2017-10-04T16:17:02Z,,NONE,"Trying to replace an hdd with another one on the same chunkserver and getting this:

`replication error: failed to create chunk (Chunk already exists)`

I've marked for removal `/export/sda` and added `/export/sdb`.
I think lizard is refusing to move from sda to sdb because are both on the same chunkserver.

This is not good, because i'm no trying to create multiple chunks on the same chunkserver, but i'm trying to move chunks from a disk to another one. In this case, chunks should be allowed to coexists on the same server.","20 years of photos, mp3 collection, home VHS movie files. past project data. installers. backups of phones. backups of servers. other garbage.  All the junk you would find on a data horders drives, about 4TB worth.  Next I'll try to automate part of the deduplication of stuff that has been backed up more than once. As a last resort I'll throw away my dos6.2 and win3.1.1 floppy backups. ;)  Currently running 2 Raspberries, a Bananna, and an ODroid2, and a few junk laptops, still need to figure out how to integrate a few old Palm Pre WebOS phones.. It isn't very pretty and things change roles all the time, haven't yet tried the corosync HA from work yet.  Disks are on USB, yes, It only has one client so it is fine.  The trick to make it usable on a Pi is nesting filesystems.  Loopbacks or iscsi or whatever container method you want to use, keeps the metadata size tiny and you can still leverage things like zfs compression.

I really appreciate when projects KISS rather than trying to do every possible thing they 'could' do.  Makes such tools more versatile and allows them to be leveraged in all sorts of unexpected and unpredictable ways.  I'm not sure I'm ready for LizardFS to be all grown up and version 6+ with built in everything. sigh, I'm sure it will be fine.",9800119
145,undergoal chunks and chunkserver not available,open,2017-10-03T15:05:13Z,2017-10-03T16:43:04Z,,NONE,"I'm trying this scenario: goal=3, some files stored properly.
I've shutdown a chunkserver, now all files are undergoal.

`OPERATIONS_DELAY_DISCONNECT` set to 30. I would expect master to start replication after 30 seconds from chunkserver disconnection.

Obviously I don't have enough chunkserver available to replicate to, thus master can't do anything.
Would be possible to add some log ? Something like:

`some chunks undergoal but insufficient chunkserver available to satisfy goal. Replication aborted`","Wouldn't you get a notification that you lost a chunkserver? And the master will have logged the loss of a chunkserver also. I think logging as you suggest could be overly and needlessly noisy. At home i have just 2CS, but i use goals of 4 and 5 for different data, to see how many of each type in the chunk matrix. I dont want to see logs and i am undergoal. I know I am, did it on purpose, and can plainly see that from various angles.  If I care, I should be notified of the loss of a CS rather than the hugh resulting undergoals caused by that loss.",9800119
146,Restarting master break mounted client,open,2017-10-03T13:27:44Z,2017-10-03T19:59:15Z,,NONE,"Restarting a master (with no shadow takeover) will break any mounted client:

```
Oct  3 15:23:02 x mfsmount[15661]: master: register error (read header: ESUCCESS (Success))
Oct  3 15:23:02 x mfsmount[15661]: master: register error (read header: ESUCCESS (Success))
Oct  3 15:23:04 x mfsmount[15661]: master: register error (read header: ESUCCESS (Success))
Oct  3 15:23:04 x mfsmount[15661]: master: register error (read header: ESUCCESS (Success))
Oct  3 15:23:06 x mfsmount[15661]: master: register error (read header: ESUCCESS (Success))
Oct  3 15:23:06 x mfsmount[15661]: master: register error (read header: ESUCCESS (Success))
```

When this happens, it's also impossible to unmount the client with `unmount /mnt`.
I have to kill `mfsmount` process and even after this the mountpoint is still stale, shown as mounted:
```
# mount | grep mfs
mfs#mfsmaster:9421 on /mnt type fuse (rw,nosuid,nodev,noexec,noatime,user_id=0,group_id=0,default_permissions,allow_other)
```

but not working (obviously):
```
# ls -la /mnt
ls: cannot access '/mnt': Transport endpoint is not connected
```

and still can't be remounted:

```
~# mfsmount -o big_writes,nonempty,nosuid,noexec,nodev,noatime,mfswritecachesize=16384,cacheexpirationtime=3600 /mnt
fuse: bad mount point `/mnt': Transport endpoint is not connected
see: mfsmount -h for help
```","but you began by restarting your master and dropping the client session state tables.  So the mount can not recover with a reconnect like it could had a shadow taken over and preserved the sessions.  I guess..

All that comes to mind is:
> MASTER_RECONNECTION_DELAY
> delay in seconds before trying to reconnect to metadata server after disconnection (default is 1)
> MASTER_TIMEOUT
> timeout (in seconds) for metadata server connections (default is 60)

I dont see anything from the client side? Shrugs. But again, i dont think it would matter. You have to do extraordinary things to kill the client which is forever lost without a valid session. Thus --force and --lazy and it should cleanly die, any activity to it sould be expected to cause hung io.",9800119
147,Clarifications on some mfsmaster.cfg settings,open,2017-10-03T12:37:48Z,2017-11-28T16:31:23Z,,NONE,"If I understood properly, `REPLICATIONS_DELAY_DISCONNECT` is used to avoid replication in case of simple chunkserver reboot.

What happens if `REPLICATIONS_DELAY_DISCONNECT` is set to a shorter value, in example, 600, and a server reboot requires 900 seconds?

Replication should be started after 600 seconds, but when the old chunkserver comes back online, the ongoing replication is stopped and already transferred chunks deleted because overgoal (as the old chunkservers already holds al transferred chunks)

and what about to any transferred and updated chunks ?


`CHUNKS_LOOP_MIN_TIME` which kind of ""check"" is done on each chunk file ? A checksum validation ? A simple exist/notexists check ?

> CHUNKS_SOFT_DEL_LIMIT: Soft maximum number of chunks to delete on one chunkserver (default is 10)

Which is the unit ? Lizard deletes `CHUNKS_SOFT_DEL_LIMIT` every how many seconds ?
Are this summed to `CHUNKS_WRITE_REP_LIMIT` and so on ? In example, if the same chunkserver is subject to deletion and replication, the max number of simultaneous operation is `CHUNKS_READ_REP_LIMIT` plus `CHUNKS_SOFT_DEL_LIMIT` ?","Is correct to say that Lizard checks chunks every `CHUNKS_LOOP_PERIOD` second for at least `CHUNKS_LOOP_MIN_TIME` ? In example:

CHUNKS_LOOP_MIN_TIME = 900
CHUNKS_LOOP_PERIOD = 1000

will result in a 900 seconds loop then 1 second paused and then another 900 seconds loop, and so on...

By setting `CHUNKS_LOOP_PERIOD = 30000`, i'll get a loop running for 900 seconds, every 30 seconds, right ?",9800119
148,Strange write speed,open,2017-10-01T09:44:31Z,2019-10-23T09:22:43Z,,NONE,"Something strange is happening here.

goal=3, 3 chunkservers with 2 gigabit nic bonded together (balance-rr). 
""iperf"" show about 2gbit in a single connection (thanks to balance-rr bonding mode)

Writing from a client (with the same bonding mode) i'm able to get about 180MB/s (that's impressive) but writing directly on the chunkserver i'm getting only about 90MB/s:

From LizardFS mountpoint:
```
# dd if=/dev/zero of=test bs=1M count=1000 oflag=dsync
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB, 1000 MiB) copied, 5.75394 s, 182 MB/s
```

Directly from chunkserver, totally skipping fuse and lizard:
```
dd if=/dev/zero of=test bs=1M count=1000 oflag=dsync
1000+0 record dentro
1000+0 record fuori
1048576000 byte (1,0 GB) copiati, 11,392 s, 92,0 MB/s
```

That's non-sense, lizard can't be faster than a single disk, when writing in sync.
Disks should be the bottleneck, as you have to wait for the ACK from each disk.
It seems that fsync is ignored or something strange is happening.
When writing through Lizard i'm able to reach about 1550mbps by using 2 gigabit nics.

`PERFORM_FSYNC` is as default (1) on all chunkservers


```
# mount | grep mfs
mfs#mfsmaster:9421 on /mnt type fuse (rw,nosuid,nodev,noexec,noatime,user_id=0,group_id=0,default_permissions,allow_other)
```",@guestisp Have you found what was the cause of the difference in write speed between writing from the host to the mfs mount point (~180 MB/s) and writing from a VM booted from LizardFS (~30 MB/s)?,9800119
149,Test if Shadow is connected to master,open,2017-10-01T01:44:32Z,2019-01-02T14:24:45Z,,NONE,"Is is possible to test if a shadow has an active connection to a master? 

I've run into a situation where a shadow is starting up and there is no master for it to connect to (first startup of cluster), then gets a command to reload as master which puts it in a failed state as it has no meta data.

I work around this by restarting the shadow so that it will always load the meta data from disk, but this often leads to missing chunks when promoting a active shadow to master

Alternatively is there a way fail the startup of a shadow if it can't connect to a master?

nb: This situation happens for my keepalived ha, since keeplived starts in BACKUP state, does elections and restarts the winner in MASTER.","Hi, drop me an email direct to mark.mulrainey@lizardfs.com

On Wed, Jan 2, 2019 at 4:39 AM Nhien <notifications@github.com> wrote:

> Could I contact sale for lizardfs-uraft or any new package?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/lizardfs/lizardfs/issues/598#issuecomment-450782398>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AYDm2cCOjHya_m92QFwQMS8Msb-l_6fTks5u_CnlgaJpZM4PpwFM>
> .
>


-- 
Regards,

Mark Mulrainey
Storage Dilemma Solver
+48 733 187 097
+1 949 299 9454

<https://lizardfs.com/>  <https://www.linkedin.com/company/3295488/>
<https://twitter.com/LizardFS>  <https://www.facebook.com/lizardfs/>
",9800119
150,Spread reads across chunkservers,open,2017-09-25T17:10:33Z,2017-09-30T09:55:20Z,,NONE,"Are reads spread across multiple chunk servers?

Let's assume a Lizard client with multiple bonded NICs.
A VM is running on top of a Lizard mount (with goal 3)

Random reads from inside this VM, are able to use multiple chunk servers or only one chunk server is used?
In example, read of ""file1"" is made by using chunkserver1 and read of ""file2"" is made by using chunkserver2, all in parallel, or ""file2"" is still read from chunkserver1 ?","""random"" is usually a rather smart choice, especially in big installations. Other solutions tend to work well with certain use cases and very bad in others.
Anyway, you can also turn this on in your chunkservers to enable prioritizing by I/O load, which makes it less random:
~~~
ENABLE_LOAD_FACTOR
if enabled, chunkserver will send periodical reports of its I/O load to master,
which will be taken into consideration when picking chunkservers for I/O operations.

~~~
Readahead works fine, but not between chunks, because each single read request is sent just to one chunkserver. Sensible readahead windows are smaller than a full chunk anyway (usually up to 16MB), so cross-chunkserver readahead is not necessary (and, because of that, not implemented).",9800119
151,EC 1+3 or standard replica?,open,2017-09-23T13:51:12Z,2017-09-27T14:51:50Z,,NONE,"Let's assume I need 3 copies of each file
Which is better, an Erasure code 1+3 or a standard replication with goal 3?

EC should be able to write in parallel from clients, while standard replication write on a single chunkserver and then chunkserver replicate to each other.

EC should be faster, right ?
Do you use any hardware accelerated function for EC, like Gluster does?","> I saw that, after my reply i edited my post. Now i am wondering in which version that changed...

I think it's in this way from the Moosefs fork, so, from the beginning.",9800119
152,mfsmount segfault,open,2017-09-21T11:51:26Z,2019-05-31T08:24:56Z,,NONE,"After upgrading lizardfs on two of our clients, they have started seeing segfaults in mfsmount randomly. It frequently happens within seconds but sometimes not for hours. I am running version 3.11.3 on Gentoo x86 (32-bit).

I caught a gdb backtrace from the last crash:

```
#0  0xf7b5e2e7 in vfprintf () from /lib/libc.so.6
#1  0xf7b5e82b in ?? () from /lib/libc.so.6
#2  0xf7b5c311 in vfprintf () from /lib/libc.so.6
#3  0x5683d26d in lzfs_vsyslog (silent=false, priority=4, 
    format=0x5684e474 ""read file error, inode: %u, index: %u, chunk: %lu, version: %u - %s (try counter: %u)"", ap=0xf68f1d58 ""ݩ\346\004"") at /usr/src/lizardfs-3.11.3/src/common/slogger.cc:92
#4  0x5683d383 in lzfs_pretty_syslog (priority=4, 
    format=0x5684e474 ""read file error, inode: %u, index: %u, chunk: %lu, version: %u - %s (try counter: %u)"") at /usr/src/lizardfs-3.11.3/src/common/slogger.cc:117
#5  0x56775c06 in print_error_msg (rrec=0xf4940a98, try_counter=1, ex=...)
    at /usr/src/lizardfs-3.11.3/src/mount/readdata.cc:261
#6  0x56776103 in read_to_buffer (rrec=0xf4940a98, current_offset=0, bytes_to_read=65536, read_buffer=..., 
    bytes_read=0xf68f1ef8) at /usr/src/lizardfs-3.11.3/src/mount/readdata.cc:331
#7  0x5677644c in read_data (rr=0xf4940a98, offset=0, size=65536, ret=...)
    at /usr/src/lizardfs-3.11.3/src/mount/readdata.cc:368
#8  0x56717cbe in LizardClient::read (ctx=..., ino=82225629, size=16384, off=0, fi=0xf5f430a0)
    at /usr/src/lizardfs-3.11.3/src/mount/lizard_client.cc:2000
#9  0x566e6f0d in mfs_read (req=0xf5f01098, ino=82225629, size=16384, off=0, fi=0xf68f219c)
    at /usr/src/lizardfs-3.11.3/src/mount/fuse/mfs_fuse.cc:414
#10 0xf7fac8d6 in ?? () from /usr/lib/libfuse.so.2
#11 0xf7fad49c in ?? () from /usr/lib/libfuse.so.2
#12 0xf7fb1396 in fuse_session_process_buf () from /usr/lib/libfuse.so.2
#13 0xf7fa923e in ?? () from /usr/lib/libfuse.so.2
#14 0xf7cd3394 in start_thread () from /lib/libpthread.so.0
#15 0xf7c0406e in clone () from /lib/libc.so.6
```

The most relevant frame seems to be #4 /usr/src/lizardfs-3.11.3/src/mount/readdata.cc:261

```C
256     static void print_error_msg(const readrec *rrec, uint32_t try_counter, const Exception &ex) {
257             if (rrec->reader.isChunkLocated()) {
258                     lzfs_pretty_syslog(LOG_WARNING,
259                                        ""read file error, inode: %u, index: %u, chunk: %lu, version: %u - %s ""
260                                        ""(try counter: %u)"", rrec->reader.inode(), rrec->reader.index(),
261                                        rrec->reader.chunkId(), rrec->reader.version(), ex.what(), try_counter);
```

Help, any ideas? I will continue to watch it in gdb so I can catch any more crashes.",It segfaulted three more times in quick succession. Each time was the same back trace. I uploaded an all-threads back trace here: https://gist.github.com/bruceg/1818f17c926bf15b9ce05f4f844235ae,9800119
153,Off Topic. Anyone into crypto? blockchain? Seen SiaCoin (Storage),open,2017-09-20T15:40:01Z,2019-05-31T08:26:54Z,,NONE,"Off Topic. Anyone into crypto? blockchain? Seen SiaCoin (Storage).
I have been monkeying around with it a little, there are a number of underlying parallel principals between it and LizardFS. Perhaps the two projects could learn some new tricks from each other.","For example.. LizardFS Version 2019:
* Eliminate Master/Shadow Single Points Of Failure such that all members are capable of full or partial metadata management.
* Dynamically Distribute Master/Shadow cores and ram capacity for metadata handling.
* Enhance and distributed secured authority over metadata, chunk servers, tape systems, clients by leveraging BlockChain as a highly adaptable and scalable data store, authority, and encryption mechanic.

BAM! 
Hell, get a coin and have a ""Rent your free brick space"" via hooks into Sia or stay completely out of the ""crypto coin as a store of value"" mindset.  Matters not to me..

Need more IOPS, More meta RAM, just point a new BlockChainMaster towards any member, request access, confirm access via self prescribed thresholds of multi-authority mechanics, wait a few minutes for the new member to catch up.  Bam CPU&Ram capacity expanded by +1 new member.  Randomly loose 50% of your master nodes... Who cares, you built it to 350% nominal load to handle that much peaking and withstand that much fault...",9800119
154,How fast should re-replication be initiated?,open,2017-09-20T12:50:37Z,2017-10-16T16:18:35Z,,NONE,"Hi,

Before putting our filesystem to real use, we figured we should stress-test it a bit. We have nice automated failover (and recovery) of the master working with keepalived, and as another test we want to see what happens if a disk just disappears.

We don't have any real precious data on it yet, but it's a live-size installation with 7 servers and some 20 million chunks over 75 disks. For now we're using 4+1 erasure coding to test the EC code, since the plan is to later move to 8+2 with more servers.

We are using separate ZFS pools for each disk, with a lizardfs directory at the top so we can detect that the path is there.

After doing ""rm -rf *"" on one of these disks with ~500GB of data, I correctly see errors both on the chunkserver and master. All files are still readable, so things are looking good there. The cgi-admin page also shows there are errors on this disk (in the last-error column, the status still says ""ok"" for the disk). 

However, even after a few hours, master still lists all chunks as stable, and I haven't seen anything about replication taking place. I would assume I should see *something* about it in the background or logs. Of course I'm well aware just removing files is a pretty nasty error (no hard device errors, etc.), but from the LizardFS p-o-v the only thing that should matter is that the files cannot be found...

So, what is the process where the chunk/master server moves from having detected a last-error to actually triggering re-replication? 

 ","@eriklindahl 
ZFS will detect a ""broken file"" on every read, not only on a scrub. If you ""zero-fill"" part of a device and you read a file with part in that zone, ZFS will detect that thank to it's bit-rot protection (the file checksum won't match)

Instead of restarting a chunkserver every night, would be better to add a manual scrub that walks the whole storage. I know that LFS is able to scrub chunks, but it does in a very very very slow way.
A manual scrub that traverse the whole storage would be great, just to be sure that all replicas are there.

All RAID (hw or sw) and even ZFS has scrub features to prevent data-loss. LFS lacks this and IMHO should be added.",9800119
155,"any plans to add the ""mfspreflabels"" option from MooseFS?",open,2017-09-11T21:34:18Z,2017-10-05T18:13:55Z,,NONE,"I was looking at the MooseFS mfsmount options, and found this option: 
```
-o mfspreflabels=LABELEXPR
specify preferred labels for choosing chunkservers during I/O
```
which seems pretty interesting to set a faster chunkserver for a given client, on a client to client basis! 

For people wanting to have a finer control over the topology, it seems like a fairly simple and quick solution for special cases, like for example, making clients prefer a SSD chunkserver for faster read.

Is there any plans to port this option over to LizardFS mfsmount? ","gotcha! 
",9800119
156,Setup for HA automatic master recovery?,open,2017-09-05T11:29:34Z,2017-09-06T00:55:41Z,,NONE,"Hi,

We have our own HA setup based on keepalived working great for the shadow server automatically taking over when the master fails. However, we would also like to automate things so the original master server can take over again the second it comes up (partly because that's a fatter server, and partly because we want things to work cleanly if there's some catastrophic failure of network, etc.).

However, the challenge is that if any files have been written while the secondary server was master, that data is obviously not yet present on the primary server. It works fine if we first restart the primary server in shadow mode to have it re-sync, so I would assume it's a matter of checking that the metadata versions match before restarting with the primary server as master?

But: This leads to a couple of challenges about always doing things in the right order (and checking they have completed), so does any of you have scripts to automate this?

","Hi Erik,

There is an official HA solution, lizardfs-uraft, that is scheduled for release with the next major version of LizardFS. That could be anywhere from a week to 6 months from now... should have already been released :)",9800119
157,Writes to samba not working from OSX,open,2017-08-29T19:20:41Z,2019-05-31T08:29:57Z,,NONE,"Hi!
We are using lizardfs for our file server with samba4 and win-ad, and it works ok, but apple computers cannot use share at all.
When writing to the samba4 share clients get ""error -36"" and nothing happens.
We tried all the same samba server and osx client, but with ext4, xfs and zfs - everything works. It's lizardfs that brakes it for macs.

Lizardfs version is latest, 3.11 from official repo, osx clients are 10.12 and 10.9.","We pushed some file lock fixes not long ago: https://github.com/lizardfs/lizardfs/commit/a6e192a9baf5e23ffb1a1f23e34ca7ec38288a39 and they are not in any official release yet.

Our locks used to pass samba's lock tests anyway, but now they pass all NFS lock tests as well, so maybe the fixes solve your issues as well.",9800119
158,"Question: Replication to *ALL* chunk nodes, preferred read from certain ones",open,2017-07-27T15:54:57Z,2017-12-17T21:37:45Z,,NONE,"Hey guys.

Maybe you know if this could be achieved with current replication goals and topology:

I want to somewhat ""abuse"" lizardfs to synchronize the content of ""n"" disks.
Scenario (now): I have 150 Client Workstations. Each client has a partition that is synced with a nfs read-only master copy using rsync. This is lame and inconsistencies between rsync runs drive me mad. I could use something along the lines of DRBD and friends :) - but in theory, lizardFS should be able to do the same (and even better ;-).

so what I want to achieve is:
All clients act as chunknodes and provide their partitions to be ""synchronized"" to the master. If a file is stored to lizardfs, the chunks are stored on every chunk node there is (if the node if offline at a given moment, it will be replicated ""later""). If a client wants to read a file, it will prefer itself (since it doubles as a client and a chunkserver) - but if it is not in sync (yet), it will be able to read from any other chunk server available.

Do I sound crazy?
Should I stick to DRBD?

I really see a lizardfs usecase here, but I'm not sure if I'm blind and don't see the obvious problem :)
","You mention that if a node goes down, it simply uses a different one.
But doesn't lizardfs immediately start copying the chunks because it now has an undergoal? I was worried about that too - then I realized: it would not make sense if it does.If you have n nodes, and you set the goal to n replicas, then all nodes have a certain chunk already. Of a node goes down, it would not make sense to store the chunk twice on another node, since it would not add redundancy. So of a node goes down, the chunks are registered as ""under goal"", but no replication is started, simply because there are no nodes left that could store it and at the same time do not have a valid copy already.I hope this answers your question.My nodes are located in student labs, sometimes they are up, sometimes they are down, some of them, all of them, totally random.As soon as a node goes down, it is of course marked as disconnected and the chunks are undergoal, but due to the situation described above, the replication does not really care.If something changes on the Filesystem and the node comes back to live, the mount will access the chunks from all other nodes, until the changed chunks in question are replicated to the node again.I run three labs for now and try to keep the traffic to their respective access switches. So my configuration is basically this (lab1, lab2 and lab3 are also configured in the topology.cfg):10 all_nodes : lab1 lab1 lab1 lab1 lab1 lab1 lab1 lab1 lab1 lab1 lab1 lab1 lab1 lab1 lab1 lab2 lab2 lab2 lab2 lab2 lab2 lab2 lab2 lab2 lab2 lab2 lab2 lab2 lab2 lab2 lab3 lab3 lab3 lab3 lab3 lab3 lab3 lab3 lab3 lab3 lab3 lab3 lab3 lab3 lab3 You will lose performance since instead or directly loading blocks from your local disk you're going to have a Userland implementation (fuse) messing around and causing context switches all over the place, but it works fine for us since it provides acceptable performance, scales and reduced our overhead for dealing with synchronized clients./)/)",9800119
159,REDUNDANCY_LEVEL is useless... :(,open,2017-07-23T08:27:25Z,2017-11-07T21:41:30Z,,MEMBER,"Actually `REDUNDANCY_LEVEL = 2` is worse than useless: it causes `write file error` ... `index: 0 - error sent by master server (No chunk servers)` for replicated goal=3 when two chunkservers are stopped.

Goal is defined as `3: $std {stor pool wks}` and there are five (5) chunkservers labelled `wks`.  
Problem happens when I stop any two `wks` chunkservers while it should have been possible to stop all `wks` chunkservers...
","We'll discuss this issue internally. I get that the way it works now is not perfect, but there are many technical barriers to simply change it. This topic is really close to two core functionalities of LizardFS, I/O mechanism and replication, and changing anything related to these two should be very careful.

Let this issue remain open, and if I (or anyone) have further comments or patches, they will be placed here.",9800119
160,Git writes fail on mount exported by NFS,open,2017-07-21T12:11:40Z,2017-07-21T21:40:21Z,,NONE,"I'm trying to set up multiple Debian machines where machine 1 should mount my LizardFS cluster and export it using NFS and machine 2 should be mounting that NFS share. Active directory is supposed to provide user accounts.
This seems to work fine except when running Git on machine 2 where commands writing to the repository fail. `git commit` for example results in
> fatal: fsync error on 'sha1 file': Permission denied

Each of the following changes result in a working setup:
- machine 1 exporting a local directory with the same contents as the LizardFS cluster (so it's a LizardFS issue?)
- machine 2 mounting the LizardFS cluster directly (or actually NFS?)
- machine 2 running git as root (or maybe permissions after all?)
No matter in what way I mounted the files, the users and groups were always correct.

What else I've tried without success:
- disabling NFS file locking so flock succeeds: [git-annex on NFS](https://git-annex.branchable.com/tips/git-annex_on_NFS)
- chmod-ing the whole contents of the cluster to 777
- various combinations of mount and export options

It seems to be some sort of communication issue between LizardFS and NFS considering leaving one of them out solves the problem. However, I'm out of ideas what else to try.","One reason is the better performance by running the FUSE client on a real machine whereas my ""machine 2"" is virtual (see #514). NFS working as some sort of cache also improves client performance a bit.
Another reason is simply that I need machine 1 anyway to serve Windows clients and decoupling clients from direct access to the cluster is not a bad idea (if it actually worked).

My goal is to have one file server (""machine 1"") on every network and as this is a home setup I'm not too concerned about fail-over situations.",9800119
161,Increasing number of goals beyond 20,open,2017-06-26T03:03:39Z,2017-07-05T05:02:01Z,,NONE,"Hello,

I am interested in increasing the number of allowed goals beyond 20. 

Are there issues with raising the limit, and how would I go about doing it?","Just as a matter of curiosity, why do you need so many goals?",9800119
162,Feature Request : Simultaneous writes for Replication Goals,open,2017-06-25T00:35:11Z,2017-07-05T04:43:50Z,,NONE,"Currently lizardfs uses chained writes for replication goals (unlike ec goals, which are simultaneous). This works well for single 1G adapter setups, but when you have multiple bonded adapters this is a sub-optimal use of them. e.g I have 3*1G adapters in balance-alb mode,  its supports simultaneous writes to 3 different servers at a full 1G. Also it probably increases latency issues.

I suspect if lizard had a option or new replication goals (r(x) maybe?) for simultaneous writes then sequential and random write performance could be increased.","Thanks for the labels Piotr

>If previously a write requires 3 seconds, after this it will require 1 second

I don't think its quite that dramatic. Chunkservers start passing on writes immediately on receiving data, before the send is finished, so writes of more than one packet should be written with just a few packets delay. This should work relatively well for large sequential writes, but for small and/or random writes delays will add up and latency will be much increased.


I had noticed I can much faster seq writes with ec modes than replica.",9800119
163,lizardfs makesnapshot is not keeping change dates of folders and files,open,2017-06-24T07:51:09Z,2017-06-26T14:29:43Z,,NONE,"When a snapshot ist created with `lizardfs makesnapshot`, then the change date of all files is updated:
```
stat original/file
  File: 'original/file'
  Size: 35142     	Blocks: 69         IO Block: 65536  regular file
Device: 2eh/46d	Inode: 2046928     Links: 1
Access: (0644/-rw-r--r--)  Uid: (10000/  user)   Gid: (10000/group)
Context: system_u:object_r:fusefs_t:s0
Access: 2017-05-24 15:59:02.000000000 +0200
Modify: 2016-05-31 09:29:03.000000000 +0200
Change: 2017-05-24 15:59:02.000000000 +0200
 Birth: -
```

```
stat file/in/snapshot
  File: 'file/in/snapshot'
  Size: 35142     	Blocks: 69         IO Block: 65536  regular file
Device: 2eh/46d	Inode: 2046928     Links: 1
Access: (0644/-rw-r--r--)  Uid: (10000/  user)   Gid: (10000/group)
Context: system_u:object_r:fusefs_t:s0
Access: 2017-05-24 15:59:02.000000000 +0200
Modify: 2016-05-31 09:29:03.000000000 +0200
Change: 2017-06-24 00:20:25.000000000 +0200
 Birth: -
```

2017-06-24 00:20:25 is the time when the snapshot was taken. At this time, the file was not changed. The updated change dates are an issue for backup tools. We use the IBM TSM System for backups of snapshots. Unfortunately, it tries to update all actually unchanged files because of their new change date. This leads to unnecessary long run times of the backup.   

For folders it is even worse, their modification times are updated. I don't know if this is also relevant for the backup procedure, but it is at least confusing. ",Thank you very much for providing this patch!,9800119
164,Status of HA and master sync replication,open,2017-06-17T08:15:02Z,2017-07-01T12:40:30Z,,NONE,Is possible to have an update on official HA solution and of sync replication between two or more masters is planned ?,"Yes, usually fuse is a huge bottleneck.",9800119
165,Add a downgrade tool or config option.,open,2017-06-04T17:22:59Z,2017-06-07T07:57:38Z,,NONE,"In case of bugs in new releases there should be an ""exit plan"", a tool or a special option in config to roll back an installation.
Now a connection of older mfsmaster shadow to a newer mfsmaster fails with `MLTOMA_REGISTER_SHADOW - rejected old client (v3.10.4)` message.",,9800119
166,`rremove` should be able to bypass trash,open,2017-05-15T18:25:36Z,2017-05-15T18:25:36Z,,MEMBER,"`lizardfs rremove` should be able to bypass trash.
As [I've said earlier](https://github.com/lizardfs/lizardfs/issues/254#issuecomment-111312562) when deleting large snapshot it is undesirable for millions of files to land into trash...
In my case it takes up to 20 hours to clean trash after snapshot removal so I always have to invoke `lizardfs settrashtime -l -r 0` before removing snapshots...",,9800119
167,CGI: show Labels in Server Charts,open,2017-05-13T23:17:10Z,2017-05-13T23:17:10Z,,MEMBER,"It would be useful to add chunkserver's Labels to Server Charts.
For example with such feature one can see where chunks are being replicated (if there are chunkservers labelled differently). Thanks.",,9800119
168,Wishlist 5.x - webmin module,open,2017-05-09T09:25:57Z,2017-08-03T14:11:19Z,,NONE,"As webmin is a web-based interface for system administration for Unix I think it would be possible to get some LizardFS infos available in a module for webmin. 

This could make sense if webmin is used for setting up remote systems and remote administration purposes.",,9800119
169,Master memory usage,open,2017-04-27T19:12:26Z,2019-03-11T23:01:32Z,,NONE,"Any ""rule of thumbs"" about master memory usage? What if I need more RAM than available ?

In example, by using a server with 128GB, how many files/chunks/terabyte i'm able to manage and what If i need more RAM without being able to add (maybe server with limited amount or memory slots)

Can we safely swap on SSD? Is possible to force master to use SSD storage without keeping the whole metadata archive in RAM ?","This paper (http://students.mimuw.edu.pl/~ps321169/mgr.pdf) seems to say that the data structure for a file takes about 500 bytes, including hashing etc. YMMV.",9800119
170,chunklost on disk full?,open,2017-04-25T14:43:53Z,2017-05-02T08:53:14Z,,NONE,"I'm getting this:

```
Apr 25 16:38:38 ale-XPS13 mfschunkserver[2852]: mfschunkserver[2852]: write_block_to_chunk: file:/mnt/chunk2/chunks00/chunk_0000000000000059_00000001.mfs - write error: ESUCCESS (Success)
Apr 25 16:38:38 ale-XPS13 mfschunkserver[2852]: write_block_to_chunk: file:/mnt/chunk2/chunks00/chunk_0000000000000059_00000001.mfs - write error: ESUCCESS (Success)
Apr 25 16:38:38 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - Chunk write error (IO error) (try counter: 1)
Apr 25 16:38:39 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 2)
Apr 25 16:38:40 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 3)
Apr 25 16:38:41 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 4)
Apr 25 16:38:42 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 5)
Apr 25 16:38:43 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 6)
Apr 25 16:38:44 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 7)
Apr 25 16:38:45 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 8)
Apr 25 16:38:46 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 9)
Apr 25 16:38:47 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 10)
Apr 25 16:38:48 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 11)
Apr 25 16:38:50 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 12)
Apr 25 16:38:53 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 13)
Apr 25 16:38:57 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 14)
Apr 25 16:39:02 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 15)
Apr 25 16:39:08 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 16)
Apr 25 16:39:15 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 17)
Apr 25 16:39:23 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 18)
Apr 25 16:39:32 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 19)
Apr 25 16:39:42 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 20)
Apr 25 16:39:52 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 21)
Apr 25 16:39:58 ale-XPS13 mfsmaster[2831]: mfsmaster[2831]: chunk 0000000000000059 has only invalid copies (1) - please repair it manually
Apr 25 16:39:58 ale-XPS13 mfsmaster[2831]: mfsmaster[2831]: chunk 0000000000000059_00000001 - invalid copy on (192.168.1.84 - ver:00000000)
Apr 25 16:39:58 ale-XPS13 mfsmaster[2831]: chunk 0000000000000059 has only invalid copies (1) - please repair it manually
Apr 25 16:39:58 ale-XPS13 mfsmaster[2831]: chunk 0000000000000059_00000001 - invalid copy on (192.168.1.84 - ver:00000000)
Apr 25 16:40:02 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 22)
Apr 25 16:40:12 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 23)
Apr 25 16:40:22 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 24)
Apr 25 16:40:32 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 25)
Apr 25 16:40:42 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 26)
Apr 25 16:40:52 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 27)
Apr 25 16:41:02 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 28)
Apr 25 16:41:12 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 29)
Apr 25 16:41:22 ale-XPS13 mfsmount[12930]: write file error, inode: 17, index: 35 - error sent by master server (Chunk lost) (try counter: 30)
Apr 25 16:41:22 ale-XPS13 mfsmount[12930]: error writing file number 17: EIO (Input/output error)
Apr 25 16:41:22 ale-XPS13 mfsmount[12930]: error writing file number 17: EIO (Input/output error)
```

by running (after a while) a simple bonnie++ test: `bonnie++ -d /mnt/lizard/ -n 1:1m:1k:6 -m 'LizardFS' -q | bon_csv2html >> /home/x/Scrivania/bonnie.html`

probably, is related to disk full.

when this happens, `df` output is broken, is showing wrong total size (178GB, but I have about 3GB) and ""0"" as used space:

```
$ df -h  | grep lizard
mfs#mfsmaster:9421           178M     0    178M   0% /mnt/lizard
```","Is it possible you're running out of space because the files you're creating are ending up in LizardFS' trash folder?

Try setting trash time to 1 or 0 on the folder you're pointing Bonnie++ at.",9800119
171,Manual scrub,open,2017-04-25T11:19:45Z,2017-10-12T16:23:35Z,,NONE,"A manual scub feature should be added.
Currenty, Lizard automatically scrub a file on read, or at defined intervals, but this last one doesn't scrube the whole storage, only some parts every execution.

Adding a full scrub feature (like ZFS or mdadm does) is really usefull.

Something like:

`lizardfs scrub full` or `lizardfs scrub /path/to/a/file`

The first one will scrub the whole storage, the second one will only scrub a single file (or a single directory).

Reading a file is not enough, most file could be served from server caches and thus doesn't trigger any scrub","Saying
> there's no point in adding such functionality to C++ code itself.

Is very different than being open to new features.

Tomorrow I'll create a PR , there are some fixes/optimizations to do for sure as I'm not a C programmer, thus I'm open to any suggestions.",9800119
172,"write file error, inode: 6, index: 0 - error sent by master server (No space left) (try counter: 1)",open,2017-04-23T16:11:01Z,2017-04-23T18:49:38Z,,NONE,"I'm trying a simple setup with one master and one chunkserver poiting to three mounted loop devices as chunks.

each loop device is 1GB, with an XFS filesystem, and mounted to /tmp/chunkX:

```
$ df -h | grep chunk
/dev/loop4                   997M   34M    964M   4% /tmp/chunk1
/dev/loop5                   997M   34M    964M   4% /tmp/chunk2
/dev/loop6                   997M   34M    964M   4% /tmp/chunk3

$ grep chunk /proc/mounts 
/dev/loop4 /tmp/chunk1 xfs rw,relatime,attr2,inode64,noquota 0 0
/dev/loop5 /tmp/chunk2 xfs rw,relatime,attr2,inode64,noquota 0 0
/dev/loop6 /tmp/chunk3 xfs rw,relatime,attr2,inode64,noquota 0 0
```

I'm able to write something there:

```
$ sudo dd if=/dev/zero of=/tmp/chunk1/test count=100 bs=1M
100+0 record out
100+0 record in
104857600 bytes (105 MB, 100 MiB) copied, 0,0457952 s, 2,3 GB/s
```

I've mounted Lizard in /mnt/lizard:

```
$ sudo mfsmount /mnt/lizard/
mfsmaster accepted connection with parameters: read-write,restricted_ip,ignore_gid ; root mapped to root:root
[ OK ] Received IO limits configuration update from master
```

but i'm unable to write anything:
```
$ sudo dd if=/dev/zero of=/mnt/lizard/test count=100 bs=1M
dd: error writing '/mnt/lizard/test': No space left on device
dd: closing output file '/mnt/lizard/test': No space left on device
```

These are Lizard logs:
```
Apr 23 17:52:27 ale-XPS13 mfschunkserver: can't load config file: /etc/lizardfs/mfschunkserver.cfg
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [WARN] configuration file /etc/lizardfs/mfschunkserver.cfg not found - using defaults; please create /etc/lizardfs/mfschunkserver.cfg to remove this warning (to get a base configuration you can copy corresponding file from /usr/share/doc/lizardfs-master/examples)
Apr 23 17:52:27 ale-XPS13 mfschunkserver: configuration file /etc/lizardfs/mfschunkserver.cfg not found - using defaults; please create /etc/lizardfs/mfschunkserver.cfg to remove this warning (to get a base configuration you can copy corresponding file from /usr/share/doc/lizardfs-master/examples)
Apr 23 17:52:27 ale-XPS13 mfschunkserver: set gid to 150
Apr 23 17:52:27 ale-XPS13 mfschunkserver: set uid to 136
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] changed working directory to: /var/lib/lizardfs
Apr 23 17:52:27 ale-XPS13 mfschunkserver: changed working directory to: /var/lib/lizardfs
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] lockfile /var/lib/lizardfs/.mfschunkserver.lock created and locked
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] hdd configuration file /etc/lizardfs/mfshdd.cfg opened
Apr 23 17:52:27 ale-XPS13 mfschunkserver: lockfile /var/lib/lizardfs/.mfschunkserver.lock created and locked
Apr 23 17:52:27 ale-XPS13 mfschunkserver: hdd configuration file /etc/lizardfs/mfshdd.cfg opened
Apr 23 17:52:27 ale-XPS13 mfschunkserver: hdd space manager: folder /tmp/chunk3/ will be scanned
Apr 23 17:52:27 ale-XPS13 mfschunkserver: hdd space manager: folder /tmp/chunk2/ will be scanned
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] hdd space manager: path to scan: /tmp/chunk3/
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] hdd space manager: path to scan: /tmp/chunk2/
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] hdd space manager: path to scan: /tmp/chunk1/
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] hdd space manager: start background hdd scanning (searching for available chunks)
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] main server module: listen on *:9422
Apr 23 17:52:27 ale-XPS13 mfschunkserver: hdd space manager: folder /tmp/chunk1/ will be scanned
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [....] connecting to Master
Apr 23 17:52:27 ale-XPS13 mfschunkserver: hdd space manager: path to scan: /tmp/chunk3/
Apr 23 17:52:27 ale-XPS13 mfschunkserver: hdd space manager: path to scan: /tmp/chunk2/
Apr 23 17:52:27 ale-XPS13 mfschunkserver: hdd space manager: path to scan: /tmp/chunk1/
Apr 23 17:52:27 ale-XPS13 mfschunkserver: hdd space manager: start background hdd scanning (searching for available chunks)
Apr 23 17:52:27 ale-XPS13 mfschunkserver: main server module: listen on *:9422
Apr 23 17:52:27 ale-XPS13 mfschunkserver: connecting to Master
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] loaded charts data file from /var/lib/lizardfs/csstats.mfs
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] open files limit: 32768
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: [ OK ] mfschunkserver daemon initialized properly
Apr 23 17:52:27 ale-XPS13 mfschunkserver: loaded charts data file from /var/lib/lizardfs/csstats.mfs
Apr 23 17:52:27 ale-XPS13 mfschunkserver: open files limit: 32768
Apr 23 17:52:27 ale-XPS13 mfschunkserver: mfschunkserver daemon initialized properly
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: mfschunkserver[16755]: connected to Master
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: connected to Master
Apr 23 17:52:27 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: chunkserver register begin (packet version: 5) - ip: 192.168.1.84, port: 9422
Apr 23 17:52:27 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: chunkserver register end (packet version: 5) - ip: 192.168.1.84, port: 9422, usedspace: 0 (0.00 GiB), totalspace: 0 (0.00 GiB)
Apr 23 17:52:27 ale-XPS13 mfsmaster[9554]: chunkserver register begin (packet version: 5) - ip: 192.168.1.84, port: 9422
Apr 23 17:52:27 ale-XPS13 mfsmaster[9554]: chunkserver register end (packet version: 5) - ip: 192.168.1.84, port: 9422, usedspace: 0 (0.00 GiB), totalspace: 0 (0.00 GiB)
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: mfschunkserver[16755]: scanning folder /tmp/chunk2/: complete (0s)
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: scanning folder /tmp/chunk2/: complete (0s)
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: mfschunkserver[16755]: scanning folder /tmp/chunk3/: complete (0s)
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: scanning folder /tmp/chunk3/: complete (0s)
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: mfschunkserver[16755]: scanning folder /tmp/chunk1/: complete (0s)
Apr 23 17:52:27 ale-XPS13 mfschunkserver[16755]: scanning folder /tmp/chunk1/: complete (0s)
Apr 23 17:58:10 ale-XPS13 mfsmount[17493]: Received IO limits configuration update from master
Apr 23 17:58:40 ale-XPS13 mfsmount[17493]: write file error, inode: 6, index: 0 - error sent by master server (No space left) (try counter: 1)
Apr 23 17:58:40 ale-XPS13 mfsmount[17493]: error writing file number 6: ENOSPC (No space left on device)

```

any hint ?
","I've created a 500MB file. 8 chunks were properly saved.

Then, a simple `mkfs.ext4 image.img` lead to the following, mkfs is still running (probably, hanged) and cgi interface shows 8 chunks with 0 copies and 0 valid copies:

```
Apr 23 20:45:10 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: release: session not found
Apr 23 20:45:10 ale-XPS13 mfsmaster[9554]: release: session not found
Apr 23 20:45:11 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:11 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:11 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 1)
Apr 23 20:45:12 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:12 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:12 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 2)
Apr 23 20:45:13 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:13 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:13 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 3)
Apr 23 20:45:14 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:14 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:14 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 4)
Apr 23 20:45:15 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:15 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:15 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 5)
Apr 23 20:45:16 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:16 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:16 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 6)
Apr 23 20:45:17 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:17 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:17 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 7)
Apr 23 20:45:18 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:18 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:18 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 8)
Apr 23 20:45:19 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:19 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:19 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 9)
Apr 23 20:45:20 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:20 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:20 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 10)
Apr 23 20:45:21 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:21 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:21 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 11)
Apr 23 20:45:23 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:23 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:23 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 12)
Apr 23 20:45:26 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:26 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:26 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 13)
Apr 23 20:45:30 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:30 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:30 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 14)
Apr 23 20:45:35 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:35 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:35 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 15)
Apr 23 20:45:41 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:41 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:41 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 16)
Apr 23 20:45:48 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:48 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:48 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 17)
Apr 23 20:45:56 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:56 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:45:56 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 18)
Apr 23 20:46:05 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:05 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:05 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 19)
Apr 23 20:46:15 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:15 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:15 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 20)
Apr 23 20:46:25 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:25 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:25 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 21)
Apr 23 20:46:35 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:35 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:35 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 22)
Apr 23 20:46:45 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:45 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:45 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 23)
Apr 23 20:46:55 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:55 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:46:55 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 24)
Apr 23 20:47:05 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:05 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:05 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 25)
Apr 23 20:47:15 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:15 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:15 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 26)
Apr 23 20:47:25 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:25 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:25 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 27)
Apr 23 20:47:35 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:35 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:35 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 28)
Apr 23 20:47:45 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:45 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:45 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 29)
Apr 23 20:47:55 ale-XPS13 mfsmaster[9554]: mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:55 ale-XPS13 mfsmaster[9554]: serious structure inconsistency: (chunkid:0000000000000001)
Apr 23 20:47:55 ale-XPS13 mfsmount[17493]: write file error, inode: 9, index: 0 - error sent by master server (Chunk lost) (try counter: 30)
Apr 23 20:47:55 ale-XPS13 mfsmount[17493]: error writing file number 9: EIO (Input/output error)


```",9800119
173,[HA] Automatic master personality,open,2017-04-23T08:25:38Z,2017-11-09T11:11:05Z,,NONE,"Currently, master personality must be specified in configuration file. This makes impossible to use a shared `mfsmaster.cfg` between nodes, because shadow needs an extra configuration line.

Would be great to add automatic personality, starting automatically in shadow mode if master ip (as defined in `MASTER_HOST`) isn't configured on the running machine.

in this way you'll have a single master configuration that could be shared across multiple servers via etcd, pacemaker can move a floating IP between metadata servers (master and multiple shadows) and `mfsmaster` daemon automatically check if the `MASTER_HOST` is pointing to the running machine. If yes, `mfsmaster` will start in master mode, if not, it will start in shadow connecting to the master.

A `SIGUSR1`, `SIGUSR2` or `SIGHUP` could trigger a master reload, checking again if floating ip is set. 
If set, shadow will be automatically promoted to master. pacemaker (or whatever) just need to trigger a reload when moving the floating ip and if you need to change the master configuration, you have to change it only in one place and not on every master/shadow servers.

This is also useful to simplify the configuration management. Just copy the same configuration on multiple nodes without adding additional lines. Can be scripted easily","By setting `PERSONALITY = ha-cluster-managed` master server doesn't start, complaining about this:

```
Nov  9 12:05:47 x mfsmaster: This installation is managed by HA cluster, one should manipulate metadata servers only using lizardfs-cluster-manager.
Nov  9 12:05:47 x mfsmaster[19182]: [ OK ] changed working directory to: /var/lib/mfs
Nov  9 12:05:47 x mfsmaster[19182]: [FAIL] This installation is managed by HA cluster, one should manipulate metadata servers only using lizardfs-cluster-manager.

```",9800119
174,"makesnapshot reports ""Invalid argument"", snapshot is incomplete",open,2017-04-13T03:11:48Z,2017-09-26T12:17:53Z,,NONE,"When I run lizardfs makesnapshot, it runs for a while and then reports ""Invalid argument"":

```
# lizardfs makesnapshot -l root snapshots/20170412224630
root->/big-root/snapshots/20170412224630: Invalid argument
```

Furthermore, the resulting snapshot is incomplete, missing a large number of files -- find reports less than 3M entries, when there should be close to 40M. Most, if not all, of the directories appear to be present, but most of the files themselves are absent.

Also, after creating the snapshot, I have observed a number of under-goal chunks. These under-goal chunks turn into lost chunks when the snapshots are deleted. So far, the affected files have fortunately only been cache and temporary log files, which can be regenerated, but it's only a matter of time before something real gets hit.

These two problems make snapshots worse than useless for us.

We are running lizardfs 3.10.6 on both the clients and servers.","> A new commit that adds couple of options to makesnapshot command was just merged.
> Now, you can ignore the fact that some content of the snapshotted folder changed during execution by adding -i option.
> It's also possible to set a size of snapshot that will be executed atomically, but at the cost of master responsiveness in case of setting big size values. (by either -s command option or in master config)


Did these commits make it into a release? I don't see the -s & -i options in 3.11.3

Also, what does ""size of a snapshot"" refer to?
* number of inodes/file?
* actual file size?",9800119
175,Questions about data resiliency and qemu,open,2017-04-11T20:26:33Z,2018-02-26T07:16:06Z,,NONE,"Some questions about datar resiliency:

1) when client writes to a chunkserver, the ACK from chunkserver to the client, is sent after ALL chunkservers have properly stored the chunk or after the first chunk is wrote ? In other words, with ""goal=3"", the ACK to the client is sent after all 3 copies are stored or only after the first one?

2) what happens in case of ""primary"" chunk server failure between replication ? Is client able to know that file isn't wrote properly ? In example:
client write to chunkserver A. Write is done properly on that chunkserver then before replication is made on chunkserver B, chunkserver A dies. What happens now ? Is client able to detect this failure ?

3) what happens in case of metalogger crash during a write ? Is client able to detect the failure ?

4) is possible to know the 'delay' between metadata master and shadows ? if master fails and shadow is not 100% replicated, this lead to data-loss as all chunks wrote before master failure but still not replicated to shadow before master failure, are lost.

5) any ETA about native module for qemu to bypass FUSE ?",">The image file is not thinprovisoned, thus any changes inside that should be transparent for Lizard and just trigger a chunk change.

I see your point in that regard and I doubt a qcow2 file would be any different, they are both images containing file systems.

Not sure what is tracked by meta data - will updating the contents of a file without changing its size etc update metadata apart from the timestamps? question for Piotr.",9800119
176,DATA_PATH on EBS volume,open,2017-03-13T15:37:16Z,2017-03-13T19:30:37Z,,NONE,"Hi, I have DATA_PATH for master and chunkserver which points to EBS volume.
When I destroy instance and create new one from scratch I can see all chunks, but when I mount lizardfs, it shows empty folder with 100% free space.
I need to have AS instances which could be destroyed at any time, but data on EBS volumes must me available on newly created instances. Am I missed something in my configuration?

```
# mfsmaster -version
version: 3.10.6
# mfschunkserver -version
version: 3.10.6
```","Sounds weird. Do you have lizardfs-cgi installed as well? Some screens from the web interface would provide much more information. Other helpful things would be: logs from syslog, list of files from both data paths (output from `ls -l` or something similar), servers' config files (mfsmaster.cfg, mfschunkserver.cfg, etc).",9800119
177,Read/Write process documentation,open,2017-03-02T19:36:42Z,2017-03-03T08:41:46Z,,NONE,"I'm trying to ""break"" my lizardfs installations in different ways to harden it before taking it into a larger production setup. Until now I only can guess how the whole process from mounting the FS, to reading and writing to it actually works.

Is there any process diagram for it (like the one for the architecture) or can one read it somewhere?","So my guessing was right. IMHO you should put this into the normal documentation. :)
On the other hand: Thanks a lot for your usual fast response!",9800119
178,Add distance penalty to chunk server configuration,open,2017-02-27T11:26:05Z,2017-05-20T10:42:59Z,,CONTRIBUTOR,"Currently when reading chunks from chunkservers, there is no way to
prioritize fast chunkservers running on SSDs over slow chunkservers
running on spinning disk.

This patch-set adds a new configuration option (DISTANCE_PENALTY)
that allows the admin to set a distance penalty for each chunkserver.
This penalty will be added to *any* distance calculations made for the
chunkserver, which (depending on its value and whether my previous
distance calculation patch, #523, which is included as the first patch in this
patch-set, has been applied) will make the chunkserver a less likely
candidate for reads.

This patch-set is now in production at our school, where I've configured
all our spinning disks to have a penalty of 100, while all our SSDs have
a penalty of 0 (the default).  Our VMs (which have some chunks on spinning
disk due to changed goals of snapshots) are now *much* faster, because
they only read from the spinning disks if the SSDs are unavailable.

This patch-set does include a network protocol change that makes it
incompatible with older LizardFS installs.  If there's any way for the
chunkserver to tell older masters that they can ignore the
LIZ_CSTOMA_REGISTER_PENALTY packet, or for the chunkserver to detect that
the master doesn't recognize that packet, I would happily implement it.","Ok, thanks so much.  I'll hold off polishing this patch until you're ready for it or we have something else that does essentially the same thing.",9800119
179,local *partial/recent* cache for remote cluster?,open,2017-02-26T17:24:14Z,2017-02-26T17:24:14Z,,NONE,"excuse me if I'm asking a known thing, my search skills didn't produce results.

What I'm wanting to do is use lizardfs as a central repository to a distributed write systems for security cameras and backups.  Upload speads are almost universally an issue so local write caching at the remote locations is required.  The security cameras have a complication though, the cameras themselves proxy the storage for the VMS software (Axis Companion).  If you remote view the cameras and the storage is the remote ,then the site itself is a proxy for the data.  not ideal because it would eat up download capacity to look at stored videos.

ideal on-site device is a small computer with an SSD that accepts writes to a samba share and then pushes those up to the cluster target.  It should keep some amount of data cached locally as well, preferably by age, keeping xGB of the newest data in the cache.  Note that I wouldn't want the local drive to be a storage target in the cluster, it should only be a buffer/cache.

Thoughts?

",,9800119
180,master: Use difference between racks to calculate distance,open,2017-02-24T16:50:50Z,2018-05-18T20:54:49Z,,CONTRIBUTOR,"Currently distance is calculated as 0 (same machine), 1 (same rack) and 2
(different racks).  This doesn't allow any way of saying that rack 1 is
closer to rack 2 than it is to rack 42.

This patch changes the topology distance calculation by having different
racks represented by the difference between the racks + 1 (since a
distance of 1 means they're on the same rack).

Rack 2 now has a distance of 2 from rack 1 (abs(2-1)+1=2) and a distance
of 41 from rack 42 (abs(2-42)+1=41), which means clients on rack 2 will
prefer chunk servers on rack 1.

The implementation idea came from https://github.com/lizardfs/lizardfs/issues/269#issuecomment-106193955

I've built and tested this patch on our local 3.10.6 cluster, and it functions as advertised.

Signed-off-by: Jonathan Dieter <jdieter@lesbg.com>","Hi,

The feature would be quite appreciated here. Do you have any plan to commit this patch any time soon?

Thanks!",9800119
181,Strange chunk distribution and replication.,open,2017-02-16T04:33:25Z,2017-05-15T21:54:33Z,,NONE,"I'm running a LizardFS cluster with three chunk servers holding approximately 18 million files.  The goal is ""3"".  This is a pre-production cluster I'm evaluating.

I'm converting the chunk servers one by one from magnetic media to SSD.  I downed the first chunk server and performed the maintenance: removed the old drives (four magnetic drives) and installed the new drives (seven SSDs).  I started the server, it joined the cluster again, and chunks were replicated.  About half-way through I noticed that space was being used on this chunk server at twice the rate expected, and this trend continued to when replication finished.  The two untouched nodes had 509 GiB of data and the new node had 1.2 TiB of data while having the same number of chunks: 19512380.

At this point I made note of the oddity and moved on to the second chunk server.  Similar maintenance was performed and the node was brought back online.  However, no chunks are being replicated to this updated chunk server.  At this point, all of our chunks are being reported as undergoal.

In an attempt to kickstart replication again I used mfssetgoal to change the goal from 3 to 2 and back to 3.  The chunks went from undergoal to stable, and back to undergoal, but no replication started to the new node.

Since then, some data has been written to the cluster and those chunks are stable and are present on all three chunk servers.  The only chunks present on the second chunk server are those written since the drives were swapped.

No errors reported on the chunk server or our metadata master.  The mfschunkserver daemon has set up the expected directory structure on the new drives.

Not sure what's going on.  Any advice?

![lizardfs-disks](https://cloud.githubusercontent.com/assets/701831/23007269/27d0fc70-f3cd-11e6-91a7-8d9af81008c4.png)

![lizardfs-info](https://cloud.githubusercontent.com/assets/701831/23007270/27e3620c-f3cd-11e6-8e3a-237078b91939.png)

![lizardfs-servers](https://cloud.githubusercontent.com/assets/701831/23007271/27e4d7ea-f3cd-11e6-890e-3ea563fee458.png)


",Did you resolve the undergoal replication issue dogshoes?,9800119
182,lizardfs-chunkserver.service becomes a zombie,open,2017-02-09T17:24:46Z,2017-02-16T10:51:12Z,,NONE,"`service lizardfs-chunkserver stop` leads to 

```
service lizardfs-chunkserver status
● lizardfs-chunkserver.service - LizardFS chunkserver daemon
   Loaded: loaded (/lib/systemd/system/lizardfs-chunkserver.service; disabled)
   Active: failed (Result: timeout) since Thu 2017-02-09 18:26:58 MSK; 4min 13s ago
     Docs: man:mfschunkserver
  Process: 9436 ExecStart=/usr/sbin/mfschunkserver -d start (code=exited, status=2)
 Main PID: 9436 (code=exited, status=2)

Feb 09 18:20:57 host-1 mfschunkserver[9436]: changed working directory to: /var/lib/lizardfs
Feb 09 18:20:57 host-1 mfschunkserver[9436]: [ OK ] changed working directory to: /var/lib/lizardfs
Feb 09 18:20:57 host-1 mfschunkserver[9436]: [FAIL] can't start: lockfile /var/lib/lizardfs/.mfschunkserver.lock is already locked by another process
Feb 09 18:20:57 host-1 systemd[1]: lizardfs-chunkserver.service: main process exited, code=exited, status=2/INVALIDARGUMENT
Feb 09 18:22:27 host-1 systemd[1]: lizardfs-chunkserver.service stop-sigterm timed out. Killing.
Feb 09 18:23:58 host-1 systemd[1]: lizardfs-chunkserver.service still around after SIGKILL. Ignoring.
Feb 09 18:25:28 host-1 systemd[1]: lizardfs-chunkserver.service stop-final-sigterm timed out. Killing.
Feb 09 18:26:58 host-1 systemd[1]: lizardfs-chunkserver.service still around after final SIGKILL. Entering failed mode.
Feb 09 18:26:58 host-1 systemd[1]: Failed to start LizardFS chunkserver daemon.
Feb 09 18:26:58 host-1 systemd[1]: Unit lizardfs-chunkserver.service entered failed state.

```

```
ps aux |grep lizard
root     29780  0.0  0.0  12728  2200 pts/2    S+   20:06   0:00 grep lizard
lizardfs 31765  0.0  0.0      0     0 ?        Z<sl 18:13   0:01 [mfschunkserver] <defunct>
```","Guess there will be an update from devs, but the (teaser) issue is related to using ZFS+enhanceIO recipe from https://github.com/lizardfs/lizardfs/issues/514",9800119
183,Status of LizardFS and OpenNebula integration,open,2017-01-17T05:24:46Z,2018-11-02T20:28:24Z,,NONE,"Hi there,

Was just wondering if there's an up-to-date guide for using LizardFS as a backing store for OpenNebula - @cloudweavers may be able to help with this.

I saw this thread which indicates that it should be possible: https://forum.opennebula.org/t/opennebula-dfs-like-glusterfs-sheepdog-ceph/125/2

However, the TM located at https://community.opennebula.org/ecosystem:moosefs hasn't been updated since late 2011. Does it still work?","> There's a datastore section (search for ""if you want to add Datastore support"") - after adding that, you should have a pull-down voice with ""lizardfs"".

@cloudweavers That's the problem, I don't have 'lizardfs' there despite following the instructions, restarting opennebula and sunstone, and then the force update as oneadmin. I do have a 'Custom' option (see my previous screenshot), where I can manually specify 'lizardfs', but this seems to create /var/lib/one/datastores/{next_number} rather than use /var/lib/one/datastores/lizardfs.",9800119
184,Exclude files regex from trash,open,2017-01-15T11:57:29Z,2017-01-16T12:40:11Z,,NONE,"Whether it is possible to switch off the files, eg. * .lock of trash system?
I wish that these files will automatically be deleted from a directory recursively","It is not possible right now. It is definitely implementable, though. So, two ways of doing it:
The free one: create a patch for lizardfs that does that, the community would be very grateful.
The paid one: support@lizardfs.com.",9800119
185,Client performance on remote machines,open,2017-01-10T21:00:11Z,2017-09-02T11:57:16Z,,NONE,"Since I wasn't really satisfied with the sequential write speed on my current setup (15MB/s just to write to 1 local chunkserver) but have seen multiple people mentioning speeds far beyond 100MB/s I've experimented a bit and observed the following:
- going by the official docs (XFS, big_writes etc.) and using only one machine for master+chunkserver+client gives me about the full write speed of my hard drives (~170MB/s)
- installing the client on a VM (still on the same machine) results in write speeds only up to ~50MB/s
- installing the client on a remote machine with about 30ms latency even results in read speeds as low as 3MB/s (as this was at home with an slow upstream, the writes fully saturated the connection) The connection itself is capable of 6MB/s through http to the same servers

Is the client very sensitive to latency and not supposed to be installed on different machines or am I missing something? I could probably enable jumbo frames for VM clients to improve performance there but obviously cannot do that for IPsec tunnels on remote locations.","One oddity I've noticed with gluster is that its fuse client has the much same performance for VM's as its native qemu driver, which is *very* good. I have no idea how they do it.

Benchmarks can be deceptive though too, they are no substitute for real work usage tests.",9800119
186,Network Configuration,open,2017-01-09T08:10:05Z,2017-01-17T09:03:11Z,,NONE,"I'm trying to figure out the best bonding network configuration for LizardFS performance. I basically have three nodes/chunkservers with 2 1G nics each (actually 3, but one is reserved for admin usage)

My understanding is that when the lizard client writes data to the cluster it:
- connects to the nearest chunkserver (as eastablished by mfstopology, random after that).
- writes data to that chunkserver
- the *chunkserver* writes the data to the other chunkservers as needed to satisfy replication requirements

What I'm not sure is whether the chunkserver parallelise its writes, or is it daisy chained?, e.g for replica 3 is it:

Daisy Chained:
`client -> CS1 -> CS2 -> CS3`


Parallel:
```
client -> CS1 -> CS2
              -> CS3
```

The difference effects what sort of bonding to use. With Daisy Chaining only balance-rr will give us the benefit of 1G * 2

Whereas with Parallel writes other bonds such as balance-alb or LCAP/balance-tcp will be more effective.
","> I assume that you saturate two gigabit ethernet port :)

Yeppers :) was running iftop and observed nice symmetrical writes maxing out both ports to the other two chunkservers.

Upgrading the network to 1GB*4 soon, should be interesting to test that.",9800119
187,Improving performance for small files?,open,2017-01-03T01:24:03Z,2019-06-08T09:42:29Z,,NONE,"Hi,

We're testing a couple of different distributed file systems, and overall I'm *very* impressed with LizardFS, both in terms of flexibility, features and documentation - good work! 

However, the one drawback we've found is that the performance when working with lots of small files is relatively low - when we tested RozoFS that was more than twice as fast for this case.

Is there anything we can do to improve this? We're using 10Gb ethernet with disks spread over six servers, and we've done the usual adjustments of the network stack. For LizardFS we're using the new chunk format, and we have disabled fsync on the chunk servers. We've also tried both duplication and erasure coding, where the latter was a bit faster.

This is absolutely not meant to be any bashing/comparisons of the two file systems, but since both of them seem to use FUSE I don't see any obvious reason for the difference (and for many small files the type of erasure coding shouldn't matter either).

Are there any other settings we're missing that we should look into?

Cheers,

Erik ","What is a RAIDZ10 ? Do you mean you use mirrors or do you use RAIDZ1 ?

On 7 January 2017 at 14:20, Blackpaw <notifications@github.com> wrote:

> Thought I'd leave in a late comment - I waned to see how my test setup
> faired eriklindahl, so I tried your test tar unpack.
>
> With a replica 3 setting I got a consistent 23 seconds, which is
> interesting because my setup is quite underpowered compared to yours. 3
> Nodes connected via 1G * 3, LACP, balance-tcp bonded.
>
> Even more interesting, it made no difference turning of sync in the
> chunkservers.
>
> The 3G vs 10G probably didn't matter - when I looked at iftop, there
> wasn't all that much lan activity going on, < 200Mbps
>
> My disk setup is probably key - like cloudweavers I use a ZFS pool, though
> I don't do JBOD, I have 4 WD Reds (3TB) in RAIDZ10, with 8GB dedicated to
> ARC.
>
> I also use a Kingston HyperX Savage (240GB) dedicated as a log device -
> they have outstanding sequential write speeds and TPW, a lot of SSD's are
> quite disappointing in that regard (Samsung pro is dreadful). 240GB is
> wasted for just a log device, but cache turned out to be useless for VM
> hosting.
>
> However my over IOPS are pretty low - only around 500 for Random write 4K.
> I'd really like to improve that. That does improve by 50% if I disable
> fsync, but I'm not comfortable with that.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/lizardfs/lizardfs/issues/509#issuecomment-271083440>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAAnAtHfZ7buY1w8uOfDfzAu7B7huwJ4ks5rP5EJgaJpZM4LZSHu>
> .
>



-- 
Michal Bielicki <cypromis@gmail.com>
",9800119
188,per-chunk goal setting,open,2016-12-22T18:09:10Z,2017-01-31T05:05:50Z,,NONE,"Dear friends, we are working on adding an external tiering system for Lizard, to allow two speeds of disks (ssd and hdd to begin with, but it can be generalized to more) and automated movements between them. It would greatly help us a command to set a goal for a single chunk; do you think it would be possible? 
We would be happy to contribute back what we are working on.",Worth the wait. Thanks!,9800119
189,goal & topology & georeplication,open,2016-12-15T11:10:17Z,2017-01-27T23:06:52Z,,NONE,"Hi,

I'm trying to setup a main and a disaster recovery site which altogether make a single lizardfs cluster. In lizardfs documents it's recommended to have a high speed (~10Gbps) link between sites which unfortunately we don't have.

So here's my solution and arising problems. Any advice would be highly appreciated.

I have three nodes in location ""ank"" (main site) and one in ""ist"" (drc). The lizardfs filesystem is used in Opennebula which are separatedly installed in both locations. masterfs is one of the nodes in ank. All four nodes are chunkservers. Chunkserver in ist registers to the master in ank via a vpn tunnel. Link is 10Mbps (makes around 1 Mbyte per second). The node in drc is also running as shadow.

Label of ank chunkservers: ank
Label of ist chunkserver: ist

There're three goal definitons:
``` 
10 ank : ank
11 ank_ank_ist : ank ank ist
12 ank_ank : ank ank
```

Goal definitons of the storage mountpoints:
```
/var/lib/one/datastores/103: ank_ank  #don't need a replica in drc
/var/lib/one/datastores/104: ank_ank_ist  #one additional replica in drc
```

Topology definiton:
```
# ip class of drc
# when working in the main site we don't want to read from drc
172.16.1.0/24                   2
```
What I want to do is; when the main site goes down and if I ever need to make the drc up, I'll just switch the shadow to master and as I'll have one replica in drc, I'll be able to use the filesystem there.

Here's the problem: when I try to write to a folder which has ank_ank_ist goal, speed goes down to my uplink speed which is 1 MBs and this is useless. 

Is it somehow possible to implement a caching solution so the system wouldnt wait for the goal to be satisfied for each write operation. Let's say just write to the chunkservers in local, queue the rest and send them later. If not possible (?yet) in lizardfs, an external solution (like ssd caching) would also be very helpful. 

Thanks,
Orhan
","@goblis - performance will always reflect the slowest element in the path. 

I'm assuming you need a DR site, are talking about low tens of gigs of data changing per day, and you are on a shoestring budget. If so, consider providing day-to-day redundancy via goals within primary site, and use the tunnel to trickle your metadata and new chunks to otherwise ""cold"" DR site. 

A. Establish a metalogger at DR site.
B. Snapshot the VMs.
C. Snapshot your LizardFS filesystem.
D. Drop off the initial copy of your chunks (or chunkserver ZFS pools) at the DR site, as in ""Never underestimate the bandwidth of a station wagon full of hard drives"".
E. Start rolling snapshots at primary site. They will provide you with new chunks that correspond to new and modified data. You can then patiently siphon them to the DR off out of band using rsync, zfs send, ...

When aliens destroy primary, you can restart LizardFS from metadata and chunks held in DR site and rollback the VMs to the last snapshot. You may find automatic snapshotting of VMs at boot/clean shutdown useful
",9800119
190,unbalanced data distribution; early chunks deletion,open,2016-12-02T21:23:27Z,2016-12-11T06:07:04Z,,MEMBER,"I had an interesting incident recently: 3TB HDD was physically disconnected (pulled out) by mistake on working chunkserver. Of course LizardFS quickly marked HDD as ""damaged"" and started replication. Somewhat 16...20 hours later I've noticed the problem by looking at LizardFS CGI interface. By that time most of the data (~90%) from unavailable HDD was replicated. I reconnected HDD (re-mounted file system etc.) and reloaded chunkserver at which time I've noticed something strange: chunkserver was scanning available chunks very slowly. As I've realised later chunkserver was deleting chunks from this HDD even during scanning (or very early) because shortly after HDD was fully scanned it ended up only 10% full. It was above 80% full HDD when I plugged it. Massive deletion of chunks manifested as spike on corresponding ""number of chunks deletions per minute"" graph. In the end reconnected HDD was almost empty so LizardFS started replicating data again and now data flows back to reconnected HDD.
What should have happened is extra overgoal chunks should have been deleted not only from reconnected HDD but from all over the cluster to achieve balanced distribution of data and to prevent needless rebalancing...

May be related to #491 but the actual problem here is not as much with data distribution but with (incorrect) massive early deletion of overgoal chunks from re-added storage.",https://github.com/lizardfs/lizardfs/issues/443#issuecomment-240623908 is related I believe,9800119
191,Real cluster,open,2016-11-15T16:48:18Z,2017-04-22T08:12:26Z,,NONE,"I'm evaluating GlusterFS and LizardFS for a new cluster. 70% as VM image hosting (qcow2) and 30% as raw file hosting

Any real production LizardFS cluster with some info about number of files, number of servers and so on?
MooseFS ha something similiar.","As asked some months ago, anyone willing to share details about their infrastructure like number of files, number of servers and so on?

Knowing some real production cluster would be nice in evaluating a SDS.",9800119
192,Prevent chunkservers of connecting to outdated master,open,2016-10-26T18:15:07Z,2016-11-27T19:12:22Z,,NONE,"I'm using 3.9.x, available in Debian 8. As per what I understand the following scenario can easily happen:
- Master fails because of a network reason.
- Through some scripting we automatically failover the master by changing the config of a shadow instance, reload it and give the shadow instance the master ip
- Changes happen to the chunks through the new master, so the metadata version changes
- If now for some reason the old master is available again and chunkservers connect, nothing fails, but instead the changes to metadata, that happened during the time the initial master was gone, are lost!

Can this be prevented? Can I somehow enable the chunkservers to know if they would be connected to an outdated master and therefore rather disconnect again?
","Sorry for such a delayed reply. My corosync pacemaker script leverages the native LizardFS master service role transition calls (personality is ha-managed, not master, nor shadow).  Corosync is made aware of the metadata version and will not allow outdated masters to be promoted nor to seed the cluster.  Since corosync controles the master IP there is no way a chunk server ( nor shadow, nor logger, nor clients ) could communicate with an old master.  I have not explicitly tested split brain due to network segmentation, I assume corosync would handle some of such a situation.  I am still running mostly 2.6 but have has success with 3.9 using my script, I have not tried 3.10 but assume it will still work, mostly...  Ill try to get caught back up so i can provide more timely feedback.",9800119
193,"3.10.2: CGI ""Filesystem check info"" shows no data",open,2016-10-20T02:34:15Z,2016-12-31T01:27:24Z,,MEMBER,"Despite having 1 missing chunk, a week after continuous operation 3.10.2 still show ""no data"" in CGI --> Info --> ""Filesystem check info"".
Possibly this regression is related to new `OPERATIONS_DELAY_DISCONNECT` option.

We have `OPERATIONS_DELAY_DISCONNECT = 172800` (48 hours) to delay rebalancing data when some chunkservers that are temporary offline.
Prior to 3.10.2 ""Filesystem check info"" was populated (at least) 2...3 times per day.
","Not really. This patch is a regression and that's the only reason we
shouldn't apply it. The proper way to fix your issue is to introduce more
effective chunk management with timestamps. Still, a quick fix I sent
should be enough to temporarily fix your problems.

On Friday, December 30, 2016, Dmitry Smirnov <notifications@github.com>
wrote:
> Thank you very much @psarna. I'll try this patch soon. Why do you think
we should not apply it to official repo temporarily? Any risks you can
think of?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or mute the thread.<
https://ci6.googleusercontent.com/proxy/BeTwD3s2JJRivG0SilLJtrh-Us2laqZTLYVKstD4Da_2RKz0mgSZwLOxh5hRUr3R8nZBAo6KY_nXoh3RlKGIhj4ghiUvIhxDCtVAl4uh4dBKWjP0_Gm-5t1ex6LhZgRqx_vsftqY6LdLq4FFOpKTi0GIWKLJag=s0-d-e1-ft#https://github.com/notifications/beacon/AJ8yFwgvFbNjPhMmYJTct_SH-2BBW396ks5rNX7IgaJpZM4KbqLG.gif>
",9800119
194,increase inode reuse delay from 1 day to 30 days,open,2016-10-17T20:37:05Z,2016-10-18T12:34:23Z,,CONTRIBUTOR,"Increase inode reuse delay from 1 day to 30 days. This makes it much harder to run into inode reuse issues, as I did in my cluster.
","This patch was initially applied to 2.5.4 a couple years ago when we switched from MooseFS. I believe the issue was as follows:

A directory is unlinked, but still open. After a day, the master reuses the inode, and the mount which still has the old directory open tries to access this inode which was reused too soon.
",9800119
195,Wishlist 5.x - Support different allocation strategies,open,2016-09-29T14:12:13Z,2016-10-03T04:14:23Z,,NONE,"Right now, the only available allocation strategy between chunk disks is ...um, lets call it ""Balance"". All data is divided equally between all chunkservers and all chunk disks between them. 
But for some cases it would be useful, if data could be stored in ""Fill"", ""Round Robin"" or ""Prioritised"" manner.

For example: a chunkserver CH has 3 chunk disks, C1, C2 and C3. They are chosen to store each new chunk on a basis of free space left (afaik). That is ""Balance""

Now, imagine the same server, but the chunk disk is selected randomly. It could lead to overfilling some of them in the long run, but it would also boost available IOPS at any given moment. And, say, a nightly rebalance job would even their load. That is ""Round Robin"". 

""Fill"" is useful to let disks spin down. Say, chunkserver gets a new chunk. It writes it to the C1 until C1 hits specified used space percent. Say, 95%. C2 and C3 are spun down. Then the C2 drive is used to write new data, unless some chunks became obsolete on C1 and erased. Then C1 is used again until it hits the %used mark again. That way, most disks could be powered down during write-mostly workloads.

Also ""Prioritised"" would be useful. For example, disks could be graded, so chance of chunkserver using them would be proportional not to their available space, but by their manually set priority. That way, disks of different kind and quality could be used across single installation. 
","Hi @gutleib,

I've added your suggestion to the [unofficial list](https://github.com/lizardfs/lizardfs/wiki/Wishlist-for-LizardFS-5.x). Thanks.
",9800119
196,Wishlist 5.x - Kubernetes native volume support,open,2016-09-25T22:37:45Z,2018-02-09T00:29:49Z,,NONE,"Are there any plans to add LizardFS as a native Volume for Kubernetes and join the ranks of CephFS and GlusterFS?
http://kubernetes.io/docs/user-guide/volumes/#types-of-volumes

Right now, I have to push /dev/fuse into my coreos rocket (rkt) containers and use a distro that supports mfsmount (e.g. Ubuntu).  It would be nice to shift and abstract the volume work to Kubernetes proper, instead of handling it within the container/pod itself.  This would allow me to use 'Alpine' for small services since the volume handling is external.

As an aside, since 'Alpine' is becoming more and more prevalent as a (small) base container image, are there any plans to add lizardfs (at minimum the client mounting utils) as an official package?
",@Zorlin does it not currently work on Openshift Origin? I mean the FlexVolume driver based solution.,9800119
197,Missing chunks from nginx access logs,open,2016-09-24T11:09:17Z,2016-09-26T06:48:42Z,,NONE,"Hey there,

A couple of instances of chunk loss occurred on our cluster, about 4 files in 2 days. Eg (taken from ""important messages""):

currently unavailable chunk 0000000000EB76E6 (inode: 409078 ; index: 1)
- currently unavailable file 400078: logs/client/access-prod-web-i-uniqueid.log
  currently unavailable chunk 0000000000E76CF3 (inode: 2144917 ; index: 1)
- currently unavailable file 2110017: logs/client/access-prod-web-i-differentid.log
  unavailable chunks: 2
  unavailable files: 2

It's a 4-node cluster, with masters and chunkservers running 3.9.4. I was just wondering if there was any known issues that could cause these chunks to go missing. I was able to repair all 4 of them but it's a little frustrating as those files become stuck with input/output errors until we notice the missing chunk, wait for the check loop, and repair the file.

Thanks
","Check your disks health, probably there is a problem.
",9800119
198,"[Question] Poor performance, how to make diagnostic?",open,2016-09-22T08:03:50Z,2016-09-23T12:00:55Z,,NONE,"So, currently I have very poor write performance ( about 2Mb/s for 5 chunk servers with regular HDDs), and try to figure out what's wrong.
I have 1Gb network, 5 VM nodes (1 Master + Chunkserver, 1 Master + Chunkserver, 3 Chunk servers) and 2 clients. Chunkservers are Debian servers (3.9,4) started over ESXi. HDDs are regular (1xAHCI/7200 HDD).
Configs are default. 

The only thing which concerns me - I make full clone of one chunk server. (By the way, how they are identified? By IP or some internal ID?).

So.. What should I do for troubleshooting?
","Hi @alxchk,
Chunkservers are identify by ip address and port on which are running.

I have some idea for you:
- check speed of your disks, looks on I/O when lizardfs is working,
- check your connectivity, latency, packet loss,
- also check your syslog :) there should be more information for you to help debug this.
",9800119
199,Wishlist 5.x - Chunk Prefetching / Multi-Fetching,open,2016-09-19T01:13:39Z,2016-10-31T01:10:55Z,,NONE,"Let's say we have 3 chunkservers. One on a GBit connection, one on 500 MBit and one on 100 MBit. There is no replication set up and only one client is constantly consuming data at 150 MBit/s.
Currently, chunks are being fetched sequentially as they are requested. This means that the 100 MBit server will be overloaded once its turn comes up and the client will slow down to 100 Mbit/s until the current chunk is done. After that, the 100 MBit server is going to idle until the next two chunks have been consumed from the other servers, then get overloaded again.
Wouldn't it be great if the LizardFS client was more intelligent and recognized that:
- the user requested the first chunk of a huge file so he is likely to request the following chunks as well
- according to previous transfers, there is one very fast server in the network, another slower one and yet another very slow one and those are the best places (well, the only places) it can get the chunks of that file from
- the 100 MBit chunkserver will most likely be the bottleneck of the current operation and its chunks should be fetched in advance with a lower priority
- prefetching chunks of the 500 MBit server should have even lower priority and those of the GBit one the lowest

Basically, the client should be trying to utilize remaining client bandwidth to speed up upcoming requests and decrease speed spikes as much as possible. An additional feature to achieve this would be to fetch different parts of the same chunk from different chunkservers when it is preferrable to prefetching like on the very first chunk of a file.
","I mostly wish for the multifetching. i.e. with a butch mechanism on the client, one bucket per chunkserver. It can start requesting from 4 servers, and if it sees that one performs at a lower rate, adjust how much it fetches from that one. the rate can be easily tracked continuously, and it would allow to safely fetch parallel etc. (parallel. not just round-robin. really parallel. ;-)
",9800119
200,serious structure inconsistency: (chunkid:0000000000000019),open,2016-09-18T16:51:05Z,2017-04-25T11:07:34Z,,NONE,"I tried to configure a fresh LizardFS install (in Arch Linux, LizardFS version 3.10.2) and I got this error: 

```
Sep 18 12:49:40 desktop mfsmaster[30132]: serious structure inconsistency: (chunkid:0000000000000019)
```

I've notice this before the error below:

```
Sep 18 12:25:23 desktop mfsmaster[14694]: chunk_delete_file_int: Trying to remove non-existent goal: 1
Sep 18 12:25:23 desktop mfsmaster[14694]: structure error - chunk 0000000000000001 not found (inode: 5 ; index: 0)
Sep 18 12:25:23 desktop mfsmaster[14694]: chunk_delete_file_int: Trying to remove non-existent goal: 1
Sep 18 12:25:23 desktop mfsmaster[14694]: structure error - chunk 0000000000000002 not found (inode: 5 ; index: 1)
Sep 18 12:25:37 desktop mfsmaster[14694]: serious structure inconsistency: (chunkid:0000000000000005)
```

And also:

```
Sep 18 14:24:11 desktop mfsmaster[13498]: release: session not found
```

Any idea about what caused that issue?

Thank you.

PS: I reinstalled LizardFS again and i got the same issue :( . (FYI LizardFS was compiled on Arch Linux with this PKGBUILD: https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=lizardfs )
",rebuilt the package by cherry-picking the patch. Now is working.,9800119
201,Wishlist 5.x - Multithreading,open,2016-09-16T05:40:04Z,2018-10-16T17:24:23Z,,NONE,"While adding multithreading is anything but trivial, multithreading on the master would be awesome. Right now I (and other users) use 100% of one core, when I have six or more at my disposal. Multithreading could help prevent the CPU from being a bottleneck.
",":+1: 

Takes the master SO long to restart.  :(",9800119
202,replication error: Chunkserver communication timed out,open,2016-09-15T22:04:59Z,2019-08-10T17:08:30Z,,NONE,"I have installed a System with 10 nodes (V 3.10.2) connected over multiple VPN connections. 
Some of the nodes are connected via a ADSL line (bandwidth 30/8)
It is working fine with one issue: While rebalancing i get the following error:
mfschunkserver[28827]: replication error: Chunkserver communication timed out: 10.26.0.2:9422
this repeats every 5 minutes. 
It seems it tries to download a chunk - the adsl upload bandwith is fully used for a few seconds then it ends with the above error message. On the other node there is no entry in the logfile.
Connecting to the other node work fine - for example downloading a file via scp works fine. So i can't see a network issue at the moment. 
Maybe a timeout because of the limited upload bandwidth? Why is anything els working fine (storing a file, retrieving a file)
Any suggestions how to fix the error ? At the moment rebalancing hangs because of this timeout.

regards Stefan

I dont know what is needed to analyse this problem - config files, network topology - please tell me and i can give the informations
","On 3.13.0rc1, using

`REPLICATION_BANDWIDTH_LIMIT_KBPS = 10240`

will prevent all nodes with this bandwidth limit to rebalance or replicate chunks.
As soon as I remove the limit, replication is started again and works fine.
I upgraded from 3.11 to 3.13.0rc1 in order to get a valid replication bandwidth limit,
it seems that this is not the case or some other constraints apply (maybe settings that cancel each other out)",9800119
203,Master with a bigger RAM than the SHADOW,open,2016-09-15T21:49:14Z,2019-05-31T08:34:46Z,,NONE,"If I understand correctly, the whole metadata of LizardFS Master is stored in RAM. 

Let's say we have four servers with this amount of RAM: 
- lfs001: 11G RAM (shadow)
- lfs002: 15G RAM (shadow)
- lfs003: 15G RAM (shadow)
- lfs004: 23G RAM (master)

2 questions:
1. What is going to happen to lfs001 (11G of RAM only) if the RAM of the master lfs004 (23G of RAM) is filled-up?  Could lfs001 have an issue to replicate the metadata from lfs004 because it has less RAM?
2. Is the MooseFS method to calculate the size of the partition /var/lib/lizardfs/ is still valid with LizardFS: SPACE = RAM \* (BACK_META_KEEP_PREVIOUS + 2) + 1 \* (BACK_LOGS + 1)[GiB]

Thanks.
","> The master's ram usage is almost exactly the size of the metadata set. It
> won't automatically fill as much space as it can. Example, with 1.5TB used
> and 4.4 million files we have only ~2GB used.

The critical point of distinction here is that the 4.4 million files is resulting in ~2GB of metadata, not the 1.5TB worth of data that number of files represents in consumed disk capacity.  A single 1.5TB sparse file will have zero chunks stored on any chunk servers and result meer MB of metadata size, perhaps 200MB.  As data is written into such a single large file, chunks will be created and the metadata size will grow only ever so slightly even when the whole 1.5TB is full of actual data.

So for example, you won't want to store a massive mail server with billions upon billions of tiny files natively in LFS or else you will need to have many 100's of GB of RAM available.  And be willing to wait for slow master start/stop and shadow sync durations.

However, storing billions of files is no problem for all modern traditional filesystems, they all read their metadata from their ""superblock"" only when needed and have no expectation of caching the entire thing in RAM like LFS does.  LFS does do large and sparse file very well and doing so would almost eliminate all of the metadata RAM consumption for a given amount of raw data, replacing it with reads from some set of chunks which store the superblock of your favorite filesystem.  Performance will be better; you can use loop-backed ext in a sparse file backed by LFS, or ZFS and compression using file block device backed by LFS, or even iSCSI initiator LUNs living in files backed by LFS.  You get the idea.. 

The only quark I have run into was that when hosting iSCSI with ZFS on Chunk Servers which were virtualized and backed by a weak RAID-5 hardware set of 6SSDs.  Something kept periodically tripping over some rare edge case causing non-fatal false-faults during chunk reads that replication would fix. Even though upon closer examination the ""faulty"" chunk files all seemed perfectly fine and valid.  Assid from that admittedly wild setup ZFS generally performs well enough on bare metal Chunk Servers and offers solid compression for chunk data where appropriate.",9800119
204,Low CPU consumption,open,2016-09-15T20:50:21Z,2017-08-24T10:22:59Z,,NONE,"My server has 16 CPUs, but it looks like 1 CPU is used in average by LizardFS (masters and chunkservers).

Do I need to modify an option in the master or chunkserver to make LizardFS use the 16 CPUs? (like CHUNKS_LOOP_MAX_CPU for example or others?)

How to make LizardFS masters/shadows/chunkservers more aggressive?
","Not to give anyone any bad ideas...
... But hypervisors could allow for multiple masters to share a single CPU so you are not wasting all those extra unspent cycles...

wink wink nudge nudge  ;)",9800119
205,Wishlist 5.x - REST service,open,2016-09-15T11:02:31Z,2017-09-24T15:59:32Z,,NONE,"The CGI Server provides access to some great data, but modern architectures need to consume JSON via REST nowadays to integrate our infrastructures around the service.

The `?masterhost=` query param should also go away.

Ideally I'd start a REST service like this:

```
$ docker run -d -p 8000:80 --env MASTER=1.2.3.4 -t lizardfs/restserver
```

And then I'd be able to query it like this:

```
$ curl -H ""Accept: application/json"" -u admin:password http://localhost:8000/v1/chunkservers
or
$ curl -H ""Accept: text/plain"" http://localhost:8000/v1/chunkservers
```

Some possible routes supported:
- /v1/chunkservers
- /v1/state-matrix?space=all
- /v1/messages
- /v1/disks
",I use corosync and pacemaker and have shared my metadataserver ofc script and some tweaks which corosync and pacemaker need on debian based dists..  would need to search to find the links.,9800119
206,Wishlist 5.x - Georeplication & Edge Caching,open,2016-09-15T07:23:48Z,2017-07-29T09:16:29Z,,NONE,"A native georeplication system would be awesome - I'm envisioning being able to select a region for a particular folder (or client) and all data for that folder (or client) being stored in the selected region. I know we have custom goals and topology which sort of provides this, but not quite completely.

All clients would be able to see those files, and could access them as though they were in their region, albeit with slower transfer rates and higher latency.

It would be interesting to cache objects in regions where they are being accessed, too - a small ""edge"" cache that provides the most frequently requested items.
","Recent issues have discussed REDUNDANCY_LEVEL, I'm somewhat behind on most issues...  I think this ""new to me"" feature may be similar to the described acknowledgment_goal. I think REDUNDANCY_LEVEL says that a write fails if not successful to the value of that setting, not sure if it is global or can be a part of a goal definition?  I doubt that it results in an ack for the write once reached.  It's likely the remaining writes needed to reach the goal, which may be higher, continue to come from the client rather than allowing replication between the chunk servers to finish the remaining writes needed. An early ack from REDUNDANCY_LEVEL would be neat.

The additional of a REDUNDANCY_COMPLETION = { Client, Chunk } would add more tunability for network traffic control and goal fulfillment latency options.

I think the other desire to have more tier control might be extended beyond current topology, goals, and labels by:
* Allowing for multiple labels per chunkserver and perhaps some basic regular expression matching.
* Adding variable proximity to topology values rather than simple binary matches.
Those are two mechanics are already in place and could maybe be extended without breaking backward compatibility.  I am sure that I have mentioned these ideas in the past in other issues (should look those up...)",9800119
207,Wishlist 5.x - Sharding,open,2016-09-15T07:17:53Z,2017-07-29T10:32:57Z,,NONE,"One of the huge advantages of a system like LizardFS is the idea of a large, contiguous filesystem - users do not have to worry about or care where or how the data is actually stored, just that it's safe and available.

Unfortunately with massive clusters (~170TB+) performance can be impacted by the limitations of the master and shadowmasters.

It would be awesome if there could be some way to split/shard data, without having to resort to running multiple masters with each having its own namespace.
","Or to be able to federate multiple masters into one name space.


Sent from my Verizon, Samsung Galaxy smartphone
-------- Original message --------From: 4Dolio <notifications@github.com> Date: 7/29/17  5:18 AM  (GMT-05:00) To: lizardfs/lizardfs <lizardfs@noreply.github.com> Cc: Subscribed <subscribed@noreply.github.com> Subject: Re: [lizardfs/lizardfs] Wishlist 5.x - Sharding (#468) 
Amen, I have resorted to manually sharding into multiple smaller datasets and miss the days when it was all a single contiguous block, sigh, some native sharding would be welcome, leverage multiple processes, allow shadows to read work, etc.. Someday perhaps...

—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub, or mute the thread.


  
  




{""api_version"":""1.0"",""publisher"":{""api_key"":""05dde50f1d1a384dd78767c55493e4bb"",""name"":""GitHub""},""entity"":{""external_key"":""github/lizardfs/lizardfs"",""title"":""lizardfs/lizardfs"",""subtitle"":""GitHub repository"",""main_image_url"":""https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png"",""avatar_image_url"":""https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png"",""action"":{""name"":""Open in GitHub"",""url"":""https://github.com/lizardfs/lizardfs""}},""updates"":{""snippets"":[{""icon"":""PERSON"",""message"":""@4Dolio in #468: Amen, I have resorted to manually sharding into multiple smaller datasets and miss the days when it was all a single contiguous block, sigh, some native sharding would be welcome, leverage multiple processes, allow shadows to read work, etc.. Someday perhaps...""}],""action"":{""name"":""View Issue"",""url"":""https://github.com/lizardfs/lizardfs/issues/468#issuecomment-318816148""}}}",9800119
208,Wishlist 5.x - Multimaster Replication,open,2016-09-15T07:13:00Z,2016-09-28T12:22:33Z,,NONE,"It would be awesome if we had a multimaster replication scheme in LFS 5.

You could use something like the Galera library, which is currently used by MariaDB Cluster and Percona XtraDB Cluster. It is designed to be fairly agnostic as to the database in use, so it hopefully wouldn't be too difficult to adapt it to LizardFS.
","@drauschenbach absolutely. I believe that integration of existing pieces would allow Lizard to grow and expand, without burdening the developers too much.
",9800119
209,LizardFS 5.x Wishlist Article,open,2016-09-15T07:02:51Z,2016-09-28T12:22:33Z,,NONE,"Given the rumblings of LizardFS 5.X being in development I thought it would be good to start an unofficial ""wishlist"" for LizardFS 5.x. You can find it in the wiki:

https://github.com/lizardfs/lizardfs/wiki/Wishlist-for-LizardFS-5.x

If you have suggestions or interesting features you'd like to see in LizardFS 5.x please follow the instructions in that wiki article and add yours :)
","@blink69 I've created a wiki article:
https://github.com/lizardfs/lizardfs/wiki/Wishlist-for-LizardFS-5.x
",9800119
210,libvirtd support (netfs),open,2016-09-14T18:04:53Z,2016-09-28T12:20:52Z,,MEMBER,"Recently I've found that I can't add LizardFS as a _netfs_ type storage pool in _libvirtd_ (using `virsh` or `virt-manager`) hence it is not possible to use VM migration. (local _dir_ type pool works perfectly but that's not the storage type that allows migration of VMs between hosts).
NFS and GlusterFS are already supported so it should be trivial to introduce LizardFS support.

See also:
- https://libvirt.org/storage.html#StorageBackendNetFS
",,9800119
211,File lock for secondary user,open,2016-09-06T14:43:24Z,2016-09-07T08:26:03Z,,NONE,"Hi ,

I have 2 lizardfs servers and file system is shared to our LAN network . Shared folder is mounted to 3 of our windows systems . File copy , paste , delete , create all are working fine . 

But , is there any option to lock a file if anyone is already using it ? Means he can read that file but no write permission . After that person saves and exit from it the next person can use it , like that .
Is there any option for ""mfsmount"" to set these like permissions..?
","How are your Windows clients accessing the shared folder? Via Samba I assume, if so, you need to look into setting up Samba to use [CTDB](https://wiki.samba.org/index.php/CTDB_Setup), that is a distributed locking database for Samba.
",9800119
212,Performance issue,open,2016-08-19T08:09:28Z,2017-09-11T23:11:28Z,,NONE,"Hi there,

I have a two nodes setup: One master, one shadow. Both are chunkservers and i don't use metaloggers.
Version: 3.10.1
Non-default options in mfschunkserver.cfg are:
NR_OF_HDD_WORKERS_PER_NETWORK_WORKER = 8
CREATE_NEW_CHUNKS_IN_MOOSEFS_FORMAT = 0

Each node have 5-6 rotational disks for chunks (all 10K SAS). And each node has one SSD for OS (centos 7) and also caching for the rotational disks. I'm using EnhanceIO. Caching has made a very slight difference. Anyway whether I use caching or not; r&w performace is not as good as I expect.
Here're the three fio test results (config is from here: https://www.winkey.jp/downloads/index.php/fio-crystaldiskmark). 1st: standard partition on the ssd, 2nd: lizardfs share, 3rd: vm on lizardfs
1st test: root partition on the SSD is used:

<script src=""https://gist.github.com/goblis/0c0f2e2b190f2a7c9e952f29f8304b4f.js""></script>


2nd test: lizardfs share is used:
Seq-Read: (g=0): rw=read, bs=1M-1M/1M-1M/1M-1M, ioengine=libaio, iodepth=1
Seq-Write: (g=1): rw=write, bs=1M-1M/1M-1M/1M-1M, ioengine=libaio, iodepth=1
Rand-Read-512K: (g=2): rw=randread, bs=512K-512K/512K-512K/512K-512K, ioengine=libaio, iodepth=1
Rand-Write-512K: (g=3): rw=randwrite, bs=512K-512K/512K-512K/512K-512K, ioengine=libaio, iodepth=1
Rand-Read-4K: (g=4): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=1
Rand-Write-4K: (g=5): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=1
Rand-Read-4K-QD32: (g=6): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=32
Rand-Write-4K-QD32: (g=7): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=32
fio-2.2.8
Starting 8 processes
Jobs: 1 (f=1): [_(7),w(1)] [56.6% done] [0KB/0KB/0KB /s] [0/0/0 iops] [eta 04m:27s]
Seq-Read: (groupid=0, jobs=1): err= 0: pid=3140: Fri Aug 19 10:44:57 2016
  read : io=1024.0MB, bw=170973KB/s, iops=166, runt=  6133msec
    slat (usec): min=4134, max=10963, avg=5976.89, stdev=607.48
    clat (usec): min=1, max=34, avg= 3.45, stdev= 1.65
     lat (usec): min=4138, max=10997, avg=5981.98, stdev=608.26
    clat percentiles (usec):
     |  1.00th=[    2],  5.00th=[    2], 10.00th=[    2], 20.00th=[    3],
     | 30.00th=[    3], 40.00th=[    3], 50.00th=[    3], 60.00th=[    4],
     | 70.00th=[    4], 80.00th=[    4], 90.00th=[    4], 95.00th=[    5],
     | 99.00th=[    8], 99.50th=[   12], 99.90th=[   23], 99.95th=[   34],
     | 99.99th=[   34]
    bw (KB  /s): min=160190, max=188416, per=100.00%, avg=171388.25, stdev=8088.56
    lat (usec) : 2=0.10%, 4=56.84%, 10=42.19%, 20=0.68%, 50=0.20%
  cpu          : usr=0.20%, sys=4.70%, ctx=8257, majf=0, minf=286
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=1024/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Seq-Write: (groupid=1, jobs=1): err= 0: pid=3142: Fri Aug 19 10:44:57 2016
  write: io=1024.0MB, bw=346293KB/s, iops=338, runt=  3028msec
    slat (usec): min=1507, max=13920, avg=2945.96, stdev=824.91
    clat (usec): min=1, max=18, avg= 2.86, stdev= 1.58
     lat (usec): min=1510, max=13925, avg=2950.15, stdev=825.18
    clat percentiles (usec):
     |  1.00th=[    1],  5.00th=[    2], 10.00th=[    2], 20.00th=[    2],
     | 30.00th=[    2], 40.00th=[    2], 50.00th=[    3], 60.00th=[    3],
     | 70.00th=[    3], 80.00th=[    3], 90.00th=[    4], 95.00th=[    4],
     | 99.00th=[   11], 99.50th=[   15], 99.90th=[   18], 99.95th=[   18],
     | 99.99th=[   18]
    bw (KB  /s): min=331776, max=385024, per=100.00%, avg=347127.50, stdev=20684.34
    lat (usec) : 2=2.64%, 4=82.23%, 10=13.87%, 20=1.27%
  cpu          : usr=2.78%, sys=4.92%, ctx=9271, majf=0, minf=28
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=1024/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Read-512K: (groupid=2, jobs=1): err= 0: pid=3144: Fri Aug 19 10:44:57 2016
  read : io=1024.0MB, bw=103768KB/s, iops=202, runt= 10105msec
    slat (msec): min=2, max=1755, avg= 4.92, stdev=42.56
    clat (usec): min=1, max=282, avg= 3.39, stdev= 6.37
     lat (msec): min=2, max=1755, avg= 4.93, stdev=42.56
    clat percentiles (usec):
     |  1.00th=[    2],  5.00th=[    2], 10.00th=[    2], 20.00th=[    3],
     | 30.00th=[    3], 40.00th=[    3], 50.00th=[    3], 60.00th=[    3],
     | 70.00th=[    3], 80.00th=[    4], 90.00th=[    4], 95.00th=[    4],
     | 99.00th=[    8], 99.50th=[   15], 99.90th=[   28], 99.95th=[   42],
     | 99.99th=[  282]
    bw (KB  /s): min= 2577, max=147456, per=100.00%, avg=122463.25, stdev=41607.23
    lat (usec) : 2=0.83%, 4=70.26%, 10=28.27%, 20=0.49%, 50=0.10%
    lat (usec) : 500=0.05%
  cpu          : usr=0.28%, sys=2.32%, ctx=8258, majf=0, minf=157
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=2048/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Write-512K: (groupid=3, jobs=1): err= 0: pid=3491: Fri Aug 19 10:44:57 2016
  write: io=984576KB, bw=16407KB/s, iops=32, runt= 60011msec
    slat (usec): min=690, max=4135.2K, avg=31192.14, stdev=214071.18
    clat (usec): min=1, max=269, avg= 3.88, stdev= 6.34
     lat (usec): min=692, max=4135.3K, avg=31198.08, stdev=214073.00
    clat percentiles (usec):
     |  1.00th=[    2],  5.00th=[    2], 10.00th=[    2], 20.00th=[    3],
     | 30.00th=[    3], 40.00th=[    3], 50.00th=[    3], 60.00th=[    4],
     | 70.00th=[    4], 80.00th=[    4], 90.00th=[    5], 95.00th=[    7],
     | 99.00th=[    9], 99.50th=[   16], 99.90th=[   40], 99.95th=[  270],
     | 99.99th=[  270]
    bw (KB  /s): min=  240, max=98488, per=100.00%, avg=27483.83, stdev=28097.23
    lat (usec) : 2=0.47%, 4=51.48%, 10=47.11%, 20=0.68%, 50=0.21%
    lat (usec) : 500=0.05%
  cpu          : usr=0.18%, sys=0.35%, ctx=9669, majf=0, minf=28
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=1923/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Read-4K: (groupid=4, jobs=1): err= 0: pid=5162: Fri Aug 19 10:44:57 2016
  read : io=301308KB, bw=5021.8KB/s, iops=1255, runt= 60001msec
    slat (usec): min=274, max=1658.4K, avg=787.60, stdev=6700.45
    clat (usec): min=0, max=327, avg= 2.58, stdev= 3.45
     lat (usec): min=278, max=1658.4K, avg=791.35, stdev=6700.50
    clat percentiles (usec):
     |  1.00th=[    1],  5.00th=[    1], 10.00th=[    1], 20.00th=[    2],
     | 30.00th=[    2], 40.00th=[    2], 50.00th=[    2], 60.00th=[    3],
     | 70.00th=[    3], 80.00th=[    3], 90.00th=[    4], 95.00th=[    4],
     | 99.00th=[    5], 99.50th=[   12], 99.90th=[   26], 99.95th=[   32],
     | 99.99th=[  247]
    bw (KB  /s): min=   92, max= 6528, per=100.00%, avg=5156.28, stdev=839.97
    lat (usec) : 2=13.47%, 4=74.23%, 10=11.76%, 20=0.39%, 50=0.13%
    lat (usec) : 100=0.01%, 250=0.01%, 500=0.01%
  cpu          : usr=1.37%, sys=2.61%, ctx=75945, majf=0, minf=202
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=75327/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Write-4K: (groupid=5, jobs=1): err= 0: pid=6643: Fri Aug 19 10:44:57 2016
  write: io=21288KB, bw=363000B/s, iops=88, runt= 60052msec
    slat (usec): min=72, max=4539.8K, avg=11273.18, stdev=115776.73
    clat (usec): min=0, max=34, avg= 2.84, stdev= 2.04
     lat (usec): min=73, max=4539.8K, avg=11277.38, stdev=115777.50
    clat percentiles (usec):
     |  1.00th=[    1],  5.00th=[    1], 10.00th=[    1], 20.00th=[    2],
     | 30.00th=[    2], 40.00th=[    2], 50.00th=[    3], 60.00th=[    3],
     | 70.00th=[    3], 80.00th=[    4], 90.00th=[    4], 95.00th=[    5],
     | 99.00th=[   10], 99.50th=[   17], 99.90th=[   26], 99.95th=[   29],
     | 99.99th=[   34]
    bw (KB  /s): min=    3, max= 4585, per=100.00%, avg=559.76, stdev=728.19
    lat (usec) : 2=17.04%, 4=58.68%, 10=23.21%, 20=0.81%, 50=0.26%
  cpu          : usr=0.16%, sys=0.29%, ctx=10686, majf=0, minf=28
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=5322/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Read-4K-QD32: (groupid=6, jobs=1): err= 0: pid=8744: Fri Aug 19 10:44:57 2016
  read : io=302536KB, bw=5042.2KB/s, iops=1260, runt= 60001msec
    slat (usec): min=284, max=228246, avg=783.87, stdev=1006.55
    clat (usec): min=6, max=254810, avg=24587.18, stdev=5927.85
     lat (usec): min=767, max=255737, avg=25372.38, stdev=6030.85
    clat percentiles (msec):
     |  1.00th=[   19],  5.00th=[   21], 10.00th=[   22], 20.00th=[   23],
     | 30.00th=[   24], 40.00th=[   24], 50.00th=[   25], 60.00th=[   25],
     | 70.00th=[   26], 80.00th=[   26], 90.00th=[   28], 95.00th=[   28],
     | 99.00th=[   30], 99.50th=[   30], 99.90th=[   32], 99.95th=[  176],
     | 99.99th=[  255]
    bw (KB  /s): min= 2800, max= 6096, per=100.00%, avg=5046.25, stdev=378.62
    lat (usec) : 10=0.01%, 1000=0.01%
    lat (msec) : 2=0.01%, 4=0.01%, 10=0.01%, 20=3.24%, 50=96.66%
    lat (msec) : 250=0.04%, 500=0.04%
  cpu          : usr=1.49%, sys=2.83%, ctx=76319, majf=0, minf=117
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued    : total=r=75634/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=32
Rand-Write-4K-QD32: (groupid=7, jobs=1): err= 0: pid=10185: Fri Aug 19 10:44:57 2016
  write: io=23996KB, bw=408558B/s, iops=99, runt= 60143msec
    slat (usec): min=73, max=3536.1K, avg=10014.68, stdev=79828.49
    clat (usec): min=15, max=4495.6K, avg=310766.22, stdev=476244.71
     lat (msec): min=2, max=4495, avg=320.78, stdev=485.45
    clat percentiles (msec):
     |  1.00th=[    4],  5.00th=[    6], 10.00th=[   24], 20.00th=[   76],
     | 30.00th=[  121], 40.00th=[  155], 50.00th=[  186], 60.00th=[  243],
     | 70.00th=[  318], 80.00th=[  420], 90.00th=[  603], 95.00th=[  832],
     | 99.00th=[ 2671], 99.50th=[ 4080], 99.90th=[ 4178], 99.95th=[ 4490],
     | 99.99th=[ 4490]
    bw (KB  /s): min=   16, max= 4674, per=100.00%, avg=488.93, stdev=560.38
    lat (usec) : 20=0.02%
    lat (msec) : 4=2.37%, 10=6.27%, 20=0.02%, 50=6.78%, 100=10.40%
    lat (msec) : 250=35.61%, 500=24.19%, 750=8.20%, 1000=2.83%, 2000=1.32%
    lat (msec) : >=2000=2.00%
  cpu          : usr=0.17%, sys=0.37%, ctx=12041, majf=0, minf=30
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.3%, 32=99.5%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=5999/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: io=1024.0MB, aggrb=170972KB/s, minb=170972KB/s, maxb=170972KB/s, mint=6133msec, maxt=6133msec

Run status group 1 (all jobs):
  WRITE: io=1024.0MB, aggrb=346293KB/s, minb=346293KB/s, maxb=346293KB/s, mint=3028msec, maxt=3028msec

Run status group 2 (all jobs):
   READ: io=1024.0MB, aggrb=103768KB/s, minb=103768KB/s, maxb=103768KB/s, mint=10105msec, maxt=10105msec

Run status group 3 (all jobs):
  WRITE: io=984576KB, aggrb=16406KB/s, minb=16406KB/s, maxb=16406KB/s, mint=60011msec, maxt=60011msec

Run status group 4 (all jobs):
   READ: io=301308KB, aggrb=5021KB/s, minb=5021KB/s, maxb=5021KB/s, mint=60001msec, maxt=60001msec

Run status group 5 (all jobs):
  WRITE: io=21288KB, aggrb=354KB/s, minb=354KB/s, maxb=354KB/s, mint=60052msec, maxt=60052msec

Run status group 6 (all jobs):
   READ: io=302536KB, aggrb=5042KB/s, minb=5042KB/s, maxb=5042KB/s, mint=60001msec, maxt=60001msec

Run status group 7 (all jobs):
  WRITE: io=23996KB, aggrb=398KB/s, minb=398KB/s, maxb=398KB/s, mint=60143msec, maxt=60143msec

3rd. And the last one is from inside a VM running on kvm on lizardfs share:
Seq-Read: (g=0): rw=read, bs=1M-1M/1M-1M/1M-1M, ioengine=libaio, iodepth=1
Seq-Write: (g=1): rw=write, bs=1M-1M/1M-1M/1M-1M, ioengine=libaio, iodepth=1
Rand-Read-512K: (g=2): rw=randread, bs=512K-512K/512K-512K/512K-512K, ioengine=l                                                                                        ibaio, iodepth=1
Rand-Write-512K: (g=3): rw=randwrite, bs=512K-512K/512K-512K/512K-512K, ioengine                                                                                        =libaio, iodepth=1
Rand-Read-4K: (g=4): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth                                                                                        =1
Rand-Write-4K: (g=5): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodep                                                                                        th=1
Rand-Read-4K-QD32: (g=6): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, io                                                                                        depth=32
Rand-Write-4K-QD32: (g=7): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio,                                                                                         iodepth=32
fio-2.2.8
Starting 8 processes
Seq-Read: Laying out IO file(s) (1 file(s) / 1024MB)
Jobs: 1 (f=1): [_(7),w(1)] [55.0% done] [0KB/0KB/0KB /s] [0/0/0 iops] [eta 05m:30s]
Seq-Read: (groupid=0, jobs=1): err= 0: pid=21135: Fri Aug 19 11:05:16 2016
  read : io=1024.0MB, bw=77973KB/s, iops=76, runt= 13448msec
    slat (usec): min=83, max=8037, avg=590.81, stdev=520.97
    clat (msec): min=4, max=1104, avg=12.53, stdev=55.01
     lat (msec): min=4, max=1104, avg=13.12, stdev=55.02
    clat percentiles (msec):
     |  1.00th=[    6],  5.00th=[    7], 10.00th=[    7], 20.00th=[    8],
     | 30.00th=[    8], 40.00th=[    9], 50.00th=[    9], 60.00th=[    9],
     | 70.00th=[    9], 80.00th=[   10], 90.00th=[   11], 95.00th=[   11],
     | 99.00th=[   17], 99.50th=[  306], 99.90th=[  955], 99.95th=[ 1106],
     | 99.99th=[ 1106]
    bw (KB  /s): min= 1302, max=133120, per=100.00%, avg=86641.22, stdev=41646.81
    lat (msec) : 10=88.77%, 20=10.25%, 50=0.20%, 100=0.10%, 500=0.39%
    lat (msec) : 1000=0.20%, 2000=0.10%
  cpu          : usr=0.18%, sys=4.49%, ctx=1026, majf=0, minf=289
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=1024/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Seq-Write: (groupid=1, jobs=1): err= 0: pid=21159: Fri Aug 19 11:05:16 2016
  write: io=1024.0MB, bw=20323KB/s, iops=19, runt= 51596msec
    slat (usec): min=132, max=6973, avg=498.20, stdev=357.64
    clat (usec): min=785, max=10165K, avg=49875.05, stdev=475252.65
     lat (msec): min=2, max=10165, avg=50.38, stdev=475.25
    clat percentiles (usec):
     |  1.00th=[ 1880],  5.00th=[ 2008], 10.00th=[ 2128], 20.00th=[ 2320],
     | 30.00th=[ 2480], 40.00th=[ 2672], 50.00th=[ 2864], 60.00th=[ 2992],
     | 70.00th=[ 3216], 80.00th=[ 3440], 90.00th=[ 3792], 95.00th=[ 4128],
     | 99.00th=[1794048], 99.50th=[3194880], 99.90th=[7176192], 99.95th=[10158080],
     | 99.99th=[10158080]
    bw (KB  /s): min=  249, max=290816, per=100.00%, avg=46229.81, stdev=79046.41
    lat (usec) : 1000=0.10%
    lat (msec) : 2=4.49%, 4=89.65%, 10=2.93%, 20=0.29%, 100=0.20%
    lat (msec) : 250=0.39%, 500=0.49%, 750=0.20%, 2000=0.29%, >=2000=0.98%
  cpu          : usr=0.15%, sys=0.88%, ctx=1033, majf=0, minf=31
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=1024/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Read-512K: (groupid=2, jobs=1): err= 0: pid=21203: Fri Aug 19 11:05:16 2016
  read : io=1024.0MB, bw=36790KB/s, iops=71, runt= 28502msec
    slat (usec): min=51, max=9093, avg=415.93, stdev=373.64
    clat (msec): min=2, max=14204, avg=13.49, stdev=318.15
     lat (msec): min=2, max=14207, avg=13.91, stdev=318.20
    clat percentiles (msec):
     |  1.00th=[    3],  5.00th=[    3], 10.00th=[    3], 20.00th=[    3],
     | 30.00th=[    4], 40.00th=[    5], 50.00th=[    5], 60.00th=[    5],
     | 70.00th=[    5], 80.00th=[    5], 90.00th=[    6], 95.00th=[    6],
     | 99.00th=[    7], 99.50th=[   10], 99.90th=[ 1418], 99.95th=[ 1614],
     | 99.99th=[14222]
    bw (KB  /s): min=   36, max=176446, per=100.00%, avg=85135.00, stdev=51471.13
    lat (msec) : 4=37.60%, 10=61.96%, 20=0.05%, 100=0.05%, 500=0.15%
    lat (msec) : 1000=0.05%, 2000=0.10%, >=2000=0.05%
  cpu          : usr=0.10%, sys=3.04%, ctx=2050, majf=0, minf=160
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=2048/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Write-512K: (groupid=3, jobs=1): err= 0: pid=21217: Fri Aug 19 11:05:16 2016
  write: io=436224KB, bw=6955.8KB/s, iops=13, runt= 62714msec
    slat (usec): min=49, max=13016, avg=788.27, stdev=643.69
    clat (usec): min=678, max=8272.7K, avg=72802.11, stdev=498053.96
     lat (msec): min=1, max=8273, avg=73.59, stdev=498.03
    clat percentiles (usec):
     |  1.00th=[ 1112],  5.00th=[ 1320], 10.00th=[ 1496], 20.00th=[ 1816],
     | 30.00th=[ 2512], 40.00th=[ 2928], 50.00th=[ 3088], 60.00th=[ 3248],
     | 70.00th=[ 3408], 80.00th=[ 3696], 90.00th=[ 9152], 95.00th=[177152],
     | 99.00th=[2056192], 99.50th=[3588096], 99.90th=[8290304], 99.95th=[8290304],
     | 99.99th=[8290304]
    bw (KB  /s): min=   61, max=75324, per=100.00%, avg=16538.95, stdev=20071.55
    lat (usec) : 750=0.12%, 1000=0.23%
    lat (msec) : 2=23.36%, 4=61.38%, 10=4.93%, 20=1.06%, 50=0.59%
    lat (msec) : 100=1.53%, 250=3.17%, 500=1.29%, 750=0.35%, 1000=0.35%
    lat (msec) : 2000=0.59%, >=2000=1.06%
  cpu          : usr=0.09%, sys=1.02%, ctx=859, majf=0, minf=31
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=852/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Read-4K: (groupid=4, jobs=1): err= 0: pid=21289: Fri Aug 19 11:05:16 2016
  read : io=146932KB, bw=2422.9KB/s, iops=605, runt= 60645msec
    slat (usec): min=17, max=9567, avg=248.41, stdev=225.60
    clat (usec): min=4, max=2720.4K, avg=1392.65, stdev=26045.82
     lat (usec): min=518, max=2720.7K, avg=1642.95, stdev=26045.10
    clat percentiles (usec):
     |  1.00th=[  474],  5.00th=[  532], 10.00th=[  620], 20.00th=[  772],
     | 30.00th=[  852], 40.00th=[  900], 50.00th=[  948], 60.00th=[  988],
     | 70.00th=[ 1032], 80.00th=[ 1096], 90.00th=[ 1176], 95.00th=[ 1256],
     | 99.00th=[ 1528], 99.50th=[ 1976], 99.90th=[ 9664], 99.95th=[162816],
     | 99.99th=[1744896]
    bw (KB  /s): min=   16, max= 5216, per=100.00%, avg=2886.88, stdev=1123.86
    lat (usec) : 10=0.01%, 20=0.01%, 100=0.01%, 250=0.01%, 500=2.45%
    lat (usec) : 750=15.72%, 1000=44.91%
    lat (msec) : 2=36.41%, 4=0.31%, 10=0.08%, 20=0.01%, 50=0.01%
    lat (msec) : 100=0.01%, 250=0.04%, 500=0.02%, 750=0.01%, 1000=0.01%
    lat (msec) : 2000=0.01%, >=2000=0.01%
  cpu          : usr=1.00%, sys=15.29%, ctx=36745, majf=0, minf=34
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=36733/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Write-4K: (groupid=5, jobs=1): err= 0: pid=21348: Fri Aug 19 11:05:16 2016
  write: io=3544.0KB, bw=60477B/s, iops=14, runt= 60007msec
    slat (usec): min=17, max=6057, avg=318.78, stdev=311.95
    clat (usec): min=144, max=36318K, avg=67392.99, stdev=1236847.50
     lat (usec): min=445, max=36318K, avg=67714.70, stdev=1236839.74
    clat percentiles (usec):
     |  1.00th=[  302],  5.00th=[  350], 10.00th=[  366], 20.00th=[  394],
     | 30.00th=[  414], 40.00th=[  438], 50.00th=[  462], 60.00th=[  490],
     | 70.00th=[  532], 80.00th=[  588], 90.00th=[  804], 95.00th=[ 8096],
     | 99.00th=[1089536], 99.50th=[2146304], 99.90th=[16711680], 99.95th=[16711680],
     | 99.99th=[16711680]
    bw (KB  /s): min=    0, max= 3850, per=100.00%, avg=311.11, stdev=903.00
    lat (usec) : 250=0.34%, 500=62.87%, 750=25.51%, 1000=3.61%
    lat (msec) : 2=1.47%, 4=0.45%, 10=0.79%, 20=0.34%, 50=0.45%
    lat (msec) : 100=0.34%, 250=1.69%, 500=1.02%, 750=0.11%, 2000=0.34%
    lat (msec) : >=2000=0.68%
  cpu          : usr=0.02%, sys=0.49%, ctx=892, majf=0, minf=30
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=886/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=1
Rand-Read-4K-QD32: (groupid=6, jobs=1): err= 0: pid=21378: Fri Aug 19 11:05:16 2016
  read : io=216328KB, bw=3603.4KB/s, iops=900, runt= 60036msec
    slat (usec): min=8, max=593, avg=28.45, stdev=14.34
    clat (msec): min=1, max=2188, avg=35.48, stdev=56.36
     lat (msec): min=1, max=2188, avg=35.51, stdev=56.36
    clat percentiles (msec):
     |  1.00th=[    4],  5.00th=[    6], 10.00th=[    9], 20.00th=[   15],
     | 30.00th=[   21], 40.00th=[   28], 50.00th=[   34], 60.00th=[   40],
     | 70.00th=[   47], 80.00th=[   53], 90.00th=[   60], 95.00th=[   65],
     | 99.00th=[   74], 99.50th=[   78], 99.90th=[  334], 99.95th=[ 2147],
     | 99.99th=[ 2180]
    bw (KB  /s): min=  677, max= 5568, per=100.00%, avg=3714.18, stdev=477.84
    lat (msec) : 2=0.06%, 4=2.99%, 10=9.81%, 20=15.95%, 50=47.17%
    lat (msec) : 100=23.75%, 250=0.14%, 500=0.04%, 750=0.03%, >=2000=0.06%
  cpu          : usr=7.68%, sys=18.81%, ctx=54054, majf=0, minf=63
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=99.9%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued    : total=r=54082/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=32
Rand-Write-4K-QD32: (groupid=7, jobs=1): err= 0: pid=21402: Fri Aug 19 11:05:16 2016
  write: io=22096KB, bw=353100B/s, iops=86, runt= 64079msec
    slat (usec): min=8, max=9618, avg=40.13, stdev=133.18
    clat (msec): min=1, max=17267, avg=371.14, stdev=1436.87
     lat (msec): min=1, max=17267, avg=371.18, stdev=1436.87
    clat percentiles (usec):
     |  1.00th=[ 1816],  5.00th=[ 3280], 10.00th=[ 5280], 20.00th=[10048],
     | 30.00th=[15040], 40.00th=[20608], 50.00th=[26752], 60.00th=[34048],
     | 70.00th=[67072], 80.00th=[164864], 90.00th=[659456], 95.00th=[2277376],
     | 99.00th=[5079040], 99.50th=[16056320], 99.90th=[16711680], 99.95th=[16711680],
     | 99.99th=[16711680]
    bw (KB  /s): min=    0, max= 5016, per=100.00%, avg=737.05, stdev=1112.98
    lat (msec) : 2=1.65%, 4=5.27%, 10=12.83%, 20=19.23%, 50=29.47%
    lat (msec) : 100=4.16%, 250=10.95%, 500=4.82%, 750=2.05%, 1000=0.89%
    lat (msec) : 2000=2.59%, >=2000=6.10%
  cpu          : usr=0.94%, sys=1.89%, ctx=5488, majf=0, minf=31
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.3%, 32=99.4%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=5524/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
   READ: io=1024.0MB, aggrb=77972KB/s, minb=77972KB/s, maxb=77972KB/s, mint=13448msec, maxt=13448msec

Run status group 1 (all jobs):
  WRITE: io=1024.0MB, aggrb=20322KB/s, minb=20322KB/s, maxb=20322KB/s, mint=51596msec, maxt=51596msec

Run status group 2 (all jobs):
   READ: io=1024.0MB, aggrb=36789KB/s, minb=36789KB/s, maxb=36789KB/s, mint=28502msec, maxt=28502msec

Run status group 3 (all jobs):
  WRITE: io=436224KB, aggrb=6955KB/s, minb=6955KB/s, maxb=6955KB/s, mint=62714msec, maxt=62714msec

Run status group 4 (all jobs):
   READ: io=146932KB, aggrb=2422KB/s, minb=2422KB/s, maxb=2422KB/s, mint=60645msec, maxt=60645msec

Run status group 5 (all jobs):
  WRITE: io=3544KB, aggrb=59KB/s, minb=59KB/s, maxb=59KB/s, mint=60007msec, maxt=60007msec

Run status group 6 (all jobs):
   READ: io=216328KB, aggrb=3603KB/s, minb=3603KB/s, maxb=3603KB/s, mint=60036msec, maxt=60036msec

Run status group 7 (all jobs):
  WRITE: io=22096KB, aggrb=344KB/s, minb=344KB/s, maxb=344KB/s, mint=64079msec, maxt=64079msec

Disk stats (read/write):
    dm-0: ios=94911/9817, merge=0/0, ticks=2013311/4015573, in_queue=6817524, util=98.83%, aggrios=94894/9824, aggrmerge=17/66, aggrticks=2012624/4467362, aggrin_queue=6479790, aggrutil=98.78%
  sda: ios=94894/9824, merge=17/66, ticks=2012624/4467362, in_queue=6479790, util=98.78%

Any ideas and advices to improve the performance would be greatly appreciated.

Thanks,
Orhan
",any news on this LizardFS 5.0 public beta ?,9800119
213,Suggestion: Make a FAQ wiki page,open,2016-08-18T20:37:53Z,2016-08-23T15:44:29Z,,NONE,"From my couple of weeks trying LizardFS, I experienced many small problems. Many of them can be solved by:
1. Reading MooseFS manual. But this maybe not enough in future.
2. Submitting questions to this issue tracker
3. Contact LizardFS team supports.

Though that I can tell that many others may are in the same processes. So a well-edited Wiki Page of FAQ will be a good go-to place before posting repeated questions. I also found that several of my issues have been discussed here (either closed or open). This again prompts for a wiki FAQ page. I will try to list out some questions in this ""issue"". And I think it is OK to quote from MooseFS documentation if needed, isn't it?
1. How to prepare the hard drives for LFS?
2. How to migrate data to LFS?
3. How to keep data/metadat safe in LFS?
   If this is hard to answer because of the unknown, the opposite is easier: What are the stupid things (may be not so obvious) I may do to destroy my data?
4. What's recommended filesystem for LFS disks?
5. How to migrate data from one chunkserver to another?
6. How to recover from mfsmaster shutdown? 
7. How to tune LFS cluster for performance?
8. What is recommended infrastructure setup for high-performance LFS cluster to deal with big data?
9. Some common problems and solutions:
   1. Problems with 'bind' mount. Solution: use mount sub-folder command.
   2. How do I use mount in /etc/fstab? Solution: check MooseFS documentation.
10. How to use goals to control number of replicates and on which chunkserver the data is saved?

etc.
","@Asher256 From which version you want to migrate ? 

Here https://github.com/lizardfs/lizardfs/blob/master/UPGRADE you will find all official LizardFS upgrade path, but migration from MFS 2.x is also possible I do that recently :) 

@biocyberman you are welcome to create this FAQ, I will help to improve that page also :)
",9800119
214,"Multiple labels on one chunkserver, or something else?",open,2016-08-14T22:06:35Z,2016-08-16T11:23:15Z,,NONE,"I am new to Lizard and trying to tune the setup. One question came up:

If I have /mnt/fastdisk and /mnt/slowdisk on one chunk server, how do I label them in mfschunkserver.cfg and configure goals in mfsgoals.cfg like this:

```
21 slowdisk : slowlabel # label from mfschunkserver.cfg
22 fastdisk : fastlabel # label from mfschunkserver.cfg
```
","It takes me some minutes to differentiate the enhancement you proposed in #382 and this one.
That enhancement deals with the need of addressing multiple chunkservers by a common name, or in other words, by chunkserver tags/categories/classes, or the like. 

Whereas, this one is trying to address the need of setting goals to target subsets of disks in chunkservers. With it, we will be able to setup something like: newly generated data is to write to fast local disks for analysis. Older data is to write to slower local disks by changing goals. This will avoid network traffic when reading/viewing this old data from that chunkserver.
",9800119
215,Lizadfs Master Server Error,open,2016-08-12T09:02:44Z,2016-08-12T09:25:06Z,,NONE,"Hi guys,
I have an issue on my lizardfs master. It still store metadata (filename, date, size, ...) although I deleted them on my storage. What should I do to solve this problem?
","Have you tried mounting and emptying the trash?

On Aug 12, 2016 17:02, ""ninorin"" notifications@github.com wrote:

> Hi guys,
> I have an issue on my lizardfs master. It still store metadata (filename,
> date, size, ...) although I deleted them on my storage. What should I do to
> solve this problem?
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/lizardfs/lizardfs/issues/445, or mute the thread
> https://github.com/notifications/unsubscribe-auth/ABTmrO8j1V1exWe9efacdSp__Tv6K0mOks5qfDa1gaJpZM4Ji5ie
> .
",9800119
216,Question: How to recover files with missing chunks?,open,2016-08-11T18:03:26Z,2018-12-17T22:32:41Z,,NONE,"I guess this can be one of FAQs. I could write to LFS support but decide to write here for better reach to other users. 

While testing LFS I abruptly stopped chunk servers. After a while I got report there are 64 missing chunks. I did rsync from the non-LFS backup storage, but the missing chunks do not disappear? 

Chunkserver 3.10 on Ubuntu 14.04. 
","This one #227 should be fix this problem 
",9800119
217,improve release management,open,2016-08-07T01:49:33Z,2019-08-30T12:22:41Z,,MEMBER,"It would be great if team could work on one release at a time where ""master"" branch is a staging branch for next (soon-to-become) release. This would be helpful to avoid [release mess](https://github.com/lizardfs/lizardfs/issues/431#issuecomment-238007129) when bugs are closed in ""master"" but corresponding patches are not included into release.
Also it would be helpful for testing as I know how stable current master is (due to testing and troubleshooting) but I can't do all the testing again on new release that is apparently behind ""master"".

Moreover after upgrade to latest release my shadow master fails to connect to ""pre-release"" master with error `Cannot register to master: Incorrect register BLOB` because ""master"" apparently represents ""unreleased+1"" version?
Frankly I'm terrified to upgrade pre-release ""master"" to current release as I have neither moral powers nor time to troubleshoot #422 again (this problem already took more than a week of my time). 

Also I'd like releases to be handled with milestones to make it clear what remains to be done in order to prepare new release.
Staging next release in ""master"" should help to release more often. Thanks.
","If ""master"" is a staging place for next release then one can branch off release tag and maintain stable release in its own branch once backporting of a fix is required. In this case numbered release branches would be made only to support older releases (but not to stage new ones). This might be helpful to ensure inclusion of ""forgotten commits"" from master.
",9800119
218,Quick issue on replication,open,2016-07-29T20:51:14Z,2016-08-05T16:21:02Z,,NONE,"Hi folks,

I'm running a quick test on LizardFS and I believe I'm not doing something right.

I created a Ubuntu 14.04 server with 2 SSD disks, which I allocated to Lizard on the mfshdd file.

I have Master and Chunkserver in the same box (again, its just for testing)

I have set the goals for 3 copies in my goals file, which looks like this:

 #1 1 : _
 #2 2 : _ _
 3 RepG : _ _ _
 #4 4 : _ _ _ _
 #5 5 : _ _ _ _ _

I created a couple files in the Lizard mount, changed the mfsmaster.cfg file to start replication after 30 seconds, restarted everything but every time I go to the interface, I still see that my files are ""endangered"" and it shows ""0 copies to replicate""

What am I missing to have the files replicated across the 2 disks, and later, across different chunkservers?

Thanks
","You still need to use:
mfssetgoal junkfiles -r /lfs/allmyjunk
mfssetgoal allssd -r /lfs/fastestfiles
",9800119
219,Single client perfomance,open,2016-07-29T11:02:10Z,2019-09-03T02:32:51Z,,NONE,"Hello!
I'm playing with test setup: 2 servers 4x400 SSDs, 128Gb RAM, 10Gbit ethernet on each.
1 SSD is under system, master and metalogger data, 3 SSD are 3 separate targets for chunkserver.
Both servers have almost the same setup except ""PERSONALITY = shadow"" on second master.

The goal is _ (same results with _  _). No replication goes in parallel.

I see that single thread is limited in performance ~100mb/s write and ~160mb/s read.
No iolimits set (I even tried to set them to much higher limits).
It doesn't meter how many chunk servers (1 or 2 ) or disks in chunk servers are used.
Playing with NR_OF_NETWORK_WORKERS and NR_OF_HDD_WORKERS_PER_NETWORK_WORKER doesn't give me significant effect.

Multithreading reads/writes gives higher performance.
Is single thread io limited by design or I'm missing some tuning?

servers: CentOS release 6.5 (Final), release 3.10.1-0el6.x86_64
clients: CentOS release 6.4 (Final), release 3.10.1-0el6.x86_64

I've done bechmarking with iozone on separate client machine (128Gb RAM, 10Gbit ethernet):
## single thread test:

/opt/iozone/bin/iozone -s 256G -i0 -i1 -i2 -r 64

```
      KB  reclen   write rewrite    read    reread    read(random)  write(random)    
   268435456      64  104299  105772   179766   162144   63948   51166 
```

same test once more time:
       268435456      64  105776  102543   154132   130671   76768   65345                           
## 20 threads test:

/opt/iozone/bin/iozone -s 20G -i0 -i1 -i2 -r 64 -t20

```
    Children see throughput for 20 initial writers  =  258736.32 KB/sec
    Parent sees throughput for 20 initial writers   =  258560.04 KB/sec
    Min throughput per process                      =   12930.20 KB/sec 
    Max throughput per process                      =   12944.59 KB/sec
    Avg throughput per process                      =   12936.82 KB/sec
    Min xfer                                        = 20948288.00 KB

    Children see throughput for 20 rewriters        =  261864.86 KB/sec
    Parent sees throughput for 20 rewriters         =  261864.24 KB/sec
    Min throughput per process                      =   13089.04 KB/sec 
    Max throughput per process                      =   13097.76 KB/sec
    Avg throughput per process                      =   13093.24 KB/sec
    Min xfer                                        = 20957568.00 KB

    Children see throughput for 20 readers          =  789217.98 KB/sec
    Parent sees throughput for 20 readers           =  789212.24 KB/sec
    Min throughput per process                      =   39354.13 KB/sec 
    Max throughput per process                      =   39534.00 KB/sec
    Avg throughput per process                      =   39460.90 KB/sec
    Min xfer                                        = 20876160.00 KB

    Children see throughput for 20 re-readers       =  789583.98 KB/sec
    Parent sees throughput for 20 re-readers        =  789579.27 KB/sec
    Min throughput per process                      =   39307.91 KB/sec 
    Max throughput per process                      =   39569.62 KB/sec
    Avg throughput per process                      =   39479.20 KB/sec
    Min xfer                                        = 20832896.00 KB

    Children see throughput for 20 random readers   =  181911.32 KB/sec
    Parent sees throughput for 20 random readers    =  181909.51 KB/sec
    Min throughput per process                      =    9038.96 KB/sec 
    Max throughput per process                      =    9195.25 KB/sec
    Avg throughput per process                      =    9095.57 KB/sec
    Min xfer                                        = 20615104.00 KB

    Children see throughput for 20 random writers   =  250035.10 KB/sec
    Parent sees throughput for 20 random writers    =  249864.63 KB/sec
    Min throughput per process                      =   12491.00 KB/sec 
    Max throughput per process                      =   12509.37 KB/sec
    Avg throughput per process                      =   12501.76 KB/sec
    Min xfer                                        = 20940736.00 KB
```

---
",does anyone have any updates on this? any tweaks on chunk servers that can be made?,9800119
220,viability of a 2 node cluster?,open,2016-07-24T17:00:00Z,2017-01-07T14:44:06Z,,NONE,"I've built a 2.5 node proxmox cluster.  2 VM hosts and a third small node that only handles quorum.  I can replace the third node once I need more compute/ram.

The cluster currently runs glusterfs to mirror local disk directories to enable live migration and some redundancy.  This works fine, but glusterfs has some limitations and performance issues in this type of configuration.  A big limitation is that having redundancy requires matched storage nodes, so to expand I have to go from 2 nodes to 4 nodes.

Lizardfs is something I'm looking to for storage for another project so going to 1 storage model makes sense.

So the question is, how viable is a 2 node lizardfs system with the master and shadow on the two 'main' nodes, as well as local storage?  

A related question, can lizardfs present as iscsi or nfs storage?  To utilize a filesystem for live migration, I either need support in proxmox (so it knows the storage is available from other nodes) or to by a dns addressed nfs or iscsi share.

Thanks.
","> A related question, can lizardfs present as iscsi or nfs storage? To utilize a filesystem for live migration, I either need support in proxmox (so it knows the storage is available from other nodes) or to by a dns addressed nfs or iscsi share.

A bit late, but with proxmox if you mount lizardfs on the same directory for each node (e.g /mnt/lizardfs) and set it up as shared directory storage in proxmox, you can live migrate between nodes, no need for nfs or iscsi.",9800119
221,"Shadow master sometimes crashes/""goes missing""",open,2016-07-14T09:29:58Z,2016-07-27T04:27:03Z,,NONE,"Hi there,

Seeing some strange behaviour on our LFS cluster. We have one master, one shadowmaster and four metaloggers (each on different boxes). Every so often master2 (our shadow) crashes - the machine is fine, but the shadowmaster service stops or crashes.

Definitely need some help with this as that could prevent us from failing over.

Running on the latest Debian 8 packages (Debian packages, not vendor), and fully up-to-date otherwise.

Also, if it helps, I had this issue on master2 before and completely reinstalled it from a fresh Debian template on AWS.
","Just the universe telling you to communicate more?
",9800119
222,CGI: no chunks replication info on volume removal,open,2016-07-12T07:12:28Z,2016-07-14T14:12:59Z,,MEMBER,"I marked some HDDs for removal and replications started -- I can tell as chunks replication charts show activity as well as number of chunks on removed volumes decline.

However ""Chunks"" tab show no chunks that need replication in ""Chunks which need replication"" section.

Noticed on head of ""master""...
","+1
",9800119
223,RAM on master still grows while using Berkley DB,open,2016-06-23T16:11:16Z,2018-07-23T07:37:58Z,,NONE,"I've been testing the latest code that has Berkley DB support. Everything seems to be working - the ""name_storage.db"" is created and grows on disk. But the RAM usage for the mfsmaster process continues to climb.

In my test environment, my script has created 144,338,064 file system objects thus far. The Master process is currently using 28Gb of RAM, while the ""name_storage.db"" is 2.5Gb on disk.

Can the RAM usage / growth be stopped and the Berkley DB be solely used?

Thanks
Jason
","What about allowing master to swap to an NVMe drive?
Anyway, which is the mean RAM requirement for a single chunck/object ? Is the same as MooseFS ? (about 300bytes/object)",9800119
224,Master defence against advanced attackers?,open,2016-05-25T11:39:07Z,2019-05-31T08:39:05Z,,NONE,"Hey there,

I'm currently laying the foundation for some wargames to possibly be run inside GetGlass and other organizations. Basically testing exercises to ensure our admins know how to deal with various disasters.

Two of the scenarios I would like to test is that of an attacker intentionally deleting the metadata set on 1) just the master or 2) the master and all shadows.

The valid response to situation 1 would be to fail over to a working shadow that still has the metadata set, and the response to situation 2 would be to recover the metadata set from the metaloggers and use it to restore the filesystem.

My question is what happens in such a scenario, with regard to the empty masters... Would there be a risk of chunkservers deleting chunks, thinking they are no longer needed? And if so, how do we protect against such a scenario?

Perhaps chunkservers could (by default) track the metadata version and refuse to delete chunks while the metadata version is lower than they ""remember"". This could of course be tuned.

Let me know your thoughts. Thanks.
","+1 for any tunable failsafe measures in general. That would be a useful first line of defense against hardware or software glitches, or an accidental operations issue (automated or not).

It is my understanding that as of today all chunkservers will follow through on information obtained from a single master, identified by site specific address and port number. A number of scenarios comes to mind where nefarious and determined admin on real, or made up master server could lead to chunks being dropped by chunkservers. If such scenario played out all consumers of particular lizardfs storage cluster instance would be affected and subject to unplanned downtime.

PITA with its own challenges, especially on busy storage, but could help reduce the length of downtime caused by malicious metadata manipulation:
- chunks: Keep a rolling set of immutable snapshots on filesystems used by chunkservers, monitor space utilization. Tune expiration schedule and triggers to match your site's needs, i.e.: only after ""all clear"" response from a set of canaries utilizing lizardfs filesystem you are trying to protect.
- metadata: Ensure you have at least one operational metalogger with plenty of available free space.

One can hope that metadata service will eventually become truly distributed and with proper deployment less likely to be altered everywhere the same time. Restarts with altered metadata (malicious or not) would be taken care of during voting by consensus and before becoming the Source of Truth.
",9800119
225,"CGI: ""socket connection broken""",open,2016-05-07T00:23:05Z,2016-05-13T10:33:56Z,,MEMBER,"One chunkserver was affected by kernel exception happened on Btrfs.
File system stopped responding and so did chunkserver that uses Btrfs as backend.

However hours after incident CGI/Servers still show unresponsive chunkserver as connected (and clients log multiple `Read from chunkserver error: connection reset by peer` errors).
Even worse, CGI/Disks view is completely broken showing only `socket connection broken` error (which makes it difficult to identify and locate the problem).

Please improve detection of live/active chunkservers, perhaps by using timeout to detect those that are still running but don't actually work.
","Also unresponsive chunkserver may log `jobs queue is full !!!` many times affecting the whole cluster yet master does not drop such chunkserver even after while (and clients keep trying to write to unresponsive chunkserver despite timeouts)...
",9800119
226,LizardFS incorporated into proprietary EditShare product : GPL violation?,open,2016-04-06T06:34:53Z,2017-05-24T20:29:24Z,,MEMBER,"It came to my attention that LizardFS is incorporated into proprietary ""EditShare"" product (_XStream EFS_).

Unreleased LizardFS v2.9.7 (!) can be found inside EditShare ""[updater](http://updates-es7.editshare.com/release/updaters/editshare-7.1.0.6.es7u)"" downloadable from the following page: http://www2.editshare.com/passprot/filestore/version7/offline_700.html

Updater is a tar.gz archive that contains binary .deb packages of LizardFS but _no attribution_,  _no license_ (other than what can be found inside packages) and _no sources_ which looks like GPL violation to me.

Perhaps LizardFS team should advise EditShare to comply with GPL-3 license requirements ([short summary can be found here](https://tldrlegal.com/license/gnu-general-public-license-v3-%28gpl-3%29)).

I wonder if there are any formal relationships between LizardFS and EditShare?
","> ""The requirement is that they have to give/keep proper credits and comply with GPL's terms.""


I'm not completely certain what the state of this topic it.  I feel that attribution is the most important and easy to comply with part of open-source. -cheers",9800119
227,Xor and erasure coding optimization techniques,open,2016-04-02T13:14:08Z,2016-04-02T22:43:22Z,,NONE,"I'm getting low performance on non-replicated datasets...
What are recommendations for optimizing xor and ec goal data for high throughput?

I'm trying raid instead of jbod on chunkservers, but it seems marginally close in our setup.
Also, do those goals have read benefit like replication?
","True, but what if one needs hundreds MB/s per single operation? What would be the correct tunables? And if one would like to get them out of ec goaled setup?
Don't need many of those concurrent, maybe 1 per each 3-5 chunkservers...
",9800119
228,"Lizardfs, Samba and HyperV",open,2016-03-30T21:09:25Z,2016-03-30T21:18:21Z,,NONE,"Hi folks

We are trying to use Lizardfs with Samba to server as the Virtual Machine repository for our HyperV virtual machines.

If we take lizardfs out of the picture, just use Samba 3.0 and point to a physical disk in our Linux box, it works very well.

But, if we try to use lizardfs and point Samba to the Lizardfs mfsmount point, for some reason it fails. HyperV can create the file but for some reason, it returns an error message like if it didn't create.

Any ideas?

Here is the output of the debug:

lookup (1,dwadawek.vhdx)
status: ENOENT (No such file or directory)
lookup (1,dwadawek.vhdx)
status: ENOENT (No such file or directory)
opendir (1)
readdir (1,129,0)
readdir (1,112,687)
readdir (1,129,687)
releasedir (1)
getattr (1)
lookup (1,dwadawek.vhdx)
status: ENOENT (No such file or directory)
create (1,dwadawek.vhdx,-rw-rw-r--:0100664)
create (58) ok -> keep cache: 1
getattr (1)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
setattr (58,0x1,[rw-rw-r--:00664,0,0,0,0,0])
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
flush (58)
release (58)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,mfs#mfsmaster:9421)
status: ENOENT (No such file or directory)
lookup (1,mfs#mfsmaster:9421)
status: ENOENT (No such file or directory)
lookup (1,mfs#mfsmaster:9421)
status: ENOENT (No such file or directory)
lookup (1,mfs#mfsmaster:9421)
status: ENOENT (No such file or directory)
lookup (1,mfs#mfsmaster:9421)
status: ENOENT (No such file or directory)
lookup (1,mfs#mfsmaster:9421)
status: ENOENT (No such file or directory)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
open (58)
open (58) ok -> keep cache: 0
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
open (58)
open (58) ok -> keep cache: 0
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
open (58)
open (58) ok -> keep cache: 0
setattr (58,0x8,[---------:00000,0,0,0,0,4194304])
lookup (1,dwadawek.vhdx)
setattr (58,0x30,[---------:00000,0,0,1459371002,1459371002,0])
lookup (1,dwadawek.vhdx)
lookup (1,dwadawek.vhdx)
open (58)
open (58) ok -> keep cache: 1
flush (58)
release (58)
flush (58)
release (58)
flush (58)
release (58)
flush (58)
release (58)
getattr (1)
lookup (1,dwadawek.vhdx)
","Btw, checking the Windows Event viewer, it only shows this error message when Hyper-V tries to create the file on top of Lizard

0x80070021
",9800119
229,lizardfs cgiserver provide an old style favicon,open,2016-03-13T19:55:22Z,2016-03-13T19:55:22Z,,NONE,"Hi,

it's not really a bug, but it could be nice if the favicon provided by the lizarfs cgi server could be the same as the favicon of the lizardfs web site.

Are you agree ?
",,9800119
230,Some clarification regarding metadata and ACL,open,2016-03-02T15:05:07Z,2018-04-05T10:34:12Z,,NONE,"Regarding metadata: a few years back we (I was not yet working on the system) were running moosefs 1.6 and we had massive issues with metadata that caused us headaches and data loss. Specifically we had a point where it was taking longer than 1 hour to write the metadata to disk, leading to it rolling over to write the next round of metadata and just overall a really really bad situation. Also there was an issue when the metadata writer just stopped rotating the metadata properly for 3 months. Did lizardfs fix these sorts of issues? For reference our system is 3-5 chunk servers running about 30 TB each. We have one good node set aside as a master/frontend node but adding a secondary/slave master wouldn't be too difficult.

Regarding ACL: How easy is it to work with ACL in lizardfs? Can I use standard unix tools like setfacl/getfacl to configure the ACLs on the fs? Do these tools work properly for recursively setting ACLs in a dir? Basically, how good is the ACL support in lizard?
",,9800119
231,LizardFS Roadmap plan,open,2016-02-24T13:45:00Z,2017-10-12T22:07:55Z,,MEMBER,"Hi, 

on https://github.com/lizardfs/lizardfs/wiki/Roadmap we put our latest plan for next releases. You are welcome to comment :)

Best
","> on https://github.com/lizardfs/lizardfs/wiki/Roadmap we put our latest plan for next releases. You are welcome to comment :)

Is this stil updated ? Seems a little be outdated, some feature being worked (like NFS Ganesha) are not wrote there.",9800119
232,Global Deployment,open,2016-02-21T12:01:40Z,2016-02-27T06:17:29Z,,NONE,"I'm about to attempt a global (worldwide) deployment of lizardfs.  Has this been done before?  Were any latency issues hit?  if so, how were they addressed?  

Thanks!

-Jake
","Just found another interesting system - [iRODS](http://irods.org). Here is their GitHub repo: https://github.com/irods/irods
",9800119
233,Enhancements for chunkserver labels and goals for additional flexibility.,open,2016-02-11T03:25:02Z,2018-03-08T16:54:35Z,,NONE,"I would like to propose a few enhancements for chunkserver labels and/or goals for some additional flexibility in tuning an environment.

I think it would be nice if chunkservers could be given multiple labels. In mfschunkserver.cfg the LABEL option could be extended to accept multiple labels such as: `LABEL = Fred SSD01 SSD`. such that any of these labels could be used to reference this server.  Another server might have `LABEL = Beth SSD02 SSD`. such that a goal could include Fred or Beth by name or SSD01 or SSD02 or reference either of these servers with SSD.

Another way to accomplish this might be to allow mfsgoals.cfg to recognize [sets] of labels.  For example a goal might accept `20 SomeGoal : [SSD01 SSD02] AnotherLabel` which would place one chunk on either SSD01 or SSD02 and another chunk on AnotherLabel server.  The same result would be possible with no change to the goal mechanics if multiple labels could be assigned to a chunkserver such as `20 Somegoal : SSD AnotherLabel`.

Another way to accomplish this might be to allow mfsgoals.cfg to recognize wild cards in label names.  For example a goal might accept `20 SomeGoal : SSD0? AnotherLabel` which would place one chunk on either SSD01 or SSD02 and another chunk on AnotherLabel, but would not place a chunk on a server with the label SSD nor SSD11.  It could also accept `20 SomeGoal : SSD* AnotherLabel` which would place one chunk on any server with the labels SSD or SSD01 or SSD02 or SSD11 and another chunk on AnotherLabel.

I think that just the addition of multiple labels for a given chunkserver would accomplish all of these objectives with the least complexity and may be the most versatile... Thoughts?
","That's the spirit :D
Alternatively: Throw them some money?

2018-03-08 17:52 GMT+01:00 4Dolio <notifications@github.com>:

> True true. I should not be complaining, I should be submitting a pull
> request...
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/lizardfs/lizardfs/issues/382#issuecomment-371548904>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADXnExXQ2yXwHSVte03Uu9A46DpuA7_Sks5tcWHMgaJpZM4HX3-L>
> .
>
",9800119
234,Replication and Manipulation of (cs)stats.mfs data files.,open,2016-02-09T02:21:19Z,2016-02-09T02:23:22Z,,NONE,"While Masters share sessions.mfs data with the shadows, and obviously the metadata itself.  They do not share their stats.mfs information.  Would it be worthwhile to add the ability for shadows to receive stats.mfs from the acting master of the cluster?

And another related issue, is there any way to manipulate the stats.mfs or the csstats.mfs data files?  I only recently noticed that about a week ago I accidentally allowed one of my chunkservers to start using a new csstats.mfs file.  So it has lost the previous 2+ years of csstats.mfs data.  I do still have the old datafile, but would like to merge the two rather than either loosing all the old, or creating a whole in the old data.  I was sort of hoping that perhaps you leverated rrdtools and so I could use that utility to unpack the data into text and then merge them and turn it back into valid rrd data... file claims that csstats.mfs is TrueType font data ;)  Another use case is that I once somehow managed to introduce a 1000% cpu datapoint into a masters data file, the only solution was to blow it away and start over with new data.
",,9800119
235,document `BGJOBSCNT_PER_NETWORK_WORKER` in `mfschunkserver.cfg`,open,2016-02-03T02:01:21Z,2016-02-03T02:01:21Z,,MEMBER,"`BGJOBSCNT_PER_NETWORK_WORKER` is to be documented in `mfschunkserver.cfg`.
",,9800119
236,Adaptive replication to increase performance,open,2016-01-31T13:07:17Z,2016-01-31T13:07:17Z,,NONE,"It would be great if LizardFS could create/move replicas to a local chunk-server when a chunk/file is accessed. Similar functionality exists in [gfarm](http://oss-tsukuba.org/gfarm/share/doc/gfarm/html/en/ref/man7/gfarm_environ.7.html) with `GFARM_FLAGS=r`:

> If gfarm clients are invoked on a filesystem node, the gfarm clients replicates files to local node, before accessing them. 
",,9800119
237,Chunkserver communication timed out,open,2016-01-27T13:43:53Z,2019-10-11T19:10:45Z,,NONE,"Hi friends. 

I have a chunkserver'hangs with the following message in the logs:

`Jan 26 16:41:43 ak34 mfsmount[20791]: read file error, inode: 4, index: 0, chunk: 97, version: 1 - Chunkserver communication timed out: 172.24.32.134:9422 (try counter: 1)`
`Jan 26 16:41:45 ak34 mfsmount[20791]: read file error, inode: 4, index: 0, chunk: 97, version: 1 - Chunkserver communication timed out: 172.24.32.134:9422 (try counter: 2)`
`Jan 26 16:41:47 ak34 mfsmount[20791]: read file error, inode: 4, index: 0, chunk: 97, version: 1 - Chunkserver communication timed out: 172.24.32.134:9422 (try counter: 3)`

This error occurs only when the read operations. 
I have a master, chunkserver(with one disk) and a client on a single server(for testing only).

I have the following setting in chunkserver conf:

NR_OF_NETWORK_WORKERS = 2
NR_OF_HDD_WORKERS_PER_NETWORK_WORKER = 8

I'm using rhel 6.6 and lizardfs 3.9.4.
I tried few different disks but it didn't help.

Unfortunately I can't reproduce this behavior on my other test stands, but to leave this problem without attention I can't either.

Maybe someone has ideas why this is happening?
","Hi, any update on this? I'm seeing this too. Even when there's not high io (though there is some).",9800119
238,mfsmetarestore infinite writes to metadata,open,2016-01-18T15:57:13Z,2016-01-20T15:44:05Z,,NONE,"After mfsmetarestore -a

```
[ OK ] file /meta/metadata_ml.mfs will be used to restore the most recent metadata
[ OK ] opened metadata file /meta/metadata_ml.mfs
[....] loading objects (files,directories,etc.) from the metadata file
[....] loading names from the metadata file
[....] loading deletion timestamps from the metadata file
[....] loading extra attributes (xattr) from the metadata file
[....] loading access control lists from the metadata file
[....] loading quota entries from the metadata file
[....] loading file locks from the metadata file
[....] loading chunks data from the metadata file
[....] checking filesystem consistency of the metadata file
[....] connecting files and chunks
[....] calculating checksum of the metadata
[ OK ] metadata file /meta/metadata_ml.mfs read
[ OK ] store metadata into file: /meta/metadata.mfs <- it grows over /meta/metadata_ml.mfs size and until the disk space ends.
```

strace shows tons of 
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096

Last words before infinite loop:

```
write(5, ""V\234\252\f\0\375u$V\234\252\f\0\375u%V\234\252\f\0\375u&V\234\252\f\0\375u'""..., 4096) = 4096
write(5, ""V\234\252\f\0\376$-V\234\252\f\0\376$.V\234\252\f\0\376$/V\234\252\f\0\376$0""..., 4096) = 4096
write(5, ""V\234\252\f\0\376\244\314V\234\252\f\0\376\244\315V\234\252\f\0\376\244\316V\234\252\f\0\376\244\317""..., 4096) = 4096
write(5, ""V\234\252\f\0\376\246\350V\234\252\f\0\376\246\351V\234\252\f\0\376\246\353V\234\252\f\0\376\246\354""..., 4096) = 4096
write(5, ""V\234\252\f\0\376\255'V\234\252\f\0\376\255)V\234\252\f\0\376\255*V\234\252\f\0\376\255-""..., 4096) = 4096
write(5, ""V\234\252\f\0\376\313kV\234\252\f\0\376\313nV\234\252\f\0\376\313qV\234\252\f\0\376\313r""..., 4096) = 4096
write(5, ""V\234\252\f\0\376\342\205V\234\252\f\0\376\342\211V\234\252\f\0\376\342\214V\234\252\f\0\376\342\222""..., 4096) = 4096
write(5, ""V\234\252\f\0\376\376\34V\234\252\f\0\376\376\35V\234\252\f\0\376\376 V\234\252\f\0\376\376!""..., 4096) = 4096
write(5, ""V\234\252\f\0\377\22\260V\234\252\f\0\377\22\261V\234\252\f\0\377\22\263V\234\252\f\0\377\22\264""..., 4096) = 4096
write(5, ""V\234\252\f\0\377&jV\234\252\f\0\377&lV\234\252\f\0\377&mV\234\252\f\0\37747""..., 4096) = 4096
write(5, ""V\234\252\f\0\377F\335V\234\252\f\0\377F\336V\234\252\f\0\377F\340V\234\252\f\0\377F\341""..., 4096) = 4096
write(5, ""V\234\252\f\0\377T\206V\234\252\f\0\377T\210V\234\252\f\0\377T\212V\234\252\f\0\377T\214""..., 4096) = 4096
write(5, ""V\234\252\f\0\377l\32V\234\252\f\0\377l\33V\234\252\f\0\377l\34V\234\252\f\0\377l\35""..., 4096) = 4096
write(5, ""V\234\252\f\0\377|\265V\234\252\f\0\377|\266V\234\252\f\0\377|\267V\234\252\f\0\377|\270""..., 4096) = 4096
write(5, ""V\234\252\f\0\377\213\33V\234\252\f\0\377\213\34V\234\252\f\0\377\213\35V\234\252\f\0\377\213\36""..., 4096) = 4096
write(5, ""V\234\252\f\0\377\245\240V\234\252\f\0\377\245\242V\234\252\f\0\377\245\243V\234\252\f\0\377\245\244""..., 4096) = 4096
write(5, ""V\234\252\f\0\377\265)V\234\252\f\0\377\265*V\234\252\f\0\377\265,V\234\252\f\0\377\265.""..., 4096) = 4096
write(5, ""V\234\252\f\0\377\3200V\234\252\f\0\377\3201V\234\252\f\0\377\3202V\234\252\f\0\377\3203""..., 4096) = 4096
write(5, ""V\234\252\f\0\377\343\322V\234\252\f\0\377\343\325V\234\252\f\0\377\343\330V\234\252\f\0\377\343\333""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
write(5, ""V\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6hV\234\252\f\1\0\6h""..., 4096) = 4096
...
```

Tried on 3.9.1-3.9.5(master)

mfsmetadump 

```
# header: LIZM 2.9 (4C495A4D20322E39)
# maxnodeid: 116110140 ; version: 2130886097 ; nextsessionid: 63
# -------------------------------------------------------------------
# section header: NODE 1.0 (4E4F444520312E30) ; length: 5201050675
....
# -------------------------------------------------------------------
# LizardFS END OF FILE MARKER
# -------------------------------------------------------------------
```
","The reason was in corrupted(malformed/truncated?) changelog which applies without any warnings and cause this behaviour. BTW, metaloggers changelogs were in the same state.
",9800119
239,metalogger not releasing lock on old deleted metadata files.,open,2016-01-13T04:29:14Z,2016-01-13T04:29:14Z,,NONE,"I may have discovered a bug with the 2.6.0 metalogger (which I realize is less important now with shadow masters). When the metalogger rotates it's metadata.mfs.1 (Or whatever is configured as max I presume) file to trash it is keeping a lock on the file which is deleted, which can be observed with lsof returning multiple instances of ""....metadata.mfs.1 (deleted)"" and with the eventual exhaustion of the disk space where it is stored. For the moment I am simply writing 0 into the file handles of the (deleted) files to free the disk space even though the files remain open until the metalogger service is stopped or restarted. Here is my workaround to empty the files:

`for D in $(find /proc/$(pgrep mfsmetalogger)/fd -lname ""*/metadata_ml.mfs.1 (deleted)"") ; do echo """" >| $D ; done # Empty the deleted files to free up space`
",,9800119
240,Unable to get shadow master into sync with master on very active cluster.,open,2016-01-13T03:54:58Z,2016-01-13T05:38:39Z,,NONE,"I just migrated from MooseFS1.6.27 to LizardFS 2.6.0 for our largest cluster. Already running two smaller 2.6.0 clusters which seem to be working fine with my new corosync ocf script. But the recent migration of a much larger and more active cluster has some problems. I can not get the shadow master to properly sync up. even outside of cosync controls it spend far far to long and encounters SIGPROF and SIGVTALRM which I must trap and even then it eventually fails the final sync verification and unloads the filesystem only to retry and fail indefinitely.

The metadata set is about 16G on disk and 50G in memory and we experience 500 to 5000 transactions per second (metadata_version increases). My initial hunch is that perhaps our metadata is moving to quickly for the final sync verification to succeed due to some asynchronicity problem with that process? Any ideas?

I was using the .deb package but have instead compiled our own and the shadow master now behaves slightly differently. This time it outputs these messages, once per metadata version with the final number increasing by one each time:
`wb00007 mfsmaster[7453]: metadata file /var/lib/mfs/metadata.mfs read (174110003 inodes including 24685959 directory inodes and 149377753 file inodes, 39683460 chunks)`
`wb00007 mfsmaster[7453]: running in shadow mode - applying changelogs from /var/lib/mfs`
`wb00007 mfsmaster[7453]: changelog.mfs.1:67298633003: error: 42 (Metadata checksum not matching)`
`mfsmaster[7453]: can't load downloaded metadata and changelogs: can't apply changelog /var/lib/mfs/changelog.mfs.1 (Metadata checksum not matching)`
`mfsmaster[7453]: some changes lost: [67300206869-67300649005], download metadata again`
`mfsmaster[7453]: Waiting for master to produce up-to-date metadata image`
That is 439,136 metadata version changes, presumably during the time it took to download the initial metadata set?

Are there any tunables I can use to compensate for this?
","Looks like I'll compile my own tomorrow...
",9800119
241,Help with large cluster issues.,open,2016-01-08T11:49:23Z,2016-01-29T08:51:35Z,,NONE,"I'm still working on pushing back up my metadataserver ocf corosync changes, and have a few new ones to try and add to <s>https://github.com/4Dolio/lizardfs-ocf_work/commits/master</s> which I'm not positive is the right way to start to prep for my pull request.  I'm not even sure I modified the right file there at the moment..

Anyway, bigger issue.  I just migrated from MooseFS1.6.27 to LizardFS 2.6.0 for our largest cluster.  Already running two smaller 2.6.0 clusters which seem to be working fine with my new corosync ocf script.  But the recent migration of a much larger and more active cluster has some problems.  I can not get the shadow master to properly sync up.  even outside of cosync controls it spend far far to long and encounters SIGPROF and SIGVTALRM which I must trap and even then it eventually fails the final sync verification and unloads the filesystem only to retry and fail indefinitely. The metadata set is about 16G on disk and 50G in memory and we experience 500 to 5000 transactions per second (metadata_version increases).  My initial hunch is that perhaps our metadata is moving to quickly for the final sync verification to succeed due to some asynchronicity problem with that process? Any ideas?

I have yet to upgrade any clients nor chunkservers until I succeed at getting a shadow master working.  Could that be a part of the problem?  Luckily the master can be stopped and started in just 9 minutes which is better than I estimated.  I'm ready to bring all chunkservers up to 2.6.0 but a few clients need to stay at 1.6.27 until they are retired entirely.

In the meantime I'm running a 2.6.0 metalogger on the intended shadow server for some redundancy, and also a 1.6.27 metalogger on one of the chunkservers.  Could this old metalogger be causing problems?

I may have discovered a bug with the 2.6.0 metalogger (which I realize is less important now with shadow masters).  When the metalogger rotates it's metadata.mfs.1 file to trash it is keeping a lock on the file which is deleted, which can be observed with lsof returning multiple instances of ""....metadata.mfs.1 (deleted)"" and with the eventual exhaustion of the disk space where it is stored.  For the moment I am simply writing 0 into the file handles of the (deleted) files to free the disk space even though the files remain open until the metalogger service is stopped or restarted.

I am also in desperate need of a back end ""mfstools"" method of purging a snapshot or just unlinking large metadata structures rather than resorting to using rm for hours to delete snapshots.

One final comment, the new master 2.6.0 service has far less idle cpu time than the 1.6.27 master which it replaced (running on identical hardware/distribution(Debian)).  Any thoughts on why that is?  Any plans for multi-threading (would like to be able to use all 12 cores/threads?

I finally have a fourth purely testing cluster updated to our current production environment so perhaps I can verify that LizardFS 3.x versions will work with corosync and can fix some of these issues by taking that route.  Can anyone comment on the compatibility of running a 2.6.0 or 3.x master with 1.6.27 chunkservers and clients?  And also more specifically I have some older clients on which I can not get beyond the old moosefs 1.6.27, how can I expect them to interact with a 2.6.0 or 3.x master?  At the moment I am not seeing any obvious fatal flaws with out current mixed version environment.

Thank you everyone for any advice on any of the above topics...
","> Well, finally began to test 3.9.4 (Vendor .deb)

I do not understand why would anyone use vendor packages, ever. Those mediocre packages don't include some fixes and packaging itself is about a year behind official Debian packages not to mention lack of QA, limited security support, bunch of Lintian warnings and other problems. Vendor packages are built for only few hardware architectures etc...

If you urgently need 3.9.4 in _jessie-backports_ I can upload later today or over the weekend... Please let me know.

> I was under the impression it absolutely would not work, yet it appears to be. Are there any known issues?

On 3.9.4 (or 3.9.2) I had issues with older chunkservers...
",9800119
242,chunkserver: please implement data compression [feature],open,2016-01-04T04:37:37Z,2017-06-01T21:28:51Z,,MEMBER,"Lizardfs chunk size is 64k which is not optimal for small files (e.g. sources). Because chunk files are not sparse HDD space utilisation is not optimal and a lot of space is wasted. To certain extent it can be compensated by using file system with compression (e.g. Btrfs mounted with `compress=lzo`) on chunkservers but it would be great if chunkservers could optionally compress their chunks natively with LZ4 (preferable) or similar compression algorithm.
",@blink69 What is the timeline for 5.x?,9800119
243,Problem on client: Cannot assign requested address. Sockets in state TIME_WAIT!,open,2015-12-24T14:50:36Z,2016-02-15T08:16:29Z,,NONE,"Hi friends. 
I and my team have great interest in your project. 
But we had critical problems during testing.

our environment:

four servers with a clean RHEL 7.1(but on RHEL 6.6 is the same)
two chunkservers have 4 SSD drives each
one separate server is the master.
one separate server is the client.

Servers have two sockets and 32 Gb of memory each.

mount options:

`# mfsmount -o mfscachemode=NO /mnt/ -H mfsmaster`

We perform tests using fio.
When we run fio sequential write or read are all very good.

example fio parameters:

`# fio --ioengine=libaio --direct=1 --buffered=0 --runtime=60 --bs=4k --filename=/mnt/read --name=sec.read --rw=read --iodepth=16 --numjobs=4 --size=6G`

Sockets in the state TIME_WITE not growing. Everything is OK:

`# cat /proc/net/sockstat`
sockets: used 261
TCP: inuse 19 orphan 0 **tw 201** alloc 25 mem 2
UDP: inuse 12 mem 3
UDPLITE: inuse 0
RAW: inuse 0
FRAG: inuse 0 memory 0

When we run fio in the random mode of writing or reading we see rapid growth in the number of sockets in the state TIME_WITE. Within thirty seconds, the number becomes equal to ~60,000 and fio hangs.

example fio parameters:

`# fio --ioengine=libaio --direct=1 --buffered=0 --runtime=60 --bs=4k --filename=/mnt/rand.read --name=ran.read --rw=randread --iodepth=16 --numjobs=4 --size=6G`

on client:

`# cat /proc/net/sockstat`
sockets: used 261
TCP: inuse 19 orphan 0 **tw 56438** alloc 25 mem 3
UDP: inuse 12 mem 3
UDPLITE: inuse 0
RAW: inuse 0
FRAG: inuse 0 memory 0

And in the logs we see the following messages:

Dec 24 16:17:47 xxxxxxxxxx mfsmount[22465]: can't connect to (AC182087:9422): EADDRNOTAVAIL (Cannot assign requested address)
Dec 24 16:17:47 xxxxxxxxxx mfsmount[22465]: can't connect to (AC182087:9422): EADDRNOTAVAIL (Cannot assign requested address)
Dec 24 16:17:47 xxxxxxxxxx mfsmount[22465]: file: 2, index: 5 - can't connect to proper chunkserver (try counter: 14)
Dec 24 16:17:47 xxxxxxxxxx mfsmount[22465]: file: 2, index: 15 - can't connect to proper chunkserver (try counter: 14)
Dec 24 16:17:52 xxxxxxxxxx mfsmount[22465]: can't connect to (AC182087:9422): EADDRNOTAVAIL (Cannot assign requested address)
Dec 24 16:17:52 xxxxxxxxxx mfsmount[22465]: file: 2, index: 7 - can't connect to proper chunkserver (try counter: 15)
Dec 24 16:17:52 xxxxxxxxxx mfsmount[22465]: can't connect to (AC182087:9422): EADDRNOTAVAIL (Cannot assign requested address)
Dec 24 16:17:52 xxxxxxxxxx mfsmount[22465]: file: 2, index: 15 - can't connect to proper chunkserver (try counter: 15)
Dec 24 16:17:52 xxxxxxxxxx mfsmount[22465]: can't connect to (AC182087:9422): EADDRNOTAVAIL (Cannot assign requested address)
Dec 24 16:17:52 xxxxxxxxxx mfsmount[22465]: can't connect to (AC182087:9422): EADDRNOTAVAIL (Cannot assign requested address)
Dec 24 16:17:52 xxxxxxxxxx mfsmount[22465]: file: 2, index: 5 - can't connect to proper chunkserver (try counter: 15)
Dec 24 16:17:52 xxxxxxxxxx mfsmount[22465]: file: 2, index: 15 - can't connect to proper chunkserver (try counter: 15)

We tried version 2.6 and 3.9.4. Each io request is executed with the new port? Moosefs does not have this behavior.
","As for the original issue, it was most likely solved in https://github.com/lizardfs/lizardfs/commit/57f8320c2ca8bf9dc62412b55e1eef6477823686 - for some reason replication limits worked fine with rebalancing, but didn't bound replication itself, hence the large number of open sockets.

@htbase _operation timed out_ is just an information that your write request timed out and was retried (and most likely succeeded the next time). Eventually, we will amend the code to start printing
this information only after the operation failed for the nth time (say, 3), so it will cause less confusion.
",9800119
244,Optimise HDD removal; unable to remove HDD,open,2015-12-20T21:31:52Z,2016-01-04T03:19:06Z,,MEMBER,"Marking HDD (i.e. mount point) for removal in `mfshdd.cfg` does nothing if chunkserver is the only one with a certain label.
In my case there is only one chunkserver labelled `poolA` - it has 10+ HDDs but nothing happens when I mark HDD for removal. I expect data from the HDD marked for removal to be distributed among local HDDs when there are no other chunkservers to send data to.
On chunkservers with many HDDs sending at least some data from HDD marked for removal to other local HDDs will be more effective than sending all data to other chunkservers.

Using local HDDs may prevent data from being moved twice -- first time when it is sent away from chunkserver and second time when data is sent back after triggering rebalancing threshold.

This bug is related to #280.
","I agree.. im not sure my coding skills are sufficient to fix the code properly, thus i resort to scripts for now...
",9800119
245,restarting master may cause read errors,open,2015-12-16T17:18:31Z,2015-12-16T17:18:31Z,,MEMBER,"An application failed with read error during restart of master. Perhaps it would be safer if I/O operations would be suspended until master _completely_ finish initialisation (including processing data from chunkservers).
",,9800119
246,client fail-over to another chunkserver,open,2015-12-11T08:18:24Z,2015-12-11T08:18:24Z,,MEMBER,"As demonstrated in #338 and #332, client may fail to save data when chinkserver becomes unavailable during write operation. 

Ideally client should try to complete operation using any available chunkserver as well as to _obey goal_ #227 whenever possible.
",,9800119
247,"3.9.4: chunkserver errors ""EBADF (Bad file descriptor)""",open,2015-12-08T23:05:35Z,2018-12-17T23:03:24Z,,MEMBER,"On start of chunkservers, during scan of assets the following errors are logged for various chunks:

```
write_block_to_chunk: file:/var/lib/lizardfs/34/chunks99/chunk_0000000006994176_00000001.mfs - write error: EBADF (Bad file descriptor)
chunk_writecrc: file:/var/lib/lizardfs/34/chunks99/chunk_0000000006994176_00000001.mfs - write error: EBADF (Bad file descriptor)
hdd_io_end: file:/var/lib/lizardfs/34/chunks99/chunk_0000000006994176_00000001.mfs - write error: EBADF (Bad file descriptor)
chunk_writecrc: file:/var/lib/lizardfs/34/chunks99/chunk_0000000006994176_00000001.mfs - write error: EBADF (Bad file descriptor)
hdd_io_end: file:/var/lib/lizardfs/34/chunks99/chunk_0000000006994176_00000001.mfs - write error: EBADF (Bad file descriptor)
replication error: failed to close chunk (IO error)
```

The following error is repeated 50 times:

```
write_block_to_chunk: file:/var/lib/lizardfs/34/chunksA2/chunk_0000000001A2FD2C_00000002.liz - crc write error: EBADF (Bad file descriptor)
```

As far as I can tell those chunk files do not exist.
","Sorry my bad, we will fix this 
",9800119
248,chunkserver: allow graceful recovery from read-only volume,open,2015-12-07T16:55:47Z,2018-12-17T23:04:23Z,,MEMBER,"One HDD began to malfunction and ext4 switched to read-only mode due to errors.
CGI marked volume as ""damaged"". I've noticed that and marked volume for removal since most files are still readable. However chunkserver started to log endless errors like the following:

```
delete_chunk: file:/var/lib/lizardfs/00/chunks77/chunk_000000000077B28E_00000001.liz - unlink error: EROFS (Read-only file system)
```

From recovery prospective it might be useful if LizardFS avoid writing to volume that is marked for removal.

Maybe chunkserver could detect read-only mount and stop trying to modify volume?

Would it be useful to recognise another state, ""read-only"" to allow recovery from damaged but still (partially) readable volume?
","> On new versions LizardFS don't remove chunks from volume marked for removal.

This is not true: 3.10.2 removes chunks from volumes marked for removal just like all releases prior to 3.10.2... Re-opening...
",9800119
249,goal change delays replication of undergoal chunks,open,2015-11-29T08:14:21Z,2015-12-05T10:00:28Z,,MEMBER,"About 400 chunks with _std3_ goal are ""undergoal"" because there are only 2 replicas while goal requires to have 3 copies. Normally LizardFS fixes undergoal chunks by creating required replicas fairly fast -- in less than an hour there will be no undergoal chunks in _std3_.

However ""undergoal"" chunks in _std3_ remain as such during goal change like from _xor2_ to _std2_  (or from _std2_ to _xor2_) until all ""endangered"" chunks are replicated.
The problem here is that _std3_ chunks can remain ""undergoal"" for a long time (a week or longer) until goal change is finished. But prioritising replication for goal change is wrong -- there are no chunkserver failures or outages and changing goal of data to or from _std2_ is not a dangerous operation hence it should not delay replication of data with greater goals. Goal change is a standard operation that should not be treated like emergency recovery.
","There is more than one problem. First of all goal change should never produce endangered chunks. Certainly not for _xor2_ to _std2_ change. There is a bug addressing this very issue: #331.

I'm not suggesting to prioritise increasing number of copies. I'm just trying to say that recovery of undergoal chunks should be more important than goal change.

Unlike goal change (which is a normal operation) undergoal chunks is a situation that requires immediate recovery (similar to replacement of failed HDD in RAID).

Normally goal change should not produce even undergoal chunks. I believe goal change should be transactional -- master should create all required chunks for new goal and only then update metadata (set new goal on a chunk).  At the moment master changes metadata first, and that is the source of fairly annoying problems like goal change before recovery and even data loss as in #331. 
",9800119
250,Missing package lizardfs-uraft,open,2015-11-02T20:43:28Z,2019-03-25T08:52:50Z,,NONE,"I was looking at the PDF docs on the main website for setting up a LizardFS HA cluster and it mentions a lizardfs-uraft package for Debian/Ubuntu. I have the official repo configured but can't seem to find that package in there?
","From what I see in repository, [`lizardfs-uraft`](https://github.com/lizardfs/lizardfs/tree/master/src/uraft) was made public on _June 21, 2018_ (roughly 9 months ago).

After [recent fix](https://github.com/lizardfs/lizardfs/issues/719) you can easily create `lizardfs-uraft_3.13.0_amd64.deb` package by running [`create-deb-package.sh`](https://github.com/lizardfs/lizardfs/blob/master/create-deb-package.sh) script.

`lizardfs-uraft` is not yet in official Debian & Ubuntu repositories, but hopefully Debian & Ubuntu communities will port it anytime soon.",9800119
251,master: hourly fork / 30 seconds down time (all clients),open,2015-10-31T14:27:26Z,2016-08-24T05:37:21Z,,MEMBER,"At the begginning of every hour (with few seconds precision) all clients timeout for ~30 seconds as follows:

```
Oct 31 20:00:08 debmain mfsmount[9250]: master: tcp recv error: ETIMEDOUT (Operation timed out)
Oct 31 20:00:09 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:10 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:11 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:12 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:13 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:14 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:15 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:16 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:17 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:18 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:20 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:21 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:22 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:23 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:24 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:25 debmain mfsmount[9250]: master: register error (read header: ETIMEDOUT (Operation timed out))
Oct 31 20:00:26 debmain mfsmount[9250]: registered to master (session id #2437)
```

Inspection of master log revealed:

```
Oct 31 20:00:00 debstor mfsmaster[20991]: fork failed: ENOMEM (Cannot allocate memory)
Oct 31 20:00:00 debstor mfsmaster[20991]: mfsmaster[20991]: fork failed: ENOMEM (Cannot allocate memory)
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.2) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.76) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.2) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.76) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.204) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.7) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: main master server module: (ip:192.168.0.250) write error: EPIPE (Broken pipe)
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.75) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.2) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.76) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.204) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.7) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: main master server module: (ip:192.168.0.250) write error: EPIPE (Broken pipe)
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.75) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.2) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.76) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.204) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.7) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: main master server module: (ip:192.168.0.250) write error: EPIPE (Broken pipe)
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.75) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.2) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.76) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.204) has been closed by peer
Oct 31 20:00:26 debstor mfsmaster[20991]: connection with client(ip:192.168.0.7) has been closed by peer
[...]
```

Fork fails because master uses about 50% of server's RAM and only about 40% is available. Master is started with `LOCK_MEMORY = 1`.

LizardFS compiled with _libJudy_ but without _tcmalloc_ (due to performance reasons).

I do not recall such problem on 2.6.0. It would be nice to avoid hourly forking as fork needs twice as much RAM as already allocated by master.
","On 3.9.4 master used about 51% of RAM on my server (where buying more RAM is not an option).
For some time I've used `vm.overcommit_memory=1` (see `sysctl(8)`) with some extra swap space.

3.10.1 optimised RAM usage so master uses a little less than 50% of RAM and does not exhibit the problem (as long as there is enough free RAM to accommodate 200% of current allocation).
",9800119
252,multi xor in goal,open,2015-09-22T20:13:05Z,2015-09-24T09:01:29Z,,NONE,"Hi,

in goal configuration can set xor3 { node1 node2 node3 nodep }
but after read documentation for dual Datacenter, can't set:
xor3 { dc1node1 dc1node2 dc1node3 dc1nodep } xor3 { dc2node1 dc2node2 dc2node3 dc2nodep }

can add easy ?
","Thx for reply, i' wait implementation, or sponsors that, thx
",9800119
253,mfsmaster segfaults - limits/quotas and best practices?,open,2015-08-11T19:26:10Z,2015-12-11T00:15:56Z,,NONE,"This could be a bug, but if I'm pushing known arbitrary limits please consider implementing error handling for less dramatic effect.

Test simulating runaway workloads on a large system moved onto creation then removal cycle of 100M files on a scratch LizardFS filesystem. Test objective was to probe for possible weak spots in the metadata server. To keep the pace up and limit the scope of testing to mfsmaster-client path I have used empty files. System behaved OK while the test was running until the directory reached 99,999,999 files and mfsmaster segfaulted. Restart attempts were made (6-8 minutes each including auto recovery) but subsequent attempts to remove, add files to, or query said mountpoint result in mfsmaster jumping to 100%CPU and segfaulting. If the test pushed a known limit an IO error to client generating the files would be welcome instead of a DoS / filesystem-wide outage.

Test machine has plenty of resources, mfsmaster uses only ~24GB out of 256 GB of available RAM, /var/lib/mfs with its 50 changelogs is at sensible 32% full. Test were done on out of the box LizardFS 2.6.0 running on ubuntu 14.04.3 LTS.

Speaking of resources and limits, quotas could be defined in production to manage risk of such issues, but to use them effectively one needs answers to the following questions:
- If one wanted to provision a filesystem with quota as close to 'unlimited' as possible - what are actual hard limits inherited from MooseFS or fuse, or ones inherent to LizardFS?
- How do snapshots affect the resources needed for housekeeping and quota accounting?
- Since quotas are per mountpoint what is considered best practices for filesystem layout and quota management on heavily populated, busy multiuser LizardFS filesystem(s) where snapshot are going to be used?

Thanks!

```
# free
             total       used       free     shared    buffers     cached
Mem:     264158956   15623420  248535536        396      23752   13536156
-/+ buffers/cache:    2063512  262095444
Swap:            0          0          0

# dmesg | grep mfsmaster
[ 1047.598586] mfsmaster[1155]: segfault at 2b3e294f4000 ip 0000000000435987 sp 00007ffd0ca82040 error 6 in mfsmaster[400000+c8000]
[ 2177.128763] mfsmaster[1402]: segfault at 2b602993e000 ip 0000000000435975 sp 00007ffcae6950b0 error 6 in mfsmaster[400000+c8000]
[ 3105.497703] mfsmaster[1643]: segfault at 2b81ed2a8000 ip 0000000000435975 sp 00007ffddac5f9b0 error 6 in mfsmaster[400000+c8000]
[ 4211.043471] mfsmaster[1697]: segfault at 2b8b6ba27000 ip 0000000000435975 sp 00007fff8189d860 error 6 in mfsmaster[400000+c8000]
[ 4685.572671] mfsmaster[1718]: segfault at 2b70ea00a000 ip 0000000000435975 sp 00007ffe68f7dea0 error 6 in mfsmaster[400000+c8000]
[ 5425.923069] mfsmaster[1740]: segfault at 2b7b0144f000 ip 0000000000435975 sp 00007fff603275d0 error 6 in mfsmaster[400000+c8000]

# file `which mfsmaster`
/usr/sbin/mfsmaster: ELF 64-bit LSB  executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.24, BuildID[sha1]=4bb098fc28ca6078090fc154631c1264125d1d28, stripped

# lizardfs-probe info zillion1 9421
LizardFS v2.6.0
Memory usage:   22GiB
Total space:    96GiB
Available space:    65GiB
Trash space:    0B
Trash files:    1
Reserved space: 0B
Reserved files: 0
FS objects: 100000002
Directories:    2
Files:  100000000
Chunks: 0
Chunk copies:   0
Regular copies: 0

# lizardfs-probe metadataserver-status zillion1 9421
     personality: master
   server status: running
metadata version: 411899533

# mfsrepquota -a /mnt/zillionfs
# User/Group ID; Bytes: current usage, soft limit, hard limit; Inodes: current usage, soft limit, hard limit;
User           0 --                    0                    0                    0                    3                    0                    0 
User        1000 --                    0                    0                    0             99999999                    0                    0 
Group          0 --                    0                    0                    0                    3                    0                    0 
Group       1000 --                    0                    0                    0             99999999                    0                    0 

```
","Is this still a problem on v3.9.4?
",9800119
254,Whitepaper missing: so some questions about small but diverse storage,open,2015-07-17T13:55:18Z,2015-07-17T19:28:18Z,,NONE,"The Whitepaper is missing (Error 404 whether I use the link on the website or the links in various places here in Github).  So, a couple of questions (which might have been answered there).  My use case is for relatively small amounts of storage (lots and lots of small files, several multi-gig files but total about 2TB, so less than one modern disk) but I am trying to achieve a diverse storage pattern for good protection without massive cost.

My ideal would be a distributed filesystem with three types of storage: online local (adjacent to clients, fast), encrypted local backup (close to clients so it can be maintained quickly and easily, but typically in another room to reduce fire/flood risk and encrypted), encrypted cloud storage (slow, high access and data transfer costs, encrypted).  With sensible access optimisations (use local copies for preference, do not copy data from cloud unless there is no alternative) but with no single point of failure (so, for example, clients can use the backup or cloud copies if their local server fails).

I am wondering how close lizardfs can get to this so have a few questions:

1) Can lizardfs be layered on top of other filesystems (for example, an encrypted filesystem or a cloud filesystem, either/both of which might also be a userspace filesystem)?  Can it access any cloud filesystems (e.g. Amazon S3) natively?

2) Can multiple logical chunkservers (or group of chunks with similar attributes) be run on the same (physical) server?  For example, could I have one server which manages chunks stored on fast disks for online access and also other chunks using an encrypted filesystem stored on NFS disks on a NAS?

3) Can I configure the multiple copies of data such that I am guaranteed one copy in an online chunk, one in an encrypted local backup and one in the cloud?

4) Can I manage data access so that local online storage is always read (and written first) unless it has gone offline, in which case local backup storage is used next, and cloud only when absolutely necessary?

5) Can I avoid the master being a single point of failure?  Would failover have to be manual?

6) In order to set this up, I need to avoid uploading all the data to the cloud using the slow and expensive access.  I can (and do) use disk import capabilities to load cloud data and there are fairly current copies of the data in the cloud already.  Can lizardfs use that data already (presumably not as it is not chunkified)?  If not, can I run some process in a server in the same cloud (which has fast access to the cloud data) to somehow convert the files to chunks and configure the metadata so that lizardfs knows that these new chunks are duplicates of the chunks in the local chunkservers and avoid moving the data itself to/from the cloud?

7) In the opposite direction, in the case of a disaster recovery I would use export-to-disk to retrieve the data from the cloud (not download it) -- could I then somehow tell lizardfs that the chunks are now available in a local chunkserver once I have received the disk from the cloud service?
","Oh, thanks for noticing 404 error, it will probably be fixed after the weekend.
Meanwhile you can use LizardFS whitepaper from CeBit webpage (in the Downloads section): http://www.cebit.de/product/lizardfs/557479/C273009

Now for the list of questions:
1. LizardFS is actually always layered on top of other file systems - chunkservers need one to store data. There is no native support of Amazon S3, but I guess you can export it via NFS.
2. Yes, they can.
3. Yes, you can.
4. You can use topology feature to assure that: https://github.com/lizardfs/lizardfs/blob/master/doc/mfstopology.cfg.5.txt

5.You can avoid that with the use of shadow master servers. Failover doesn't have to be manual.
1. It's tricky, but I think it may be doable. If it's possible for you to create a temporary local chunkserver and mount point on a server in the same cloud, all needed actions would be to copy all the data to mount point.
2. Let's assume that you have a chunkserver, which uses _/mnt/chunk1_ directory to store data and _/mnt/chunk1_ is mounted as a remote file system on some cloud.
   Then those actions:
   - turn chunkserver off
   - unmount _/mnt/chunk1_
   - export-to-disk from a cloud and put all the data in _/mnt/chunk1_
   - turn chunkserver on

would result in chunkserver having all its chunks like nothing happened, not even realising that it uses local data now instead of contacting the cloud.
",9800119
255,stopping chunkserver suspends replication,open,2015-07-13T11:24:31Z,2015-07-14T10:08:24Z,,MEMBER,"On 2.6.0 I've changed custom goal from 

```
16 16 : tst1 tst2
```

to

```
16 16 : tst1 tst2 _
```

and re-loaded master. I observed how all chunkservers (except `tst1` and `tst2`) started receiving chunks until some chunkservers filled over 99%. Replication continued on chunkservers with available free space, as expected, but suddenly stopped when I've taken down server with already full chunkserver.
To summarise, replication to all chunkservers for goal `16` stopped when only one chunkserver (that was not even receiving new chunks due to lack of free space) was terminated.
Re-starting chunkserver again resumes replication to other chunkservers.
","Thanks! :) I'll test and confirm once path become publicly available.
",9800119
256,What are the plans for HA failover moving forward?,open,2015-07-08T06:06:50Z,2018-12-17T22:32:59Z,,NONE,"In reference to this commit: https://github.com/lizardfs/lizardfs/commit/02747c012d33ce8e79d9b3cb6e080af8b46e3b8e#diff-25d902c24283ab8cfbac54dfa101ad31 ha-cluster: Drop pacemaker/corosync stack from 2015 Jun 2nd.

I only just now noticed that you are dropping these corosync/pacemaker specific ""ha-cluster"" scripts.  I am using corosync+pacemaker for my cluster and am using your ocf metadataserver script.  But I did not use your lizardfs-cluster-manager ""wizard"" script, rather I manually configured the corosync/pacemaker services and configurations.  With that said, I do agree that they are not very user friendly.  I'm still at a lose for how it is supposed to coldboot/bootstrap/initialize the lizard cluster, I just haphazardly manage to bring it online.  I would love an explanation about how this was supposed to work.

I also notice that there is an error in the ocf metadataserver script which would prevent it from working properly without patching that error. Perhaps this was intentional in order to prevent the less motivated from using this method and getting into trouble. I'm not sure you would want me to share the fix...

I also learned that the way the Debian7 distributes corosync and pacemaker is not compatible with the way your ha-cluster scripts expect those services to behave and interact.  So I have a set of sed commands which fix the stock corosync and pacemaker services such that they work as expected.  I had intended to share these after verifying that they allow for the lizardfs-cluster-manager script to work as intended.

In my experience my corosync/pacemaker driven ha-cluster had been very reliable (with the bootstrapping exception).  I can confidently reboot any member of the cluster at will without worrying about the effects, which are imperceptible.  I'm still working on migrating two moosefs systems to lizardfs, but I would really like to have automatic failover, so for the moment I intend to continue to use corosync+pacemaker.

I understand your dropping this stuff, but it appears that you kept the ha-cluster-managed personality for the master service intact?  Which I expect needs to stay in place regardless of which type of ha cluster technology you end up using.  I sort of expected LizardFS to remain semi failover technique agnostic such that one might use carp, or corosync/pacemaker, or any number of raft implementations.  Can you speak to that point?  I see mention of uraft, but would appreciate pointers to exactly what that is.
","Hi @blink69 3.12.0 is out. Providing uraft soon will give you lots of attention. I began testing lizardfs, because of your comment and I'm impressed. I tested a lot of SDNs/HA storage (Ceph, DRBD, Gluster etc.) and LizardFS is a good mixture. ",9800119
257,Data loss while restarting a chunkserver during a rebalance/replicate,open,2015-06-16T15:20:12Z,2015-06-20T03:00:15Z,,CONTRIBUTOR,"So, this is a bit long.  We've had this bug happen 3 times.  Once long ago, then twice within a week.

I had originally noticed the problem, when restarting a chunkserver, that chunks were lost.  In that scenario, I wasn't fully aware of what was going on.  I was annoyed, but not too worried, as the missing data was in throw-away files, so it was no big deal.

Then, recently, we came to the realization that combining 4 drives into a raid0 was a poor decision on our part.  It increased single-thread performance, but it meant that when a drive died(which they always do), we would have to replicate much more data to become safe again(we use goal=2).  So, I had gone about splitting each raid0 into 4 separate directories for mfshdd.cfg.

The general procedure for this, is to mark the directory for removal(prefix it with ""_"" in mfshdd.cfg), let moosefs duplicate all the chunks, then remove the drive.  However, this replication is *slow_, and hurts performance.  The writes are placed randomly onto the other nodes, and all these writes are starving the cluster.  So, I decided to do a plain rsync of the chunk folders, spreading them out manually onto spare free space around the cluster.  This seemed to work well.  Since one single drive did not have sufficient space to hold the mount point I was trying to remove, I had to split the chunk folders up.  I did [0-7]\* to one machine, and [8-F]\* on another.  After the initial rsync, while the chunkserver was active, I ran it again(less copied this time).  Then, on the other nodes that were holding the duplicate copies, I added those folders into their mfshdd.cfg, but I marked those as removal when I did so.  This would keep new data from being written to those locations(in theory).  I also marked the original location to be removed, then waited for all chunks to have no counts of 1.  Once that was hit, I stopped the original chunkserver.  While that original chunkserver was stopped, moosefs(we aren't yet upgraded to lizardfs) continued to rebalance those half-copied to-remove locations.

As I went about splitting the raid0, I also needed to repartition each of the 4 drives.  That's a standard practice, and I had no problems with that.  I failed one drive of the raid5 OS, killed the raid0, repartitioned that 1 drive, created a raid1 with 3 missing drives, did a pvmove, then added one mount point into mfshdd.cfg(what was on sdd).  Moosefs started to move data onto this drive.

I then did the same procedure to the remaining 3 drives, repartition, add to the raid1, add all 3 mount points to mfshdd.cfg.  I then restarted the chunkserver on this original machine.  At this point, I lost data.  I didn't know what the cause was at this time.  mfsfilerepair did not fully help; in some cases, it erased the block, in others it found a different version.  This installation primarily runs a backuppc installation, which meant that we had a corrupted backup set.

I then proceeded to do the same to production, not knowing the full scope of what was to come.

Of course, we had data loss there as well.  And, as luck would have it, the machines that had corrupted filesystems(when a 64M chunk suddenly becomes all 0, bad things happen), required restore from the backup system, but that was corrupted to, and it was preventing proper reads.  Much hair pullling at this point.

During all this, my coworker thought that the problem might have stemmed from me manually copying and splitting those chunked folders around . Not knowing exactly, I took his suggesting to remove them from the other nodes(we are working on production at this point).  Moosefs is rebalancing, some chunks have a single copy, most have 2, some have 3.  I restart the chunkservers.  _More_ data loss.

More chunks are at 0 copies.  I restart the primary chunkserver(the one I was trying to do work on). _More_ data loss.

At this point, I begin not to trust moosefs at all.  I investigate some of these chunks, and notice that they are truncated.  Not full size(like they should be).  This is also based on reading syslog.  So, I think to myself, why would a chunk be truncated.  And then it hits me.  Maybe when a chunkserver is restarted, while it is replicating, and a chunk is coming in, the full data is not received from the network, and therefor not written to disk.  Or, the full data has not yet been written to the remote note, before the process is stopped.

I attempt to read the lizardfs code(hoping I could see something that works better), and it appears that the worker that is responsible for copying the data does not have any incoming signals to tell it to abort the transfer(signals being a general term, I understand that pthreads are in use).  And, I don't see how such a restart during a replication would be communicated back to the master, to have it abort the replication transaction on all nodes involved.

If this ""restart during replication"" actually causes data loss, then that is a _huge_ issue.  Because under normal situations, disks will need to be marked for removal, and it's entirely possible that other unrelated nodes might experience an outage, or other maintenance could be occuring.  Even if the goal was increased, I don't think that would help.
","I have had success doing what you describe, In our large production environment. But I do not recommend doing this if you do not understand how the chunk server service keeps track of it's chunks. It is critical to understand that the chunk service does not recognize the addition of nor the removal of ""side-loaded chunk files"" until it is restarted and re-scans it's disks.  Also be aware that the chunk server disks keep chunks in sub-folders named {00..FF} and that the last 2 characters of a given chunk file indicate which folder it is stored in.  I always took care to make sure that files stayed inside the proper sub-folder.

I believe not restarting the chunk server service (to re-scan the disks) is where you went wrong with your process.  Your source servers did not report their lost chunks and your target server did not report their gained chunks because you did not restart the service so it could re-scan the disks. I believe that had you restarted your target chunk server service before you removed the files your lost chunks would have returned.

We have a 24+21 disk JBOD with 2 controlling chunk servers.  I moved chunks between various file systems on those disks by hand numerous times.  I could move ~24TB in ~12 hours with rsync while natural re-balancing would take many weeks.  Keep in mind that this is always done withing the scope of a single chunk server.  Doing so between chunk servers would interfere with the goal values. You can do it between chunk servers, but you must be aware of the resulting changes in goals and copies.  It is also technically possible to run 2 chunk server services on a single server by modifying the default port on which the chunk server communicates (If for example you wanted one server to store 2 copies during such a manual replication process).  Anyway...

My method for moving chunk data around on a single chunk server is roughly:  rsync from source to target disks (you can do it live). Stop or mark for removal the source disks, restart/HUP chunk server service to make it re-scan. rsync again to pickup smaller number of changes during original transfer.  Remove source disks, restart/HUP chunk server service. Once you verify you have not lost anything, then remove the source disk permanently, reformat, whatever, repeat to move the chunks back.  Keep in mind that I do this with at least a goal of 2 just to be sure not to lose anything.
",9800119
258,"cgi: improve Disks/""last error"" information.",open,2015-06-11T02:21:53Z,2016-08-02T08:44:43Z,,MEMBER,"CGI interface have a very useful field _last error_ under _Disks_ tab. When error is detected _last error_ captures the time when it happened. However tooltip on last error is not very useful because it shows the chunk number where error occurred -- a particular chunk number is not important and almost never needed to be known.
I suggest improving tooltip by adding information about total number of errors and the type of the last error (e.g. read error, CRC error etc.).
Thanks.
","Number of errors counter would be very useful to have. Last error could be relatively harmless when it is just a rare/standalone error. However at the moment there is no way of telling how many errors there were prior to last error. (a few? a hundred?)
",9800119
259,"chunkserver: incorrect handling of ""damaged"" volumes",open,2015-06-10T01:56:22Z,2015-06-10T10:17:45Z,,MEMBER,"It seems that 6ec055cb0d089cf299089f8f02f5c5595bba1ff8 is not working as it supposed to. I (re-)started server with 5 volumes none of which were mounted due to unrelated problem. Chunkserver started and marked 3 volumes as ""damaged"" (expected) but two volumes were incorrectly marked ""ok"" so chunkserver started with two empty volumes and just logged multiple errors like

```
replicator: hdd_create status: IO error
hdd_io_begin: file:/var/lib/lizardfs/02/29/chunk_0000000005069D29_00000000.mfs - open error: ENOENT (No such file or directory)
```

etc. All five mount points are owned by `root` and not writeable to chunkserver so they are empty. Chunkserver should have been started with all five volumes marked as ""damaged"".
","Piotr, you are right, I found leftover '.lock' files in those mount points that were mistakenly not marked as ""damaged"". Thank you for advise. Those `.lock` files were forgotten since December 2014 when mountpoints had incorrect ownership.
Would it be possible to include `.lock` files into check for damaged volumes?
Yes, it is correct that it was not possible to write to mountpoints but apparently existing `.lock` files confused chunkserver.
",9800119
260,Various questions,open,2015-06-02T09:58:59Z,2016-05-13T23:24:40Z,,NONE,"Hello,

I've just set up a test environment from the QuickStart.
https://github.com/lizardfs/lizardfs/wiki/Quick-Start-Guide

I've some basic question I can't find answer for.
- Can I assign a goal to an entire export from the master/chunk side? Or do I need to define it with mfssetgoal client-side?
- I guess I can't have more than 1 namespace in the same installation?
- HA for master with shadow master is configured part from lizard with corosync or something like that? Or there is an integrated mechanism to have H.A. automatically?
- Due to the lack of documentation, can I assume than moosefs docs are applicable to Lizardfs?
- After some research I assume that there is no way to let Lizardfs handle automatically SSD disks for best performance, I must manually assign disks to goals, files to goals, etc., right?
  - There is any previous experience using Lizardfs for VMWARE datastores?

Thank you very much.
XAvier ROmero
","""Transparent"" attach (without creating cached device) is a killer feature of _EnhanceIO_. :)
However we found _EnhanceIO_ to be unreliable for write caching, especially in regards to risks of accidental mount of device with(out) dirty (unflushed) cache...
_dm-writeboost_ used to be unforgiving to HDD media defects until 2.1.2 which fixed issue 98.
IMHO _dm-writeboost_ is also great with RAM disks as caching devices.
As for SSDs, we use them for write buffering on some nodes by placing ext4 external journals to SSD.
_EnhanceIO_ is great for read (only) caching but for _LizardFS_ with its highly randomised workload I consider read caching ineffective...
",9800119
261,shadow master fails to (re-)start after abnormal termination,open,2015-05-28T15:09:40Z,2016-01-30T14:26:18Z,,MEMBER,"Power surge rebooted shadow master. After reboot shadow master (2.6.0) failed to start:

```
May 29 00:29:39 deblab mfsmaster[1856]: calculating checksum of the metadata
May 29 00:29:45 deblab mfsmaster[1856]: metadata file /var/lib/lizardfs/metadata/metadata.mfs read (28571217 inodes including 1511321 directory inodes and 26873268 file inodes, 6039852 chunks)
May 29 00:29:45 deblab mfsmaster[1856]: running in shadow mode - applying changelogs from /var/lib/lizardfs/metadata
May 29 00:29:46 deblab mfsmaster[1856]: /var/lib/lizardfs/metadata/changelog.mfs.1: 231774 changes applied (2056346476 to 2056578249), 0 skipped
May 29 00:29:46 deblab mfsmaster[1856]: terminate called after throwing an instance of 'std::invalid_argument'
May 29 00:29:46 deblab mfsmaster[1856]: what():  stoull
```

I had to delete `changelog.mfs.*` to fix the problem.

It would be nice to automatically dismiss changelog with failed CRC and re-download it from master.
Ideally shadow master should be able to initialise by re-downloading data from master as necessary.
","7 months later I'm still hoping for this improvement to happen...
Perhaps LizardFS team should consider organising a bug squashing party? ;-)
",9800119
262,please make HDD_ADVISE_NO_CACHE=1 the default,open,2015-05-23T11:35:43Z,2018-04-05T10:34:01Z,,MEMBER,"At the moment chunkserver do not use `HDD_ADVISE_NO_CACHE` by default.
`HDD_ADVISE_NO_CACHE = 1` is a better default because on dedicated chunkserver there is nothing to compete for cache hence advisory OS recommendation to dismiss cache is not forcing cache release. That's why dedicated chunkserver performs equally well disregarding of this option. 
On non-dedicated chunkserver where other processes are running using `HDD_ADVISE_NO_CACHE` helps nicer co-existance with other processes by reducing chunkserver cache pressure (i.e. aggressive cache displacement). Basically it leaves optimisation to OS which is more capable to decide what to dismiss.

I'd argue that `HDD_ADVISE_NO_CACHE` is entirely unnecessary but even if you choose to preserve it `HDD_ADVISE_NO_CACHE = 1` is much better and much more universal default. See rationale in #212 and #225. Thanks.
","Official Debian packages have `HDD_ADVISE_NO_CACHE=1` by default ever since -- I've never had any problem with it... As I've said before, I think it is safe to  make this setting default upstream...
",9800119
263,continuous snapshotting,open,2015-05-23T07:25:17Z,2016-01-29T09:17:13Z,,MEMBER,"It would be great if LizardFS could be set to perform continuous snapshotting for some folders with (pre-)defined frequency period. Such system will create a new shapshot only when there are changes and automatically recycle oldest snapshot based on maximum age and available space. Since snapshots could take considerable amount of RAM perhaps limiting maximum memory occupied by snapshots makes sense.

Continuous snapshotting could replace current implementation of trash bin.

Continuous snapshotting is a key feature of [NILFS2](http://nilfs.sourceforge.net/en/) file system.
","What if trash directory separators | were turned back into actual directories and versions added with each revisions of files as they were updates/replaced for the duration of their trash retention.  Move the inode/chunkID/whatever it is from the front to the end with an epoc stamp at the end of the path.  The trash files contents is the path, along with the ID+EPOC should be enough that moving it to undel could still be the recovery mechanic.  Rather than renaming the contents of the file to change the restore location you could execute that by modifying the name as you mv to undel.  If changing the basic behaviour would break compatibility then invent a new `mfsmeta/snapshot` pseudo structure. It would fix the massive flat trash structure and allow for continuous snapshots as operations occur while keeping it out or the normal filesystem view.  perhaps choose between continuous shapshots and normal trash with a new mfssetattribute bit to enable it where desired.  And/or enable/disable trash retention and/or new snapshot retention as a master configuration setting.

`mfsmeta/trash/folder/directory/file/12345678-epoc`
`mv mfsmeta/trash/folder/directory/file/12345678-epoc /mfsmeta/trash/undel/`
`mv mfsmeta/snapshot/folder/directory/file/12345678-epoc /mfsmeta/snapshot/undel/12345678-epoc|alternate|folder|torestoreto|newfilename`
",9800119
264,uneven distribution of data between HDDs,open,2015-05-23T06:07:45Z,2015-12-15T04:57:01Z,,MEMBER,"I had a chunkserver with 6 HDDs utilised over 90%.
Weeks ago I've added 3 new HDDs and after a while they are utilised only up to 30% and remain on that level. There is no active replication. Utilisation of old HDDs dropped to 85...90%.
I'd like to spread data more evenly among HDDs. Is there a way to do so?
Default `ACCEPTABLE_DIFFERENCE = 0.1` (i.e. 10%) does not seems to apply to distribution of data among local HDDs... Is there a way to control local replication?
Thanks.
","There is a typical situation when bad distribution of data between local HDDs hurts:
suppose I have 10 HDDs on a chunkserver -- I mark one HDD for removal and instead of re-distributing data among 9 HDDs, data from HDD marked for removal is sent to other chunkservers. This is causing unnecessary network load and sometimes can trigger `ACCEPTABLE_DIFFERENCE` threshold so eventually data might be sent back...
",9800119
265,reloadable options,open,2015-05-23T05:00:14Z,2017-11-23T13:17:28Z,,MEMBER,"One thing I find quite confusing in LizardFS is how to determine which parameters are re-loadable. For example changes in `mfshdd.cfg` are effective on `systemctl reload lizardfs-chunkserver.service` but changes to options in `mfschunkserver.cfg` (at least `HDD_TEST_FREQ`) needs restart of the daemon. Is it possible to make sure that all configuration from .cfg files are re-loadable by master and chunkserver?
","This is what i've found, maybe i'm wrong.
All of these are reloaded.

it seems that also `HDD_LEAVE_SPACE_DEFAULT` is reloadable as your are triggering a folder scan with `hdd_folders_reinit()` even on reloads, in `hdd_reload`: https://github.com/lizardfs/lizardfs/blob/ebd0c1dec8ec5c75211943048be5eccb8acc1681/src/chunkserver/hddspacemgr.cc#L3756


```
MASTER:
  BACK_LOGS
  TOPOLOGY_FILENAME
  PREFER_LOCAL_CHUNKSERVER
  AUTO_RECOVERY
  DISABLE_METADATA_CHECKSUM_VERIFICATION
  MAGIC_AUTO_FILE_REPAIR
  NO_ATIME
  BACK_META_KEEP_PREVIOUS
  METADATA_CHECKSUM_INTERVAL
  METADATA_CHECKSUM_RECALCULATION_SPEED
  MFSMETARESTORE_PATH
  MAGIC_PREFER_BACKGROUND_DUMP
  REPLICATIONS_DELAY_INIT
  REPLICATIONS_DELAY_DISCONNECT
  OPERATIONS_DELAY_INIT
  OPERATIONS_DELAY_DISCONNECT
  REPLICATIONS_DELAY_INIT
  REPLICATIONS_DELAY_DISCONNECT
  EXPORTS_FILENAME
  MATOTS_LISTEN_HOST
  MATOTS_LISTEN_PORT
  MASTER_HOST
  MASTER_PORT
  BIND_HOST
  ENABLE_LOAD_FACTOR
  MASTER_RECONNECTION_DELAY
  MATOML_LISTEN_HOST
  MATOML_LISTEN_PORT
  MATOML_LOG_PRESERVE_SECONDS
  MATOML_LISTEN_HOST
  MATOML_LISTEN_PORT
  MATOML_LOG_PRESERVE_SECONDS
  MASTER_TIMEOUT
  BACK_META_KEEP_PREVIOUS
  META_DOWNLOAD_FREQ
  MATOCS_LISTEN_HOST
  MATOCS_LISTEN_PORT
  LOAD_FACTOR_PENALTY
  REJECT_OLD_CLIENTS
  SESSION_SUSTAIN_TIME
  MATOCL_LISTEN_HOST
  MATOCL_LISTEN_PORT
  MATOCU_LISTEN_HOST
  GLOBALIOLIMITS_FILENAME
  GLOBALIOLIMITS_ACCUMULATE_MS
  GLOBALIOLIMITS_RENEGOTIATION_PERIOD_SECONDS

CHUNKSERVER:
  READ_AHEAD_KB
  MAX_READ_BEHIND_KB
  CSSERV_LISTEN_HOST
  CSSERV_LISTEN_PORT
  HDD_ADVISE_NO_CACHE
  PERFORM_FSYNC
  HDD_TEST_FREQ
  HDD_PUNCH_HOLES
  HDD_LEAVE_SPACE_DEFAULT
  REPLICATIONS_DELAY_INIT
  REPLICATIONS_DELAY_DISCONNECT
  OPERATIONS_DELAY_INIT
  OPERATIONS_DELAY_DISCONNECT
  AVOID_SAME_IP_CHUNKSERVERS
  REDUNDANCY_LEVEL
  DISABLE_CHUNKS_DEL
  CHUNKS_SOFT_DEL_LIMIT
  CHUNKS_HARD_DEL_LIMIT
  CHUNKS_WRITE_REP_LIMIT
  CHUNKS_READ_REP_LIMIT
  CHUNKS_LOOP_PERIOD
  CHUNKS_LOOP_MAX_CPU
  CHUNKS_LOOP_TIME
  CHUNKS_LOOP_MIN_TIME
  CHUNKS_LOOP_MAX_CPS
  ENDANGERED_CHUNKS_PRIORITY
  ENDANGERED_CHUNKS_MAX_CAPACITY
  ACCEPTABLE_DIFFERENCE
  CHUNKS_REBALANCING_BETWEEN_LABELS
```",9800119
266,infiniband/rdma/verbs,open,2015-05-21T16:56:00Z,2020-02-24T23:59:38Z,,CONTRIBUTOR,"Is there any interest in support for the above?  We've always wanted direct support in moosefs, but now we feel much better about lizardfs upstream, so might be willing to investigate this ourselves.
","Our basic assumption was to assume that the 1U machine *would fail*.  Power code, network cable, SSD backplane(we once had a capacitor burst, and set the circuit board on fire!).  So our local installs placed the OS on a 4-drive raid10, with a small partition, and the rest of the data on each was *directly* exposed to lizardfs via separate mounts on in mfshdd.cfg.

There is no local redundancy at all on a chunkserver.

We then had 2 power drops into the cabinet, split the servers into an A/B setup, and added a custom goal called ""dual_ps"", where we had 3 chunk copies, one on A, one on B, and the third someplace else.


",9800119
267,Prefer local chunkserver+debian updates,open,2015-05-19T21:25:26Z,2015-05-21T16:44:00Z,,CONTRIBUTOR,"This adds the enhancement mentioned in #250.  It also improves the debian packaging a little bit.
","Hmm.  Test cases, eh?  I need to make some time to understand the test case system for lizardfs.  I saw other things in gerrit that interest me.
",9800119
268,Small file speedup paper,open,2015-05-18T02:37:04Z,2017-07-15T16:10:08Z,,NONE,"Hi!, came across an interesting paper:

SepStore: Data Storage Accelerator for Distributed File Systems by Separating Small Files from Large Files

Seems to be a great improvement for MooseFS/LizardFS. The reference implementation was based on MooseFS

Ref:
http://link.springer.com/chapter/10.1007%2F978-3-319-11167-4_27
","Interesting, but we need to keep in mind that leveldb do not scale very well and that LevelDB-based implementation of OSDs in Ceph was a failure from performance prospective...
",9800119
269,CGI: Auto scale charts in [Server Charts / traffic to clients and other chunkservers],open,2015-05-13T00:13:06Z,2015-05-13T01:56:25Z,,MEMBER,"Charts in ""Server Charts"" --> ""traffic to clients and other chunkservers"" are difficult to compare because each servers' chart is auto-scaled on Y-axis. 
For example it is very difficult to find slowest (or most active) server by looking on charts. It would be nice to be able to pin all graphs to certain scale -- user-defined or auto-detected maximum.
","+1.
On 13/05/2015 8:13 am, ""Dmitry Smirnov"" notifications@github.com wrote:

> Charts in ""Server Charts"" --> ""traffic to clients and other chunkservers""
> are difficult to compare because each servers' chart is auto-scaled on
> Y-axis.
> For example it is very difficult to find slowest (or most active) server
> by looking on charts. It would be nice to be able to pin all graphs to
> certain scale -- user-defined or auto-detected maximum.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/lizardfs/lizardfs/issues/273.
",9800119
270,logging rate limiting,open,2015-05-12T01:37:50Z,2015-05-13T22:49:18Z,,MEMBER,"When shadow master run out of disk space it logged over 10 GiB of errors (when interrupted) at a threatening rate: `fwrite error` was logged over 31,000 times per second!
","Not all systems use this rsyslog functionality. I'm on Debian where rsyslog is used by default yet my log was flooded. Besides it would be weird to deploy LizardFS with special rsyslog configuration.

Logging at the maximum speed is a bug. Even if rsyslog can do rate limiting it is silly to use 100% CPU for wrestling with syslog and for logging the same error over long period of time. Is it really that hard to log error once and then wait for some time for situation to improve? You know, logging one such message per minute about 1,860,000 (60*31000) times better. ;)

IMHO there is no need for smarter logging _library_ but there is very apparent need for smarter error _handling_.
",9800119
271,cache tier (master level),open,2015-05-11T05:01:18Z,2020-02-22T20:49:50Z,,MEMBER,"One of the LizardFS features we are badly missing is global cache tier. We have some fast servers with little storage capacity but fast network and SSDs. We would like to use them as LizardFS cache tier to direct most (or maybe all) client requests to those fast servers in order to improve performance. However currently there seems to be no way to raise priority of some chunkservers to make them serve clients first.
From implementation prospective such frontend chunkservers could act as specially labelled nodes taking most writes and then pushing data to other labels, possibly caching some. It could be easy to keep hints of frequently accessible data on master and use this data to optimise data placement with prioritising client access to faster chunkservers.
","Anyone hear anything in this regard now that it's been the better part of half a decade since this was opened?

I also would love to implement an SSD caching mechanism at the master level. Sitting on 2TB of SSD storage in the master just begging to become a cache. 

What if any workarounds have other used?",9800119
272,cache tier (chunkserver),open,2015-05-11T04:51:31Z,2016-03-07T12:16:01Z,,MEMBER,"It could be very helpful for performance if chunkservers could place frequently accessible data to faster HDDs and move chunks between local hard disks according to cache hits. It would allow chuinkservers to perform very well on servers with many rotational HDDs and just one SSD.
","As @onlyjob wrote, for a use case like this most standard local caching solutions do not add a large improvement. tracking the (non-checkloop!) accesses to a chunk allows to cache what is a ""hot zone"". 
This is what any ""real"" storage does, and what is currently not implemented in bcache/enhanceio/lvm/flashcache. zfs is a very large hammer for this problem.
Also please keep in mind it less efficient to that end than having some cache layer at the distributed fs level:
You have lower hitrate and slower cache warmup - even if one chunkserver already cached this block it will still be uncached on another.

The other bad mistake is to read through the cache at all times (i.e. flashcache, the first version of the ceph cache tier) or blindly skip sequentatial IO. 
",9800119
273,controlling chunkserver/client priority,open,2015-05-11T04:45:23Z,2016-08-09T06:59:33Z,,MEMBER,"Some of our servers have high storage capacity to network bandwidth ratio. When such server is online LizardFS may feel slower because apparently more requests queued to server with slow link and many HDDS than to servers with less HDDs but faster network connection. Taking down chunkserver behind slower link seems to improve access time.

Another case is server with slow HDDs but fast network. Similar to the above situation taking down this server seems to improve access time and user's experience.

Is it possible to provide hints to some chunkservers in order to reduce or minimise their communication with clients? Maybe an option to limit maximum number of requests to clients per second? We would still like to use those servers for redundancy and replication but would like to reduce their interaction with clients to have better control over distribution of load. 

Thanks.
","Bump for topology proximity and CS label wildcards and/or multiple CS label support.
",9800119
274,broken group permissions,open,2015-03-28T07:17:12Z,2019-10-28T10:26:15Z,,MEMBER,"User who is a member of group `common` is unable to access a folder on LizardFS:

```
drwxrws---   2 root common  0 Mar 28 17:47 test
```

Similar folder in `tmpfs` is perfectly accessible. LizardsFS is mounted with default mount options: `rw,relatime,user_id=0,group_id=0,default_permissions,allow_other`.

Same problem exist with group-writeable files: user can modify them only when I set `o+w` flag.
","Hello.

I added a comment on #394 since it should not be closed for me since it's not working.

The `getfacl` and `setfacl` commands work properly but a simple execution test fails.",9800119
275,test suite mess,open,2015-03-18T21:47:17Z,2019-08-27T08:57:10Z,,MEMBER,"I'm struggling to run _automated_ tests and I recognise numerous problems with test suites.
### Test suite is not suitable for automated testing.

Due to problems listed below.
### Lack of post-build unit tests.

Some tests are better to run after successful build. For instance code style test ""SanityChecks.test_code_style"" is better to run using CMake's `ctest` by adding `ENABLE_TESTING()` and `ADD_TEST` to `CMakeLists.txt`.

Such test may require `cppcheck` but should not depend on `git` because compilation may be done from source tarball. 

Besides not only code style check do not belong to run-time test suite but also it unnecessary depends on `git` and do not fail when the latter is missing.

Let's start by adding test for internal CRC function(s) (not crcutil's) to validate that they are working correctly on big-endian and little-endian architectures?
### Non-packageable/non-installable run-time tests.

Run-time test suite should be packageable/installable so it could run without access to source code which may not be available on build server where running run-time tests may not be possible.

Packaging run-time tests allows to define dependencies.
### Undocumented requirements.

Tests assume presence of the following packages (on Debian):

```
attr
dbench
fuse
netcat-openbsd
socat
sudo
valgrind
```

I had to restart test suit at least six times because some dependency was missing.
### Run-time tests require internet access/

It should be possible to run tests in clean environment without internet access.

Please don't `git clone` as such tests are doomed to fail in secure environment.
Build tests and run-time tests should be separated.
### Wireshark plugin.

SanityChecks.test_wireshark_plugin_generation do not belong to run-time tests.
Besides it should be possible to build plugin without wireshark sources but only with packages `libwireshark-dev`/`wireshark-dev` (on Debian).

I would like to see wireshark plugin shipped in the dedicated package.
### Non-trivial tests (not separated to dedicated test suite)

On standard system tests that require something special should be separated to special test suite. One of such tests is
- ShortSystemTests.test_acl_permissions (requires patched libfuse)

The following tests are optional (it is not necessary to build LizardFS with polonaise):
- ShortSystemTests.test_polonaise
- ShortSystemTests.test_metadata_polonaise

hence they should be separated in standalone test suite.
- ShortSystemTests.test_moosefs_upgrade

is also optional -- it requires to build patched services so it is hard run especially if no source code and build tools are available. This test should not be run by default because it has special requirements.
### Multiple minor `setup_machine.sh` problems.
- ""Prepare loop devices"" fails to mount loop devices because `loop` module is not loaded automatically (required `modprobe -v loop`)
- It is no longer needed to add users to group `fuse` because (on Debian) all users can mount fuse file systems by default since ""fuse_2.9.3-5"" uploaded in December 2013.
- Misuses `/etc/lizardfs_tests.conf`: it appears to me that once configuration file exists `setup_machine.sh` should use it to handle mount points and other defaults.
- Second invocation of `setup_machine.sh` may produce unexpected results.
- If _setup_ action performed with volatile mount points (e.g. `/tmp/hda`) then next reboot will fail into recovery mode because mount points from `/etc/fstab` do not work. Somwhere it is worth mentioning that mount points should be persistent and that it is not a good idea to use `/tmp` because it will be cleared on reboot...
### Misuse of `fuse` group.

Some tests depend on existence of group ""fuse"" which is wrong because such group may not exist (or exist for another purpose). As I mentioned earlier `fuse` group is not needed any more hence tests should not rely on it to avoid collisions.
",,9800119
276,undergoal chunks on (normal) restart of chunkserver,open,2015-02-17T05:03:00Z,2018-05-19T15:49:55Z,,MEMBER,"On latest 2.5.2 some (usually about 4...12) _undergoal_ chunks appear every time I restart any chunkserver. When there are enough chunkservers available all pending writes should be successfully re-directed to other chunkservers when a particular chunkserver is stopped.

Please make chunkservers' restart more robust to avoid risk of having endangered/undergoal chunks on normal restarts. I'm especially concerned about danger of simultaneous restart of two or more chunkservers...
Thanks.
",Is this still open?,9800119
277,"FTBFS on ppc64 (powerpc, big-endian)",open,2015-02-03T17:29:06Z,2015-04-03T13:59:53Z,,MEMBER,"Master FTBFS on ppc64:

```
Linking CXX executable mfsmount 
cd /var/tmp/src/lizardfs/githead/build/src/mount/fuse && /usr/bin/cmake -E cmake_link_script CMakeFiles/mfsmount.dir/link.txt --verbose=1 
/usr/bin/c++   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2  -pipe -std=c++0x -pthread -Wall -Wextra -fwrapv -pedantic -O3 -DNDEBUG -O3 -DNDEBUG -g   -Wl,-z,relro -Wl,-z,now -Wl,--as-needed CMakeFiles/mfsmount.dir/main.cc.o  -o mfsmount -rdynamic libmount_fuse.a -lfuse ../libmount.a ../../common/libmfscommon.a ../../../external/libcrcutil.a -lz -lrt  
../libmount.a(lizard_client.cc.o): In function `std::__atomic_base<unsigned long long>::load(std::memory_order) const': 
/usr/include/c++/4.9/bits/atomic_base.h:500: undefined reference to `__atomic_load_8' 
/usr/include/c++/4.9/bits/atomic_base.h:500: undefined reference to `__atomic_load_8' 
../libmount.a(lizard_client.cc.o): In function `std::__atomic_base<unsigned long long>::operator++()': 
/usr/include/c++/4.9/bits/atomic_base.h:408: undefined reference to `__atomic_fetch_add_8' 
/usr/include/c++/4.9/bits/atomic_base.h:408: undefined reference to `__atomic_fetch_add_8' 
/usr/include/c++/4.9/bits/atomic_base.h:408: undefined reference to `__atomic_fetch_add_8' 
/usr/include/c++/4.9/bits/atomic_base.h:408: undefined reference to `__atomic_fetch_add_8' 
/usr/include/c++/4.9/bits/atomic_base.h:408: undefined reference to `__atomic_fetch_add_8' 
../libmount.a(tweaks.cc.o): In function `std::__atomic_base<unsigned long long>::store(unsigned long long, std::memory_order)': 
/usr/include/c++/4.9/bits/atomic_base.h:478: undefined reference to `__atomic_store_8' 
../libmount.a(tweaks.cc.o): In function `std::__atomic_base<unsigned long long>::load(std::memory_order) const': 
/usr/include/c++/4.9/bits/atomic_base.h:500: undefined reference to `__atomic_load_8' 
collect2: error: ld returned 1 exit status 
```
","@psarna: Piotr, I modified your patch as follows:

```
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -101,8 +101,14 @@
 if(ENABLE_WERROR)
   set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Werror"")
 endif()

+# Check if architecture needs atomic library
+check_function_exists(__atomic_load_8 HAVE___ATOMIC_LOAD_8)
+if(NOT HAVE___ATOMIC_LOAD_8)
+  check_library_exists(atomic __atomic_load_8 """" HAVE_LIBATOMIC)
+endif()
+
 if(ENABLE_TESTS AND NOT THROW_INSTEAD_OF_ABORT)
   message(STATUS ""Tests require THROW_INSTEAD_OF_ABORT to be set to YES, changing passed value:"")
     set(THROW_INSTEAD_OF_ABORT YES)
   message(STATUS ""THROW_INSTEAD_OF_ABORT: ${THROW_INSTEAD_OF_ABORT}"")
--- a/src/common/CMakeLists.txt
+++ b/src/common/CMakeLists.txt
@@ -12,5 +12,8 @@
 endif()
 if (UNIX AND NOT APPLE)
   target_link_libraries(mfscommon ${RT_LIBRARY})
 endif()
+if(HAVE_LIBATOMIC)
+  target_link_libraries(mfscommon ""atomic"")
+endif()
 add_tests(mfscommon ${COMMON_TESTS})
```

It worked on _powerpc_ but added `-latomic` on _amd64_ as well...
",9800119
278,Feature Request - CHUNKS_IGNORE_MISSING_LABEL option.,open,2015-01-27T23:58:15Z,2015-01-28T00:03:10Z,,NONE,"Related to #245 I would like to request a new feature:

> marcinsulikowski commented:
> When designing this mechanism we were thinking like this: if there are no servers to satisfy given constraints (e.g., someone requested creation of copies on servers labelled xxx, but there are no such servers), then something must be really wrong. If something is really wrong, there is no possibility to act correctly, so LizardFS should at least save data from being lost. Really, no servers is definitely not a normal situation :). In such case, LizardFS tries to make data safe by keeping number of replicas greater of equal to the desired number of replicas, even if it has to choose a server that user doesn't expect to be chosen. For example in case of a goal some_goal: locationA locatioB (a typo in the second label) keeping only a single replica in locationA is probably never a good idea -- any single failure might cause a loss of data. Moreover, in case of two typos some_goal: locatioA locatioB, creating chunks with no copies is probably even worse.

With that said, specifically ""there is no possibility to act correctly"", adding an explicit CHUNKS_IGNORE_MISSING_LABEL option allows for operators to at least determine what the ""best"" correct action to take might be for their situation. Perhaps ""something must be really wrong"" and the current action is appropriate, but an operator may be able to decide that doing nothing is the most correct action.

A new CHUNKS_IGNORE_MISSING_LABEL option could be assigned a value of {0..99}, 99 being the default.
- For each missing labeled chunkserver, this value would dictate how many times _ can be used for a given missing labeled chunkserver.
- 0 == no further replication to non-matching labels would be attempted, 0 missing labels will be assumed to be _.
- 1 == 1 missing label will be treated as a _, 2 missing labels would not cause additional replication.
- 2 == 2 missing labels would be tolerated without causing any undergoal chunks.
- 99 == DEFAULT, conceivably all missing labels will be replicated to _.
  : This might be more powerful and versatile if it were given as an option for each custom goal definition in mfsgoals.cfg
  :: 11 GoalLabel : A B C : {0..99}, default of 99 if not defined.

At https://github.com/lizardfs/lizardfs/commit/7a30ed1c179e6e3f2b0afab24bf0278a6dbcec30#diff-6b9999dcc3387035fb4462c6703de04dR1949

```
 -      } else if (vc + skippedReplications >= c->expectedCopies()) {
 +      } else if (vc + skippedReplications >= c->expectedCopies()) || (CHUNKS_IGNORE_MISSING_LABELS < skippedReplications) {
 +                 // Don't create copies on non-matching servers if ignore count less than missing replicas.
```

The CHUNKS_IGNORE_MISSING_LABEL option should come with a warning which indicates that it's use could lead to chunks becoming undergoal, endangered, missing, or even never_created.
- A value of 0 would be discouraged as in the case where all labels were invalid then 0 chunks of a new file might be written.
- A value of 1 might leave chunks in an endangered state, but this may be acceptable in some situations and it is impossible for the LizardFS master process to know that.
- A value of 2 would always guarantee no chunks become endangered but might leave them undergoal depending on the number of chunkservers defined for a given goal label.

I can imagine a number of cluster use cases where it would be more desirable to not spend the bandwidth and time replicating such chunks to inappropriate chunkservers.

In Congruent - A Cluster composed of 5 \* 20TB with label A and 2 \* 100TB with labels B and C.
- Without custom labels a goal=3 is used, each of B & C absorb ~100% of chunks, but this can not be ensured.
- Given a custom label of ""A B C"", it can be guaranteed that B & C each have a complete set of all chunks.
- If it is acceptable for B and/or C to go offline, CHUNKS_IGNORE_MISSING_LABELS=0
- Note that prior to using a custom label, the goal would need to be reduced from 3 to 2, which can be done with no service transitions.
  *\* Using custom goals, the custom label would need to be modified to exclude the missing B and/or C and services reloaded.
  *\* Given such a new IGNORE option, no intervention would be required given the transient or permanent loss of B and/or C.
  *\* Should this new state of -B or -C persist then an operator would be required to make a more lasting configuration change, but no 3AM intervention needed because the cluster know to just do nothing.
  *\* Doing nothing is acceptable because if one of B or C goes offline then there remains 2 copies.
  *\* The Set of servers in A may not have sufficient storage to satisfy replication to 3 copies.
  *\* Attempting to replicate the missing chunks would add to disk and network load lowering that available to clients and perhaps introducing additional disk full situations within other parts of the cluster.
- Presuming that A does have the available capacity, you might choose CHUNKS_IGNORE_MISSING_LABELS=1 to avoid endangered chunks.

Slow Nodes - A Cluster composed of a core of 2 \* 100TB Labeled A & A, 20TB Labeled B, 20TB Labeled C, etc...
- Each remote copy at B or at C is know to periodically go offline or become inaccessible for some acceptable period of time.
- It is acceptable for users at sites B and C to loose this resource should connectivity become a problem. (READ_ONLY_SHADOW would be neat here)
- It's shadow masters do not care as they resume and update the metadata when they return, and are not a part of any meta failover.
- CHUNKS_IGNORE_MISSING_LABELS=0 would be used to avoid attempting to replicate missing site B to slow site C, potentially saturating C and causing outage.

Cold Storage - A Cluster composed in part of label A B C D E each with a full set of chunks and a shadow master or metalogger.
- There could be additional subset servers and labels with varying usages.
- Any number of the set A B C D E could go offline for cold storage with everything required to run independently.
- When a previously offline server returns it would get new metadata and changes to chunks would be replicated.
- CHUNKS_IGNORE_MISSING_LABELS=0 would be desirable in this situation.
","Directly related, it would be nice if the cgi ""Config"" page ""Goal definitions"" table would show defined labels which are missing in red text. In a manor similar to the cgi ""Servers"" page reporting of ""disconnected !!!"" chunk servers.
",9800119
279,client: sparse file support [feature],open,2014-12-29T01:35:46Z,2018-04-16T10:08:21Z,,MEMBER,"Please implement support for [sparse files](https://en.wikipedia.org/wiki/Sparse_file). Thanks.
","A fraught area, could well be an issue for qemu rather than lizardfs.",9800119
280,"[Minor] We need an official name for ""disks"" to help distinguish them from other datastores",open,2014-12-26T09:44:43Z,2018-08-22T03:06:47Z,,NONE,"Hi there,

I think it would be handy to decide on an official name for ""disks"". The name isn't always accurate as the actual datastore could be an SSD, tape (unlikely), a hard drive, or even a RAMdisk (if you're flat out crazy). I'm totally fine if this name ends up being ""disks"", but I have a suggestion.

I think it would be a great idea to call volumes ""bricks"". One downside is obviously the negative associations with the term, but it makes a lot of sense in metaphors and marketing - it could lead to slogans like ""A solid foundation for your data"" or something cheesy like that. At work we actually already use ""/mfsbrick/"" as the disk name on many of our systems, as well as others like ""/mfsbrick.jbod.01/"" and ""/mfsbrick.raid/"". Brick ends up keeping things much less ambiguous for us, which is important because we have MooseFS bricks of pretty much every variety you could imagine at the moment (and are working on standardizing).

Just a thought :)
",Bumping this as it's still relevant today.,9800119
281,clean-up python code,open,2014-12-21T23:27:31Z,2019-08-27T10:13:11Z,,MEMBER,"`pylint` reports too many warnings in `mfscgiserv`, `chart.cgi` and `mfs.cgi`.
All python code rated very low by _pylint_. 
In most cases improvements are trivial but nevertheless important.
","I made CGI script `src/cgi/lizardfs-cgiserver.py.in` [`pylint`][1]-compliant in commit 5f8bb8b8e689060558. I will double check rest of the `*.cgi` files. We should migrate CGI scripts to Python 3 anyway (#665), so this fix will be rather temporary.

[1]: https://www.pylint.org",9800119
282,cgi: please expose build information,open,2014-12-14T18:03:29Z,2014-12-29T12:41:02Z,,MEMBER,"CGI interface expose very useful version info in ""Servers"" tab. In addition to numeric version it will be useful to expose custom build string passed as `-DBUILD_INFO=-3~local1 (amd64)`, for example. The purpose of such string is to allow package maintainer to expose build date, architecture and (most important) package revision. Package may contain important fixes so it is useful to see its revision. It would be convenient to output build information appended to numeric version in ""version"" column of the CGI  interface. Thanks.
",,9800119
283,Multimaster Shadowmaster Read Scaling,open,2014-12-13T01:50:24Z,2019-08-27T09:39:42Z,,NONE,"Hey there,

Would it be possible to scale read requests by simply allowing clients to connect to any of the masters? Since they have the full dataset they should be able to serve those requests just fine.

For extra fancy sauce, they could send writes along to the actual Master transparently. This way we would achieve much of what people are describing when they talk about loadbalancing MFS/LFS.

Just thoughts :)

Thanks,
~ Benjamin
","As was commented in #227 by @marcinsulikowski the protocol between the master and the shadows is asynchronous. Wouldn't it be a problem if you allow a client to send a read query and get stale data since the shadow hasn't caught up with master?

Personally I find this to be the main deficiency in LizardFS and it makes it lot harder ""sell"" to my organization. I would love to have the ability to enable synchronous mode between the replicas. This way you could achieve read scalability as well as, more importantly for me, eliminate the risk of loosing/corrupting data in the case of a master failover as we have today.
",9800119
284,please drop redundant service management features,open,2014-12-12T10:53:36Z,2019-08-27T09:36:08Z,,MEMBER,"Daemons have not very useful features duplicating functionality of system service managers and not doing the job very well (e.g. #215). For instance ""stop"" and ""restart"" options seems to merely send signals ""SIGTERM"" and ""SIGHUP"" respectively. The following fragment of code from .service file 

```
ExecStop=/usr/sbin/mfschunkserver stop
ExecReload=/usr/sbin/mfschunkserver reload
```

can be reduced to 

```
ExecReload=/bin/kill -HUP $MAINPID
```

unless services do anything more intelligent than sending signal to the main process identified by PID file on stop/restart. Why not let service manager do the job?

Please note that systemd.service(5) says the following:

> Note however that reloading a daemon by sending a SIGHUP signal is usually not a good choice, because this is an asynchronous operation and hence not suitable to order reloads of multiple services against each other. It is strongly recommended to set ExecReload= to a command that not only triggers a configuration reload of the daemon, but also synchronously waits for it to complete.

I recommend to drop stop/restart code from daemons or make sure it does what's described above. At the moment it seems unnecessary to maintain redundant service management functionality in daemons.

Also it would be helpful/useful to implement `--pid` option to specify location of PID file.

P.S. I couldn't find any information regarding actions `test` and `isalive` -- how do they work? They seems to be unnecessary as well. How `kill` is different from `stop`?
","> maybe submit a pull request with new init scripts?

Sigh... I feel discouraged to do that: 
- At least one new (cgiserv) script needs obsolete service management functionality to be removed as per #219.
- Other scripts are probably needed only until LizardFS makes into official Debian repository. After that I have no intentions to maintain scripts in two places.
- It is a fairly big rewrite -- I anticipate that Marcin Sulikowski will say something like that it is incompatible and the pull request will probably share destiny of other rejected pull requests...
- At the moment I'd like to focus on more important requests...
",9800119
285,"cgi: feature ""safe to remove""",open,2014-12-11T23:20:59Z,2014-12-29T12:53:00Z,,MEMBER,"""Disks"" section show volumes' status as ""ok"" or ""marked for removal"" -- the latter apparently never changes.
It would be nice to provide indication when volume is safe to remove.
Thanks.
","Currently you can consider disks as ""safe to remove"" when there are no endangered/undergoal (depending on your requirements) chunks in the ""regular chunks state matrix"" (`http://<cgi-host>:9425/mfs.cgi?sections=IN&INmatrix=1`). This matrix shows number of copies for each goal, but counts only copies on disks that aren't marked for removal.
",9800119
286,Please expose recursive directory size,open,2014-12-10T09:42:36Z,2019-05-31T08:43:27Z,,MEMBER,"`linux-doc-3.16/Documentation/filesystems/ceph.txt`
describes a very nice CephFS feature:

> Ceph also provides some recursive accounting on directories for nested
> files and bytes.  That is, a 'getfattr -d foo' on any directory in the
> system will reveal the total number of nested regular files and
> subdirectories, and a summation of all nested file sizes.  This makes
> the identification of large disk space consumers relatively quick, as
> no 'du' or similar recursive scan of the file system is required.

I'd very much like to see this awesome feature implemented in LizardFS.
I reckon it should be relatively easy to implement because all metadata is already in RAM...
","@onlyjob @Zorlin Wiki on GitHub is now available (https://github.com/lizardfs/lizardfs/wiki). It's not much there, but I hope that it would gradually change :).
",9800119
287,"Rename config files (drop ""mfs"" prefix)?",open,2014-12-08T08:08:56Z,2019-08-27T09:33:39Z,,MEMBER,"I'm wondering whether dropping ""mfs"" prefix from config files' names would worth it?
From aesthetic prospective names like `exports.cfg` (instead of `mfsexports.cfg`) are nicer and could be helpful to distinguish LizardFS from MooseFS. On the other hand compatible names of config files may be handy for those who migrates from MooseFS to LizardFS but in such case renaming or symlinking files is just a little effort which is probably not worth considering... Any thoughts?
","Years passed and LizardFS should no longer delay resolving conflicts with MooseFS.
Hijacking MooseFS' name space is hostile and the problem still affects names of the executables, config files, etc. It should be possible to install LizardFS and MooseFS side by side without conflicts - at least chunkservers and clients.",9800119
288,Finish removing references to MooseFS and MFS,open,2014-11-24T12:21:56Z,2019-08-27T08:43:52Z,,NONE,"Hi there,

It would be awesome if we could completely strip ""MFS"", ""mfs"", ""MooseFS"" and variants thereof from LizardFS (with the exception of the CHANGELOG and other crucial areas) - at this point it just serves to add confusion.

I can't easily do a pull request right now, so instead here's the diff where I made these changes. I also documented below the best way to make the changes. https://github.com/Zorlin/lizardfs/commit/bf4e09510a0cb725ca449b09d512b3befd610b15

Thanks,
~ Benjamin
","There are 2180 `MFS` and 545 `MooseFS` strings in the project (case insensitive search)...

It requires much work to verify which ones can be substituted with `lfs` or `LFS`. Current development is focused on bug fixing in `3.13-rc1`, thus any help (_pull requests_) will be greatly appreciated. :1st_place_medal: 

    patryk@patryk:/tmp$ git clone https://github.com/lizardfs/lizardfs.git
    Cloning into 'lizardfs'...
    remote: Enumerating objects: 5, done.
    remote: Counting objects: 100% (5/5), done.
    remote: Compressing objects: 100% (5/5), done.
    remote: Total 21750 (delta 0), reused 1 (delta 0), pack-reused 21745
    Receiving objects: 100% (21750/21750), 11.66 MiB | 3.24 MiB/s, done.
    Resolving deltas: 100% (16535/16535), done.
    patryk@patryk:/tmp$ grep -Ri mfs lizardfs | wc -l
    2180
    patryk@patryk:/tmp$ grep -Ri moosefs lizardfs | wc -l
    545",9800119
289,Questions about testing configure and installation,open,2014-09-16T10:17:39Z,2019-08-27T09:14:37Z,,NONE,"1. Could you please add `clean` function for `setup_machine.sh`?<br />
Because I found that the output files generated by `setup_machine.sh` is different from the first ones if I ran it again. For example, there are only 3 lines in `/etc/sudoers.d/lizardfstest`:

       [root@mfs4 tests]# cat /etc/sudoers.d/lizardfstest
       
       ALL ALL = (lizardfstest) NOPASSWD: ALL
       ALL ALL = NOPASSWD: /usr/bin/pkill -9 -u lizardfstest
       lizardfstest ALL = NOPASSWD: /bin/sh -c echo\ 1\ >\ /proc/sys/vm/drop_caches

When the testing is done, I would like to remove the testing environment. The several tests tools and lizardfs programs generated by `cmake`, `make` and `make install` from the directory `lizardfs/tests` are also considered to be cleaned.

2. It seems that the file `/etc/lizardfs_tests.conf` does not work. I set: `${LIZARDFS_ROOT:=/var/lib/lizardfstest/local}`, but it is still installed into `/usr/local`. I have to use `cmake .. -DENABLE_TESTS=YES -DCMAKE_INSTALL_PREFIX=/var/lib/lizardfstest/local/`.","Related bug: #258 ""_test suite mess_"".
",9800119
290,Chef cookbook for lizardfs,open,2013-10-24T19:27:09Z,2017-11-24T09:11:48Z,,CONTRIBUTOR,"Related link:
https://github.com/menglifang/cookbooks/blob/master/moosefs/libraries/helper.rb
","OK, it does make sense now! And the reason is then simple - your ticket was so sparse that we haven't deciphered it in four years. Could you please edit the opening message with more details like what you propose and possibly a working example?",9800119
291,Feature/optimization of slashing,open,2020-03-27T10:31:15Z,2020-03-27T11:34:05Z,,CONTRIBUTOR,,rerun tests,161629033
292,"Initiated parameter proposal, ‘zeroProduceCumulativeTime’ boundary value problem；",open,2020-03-27T09:59:00Z,2020-03-27T09:59:00Z,,NONE,"Consensus rounds is 4, zeroProduceCumulativeTime set to 5, 
Initiate parameter proposal successfully",,161629033
293,"Two new parameters of zero production set to 1, 1， Penalty block problem；",open,2020-03-27T09:47:21Z,2020-03-27T10:01:21Z,,NONE,@WeiLoy ,,161629033
294,bad block when upgrade to version 0.11.0,open,2020-03-25T12:20:19Z,2020-03-27T08:01:53Z,,NONE,"Hi there,

from 0.10.1 upgrade to version 0.11.0, send wasm contract before and after upgrade ， bad block when synchronized with version 0.11.0 

",,161629033
295,"Submit parameter proposal and read the generated configuration;""paramProposalVoteDurationSeconds""",open,2020-03-25T10:15:19Z,2020-03-27T06:45:45Z,,NONE,"Submit parameter proposal and read the generated configuration; ""paramProposalVoteDurationSeconds""

",fixed. apply version 0.10.0 (update the paramProposalVoteDurationSeconds value) only for chainID=101 or 299.,161629033
296,"Upgrade node, appears “BAD BLOCK”；The error message is“invalid vrf prove”",open,2020-03-23T10:12:56Z,2020-03-25T02:13:14Z,,NONE,@WeiLoy ,"this is a bug when snapotdb open before blockchain,so snaoshot db can't recover from blockchain.
fix：
the snapshotdb will open baseDB instad ,https://github.com/PlatONnetwork/PlatON-Go/pull/1288",161629033
297,Feature/delegation rewards onchain,open,2020-03-23T10:12:03Z,2020-03-23T10:30:31Z,,CONTRIBUTOR,,,161629033
298,"The slashblocksreward parameter is set to 0, causing the chain to stop exiting",open,2020-03-23T05:49:33Z,2020-03-27T05:53:30Z,,CONTRIBUTOR,"version：0.11.0
*commitID | d562fd3b66af9f374c86d4069a9bc28970655bef


",already fixed,161629033
299,"In the abnormal scenario test, the chain stops producing blocks.",open,2020-03-16T03:04:26Z,2020-03-27T05:53:57Z,,CONTRIBUTOR,"Hi there,

In the abnormal scenario test,The initial 25 consensus nodes simulate a scenario where zero blocks are penalized. In the end, only the last node is left, but the chain stops producing blocks.

#### System information

PlatON version: 0.10.2
OS & Version: Windows/Linux/OSX
Commit hash : https://github.com/PlatONnetwork/PlatON-Go/commit/e5ea70a9db7efe962a7cd310efa7affe1a71bc5d

#### Expected behaviour

there is only one consensus node and continue to produce blocks.

#### Actual behaviour

 stops producing blocks

#### Steps to reproduce the behaviour

The initial 25 consensus nodes simulate a scenario where zero blocks are penalized

#### Backtrace

````
DEBUG[01-01|20:27:01.035|x/handler/vrf_handler.go:139]              Storage previous nonce                   current blockNumber=1230 parentHash=15c6c488364559640773eacc4c6a5c750181ad995b98f67f4fd21774b9839f00 current hash=0000000000000000000000000000000000000000000000000000000000000000 valueLength=25 MaxValidators=25
INFO [01-01|20:27:01.035|x/handler/vrf_handler.go:160]              Storage previous nonce Success           current blockNumber=1230 parentHash=15c6c488364559640773eacc4c6a5c750181ad995b98f67f4fd21774b9839f00 current hash=0000000000000000000000000000000000000000000000000000000000000000 valueLength=25 MaxValidators=25 nonce=02dec1fff070f9b70492d2c902153e0f84c44734d3b3176de822c1d0e4d4616556b77a26c31f41dcc2fb56216b200b59022a795b90d8f3ced7d3b5fa695c1e4a436cc9c13655418991f0f8d6c5612310a2   firstNonce=927d5e489deba4e4f44dc19ef6e4b781aaf8094b4eb3ea4f0c6b4ace6bc8bf2a lastNonce=dec1fff070f9b70492d2c902153e0f84c44734d3b3176de822c1d0e4d4616556
ERROR[01-01|20:27:01.035|x/plugin/staking_plugin.go:3083]           Not Found epoch validators, the queue is zero isCommit=false start=1 end=10000 current blockNumber=1230 current blockHash=0x0000000000000000000000000000000000000000000000000000000000000000
ERROR[01-01|20:27:01.035|x/plugin/reward_plugin.go:450]             AllocatePackageBlock IsCurrVerifier fail err=""The validator is not exist""                                                                                                                                               blockNumber=1230 blockHash=000000…000000
ERROR[01-01|20:27:01.035|miner/worker.go:1265]                      Failed to GetReactorInstance EndBlocker on worker blockNumber=1230 err=""The validator is not exist""
ERROR[01-01|20:27:01.035|miner/worker.go:1140]                      Failed to commitNewWork on worker: call commit is failed blockNumber=1230 err=""The validator is not exist""
DEBUG[01-01|20:27:01.035|consensus/cbft/cbft.go:1178]               Calc next block time                     epoch=5 view=1584                                                                                blockTime=2020-01-01T20:27:01+0800  now=2020-01-01T20:27:01+0800 produceInterval=1s period=10000 amount=10                      interval=3.232722ms         rtt=200ms         executeTime=400ms
DEBUG[01-01|20:27:01.035|miner/worker.go:1053]                      Next block time                          time=""2020-01-01 20:27:01.632""
DEBUG[01-01|20:27:01.176|consensus/cbft/cbft.go:432]                Receive synchronization related messages from peer epoch=5 view=1584                                                                                peer=ce9ca7e745cd783e type=*protocols.GetLatestStatus      msgHash=2f1386…dbc0d8 BHash=15c6c4…839f00 msg={BlockNumber:1229,BlockHash:0x15c6c488364559640773eacc4c6a5c750181ad995b98f67f4fd21774b9839f00,QuorumCert:{Epoch:5,ViewNumber:26,Hash:15c6c4…839f00,Number:1229,Index:8,ValidatorSet:BA{1:x}},LBlockNumber:1228,LBlockHash:0xb6143486c78e5927589d7010ba1175f5fee47f560546e55dd602e95fe1636db3,LQuorumCert:{Epoch:5,ViewNumber:26,Hash:b61434…636db3,Number:1228,Index:7,ValidatorSet:BA{1:x}},LogicType:1}                                                 syncMsgCh=0
DEBUG[01-01|20:27:01.176|consensus/cbft/sync_process.go:388]        Received message on OnGetLatestStatus    epoch=5 view=1584                                                                                from=ce9ca7e745cd783e logicType=1 msgHash=2f1386…dbc0d8 message={BlockNumber:1229,BlockHash:0x15c6c488364559640773eacc4c6a5c750181ad995b98f67f4fd21774b9839f00,QuorumCert:{Epoch:5,ViewNumber:26,Hash:15c6c4…839f00,Number:1229,Index:8,ValidatorSet:BA{1:x}},LBlockNumber:1228,LBlockHash:0xb6143486c78e5927589d7010ba1175f5fee47f560546e55dd602e95fe1636db3,LQuorumCert:{Epoch:5,ViewNumber:26,Hash:b61434…636db3,Number:1228,Index:7,ValidatorSet:BA{1:x}},LogicType:1}

````
",all validators has been slashing，this mechenism will be changed in 0.11.0,161629033
300,Restart node panic,open,2020-03-12T02:08:16Z,2020-03-27T06:03:00Z,,CONTRIBUTOR,"Hi there,

When I restart after modifying ""NetworkId"", the node appears panic

#### System information

PlatON version: `0.10.2`
OS & Version: Windows/Linux/OSX
Commit hash : e5ea70a9db7efe962a7cd310efa7affe1a71bc5d

#### Expected behaviour

restart success
#### Actual behaviour
panic

#### Steps to reproduce the behaviour

1.init 
2.start node
3.stop node
4.update NetworkId
5.start node
#### Backtrace

````
INFO [03-11|14:22:57.412|core/snapshotdb/snapshotdb.go:238]      begin recover                            package=snapshotdb path=/home/platon/trantor_test/node-16789/data/platon/snapshotdb
INFO [03-11|14:22:57.412|core/snapshotdb/snapshotdb.go:242]      load current                             package=snapshotdb base=&{Num:+230} high=""&{Num:+240 Hash:[241 211 30 150 189 59 248 142 69 186 5 0 199 143 226 255 166 114 18 76 169 89 18 25 113 30 59 94 235 239 211 137]}""
ERROR[03-11|14:22:57.412|core/snapshotdb/snapshotdb.go:244]      recover db fail:                         package=snapshotdb error=""current baseNum and highestNum is not same,but not wal find""
ERROR[03-11|14:22:57.412|core/snapshotdb/snapshotdb.go:192]      init db fail                             package=snapshotdb err=""current baseNum and highestNum is not same,but not wal find""
panic: current baseNum and highestNum is not same,but not wal find

goroutine 1 [running]:
github.com/PlatONnetwork/PlatON-Go/core/snapshotdb.Instance(0x0, 0x0)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/core/snapshotdb/snapshotdb.go:193 +0x33f
github.com/PlatONnetwork/PlatON-Go/x/gov.get(0x84c1dc5b4e8bc5db, 0x68a85898147c50ab, 0x27b9c772b0c31e9, 0x17b806a004157381, 0xc0000381e0, 0x21, 0x21, 0x21, 0x21, 0x60, ...)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/x/gov/gov_snapdb.go:29 +0x26
github.com/PlatONnetwork/PlatON-Go/x/gov.findGovernParamValue(0x13caf21, 0x5, 0x13dd7a9, 0x10, 0x84c1dc5b4e8bc5db, 0x68a85898147c50ab, 0x27b9c772b0c31e9, 0x17b806a004157381, 0xc000000008, 0x1432958, ...)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/x/gov/gov_snapdb.go:255 +0x1a0
github.com/PlatONnetwork/PlatON-Go/x/gov.GetGovernParamValue(0x13caf21, 0x5, 0x13dd7a9, 0x10, 0x0, 0x84c1dc5b4e8bc5db, 0x68a85898147c50ab, 0x27b9c772b0c31e9, 0x17b806a004157381, 0x27b9c772b0c31e9, ...)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/x/gov/gov.go:644 +0x84
github.com/PlatONnetwork/PlatON-Go/x/gov.GovernMaxBlockGasLimit(0x0, 0x84c1dc5b4e8bc5db, 0x68a85898147c50ab, 0x27b9c772b0c31e9, 0x17b806a004157381, 0x41, 0x34630b8a000, 0x1)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/x/gov/gov.go:774 +0x73
github.com/PlatONnetwork/PlatON-Go/eth.New(0xc000198000, 0xc0004b4000, 0x30, 0x136b600, 0xc0004a97c8)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/eth/backend.go:237 +0xb8f
github.com/PlatONnetwork/PlatON-Go/cmd/utils.RegisterEthService.func2(0xc000198000, 0xc00019c3f0, 0xc0004a9980, 0xc0001a62c0, 0x2)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/cmd/utils/flags.go:1281 +0x46
github.com/PlatONnetwork/PlatON-Go/node.(*Node).Start(0xc000452000, 0x0, 0x0)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/node/node.go:197 +0x414
github.com/PlatONnetwork/PlatON-Go/cmd/utils.StartNode(0xc000452000)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/cmd/utils/cmd.go:67 +0x2f
main.startNode(0xc0000e5b80, 0xc000452000)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/cmd/platon/main.go:304 +0x7e
main.geth(0xc0000e5b80, 0xc0000e5b80, 0xc0004a9b9f)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/cmd/platon/main.go:292 +0xe3
github.com/PlatONnetwork/PlatON-Go/vendor/gopkg.in/urfave/cli%2ev1.HandleAction(0x11f5cc0, 0x1434fc8, 0xc0000e5b80, 0xc000310cc0, 0x0)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/vendor/gopkg.in/urfave/cli.v1/app.go:490 +0xc8
github.com/PlatONnetwork/PlatON-Go/vendor/gopkg.in/urfave/cli%2ev1.(*App).Run(0xc00030e340, 0xc0000321c0, 0x1b, 0x1c, 0x0, 0x0)
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/vendor/gopkg.in/urfave/cli.v1/app.go:264 +0x58c
main.main()
	/home/platon/jenkins/workspace/PlatON/Build_Ubuntu/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/cmd/platon/main.go:278 +0x55

````
","Lack of environment, can not confirm what the problem is, there are two doubts from the log. 1. The block height of the current chain has been confirmed to 330, but when restarting, the block height obtained by calling blockchain.CurrentHeader () is 240 2. When restarting, the snapshot DB is missing the wal log between 230-330, causing an error current baseNum and highestNum is not same, but not wal find, but from the log, 230-330 logs are written successfully",161629033
301,u128 does not support power operations in WASM contracts,open,2020-03-10T08:30:05Z,2020-03-10T08:30:05Z,,MEMBER,"Hi there,

u128 does not support power operations in WASM contracts.

#### System information

PlatON version: `platon version`
OS & Version: Windows/Linux/OSX
Commit hash : (if `develop`)

#### Expected behaviour


#### Actual behaviour


#### Steps to reproduce the behaviour


#### Backtrace

````
[backtrace]
````
",,161629033
302,Missing int128 data type in wasm contract,open,2020-03-09T09:50:55Z,2020-03-09T09:50:55Z,,MEMBER,"Hi there,

Missing int128 data type in wasm contract.

#### System information

PlatON version: `platon version`
OS & Version: Windows/Linux/OSX
Commit hash : (if `develop`)

#### Expected behaviour


#### Actual behaviour


#### Steps to reproduce the behaviour


#### Backtrace

````
[backtrace]
````
",,161629033
303,"Can not send LAT to inner contract, such as '0x1000000000000000000000000000000000000006'",open,2020-02-23T13:02:33Z,2020-03-27T05:56:11Z,,MEMBER,"Hi there,

Can not send LAT to inner contract, such as '0x1000000000000000000000000000000000000006'， the transaction receipt return status=0 and the log output 'out of gas'

@GavinXu520 @cheng762 

please note that this is an issue tracker reserved for bug reports and feature requests.

#### System information

PlatON version: `platon version`
OS & Version: Windows/Linux/OSX
Commit hash : (if `develop`)

#### Expected behaviour


#### Actual behaviour


#### Steps to reproduce the behaviour


#### Backtrace

````
[backtrace]
````
",this issue will be fixed in 0.11.0,161629033
304,Dockerfile is unavailable,open,2020-02-21T03:37:47Z,2020-02-21T03:37:47Z,,NONE,"Hi there,

please note that this is an issue tracker reserved for bug reports and feature requests.

#### System information

PlatON version: `platon version`
OS & Version: Ubuntu
Commit hash : (if `develop`)

#### Expected behaviour
docker build .

#### Actual behaviour
Step 4/9 : RUN cd /PlatON-Go && make platon
 ---> Running in 410f092cf5dd
build/build_deps.sh
build/build_deps.sh: line 14: git: command not found
/PlatON-Go/build//build_bls.sh: line 13: git: command not found
make[1]: *** No targets specified and no makefile found.  Stop.
make[1]: Entering directory '/PlatON-Go/crypto/bls/bls_linux_darwin/src/bls'
make[1]: Leaving directory '/PlatON-Go/crypto/bls/bls_linux_darwin/src/bls'
build/build_deps.sh: line 48: cd: /PlatON-Go/life/resolver/softfloat/build/Linux-x86_64-GCC: No such file or directory
make: *** [Makefile:15: platon] Error 1
The command '/bin/sh -c cd /PlatON-Go && make platon' returned a non-zero code: 2


#### Steps to reproduce the behaviour


#### Backtrace

````
[backtrace]
````
",,161629033
305,Calling ppos system contract query method across contracts may cause `BAD BLOCK` to appear,open,2020-02-10T13:19:49Z,2020-03-17T10:22:40Z,,CONTRIBUTOR,"Hi there,

When the query method of the ppos system contract is called across contracts through the evm sol contract or the wasm cpp contract, the results obtained on multiple nodes may be inconsistent, which will cause the appearance of `** #BAD BLOCK**`.
the reason:
The ppos-related query interfaces all use snapshotdb's `commit data`, and the `commit block` heights on different nodes will be different, resulting in inconsistent data.

#### System information

PlatON version: `0.8.0`
OS & Version: Windows/Linux/OSX


#### Expected No `BAD BLOCK`


#### Actual Maybe appear `BAD BLOCK`
",,161629033
306,invoke contract with function platon_assert  occurred error,open,2020-02-03T06:41:31Z,2020-02-03T11:54:34Z,,NONE,"Hi there,

invoke contract with function platon_assert  occurred error

#### System information

PlatON version: `platon version`
OS & Version: Windows/Linux/OSX
Commit hash : (if `develop`)

#### Expected behaviour

invoke success

#### Actual behaviour

when invoke contract,return status is 0x0

#### Steps to reproduce the behaviour
1.Contract_termination.cpp contract like this
````
#include <platon/platon.hpp>
#include <string>
using namespace platon;

extern char const string_storage[] = ""stringstorage"";
/**
 * platon_assert
 */
CONTRACT ContractTermination : public platon::Contract{
   public:
      ACTION void init(){
      }
 
      ACTION bool transfer_assert(std::string name, uint64_t value){
            platon_assert(value >= 100, ""bad value"");
            stringstorage.self() = name;
      }

      CONST std::string get_string_storage(){
          return stringstorage.self();
      }

   private:
      platon::StorageType<string_storage, std::string> stringstorage;
};

PLATON_DISPATCH(ContractTermination, (init)(transfer_assert)(get_string_storage))
````
invoke contract as below:
invoke --addr 0x093acd8fF0b8aC2Cd1491571142205474342e887 --func transfer_assert --params {""name"":""2"",""value"":112}

return value as below:
{
 ""root"": ""0x"",
 ""status"": ""0x0"",
 ""cumulativeGasUsed"": ""0x2faf080"",
 ""logsBloom"": ""0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"",
 ""logs"": [],
 ""transactionHash"": ""0xa143b8bcb5444fb66a5ba2986aadcc076f9d9c64c6dcd734ce7f32fb55dc4c4e"",
 ""contractAddress"": ""0x0000000000000000000000000000000000000000"",
 ""gasUsed"": ""0x2faf080""
}

````



#### Backtrace

````
[backtrace]
````
","clang++ will report a warning and generate call llvm.trap() instruction when lack return value,  cound lift warning to error, let developer add return statement.",161629033
307,Partially merged dual virtual machine code,open,2020-01-11T06:40:24Z,2020-01-14T02:20:01Z,,MEMBER,,Can one of the admins verify this patch?,161629033
308," Stress test contract interface with the generated empty wallet, causing the system to crash",open,2020-01-03T12:39:22Z,2020-03-27T05:57:38Z,,NONE,"Hi there,


Stress test contract interface with the generated empty wallet, causing the system to crash

#### System information

PlatON version: `0.7.5.0`
OS & Version: Linux
Commit hash : 0b1494

#### Expected behaviour


#### Actual behaviour


#### Steps to reproduce the behaviour


#### Backtrace

panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x6892f7]

goroutine 215540 [running]:
container/list.(*List).remove(...)
	/home/juzix/source/go/go/src/container/list/list.go:110
container/list.(*List).MoveToFront(0xc0025dad80, 0xc00ffa8180)
	/home/juzix/source/go/go/src/container/list/list.go:173 +0x47
github.com/PlatONnetwork/PlatON-Go/trie.(*LRU).Get(0xc0025d8540, 0xc043e47b28, 0x20, 0x1f61020, 0x1270d69, 0x8, 0x48340e)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/trie/bigcache.go:88 +0x81
github.com/PlatONnetwork/PlatON-Go/trie.(*BigCache).Get(0xc0004dc320, 0xc043e47b28, 0x20, 0x20, 0xc043e47b28, 0x20, 0xc047091500)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/trie/bigcache.go:191 +0x60
github.com/PlatONnetwork/PlatON-Go/trie.(*Database).node(0xc000252240, 0x1489ec3684e8d919, 0x1bf51d7856a4db70, 0xe8c7558a07f52cdc, 0x8bfce05d6cffd35a, 0x1, 0x0, 0x10a54e0)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/trie/database.go:374 +0x3b3
github.com/PlatONnetwork/PlatON-Go/trie.(*Trie).resolveHash(0xc04e63d490, 0xc002dc9e20, 0x20, 0x20, 0xc044a9f9f0, 0x1, 0x41, 0xc044ced080, 0x5, 0xc043e47d30, ...)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/trie/trie.go:440 +0x140
github.com/PlatONnetwork/PlatON-Go/trie.(*Trie).tryGet(0xc04e63d490, 0x14f5960, 0xc002651f60, 0xc044a9f9f0, 0x41, 0x41, 0x1, 0xd1fe7c35def1d695, 0xfce701d317f80949, 0xc043e47e98, ...)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/trie/trie.go:173 +0xef
github.com/PlatONnetwork/PlatON-Go/trie.(*Trie).tryGet(0xc04e63d490, 0x14f3560, 0xc04e631180, 0xc044a9f9f0, 0x41, 0x41, 0x0, 0x20, 0xc04e63d4d0, 0x20, ...)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/trie/trie.go:165 +0x6ce
github.com/PlatONnetwork/PlatON-Go/trie.(*Trie).TryGet(0xc04e63d490, 0xc04e63d4d0, 0x20, 0x20, 0xc04e63d4d0, 0x20, 0x20, 0x2, 0x2)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/trie/trie.go:139 +0x7b
github.com/PlatONnetwork/PlatON-Go/trie.(*SecureTrie).TryGet(0xc04e63d490, 0xc043dd9040, 0x14, 0x14, 0x0, 0x0, 0x0, 0x0, 0x0)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/trie/secure_trie.go:80 +0x7b
github.com/PlatONnetwork/PlatON-Go/core/state.(*StateDB).getStateObject(0xc0523b1d40, 0x10, 0x0, 0x3000000, 0xc044aadad0)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/core/state/statedb.go:622 +0x1ca
github.com/PlatONnetwork/PlatON-Go/core/state.(*StateDB).GetOrNewStateObject(0xc0523b1d40, 0x10, 0x0, 0x3000000, 0x6)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/core/state/statedb.go:647 +0x3f
github.com/PlatONnetwork/PlatON-Go/core/state.(*StateDB).AddBalance(0xc0523b1d40, 0x10, 0x0, 0xc003000000, 0xc043e590c0)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/core/state/statedb.go:383 +0x3f
github.com/PlatONnetwork/PlatON-Go/core.(*StateTransition).TransitionDb(0xc04e63d650, 0x14fbb80, 0xc05cc51ec0, 0xc044a84220, 0xc04e63d650, 0xc05cc51ec0, 0xc05cc51ec0, 0x1228f80)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/core/state_transition.go:231 +0x558
github.com/PlatONnetwork/PlatON-Go/core.ApplyMessage(0xc0505bea80, 0x14fbb80, 0xc05cc51ec0, 0xc044a84220, 0xc0505bea80, 0xc0523b1d40, 0xc04e5dad80, 0x0, 0x0, 0x0, ...)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/core/state_transition.go:130 +0x5b
github.com/PlatONnetwork/PlatON-Go/internal/ethapi.(*PublicBlockChainAPI).doCall(0xc002dda1e0, 0x14f6c60, 0xc05cc51da0, 0x10, 0x0, 0x1000000, 0xc04806ef20, 0x0, 0x0, 0x0, ...)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/internal/ethapi/api.go:639 +0x659
github.com/PlatONnetwork/PlatON-Go/internal/ethapi.(*PublicBlockChainAPI).Call(0xc002dda1e0, 0x14f6ca0, 0xc0403d96b0, 0x10, 0x0, 0x1000000, 0xc04806ef20, 0x0, 0x0, 0x0, ...)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/internal/ethapi/api.go:649 +0xdb
reflect.Value.call(0xc000240b00, 0xc00023d780, 0x13, 0x126b627, 0x4, 0xc05cc51d40, 0x4, 0x4, 0x0, 0xc05cc51d40, ...)
	/home/juzix/source/go/go/src/reflect/value.go:447 +0x449
reflect.Value.Call(0xc000240b00, 0xc00023d780, 0x13, 0xc05cc51d40, 0x4, 0x4, 0x2, 0x2, 0xc02f6364e0)
	/home/juzix/source/go/go/src/reflect/value.go:308 +0xa4
github.com/PlatONnetwork/PlatON-Go/rpc.(*Server).handle(0xc008ed12c0, 0x14f6ca0, 0xc0403d96b0, 0x14fc960, 0xc0401008c0, 0xc05cc51ce0, 0xc05ddfdf38, 0x467112, 0x8)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/rpc/server.go:309 +0x6c3
github.com/PlatONnetwork/PlatON-Go/rpc.(*Server).exec(0xc008ed12c0, 0x14f6ca0, 0xc0403d96b0, 0x14fc960, 0xc0401008c0, 0xc05cc51ce0)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/rpc/server.go:330 +0x1a6
github.com/PlatONnetwork/PlatON-Go/rpc.(*Server).serveRequest.func2(0xc04080e890, 0xc008ed12c0, 0xc0408164a0, 0x14fc960, 0xc0401008c0, 0xc051574bc8, 0x1, 0x1, 0xc00000f200)
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/rpc/server.go:204 +0xad
created by github.com/PlatONnetwork/PlatON-Go/rpc.(*Server).serveRequest
	/home/juzix/source/PlatON-Go/build/_workspace/src/github.com/PlatONnetwork/PlatON-Go/rpc/server.go:199 +0x28a

",this issue has been fixed at #1045,161629033
309,Light client protocl,open,2019-11-25T03:54:37Z,2019-12-20T06:49:28Z,,MEMBER,Support for les protocl in v0.7.5,,161629033
310,Transactions parallel,open,2019-11-25T03:49:44Z,2019-12-20T06:49:39Z,,MEMBER,Transactions can be executed in parallel by recognizing the mutually exclusive resources of each transactions into DAG(Directed Acyclic Graph),,161629033
311,catalog updates tcp port always uses udp port + 1,open,2020-03-26T15:01:45Z,2020-03-26T15:04:53Z,,MEMBER,"For example:

CATALOG_UPDATE_PROTOCOL=tcp   makeflow -C mycatalog:9097

Updates will be sent to mycatalog:9098, which is confusing. There is no way to specify the tcp port by itself.",,8759937
312,Return the number of physical CPUs from load_average_get_cpus(),open,2020-03-19T22:11:49Z,2020-03-25T18:48:17Z,,CONTRIBUTOR,"Currently, we count *logical* cores when examining the host system. On systems that make use of hyper-threading (especially extreme cases like Xeon Phi), this can lead to accidental over-subscription issues.

This PR changes `load_average_get_cpus()` to return the number of *physical* cores on the machine. For Macs, this is a simple change. For Linux, it requires some digging into `/sys`. I've tested a number of CPU configurations:
- Single-socket, no HT (cclws20)
- Single socket, HT (cclws19)
- Multi-socket, no HT (crcfe02)
- Multi-socket, HT (earth)

From what I could find online, the Xeon Phi registers as a normal CPU with HT, but I don't think I have access to any real machines to test on.

Resolves #2262",,8759937
313,Conda package issues,open,2020-03-19T18:30:53Z,2020-03-20T14:03:49Z,,CONTRIBUTOR,"Hello there,

I hope everyone is staying safe and healthy during this crazy time.  I just wanted to report an issue I found with installing cctools via conda.  There's no particular reason why I'm seeing this issue today; I was previously building from source and installing using a custom script.

This is in a clean Python 3.7.6 environment, using miniconda downloaded and installed just a few minutes ago. Prior to installing cctools, I did install the mercurial package.

```
(main) leeping@bright:/opt/intel/licenses$ conda install -c cclnd cctools
Collecting package metadata (current_repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
  current version: 4.8.2
  latest version: 4.8.3

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: /home/leeping/opt/miniconda3/envs/main

  added / updated specs:
    - cctools


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    cctools-927.0.2            |       h5a3fd96_4         5.4 MB  conda-forge
    ld64-450.3                 |       h3104ce3_4         8.7 MB  conda-forge
    libllvm9-9.0.1             |       hc9558a2_0        25.1 MB  conda-forge
    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge
    tapi-1000.10.8             |       hc9558a2_4         1.1 MB  conda-forge
    ------------------------------------------------------------
                                           Total:        40.3 MB

The following NEW packages will be INSTALLED:

  cctools            conda-forge/linux-64::cctools-927.0.2-h5a3fd96_4
  ld64               conda-forge/linux-64::ld64-450.3-h3104ce3_4
  libllvm9           conda-forge/linux-64::libllvm9-9.0.1-hc9558a2_0
  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000
  tapi               conda-forge/linux-64::tapi-1000.10.8-hc9558a2_4


Proceed ([y]/n)? y


Downloading and Extracting Packages
ld64-450.3           | 8.7 MB    | ################################################################################################################################################################## | 100% 
tapi-1000.10.8       | 1.1 MB    | ################################################################################################################################################################## | 100% 
cctools-927.0.2      | 5.4 MB    | ################################################################################################################################################################## | 100% 
libuuid-2.32.1       | 26 KB     | ################################################################################################################################################################## | 100% 
libllvm9-9.0.1       | 25.1 MB   | ################################################################################################################################################################## | 100% 
Preparing transaction: done
Verifying transaction: done
Executing transaction: done
(main) leeping@bright:/opt/intel/licenses$ ls
l_PHVH88ZP.lic  NCOM_L__CPPFOR_NHXH-GGJZJKHK.lic
(main) leeping@bright:/opt/intel/licenses$ python
Python 3.7.6 | packaged by conda-forge | (default, Mar  5 2020, 15:27:18) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import work_queue
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'work_queue'
```","If you do try it, note that as of  March 20 you should get cctools version 7.1.0, rather than the most recent 7.1.1. I have not been able to update the version in conda, as currently conda-forge is trying to globally update pypy.

That said, the only change in 7.1.1 from 7.1.0 is a bug fix when linking parrot, which I don't think you use.",8759937
314,Worker Resource Detection and Hyperthreads,open,2020-03-16T18:38:46Z,2020-03-16T18:41:02Z,,MEMBER,"Currently, the worker detects the number of ""virtual CPUs"" which corresponds to hyperthreads.  It should probably detect physical cores instead, or at least have an option to do so.
","In `dttools/src/load_average.c` we use `sysctlbyname(""hw.ncpu"",....`.  For physical cores we want `hw.physicalcpu`.",8759937
315,wq_bwa: fixed off by two error when submitting tasks,open,2020-03-13T12:45:00Z,2020-03-25T15:37:18Z,,CONTRIBUTOR,There was an error where two of the splits were not being created/submitted as tasks. A simple fix is proposed here.,,8759937
316,Python Packaging Cleanup,open,2020-03-04T22:37:53Z,2020-03-04T22:37:53Z,,CONTRIBUTOR,"This PR reworks the `python_package_*` tools. For `python_package_create` and `python_package_run` I mostly fixed the sharp corners and cleaned generally cleaned things up. I more or less rewrote `python_package_analyze`, though. In particular,
- Rather than just grepping the program text, I used the `ast` module to parse the code, which allows more control (e.g. only add imports from inside a function).
- Switched the spec format to use that of `conda env export`, which can be fed directly to `conda env create`. This captures both Pip and Conda packages.
- Rather than using the external dependency `stdlib_list`, I checked the path of each imported module against `{sys.prefix}/pythonX.Y/site-packages` to determine if Pip/Conda should know about it.
- In some cases, the imported name does not match the package name in Pip/Conda (e.g. `ndcctools` provides `work_queue`). I put in more complicated logic to list package contents and try to match things up if using the name doesn't work.
- In case of weird metapackage layouts, added a command line option for manually specifying which package provides a given import name.
- Prints a warning if a package can't be found. This could indicate a weird name/layout, or something installed outside the Conda environment.

It's been working for me under a variety of packaging setups, and can handle a lot more than the original implementation. Unfortunately, the performance is worse (since we have to list the full contents of all packages). We might make this extra (expensive) check optional so that users can just manually specify packages when the names don't match up.",,8759937
317,Python interface for wq factory,open,2020-02-18T16:32:41Z,2020-03-25T14:11:16Z,,MEMBER,"@trshaffer From your code I renamed it to work_queue.Factory, made a distinction between command line only and config file options, and made some variables and functions class attributes.

Let me know if it still works as you intended.","@trshaffer, Tim, does this pr looks good to you?",8759937
318,WQ Master and Python API using JSON API for workqueue,open,2019-12-21T19:00:06Z,2020-03-13T14:37:18Z,,CONTRIBUTOR,,Tested with BWA as a real-world example.,8759937
319,Feature request - Short Job Bundling,open,2019-12-16T17:29:33Z,2020-02-13T19:03:18Z,,NONE,"When jobs are relatively short and there is a large number of them, Makeflow (running in `wq` mode) will be overloaded to the point that most workers are not able to connect the the master at all (can not even `telnet` the master). In the worst case, we only see 15~30 1-core workers connected, which is less efficient that running in `-T local` on a large instance). The issue is there are too many chitchat in between workers and master for file transfer, and master is not able to respond to worker connection due to the single thread implementation.

In our use case, we moved away the transfer of input data to an HTTP server, and bundle multiple jobs to into a single one to sent to worker in a single chunk to execute sequentially (there are many more short jobs than workers).

It would be very helpful if Makeflow are able to do bundling upon configured/hinted. A pesudo implmentation for JX syntax would be having an extra attribute inside the rule that hint bundling, and also an attribute on the category side to hint bundling (so that user are able to bundle jobs from different rule into one). Makeflow would be bundling multiple (number specify by the attribute) bundle-able jobs into one when executing.

Also implement the bundling with subworkflow feature is not very clean, because we have to bundle a fixed number of jobs manually into a subworkflow, which makes later modification harder to accomplish.

@btovar ","Thanks for the details.
One more question: about how big was the output of each task?


On Thu, Feb 13, 2020 at 9:43 AM John Xu <notifications@github.com> wrote:

> I was a student in Eric and Nirav's class last semester.
> It was related to our final project of the class, most task takes about
> 1~3 mintues, the size of a single data set is about 20MB, and we have ~9000
> data sets for one of the pipelines.
> Just give more context, this is related to an issue we encountered during
> the final project. The issue is that we seems to encounter a limitation of
> the Makeflow's implementation as a Work Queue master, because most workers
> are not able to connect to the master (say only ~30 out of 100), so Ben and
> some other software devs suggests that we move the outbound data transfer
> away from Makeflow and bundling smaller task into big one. The problem does
> seem to go away after we implement our own bundling, which is just a rule
> that execute a python script for each bundle that takes a fixed number of
> data sets (a bundle), and execute a bash script on each data sets. Our
> implementation, while works, somewhat defeat the purpose of using a
> workflow manager, so it would be nice to have some support from Makeflow
> and JX.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/cooperative-computing-lab/cctools/issues/2232?email_source=notifications&email_token=AAVADET5W4OVC5T72STUPKTRCWBDRA5CNFSM4J3NL7QKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELV5YNY#issuecomment-585882679>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAVADEVF6BZMIMZZ35NTCF3RCWBDRANCNFSM4J3NL7QA>
> .
>
",8759937
320,How to exchange results of tasks in WorkQueue,open,2019-12-05T06:59:48Z,2019-12-05T15:52:26Z,,NONE,"I want to submit two tasks - t1, t2 to the workqeue and i have two workers each running on separate machine (with good network between them), such that worker 1 pull t1 and worker 2 pull t2. I want to use MPI to exchange output results that were generated by t1 with t2. Then I want to resubmit tasks again to workqueue and pull them again automatically by the workers. I basically want to create sort of loop, where each step exchange results with MPI and resubmit back tasks to WQ

Does WQ or some other cctools package supports to use MPI to exchange results between t1 and t2? Does WQ support the entire scenario with task resubmisisons? or i need to implement this logic outside of work queue?
Thanks","work queue's design assumes that tasks don't communicate with each other during their execution, thus there is not a way to coordinate mpi tasks that need to communicate to other mpi tasks.

Of course,  mpi tasks that execute in a single host using multiple process are supported, but in such case the fact that the tasks use mpi is not very relevant.",8759937
321,Fast abort and non-homogenous categories,open,2019-11-22T13:38:17Z,2019-11-22T13:38:17Z,,MEMBER,"In a workflow from NDCMS the fast abort statistic where skewed by fast finishing tasks. Many slower tasks where retried because of fast abort until they failed by hitting the number of max retries. No slower tasks were able to influence the statistics, as they were terminated before they could finish.

The problem was hard to debug because of the history of retries was not explicitly kept (#2209)

Also, the fast abort multiplier and statistics did not adapt to the changing distribution. If we do not want to make them adaptive, and recommend its use for homogeneous categories, at least work queue should signal that fast abort may not be working as intended. In the current use case, there was no warning from work queue that a large number of workers and tasks were being terminated given fast abort. Fast abort assumes that only a small fraction of workers will be terminated.



",,8759937
322,Taks retries history is not kept explicitely kept,open,2019-11-22T13:37:38Z,2019-11-22T13:37:38Z,,MEMBER,"The reasons for retrying a task (i.e., resource exhaustion, and workers' disconnection, failures, and fast aborts), are not explicitly recorded in any structure, or in the logs. The history of retries can be reconstructed from the transactions log, but this can only be done by an expert.

Here are some suggestions:

- On the final transaction of the task, that is the DONE transaction, we can also print the history of retry causes.
- The causes may be made available in either the task structure, or by a request to the work queue object.",,8759937
323,makeflow's DAG_FILE_TYPE_TEMP does not delete temp files,open,2019-10-28T13:26:59Z,2020-02-11T13:57:29Z,,MEMBER,"Test TR_makeflow_040_sub_workflow_jx.sh is not deleting its arguments file `makeflow.jx.args.XXXXX` created [here](https://github.com/cooperative-computing-lab/cctools/blob/29c37b84c3b34a471907cfc079f97678914077e1/makeflow/src/makeflow.c#L216).

That file is registered with `makeflow_hook_add_input_file` as a temp file, but that does not seem to have any effect.","Ok, I see what you mean now -- the underlying architectural problem is that
clean on a sub-makeflow doesn't really ""submit"" and ""complete"" a job.  I
see a couple of ways forward on this:
1 - Make the clean operation really use the job execution interface in the
normal way.
2 - Duplicate some functionality from job execution into the sub-makeflow
clean operation.
3 - Pass arguments to sub-makeflows in a way that doesn't result in the
generation of extra files.  (but maybe there will still be


On Tue, Feb 11, 2020 at 8:50 AM Benjamin Tovar <notifications@github.com>
wrote:

> I think that's the problem. The cleanup is done only on job completion,
> but with -c or -k no job is ever completed.
>
> That is why I added the manual fix to delete args.XXXXX files, which are
> created during parsing. A better solution would be to delete all temps
> regardless of job completion, but I did not want to do that just now
> because you were studying how slow is to stat files in the disk when
> ignoring the log.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/cooperative-computing-lab/cctools/issues/2185?email_source=notifications&email_token=AAVADEQ6AKXIKOOF2HR33NTRCKUJHA5CNFSM4JF2DYBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELMPDNQ#issuecomment-584642998>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAVADESMZV5IFT6BTUKBUJ3RCKUJHANCNFSM4JF2DYBA>
> .
>
",8759937
324,makeflow memory leaks,open,2019-10-25T15:54:02Z,2019-12-19T17:14:39Z,,MEMBER,"The new code for jx parsing and hooks has some memory leaks as detected by valgrind. From `cctools/makeflow/example` these can be seen with:

```sh
../src/makeflow -c --jx example.jx; valgrind --leak-check=full -- ../src/makeflow --jx example.jx
```",,8759937
325,WORK_QUEUE_PREEXIST  is confusing in the API documentation,open,2019-10-24T18:53:44Z,2019-10-24T18:53:44Z,,MEMBER,"We use it internally to prevent `WORK_QUEUE_THIRDGET` files to be deleted, however, a user should not be able to set it manually. This is not clear from their documentation:

```C
WORK_QUEUE_PREEXIST = 4, /**< If the filename already exists on the host, use it in place. */
WORK_QUEUE_THIRDGET = 8, /**< Access the file on the client from a shared filesystem */
```

In fact, `WORK_QUEUE_THIRDGET` does not care whether the file is in a shared filesystem, only that the path is accessible at the worker. 

Currently, it seems that  `WORK_QUEUE_THIRDGET | WORK_QUEUE_SYMLINK` is closer to what the documentation of `WORK_QUEUE_PREEXIST` describes. 
",,8759937
326,Coffea and Work Queue,open,2019-10-10T17:34:44Z,2019-11-05T14:05:27Z,,MEMBER,"Build a prototype of the Coffea data analysis framework using Work Queue as an underlying execution method.

(@zsurma you are already working on this, but I want to make sure it is captured in git properly.)

Ben's initial instructions:
```
Official installation instructions:
        https://coffeateam.github.io/coffea/index.html

Github repository:
        https://github.com/CoffeaTeam/coffea

Presentation of coffea:
    https://indico.cern.ch/event/822074/contributions/3471455/attachments/1865262/3067208/CoffeaNYU_LindseyGray_19062019.pdf



How I installed it:

One-time steps:

1) Install for python3 from: https://docs.conda.io/en/latest/miniconda.html
2) conda create -n coffea-env python=3.6
3) conda activate coffea-env
4) python3 -m pip install coffea
5) python3 -m pip install parsl
6) python3 -m pip install jupyter
7) conda install xrootd
8) Download the example: curl -O https://raw.githubusercontent.com/CoffeaTeam/coffea/master/binder/muonspectrum_v3.ipynb
9) conda activate coffea-env
10) jupyter notebook
11) A browser tab should have opened. If not, go to: http://localhost:8888/tree
12) Upload muonspectrum_v3.ipynb
13) Open muonspectrum_v3.ipynb

Steps to execute everytime for coffea:

1) conda activate coffea-env
2) jupyter notebook

All this will run the examples using the local executor. To run with parsl, change run_uproot_job to run_parsl_job.
```","(@zsurma I meant to assign you to this early, but you weren't in the system.)

Working code:
https://github.com/zsurma/coffea/commits/workqueue",8759937
327,Update Install Instructions,open,2019-10-10T15:32:53Z,2020-02-07T20:25:59Z,,MEMBER,"Now that the readthedocs format has been merged into master, we need to do a major update of the install instructions.  Here is what I want it to look like:

1 - Install via Conda
2 - Install via Spack
3 - Install via Git
4 - Special Cases (as already given)

And then let's drop the source/binary tarball installation instructions, I think that's outmoded.

@tjuedema please make a PR with the Spack instructions added ASAP, then pass it off to @btovar to add the rest.
",See #2252 ,8759937
328,nest resource_monitor invocations to group sets of processes,open,2019-10-03T21:16:55Z,2019-10-03T21:16:55Z,,MEMBER,"Currently with nested invocations of `resource_monitor`, the inner instance does not do any monitoring, and simply executes its payload. With this, the outer instance does all the work.

A better solution is to let the inner instance to monitor the processes of its payload, and communicate summary information to the outer instance.",,8759937
329,"on resource exhaustion, Work Queue only returns logs from the resource monitor",open,2019-09-11T16:33:56Z,2019-09-11T16:56:56Z,,MEMBER,"Currently Rachael from ND CMS has some tasks which fail given resource exhaustion. The exhaustion may be a result of a bug in the tasks, as the task are running for an unexpected 20+ hours before failing. On failure all the user logs the tasks writes are not brought back to the master, which makes debugging more difficult.","It is not clear to me what we want. 

In the final retry, we most likely want to treat it as any other task failure, that is, return everything to the user.

With intermediate retries, it is not clear. We do not want to return them as final outputs, as they will be overwritten by further retries. However, we do want to return them to the user in some way to ease debugging, perhaps with something similar to how we handle failed rules in makeflow.",8759937
330,perl api doc's are not being generated,open,2019-08-26T15:23:50Z,2019-08-26T15:23:50Z,,MEMBER,"It seems that when we re-organized the bindings directories, we did not update the paths in doc/Makefile for perl.

E.g., for work_queue, they point to src/perl, rather than src/bindings/perl",,8759937
331,WIP: External Test Setup,open,2019-08-12T20:27:33Z,2019-08-13T14:16:29Z,,MEMBER,"This is a first pass at cleaning up the test suite to operate external to the build system.  I didn't convert all the tests in one pass, but did a handful to test the idea.  I'm attempting to minimize what each component knows about the whole system, so it ought to be possible to run the test suite on a build that came via pip, conda, spack, etc without knowing the config.mk that came out of that build.

Here is the idea:

1 - Each test operates independently and expects that the appropriate `PATH`, `PYTHONPATH`, etc has already been set to point to the target test installation.  The test only uses assets in the cwd and relies on binaries, etc already being in the path. If it needs to compile something, it uses `CCTOOLS_COMPILE` or `CCTOOLS_COMPILE_STDIN` to invoke the compiler appropriately.

2 - `test_runner_common.sh` expects only `CCTOOLS_INSTALL_DIR` to be set, and uses that to construct an appropriate `PATH`, `PYTHONPATH`, etc for each test.  `CCTOOLS_COMPILE` is defined to use `CCTOOLS_INSTALL_DIR` to find `include`, `lib`, etc.

3 - `run_all_tests.sh` does not attempt to parse `config.mk` but just relies on `CCTOOLS_INSTALL_DIR` being set, and then invokes all of the tests.  (Still need to pass in what subset of tests to run somehow.)

@btovar @trshaffer  does this seem like a sensible approach?
Anything missing here?
",#2035,8759937
332,Work Queue Factory --wrapper-input Does Not Transfer Input,open,2019-07-22T14:08:28Z,2019-07-22T14:12:06Z,,CONTRIBUTOR,"The file specified by --wrapper-input is correctly added to Condor submit script (when using Condor), but the file is not copied over to the /tmp directory used to submit the workers.",,8759937
333,Future Factory Evolution,open,2019-07-18T15:45:53Z,2019-07-18T15:45:53Z,,MEMBER,"The current `work_queue_factory` is effective at starting WQ workers, but we are seeing an increased need for scaling up arbitrary applications.  Evolve the factory into a more generalized tool that does the following:

_Start N instances of program P on batch system B using container C.  The number of instances may vary according to the concurrency needs expressed in the catalog and modulated by local policy P._

Then, we can take the improved tool and use it to deploy a wide variety of execution systems such as Parsl, Spark, Dart, etc...

",,8759937
334,JX Multi Database,open,2019-07-16T14:10:44Z,2019-07-16T14:10:55Z,,MEMBER,"Splits catalog history into multiple databases on the fly, one for each record type.
Still needs a conversion routine to move the old format into the new format.
Addresses #2016 ","WIP
",8759937
335,blacklist hostnames from wq factory config file,open,2019-06-28T16:00:22Z,2019-07-01T16:56:38Z,,MEMBER,"Feature request from Andrew Wightman (NDCMS). Currently, blacklisting can only be done by an API call from the master, that then communicates the blacklisted hosts to the catalog.

Adding a blacklisted hostname in the config file does not necessarily has to be communicated to the catalog. ","The file is already periodically re-scanned, thus if a host is removed from the list, such host will not be blacklisted anymore.",8759937
336,Some Travis Builds Fail at Curl Step,open,2019-06-24T19:33:41Z,2019-07-03T14:07:51Z,,MEMBER,"Autobuilds for cctools fail in the test stage on docker image cclnd/cctools-env:x86_64-centos7 when the basic makeflow test attempts to use curl to download capitol.jpg from ccl.cse.nd.edu.

For example:
https://travis-ci.org/cooperative-computing-lab/cctools/builds/549881197?utm_medium=notification&utm_source=email

This appears to be some sort of environmental problem, like dns or networking not set up right in the container, because the curl fails after almost exactly 30 seconds with SIGTERM.
","To clarify, this is an intermittent problem.
",8759937
337,Update Release Procedure,open,2019-06-24T18:07:42Z,2019-08-08T18:09:33Z,,MEMBER,"@btovar please update our release/deploy procedure to match the current infrastructure, and also include whatever bits are needed to update the Conda deployment package:
http://ccl.cse.nd.edu/internal/deploy.php
","More broadly: write out our overall deployment procedure that maps out the steps from issuing a release in github to seeing packages deployed in condor/pip/spack/cvmfs, and then performing tests on the installation.",8759937
338,JX Object Implementation,open,2019-04-16T19:58:00Z,2019-06-24T18:44:15Z,,CONTRIBUTOR,"Continuing from #2050

There are several related issues with objects in JX at present:
- non-string keys are allowed (this is forbidden in JSON)
- duplicated keys are allowed (this is allowed in JSON, but awkward to work with)
- the implementation uses an ordered list of pairs

For point 1, there weren't objections to restricting JX to string keys. Doing this, however, we might lose the ability to define keys via expression. I don't know of any time this is actually done. To support this, we would need to be able to use unevaluated expressions as object keys. It's not clear how lookups and type or uniqueness validation would interact with expressions.

The issue in #2050 is partially due to duplicate keys, and there doesn't seem to be much interest in supporting that at all.

For the implementation, there are concerns about performance, especially with small JX objects. Currently, insert is O(1) and lookup is O(n). To ensure uniqueness of keys, we might need to make inserts more expensive or otherwise change the operations. If we're only working with string keys, a hash table might help here, but that could bring increased overhead and strangeness with expressions as keys.

cc @dthain @btovar ",,8759937
339,Makeflow allow grouped containers/wrappers,open,2019-04-05T18:47:20Z,2019-07-19T21:48:23Z,,CONTRIBUTOR,"In discussion on the forum, the question has been raised about the ability apply specific containers per rule or to a group of rules. I think this could be addressed in a number of ways, but we may decide that this functionality breaks with Makeflow's design, hence opening this to discussion.

There is a related issue #1542, but this views the topic more widely including containers.

First Question: Does this hold with Makeflow's design goals of static definitions that are
known prior to/at runtime? How do individual or group scripts change this, if at all? Would this functionality be widely useful and/or has interesting research value?

Second Question: How would this be implemented and when does the change apply to the task. Is it a hook that is evaluated at submit, or an expansion that is always included in the rule at parse for purposes of archiving and consistency? I think the answer to there depends heavily on what we choose as a solution and how it is implemented.

Here are possible proposals:
- To support in Make and JSON/JX syntax we extend categories to have field or options to containers and or scripts. Or use Makeflow directives in Make and fields in JSON.
- To support only in JSON/JX parse additional elements for containers and scripts. The JSON parser would be able to accommodate this, but we would want to consider how we check to this. This option could operate similar to the singularity or wrapper code now, but applies it at parse time instead of submit time. 
- Layered JSON definitions where an attribute applies all of the rules contained. This would allow you to ""apply"" wrapper and containers to groups and the parser has to sort them out and flat them with some concatenation. This I think is the messiest and should be avoided as the concatenation may be unpredictable to the user.
- Others... There are likely several other options that I am missing and if you think they are viable candidates comment below.

Overall, this could be a large discussion but I wanted to put this up with some first thoughts just to keeps track of as we discuss.","IMHO, there is really only one way to get this in the current architecture:  Allow individual rules to state a property like `container` or `sandbox`, and then modify the hook infrastructure slightly to fire individually on jobs with that property.  The user would still have to mark them individually in some way.  (Or perhaps by category.)

Looking farther ahead, we could thing of a different sort of workflow syntax where we have ""scopes"" in which properties can be set for multiple jobs at once.


",8759937
340,R WQ bindings,open,2019-03-29T15:19:10Z,2019-04-04T17:02:58Z,,MEMBER,Supersedes #2033 ,,8759937
341,Makeflow caching reused files.,open,2019-03-07T15:44:02Z,2019-03-07T15:44:02Z,,CONTRIBUTOR,"Makeflow has foreknowledge of which files are reused and therefore cache-worthy in Work Queue. It would be useful to define this information and exploit this.

This would require changes to:
- `makeflow_module_caching.c` to identify files as unique vs cache-worthy. This could be done in `makeflow.c`, but this would be a good enhancement to do in a module.
- `batch_job_file.c` to define a file as cache-able (likely a boolean).
- `batch_job_work_queue.c` to interpret `batch_job_file` cache-able field.

Thoughts or concerns?",,8759937
342,Test suite outside compilation process,open,2019-03-01T15:09:27Z,2019-08-08T18:12:12Z,,MEMBER,Develop a test suite that runs outside the building process of CCTools. These test should run on the result of make install. ,"Some initial thoughts:
1 - Move all of the `*/test` files into `/test`.
2 - Modify the tests to rely on the existing PATH and PYTHONPATH, rather than relative paths.
3 - Take the various test programs (`auth_test`,`jx_test`,etc) out of the source tree, and put them in the test tree to be built upon demand, not installed.
4 - Set up the top-level test script to perform a make install, then set the PATH/PYTHONPATH and run the tests on the installed version.
",8759937
343,Split Off Work Queue Examples,open,2019-02-26T16:05:33Z,2019-06-24T18:45:13Z,,MEMBER,"@nhazekam @btovar please split off the work queue examples into a separate repo.  I think that includes apps/*, allpairs, wavefront, sand, and perhaps others.  Please complete this weekl.

It looks like Ben already created it: https://github.com/cooperative-computing-lab/work-queue-examples
","@nhazekam How close is this to complete?
",8759937
344,R Bindings for Work Queue,open,2019-02-22T21:00:03Z,2019-07-03T14:10:36Z,,CONTRIBUTOR,"Approached about interest in R bindings and to see what the distance exists to export them.

Prior to this we may need to explore the ease of export, stability of version/binding, and what examples would look like in R.",I just added the swig r binding for work_queue and chirp #2033. @trshaffer Could you help me to review the PR ?,8759937
345,golang binding Makefile's do not have a proper install,open,2019-02-13T17:40:41Z,2019-06-24T18:46:40Z,,MEMBER,"make install does nothing for the golang bindings. I modify them to print a warning, but they should be installed to proper locations.",,8759937
346,Catalog: Split History by Type?,open,2019-02-05T17:43:03Z,2019-07-16T14:08:54Z,,MEMBER,"Historical queries of the catalog are often expensive b/c of the need to scan data irrelevant to the current query.  e.g. There are lots of chirp loadavg/storage updates that dominate the wq_master updates by an order of magnitude.

One solution to this is to partition data by the static ""type"" field of the advertisement, and then store these in separate instances of `jx_database`, so the directory structure becomes `history/{type}/{year}/{day}.[log|ckpt]`

This would require several steps of evolution:
1 - Implement a new parent object `jx_multi_database` which creates and writes`jx_database` instances on demand as new type fields are observed, and merges states on reads.
2 - Modify the catalog server to use `jx_multi_database` instead of `jx_database`.
3 - Write a tool to convert historical data to the new format, by reading it from a `jx_database` and inserting into a `jx_multi_database`

It may not be necessary to modify `deltadb_query` b/c it will still work against the single database format, which remains unmodified.

Builds on PR #2009 ",,8759937
347,Specifying Intermediate files in JX,open,2018-10-11T19:08:25Z,2018-10-11T19:08:25Z,,CONTRIBUTOR,"There is no way to specify files are workflow inputs, intermediates, and outputs. This is possible in make-style Makeflow, but not supported in JX.

Makeflow's default behavior is to label any file that is not created in the workflow as an input, any file that is not used by a rule as an output, and all other files as intermediates. Garbage collection uses these labels when determining if a file can be removed.

The goal of this would be to allow files to be specified as intermediates and removed when garbage collection is enabled, or outputs even when used in a job.

Proposal:
Using the extend file specification in JX, parse a ""type"" field that specifies it as a workflow {input, output, intermediate}. This would allow the user to overrule Makeflow's default behavior.

Alternatively we could add global labels (such as categories or tasks) for workflow outputs and inputs, which is what is done currently in make-style Makeflow. This would be useful in the submakeflow case, which may need to be revisited in this discussion #1749 .
",,8759937
348,API call for work queue and batch interface to mark files as 'no cache',open,2018-07-27T14:41:59Z,2018-07-27T14:42:24Z,,MEMBER,"The idea is for masters such as makeflow to advise the batch system that a file won't be used anymore.  In the work queue case, this would mark a file as not to be cached, and to delete the file in the workers it exists.",,8759937
349,"Allowing edits to the makeflow file ""on the go""",open,2018-02-27T06:52:24Z,2018-12-06T19:08:34Z,,NONE,"Hello,

while enjoying your makeflow, I am thinking more and more to the limitations due to this feature.

For pipeline developing, is often necessary to insert a new algorithm of combination of parameters for an algorithm within the already executed pipeline. In my case for example, I test an algorithm with a combination of parameters:

a = 1:100
b = 1:10
c=""mode1"", ""mode2"", ""mode3""

100*10*3 combinations 

It is pretty common after having executed the whole pipeline, to be wanting to test for b = 11

Now,  your team already suggested to produce many makefiles instead of pissing together a high number of commands. Although this can be more modular and robust generally speaking, in a grid of parameters like this (and even more complex bioinfomatics pipelines) deciding where to modularize is non easy, and sometime even not feasible.

To gain momentum and adoption by researchers like me, Makeflow should be able in my opinion (like GNU make) to be robust to changes in the makefile after full or partial execution, simply updating the existing dependencies. 

Some genomics pipelines (is not my case) take 3 weeks to complete (mine takes 2 days), and there cannot be the risk of having to run from the top if a secondary small change is required.

Makeflow is easy to use and almost no brainier for queuing systems. I hope you can consider this request, as I would like to stick to your system, however at the moment is requiring a lot of time and mental energies to go around this hard needs of makeflow.

Thanks.

",That's an option. This sort of got away from me when the focus shifted to archive mode.,8759937
350,CCTools CVMFS repo,open,2017-11-16T21:31:35Z,2018-02-02T15:52:15Z,,CONTRIBUTOR,"As @PerilousApricot pointed out recently, it should be pretty straightforward to get software installed in CVMFS [1]. Would maintaining a CCTools repo in CVMFS be feasible? This would simplify the recipe for sharing my analysis code (which relies on Makeflow+WQ) with non-ND collaborators at CERN.

[1] [http://openhtc.org/docs/data/external-oasis-repos/](http://openhtc.org/docs/data/external-oasis-repos/)
","It looks like someone added some older versions here:

```
[dthain@cclws15 dropbox]$ ls -la /cvmfs/oasis.opensciencegrid.org/osg/modules/cctools
total 9
drwxrwxr-x 1 root root 1024 Oct 20  2016 .
drwxrwxr-x 1 root root 1024 Jan 16 19:07 ..
drwxrwxr-x 1 root root 1024 Feb 25  2016 4.4.2
drwxrwxr-x 1 root root 1024 Feb 25  2016 5.2.3
drwxr-xr-x 1 root root 1024 Mar 24  2016 5.4.7
drwxr-xr-x 1 root root 1024 Oct 18  2016 6.0.7
```

",8759937
351,Makeflow: Handle Sub Workflows via JX,open,2017-09-25T17:32:31Z,2018-12-06T18:35:07Z,,MEMBER,"Using JX, we now have the plumbing necessary to invoke sub-workflows.

A sub-workflow invocation should look like this:
```
type: ""workflow"",
inputs: { ... },
outputs: { ... }.
workflow: ""something.workflow"",
args: { ... }
```

Where `args` is incorporated into the ""context"" of the sub-workflow specification.

Then, when executing a sub-workflow, we need to create a script along these lines:

```
mkdir $t
cd $t
copy in input files
create a context file
makeflow -T local ...
copy out the output files
cd ..
rm -rf $t
```

This should happen as part of the job expansion that occurs in `makeflow_node_expand` before the job actually gets submitted.  Then, it has the appearance (to the batch system) of just being a simple job.

@AndrewLitteken please chat with @nhazekam @trshaffer  about the details of this.
",I think this is work that was/is in progress related to #1808.,8759937
352,Add Resources to Examples,open,2017-08-01T15:08:02Z,2017-08-01T15:10:12Z,,MEMBER,"The makeflow example generator scripts should be labelled with the CORES, MEMORY, and DISK needed by each task, so as to facilitate better resource management.

",,8759937
353,API call from work queue to add custom fields to the resource monitor summary,open,2017-06-12T16:10:27Z,2017-06-12T16:12:18Z,,MEMBER,"Use case:  Label resource summaries with the work units of a lobster task.

As requested by @klannon, @t172 ",,8759937
354,dag_to_file (for makeflow_analyze --bundle) is out of date,open,2017-01-18T17:21:08Z,2017-05-17T15:36:35Z,,MEMBER,"In particular it currently has this two bugs:

It uses variables instead of directives for categories.
All variables and exports are set from the default category.",,8759937
355,Allow wrappers to apply to groups of Makeflow rules,open,2016-11-18T16:22:32Z,2016-11-18T17:46:38Z,,CONTRIBUTOR,"This is a feature request. In a demonstration of just how convoluted my workflow is, I need different wrappers for various rules in my Makeflow script. After discussion with @hmeng-19 we decided a wrapper was the right tool for the job here rather than umbrella because I'm just setting up environment variables. It would be convenient to be able to specify different wrappers for groups of rules in the Makeflow script, in an analogous way to how umbrella specifications are handled. ","@dthain: Yes.
",8759937
356,Ability to ask workers to report metric,open,2016-10-20T18:55:13Z,2016-10-20T18:55:13Z,,NONE,"Hi Ben @btovar per our conversation, I am filling this feature request.
It would be good if the master could request:
1. Run commands like uptime, df -h , ps etc to see activity and metrics on a worker. Currently we do a lot of that using ansible, but doing it natively will be great (from master)
2. It would be great to connect master and worker using public private key (allow workers to be submitted with public keys), this also will make #1 more secure.

Let me know if you need more specifics.

Regards,
Nirav
",,8759937
357,Argument array for WorkQueue tasks,open,2016-10-19T15:17:10Z,2017-09-26T16:21:00Z,,NONE,"Currently, the task creation passes a string directly to the shell:

```
Task(""sim.exe -p 50 in.dat > out.txt"")
```

However, shell quoting rules are really difficult!  It would be a lot better if we could build argument lists directly.

```
Task(""sim.exe"", args=[""-p"", ""50"", ""in.dat""])
```

(@btovar @nhazekam)
",,8759937
358,more than one wq factory per master is not well supported,open,2016-08-22T18:30:57Z,2017-05-17T15:37:15Z,,MEMBER,"Most of the code from work_queue_factory assumes that there is a single factory per master. This causes some problems. For example, somewhere we have the line:

workers_submitted > new_workers_needed + workers_connected

where workers_submitted is particular to the factory, but new_workers_needed and workers_connected come from the master, and thus depend on other factories.

One particular problem is that if the master requires N new workers, all the factories may submit such N workers, which is inefficient. 
",,8759937
359,Feature: Combine multiple runs in parrot_package_create,open,2016-07-08T15:45:03Z,2016-07-18T15:06:15Z,,MEMBER,"Allow the user to add the contents of another run to an existing parrot package, detecting conflicts if necessary.  For example:

```
parrot_package_run   somecommand
parrot_package_create mypackage
parrot_package_run   anothercommand
parrot_package_create  --add mypackage
```
","@hmeng-19 , `parrot_identity_box` does something similar to this, perhaps that capability can be integrated into `parrot_package_run`.  Also note the `--uid` and `--gid` options to parrot, which let you pretend to be a different user...
",8759937
360,Have factory autosize workers based on category requirements,open,2016-06-28T15:44:13Z,2018-12-06T21:43:41Z,,CONTRIBUTOR,"I know that we've discussed this idea in the past, but I don't see it in the issue tracker yet-- please close if I've missed it.

We have a number of undergrads doing HEP research this summer and using Lobster + WQ for the first time. With the fantastic progress made by the cctools team on the resource monitor, it's easier and faster than ever to spot resource allocated-vs-utilized inefficiencies. I am observing, however, that a big stumbling block for new users is determining the optimal factory configuration. The benefit of automatically adjusting the requested resources per-task is lost if the user has the per-worker factory request too low. More advanced users can spot this almost immediately with the (awesome!) `work_queue_status -A` option, but new users are often confused about why they don't have any able workers. The whole picture is complicated by certain workflows that have the ratio of various categories running changing over time. It seems that the factory resource specification itself is redundant, since the resource monitor already knows what it SHOULD be. Would it be possible to have an automatic flag that would instruct the factory to submit workers with either 1) a given multiple of the necessary resources per category or a fancier variation 2) the maximum multiple of the necessary resources per category in order to pack optimally into available slots?

I realize that added complexity comes at a cost and is not always an improvement, but in this case I think there is a potential for a huge increase in user friendliness.

@klannon @matz-e @WenzhaoLi
","Oh, sorry! I didn't think about it from a more worker+task level. I was thinking more of a fit-from-largest-task perspective. But that does seem to be an open question then, comparing service performance from differently sized worker partitions.

It could be interesting to try and tie in the capacity feature on the factory as well. That way the factory and the master could communicate with one another, and the factory can get the right-sized workers created to handle most of the jobs while not overwhelming the master with too many workers.",8759937
361,wq_factory does not check protocol version,open,2016-05-19T19:58:57Z,2017-05-17T16:14:34Z,,MEMBER,"The master rejects workers that report a different protocol version. This means that the factory keeps creating workers that fail trying to satisfy the master. Ideally, master should report its version to the catalog, and the factory should not spawn workers on a version mismatch. The factory should print a warning indicating the version mismatch.
",,8759937
362,Idea: add crash/statistics reporting to CCTools,open,2016-02-23T14:44:07Z,2016-03-04T19:34:44Z,,MEMBER,"A common problem we have is that Parrot ""broke"" but the user does not know how (easy it is) to report the problem or what we need to address the issue. A useful addition to CCTools would be crash and/or statistics reporting. This could allow us to easily gather:
- stack backtraces on aborts
- last lines of the debug log on failure
- system calls made by Parrot (or other tool specific data)
- cvmfs cache size / number of objects in cache
- forks  and clones made / max number of processes / Is the application using threading?
- number of file descriptors in use

A core question that must be answered first though is (a) when do we send reports and (b) how does the user turn them on (opt-in only!).

Probably (a) is on exit. (b) is the tricky part. Probably the default is don't send anything and we would have a configure flag to change the default to ""on"" . There would be a few command line switches to turn it on/off. The default might be to ask (if there is a controlling terminal) the user if they want to submit a bug report. Otherwise drop the report in $PWD and let the user decide what to do.

Please comment here with ideas or concerns.
","Of course, going that route, there will always be the danger that the user misconfigures their script and spams you with many bug reports for the same issue...
",8759937
363,Ability to drain a Work Queue Factory,open,2016-01-15T18:25:59Z,2019-05-10T17:11:54Z,,NONE,"I'm not sure if this is even a sensible request, but it would be really nice if you could ""drain"" a work queue factory--in other words, tell a factory that it should reduce the number of connected workers, but only by removing workers after they complete their current tasks.
","With the --feature option of the worker, we can have a more general solution, and not a condor specific solution.

1. The factory generates some kind of id name FACTORY_NAME. This name can be set as an option to the executable.
2. The workers are launched with --feature FACTORY_NAME.
3. When downsizing, the factory sets ""draining"":true, in its report to the catalog.
4. The master reads the info of the catalog for the factories its workers come from.
5a. Master sends shutdown to workers from that factory not running any task.
5b. No task is dispatched to workers from that factory, until ""draining"" is false.

",8759937
364,no-worker-for-task policy,open,2015-12-03T17:57:54Z,2017-05-17T16:14:34Z,,MEMBER,"If a task cannot be dispatched given its resource requirements (i.e., there is not a worker big enough connected to the master), what should we do?

We need to communicate with the user somehow. We cannot cancel the task right away, as a worker may be in the process of connecting. I propose to have calls such as:

work_queue_cancel_task_on_memory(q, N);
in which tasks which use N memory or more, are cancelled.

Also:
r = work_queue_missing_resources(q);
in which r is a summary struct with the maximum resources missing. If no resource is missing, r is NULL.

@dthain, @nhazekam 
","Cool, though in that case I agree with your above statement that we need to communicate whats happening to the users, so in cases of task cancellation they are aware it was removed.
",8759937
365,Investigate potential Parrot speed ups with PTRACE_SECCOMP,open,2014-07-04T18:20:38Z,2018-12-11T19:07:29Z,,NONE,"Here's a clever (possibly overly-clever?) idea to speed up the execution of tracee processes when using parrot.

seccomp mode 2 allows one to attach a BPF program to system calls (see http://lwn.net/Articles/475049/).  This allows one to do rudimentary filtering of system calls within the kernel context before the kernel actually executes the syscall.  You can do things like return a value immediately, send a signal to the process, or returning SECCOMP_RET_TRACE.  If you enable PR_SET_NO_NEW_PRIVS, you can install a syscall filter as an unprivileged user.

Next, the PTRACE_O_TRACESECCOMP option to PTRACE_EVENT (argh - helpfully underdocumented outside of the commit message http://people.debian.org/~sf/seccomp-filter-backport/0012-ptrace-seccomp-Add-PTRACE_SECCOMP-support.patch) stops the tracee only when SECCOMP_RET_TRACE is invoked.

Put together, you could whitelist the system calls here https://github.com/cooperative-computing-lab/cctools/blob/3f28d98be47211d13c27771269f616f2cd03c846/parrot/src/pfs_dispatch.cc#L2799 and only incur the ptrace overhead for system calls that parrot must manage by using PTRACE_EVENT instead of PTRACE_SYSCALL.  Further, one could also do the automatic handling here https://github.com/cooperative-computing-lab/cctools/blob/3f28d98be47211d13c27771269f616f2cd03c846/parrot/src/pfs_dispatch.cc#L2799 completely within the kernel context (although that is probably superfluous - the whole point of doing this is to speed things up and none of those appear to be common syscalls).

This would be especially helpful for tracee programs like Java which are much slower under parrot because they heavily use syscalls like clock_gettime and futex.

Something for a rainy day...
",This could potentially improve Parrot's performance. Enough time has passed that the necessary kernel features should be available. Investigating this feature might make for an interesting student project or something. No reason to close this issue now.,8759937
366,Work Queue: Improved Persistence,open,2013-10-11T20:29:19Z,2018-12-06T21:14:17Z,,CONTRIBUTOR,"Hi there,

I wanted to say thanks again for letting me present my work at your workshop!  I wanted to be there in person, but I couldn't because I was filling out some job applications.  I hope the remote presentation went okay.

This is slightly related to issue #213 .  Recently I have found that workers are much more robust against failed connections (i.e. less likely to get ""Failed to read from worker"") if they connect through SSH port forwarding.  Previously I had been using this method to get around firewall restrictions, but they seem to be useful even when there is no firewall.

For compute nodes at Stanford connecting directly to my workstation, I can lose up to 500 out of 1000 workers overnight due ""Failed to read from worker"" errors.  I am not sure when this issue started but it seems to have become more severe in recent weeks.  I think it might be due to faulty network traffic but I'm not really sure.  On the other hand, if I forward the connections through SSH on the cluster's head node, all of the workers stay connected.  The drawback is that if there are too many connections being forwarded through the head node, all of the connections are broken when ssh on the head node crashes, due to a ""broken pipe"" or ""too many open files"" error.

Since the SSH port forwarding operation is a bit complicated, I'll reiterate it here.  When the compute node connects directly to my workstation (no forwarding), the command is:

```
./work_queue_worker leeping.stanford.edu 7329
```

With port forwarding, the command is:

```
ssh -N -f -o ServerAliveInterval=180 -L7329:leeping.stanford.edu:7329 leeping@leeping.stanford.edu (on hs-ln01, the head node)
ssh -o ServerAliveInterval=180 -N -f -L7329:localhost:7329 hs-ln01 (on compute node)
./work_queue_worker localhost 7329
```

I have this integrated into my custom job script so it doesn't require any extra work from the user, just an extra `-f` argument to torque_submit_workers.
- Lee-Ping

PS:  In all of my masters, I disabled the Work Queue ""keepalive packet"".  This is because the keepalive packet appeared to cause more disconnections rather than prevent them, since the master will disconnect the workers that don't send a keepalive response.  However, the keepalive packet might also create network traffic so that the OS won't kill the connection due to idle time.  Is this correct?
","Still a problem.  Disconnections can occur even with the timeout set high.
",8759937
367,Job wall time as a resource advertised by the worker.,open,2013-09-03T16:30:19Z,2018-12-06T20:41:19Z,,MEMBER,"This feature request comes from Lee-Ping as below:

This resource would basically keep track of the seconds that are left in the batch job.  Since my jobs are long (20000s - 72000s) and the nodes have a 24 hour wall time limit (86400s), it creates a bad situation when a worker first finishes a 20000s job, then it starts a 72000s job but is killed after 66400 seconds.  Since I have an idea of how long jobs are expected to take, I can submit my tasks with a ""walltime resource"" requirement of 80000 seconds, and they will only start running on relatively fresh workers.  Again, this shouldn't affect how long the task actually takes to run, it will just prevent the job from running on workers which don't have enough time left.
",@btovar this can be closed. It seems that  it would cause too much of a change to WQ to effectively do. Maybe it's something for WQ2?,8759937
368,Work Queue Temporary Files,open,2013-07-24T18:02:17Z,2018-12-06T20:39:02Z,,MEMBER,"Per Lee-Ping, add a mode to Work Queue to optimize the case of a large file that is generated by one task and then consumed by another, but is not necessary to return back to the submission point.

This can be done by adding a flag WORK_QUEUE_TEMPORARY to a file, which indicates that the master should remember its presence, but not transfer it back.  A task that indicates the same file as an input dependency must then be matched to the worker that has the file.

Complication: What happens if a worker storing a temporary file crashes and loses the file?
",@btovar I think the primary requested functionality of this is handled by the request of #145 and a shared file system. This can probably be closed.,8759937
369,Work Queue Cache Recovery,open,2013-07-24T18:00:09Z,2018-12-06T20:37:22Z,,MEMBER,"Lee-Ping would like a mode in which the worker does not delete its cache directory, since the worker may be evicted but return to the node later.

Upon restart, the worker would connect to the master, then iterate over the contents of the cache directory, verifying that each file present matches what the master has, deleting those that do not match.

This would allow the master to stage very large expensive files once, and not re-transmit them in the event of a transient failure.
","@btovar I don't know if I would recommend closing this issue from 4 years ago. It's a good idea that someone hopefully should get around to doing, but if I'm not mistaken, this is going to be covered in WorkQueue 2 no?",8759937
370,Fails to call atm = ATM()?,open,2020-03-20T16:45:44Z,2020-03-20T18:47:48Z,,NONE,"```
OperationalError                          Traceback (most recent call last)
/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1248                     self.dialect.do_execute(
-> 1249                         cursor, statement, parameters, context
   1250                     )

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/default.py in do_execute(self, cursor, statement, parameters, context)
    579     def do_execute(self, cursor, statement, parameters, context=None):
--> 580         cursor.execute(statement, parameters)
    581 

OperationalError: database is locked

The above exception was the direct cause of the following exception:

OperationalError                          Traceback (most recent call last)
<ipython-input-141-90878e6be8ba> in <module>
      3 
      4 from atm import ATM
----> 5 atm = ATM()

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/atm/core.py in __init__(self, dialect, database, username, password, host, port, query, access_key, secret_key, s3_bucket, s3_folder, models_dir, metrics_dir, verbose_metrics)
     51     ):
     52 
---> 53         self.db = Database(dialect, database, username, host, port, query)
     54         self.aws_access_key = access_key
     55         self.aws_secret_key = secret_key

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/atm/database.py in __init__(self, dialect, database, username, password, host, port, query)
     99 
    100         # create ORM objects for the tables
--> 101         self._define_tables()
    102 
    103     def _define_tables(self):

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/atm/database.py in _define_tables(self)
    506         self.Classifier = Classifier
    507 
--> 508         Base.metadata.create_all(bind=self.engine)
    509 
    510     # ##########################################################################

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/sql/schema.py in create_all(self, bind, tables, checkfirst)
   4302             bind = _bind_or_error(self)
   4303         bind._run_visitor(
-> 4304             ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables
   4305         )
   4306 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _run_visitor(self, visitorcallable, element, connection, **kwargs)
   2044     ):
   2045         with self._optional_conn_ctx_manager(connection) as conn:
-> 2046             conn._run_visitor(visitorcallable, element, **kwargs)
   2047 
   2048     class _trans_ctx(object):

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _run_visitor(self, visitorcallable, element, **kwargs)
   1613 
   1614     def _run_visitor(self, visitorcallable, element, **kwargs):
-> 1615         visitorcallable(self.dialect, self, **kwargs).traverse_single(element)
   1616 
   1617 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/sql/visitors.py in traverse_single(self, obj, **kw)
    136             meth = getattr(v, ""visit_%s"" % obj.__visit_name__, None)
    137             if meth:
--> 138                 return meth(obj, **kw)
    139 
    140     def iterate(self, obj):

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/sql/ddl.py in visit_metadata(self, metadata)
    779                     create_ok=True,
    780                     include_foreign_key_constraints=fkcs,
--> 781                     _is_metadata_operation=True,
    782                 )
    783             else:

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/sql/visitors.py in traverse_single(self, obj, **kw)
    136             meth = getattr(v, ""visit_%s"" % obj.__visit_name__, None)
    137             if meth:
--> 138                 return meth(obj, **kw)
    139 
    140     def iterate(self, obj):

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/sql/ddl.py in visit_table(self, table, create_ok, include_foreign_key_constraints, _is_metadata_operation)
    824                 table,
    825                 include_foreign_key_constraints=  # noqa
--> 826                     include_foreign_key_constraints,
    827             )
    828             # fmt: on

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/base.py in execute(self, object_, *multiparams, **params)
    986             raise exc.ObjectNotExecutableError(object_)
    987         else:
--> 988             return meth(self, multiparams, params)
    989 
    990     def _execute_function(self, func, multiparams, params):

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/sql/ddl.py in _execute_on_connection(self, connection, multiparams, params)
     70 
     71     def _execute_on_connection(self, connection, multiparams, params):
---> 72         return connection._execute_ddl(self, multiparams, params)
     73 
     74     def execute(self, bind=None, target=None):

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_ddl(self, ddl, multiparams, params)
   1048             compiled,
   1049             None,
-> 1050             compiled,
   1051         )
   1052         if self._has_events or self.engine._has_events:

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1251         except BaseException as e:
   1252             self._handle_dbapi_exception(
-> 1253                 e, statement, parameters, cursor, context
   1254             )
   1255 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception(self, e, statement, parameters, cursor, context)
   1471                 util.raise_from_cause(newraise, exc_info)
   1472             elif should_wrap:
-> 1473                 util.raise_from_cause(sqlalchemy_exception, exc_info)
   1474             else:
   1475                 util.reraise(*exc_info)

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/util/compat.py in raise_from_cause(exception, exc_info)
    396     exc_type, exc_value, exc_tb = exc_info
    397     cause = exc_value if exc_value is not exception else None
--> 398     reraise(type(exception), exception, tb=exc_tb, cause=cause)
    399 
    400 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/util/compat.py in reraise(tp, value, tb, cause)
    150             value.__cause__ = cause
    151         if value.__traceback__ is not tb:
--> 152             raise value.with_traceback(tb)
    153         raise value
    154 

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)
   1247                 if not evt_handled:
   1248                     self.dialect.do_execute(
-> 1249                         cursor, statement, parameters, context
   1250                     )
   1251         except BaseException as e:

/anaconda/envs/azureml_py36/lib/python3.6/site-packages/sqlalchemy/engine/default.py in do_execute(self, cursor, statement, parameters, context)
    578 
    579     def do_execute(self, cursor, statement, parameters, context=None):
--> 580         cursor.execute(statement, parameters)
    581 
    582     def do_execute_no_params(self, cursor, statement, context=None):

OperationalError: (sqlite3.OperationalError) database is locked
[SQL: 
CREATE TABLE datasets (
	id INTEGER NOT NULL, 
	name VARCHAR(100) NOT NULL, 
	class_column VARCHAR(100) NOT NULL, 
	train_path VARCHAR(200) NOT NULL, 
	test_path VARCHAR(200), 
	description VARCHAR(1000), 
	n_examples INTEGER NOT NULL, 
	k_classes INTEGER NOT NULL, 
	d_features INTEGER NOT NULL, 
	majority NUMERIC(10, 9) NOT NULL, 
	size_kb INTEGER NOT NULL, 
	PRIMARY KEY (id)
)

]
(Background on this error at: http://sqlalche.me/e/e3q8)
```",I have transferred the issue as it does not related to ATMSeer,70934951
371,Running on Windows without a VirtualBox,open,2019-11-19T04:36:59Z,2019-12-11T06:39:11Z,,NONE,"* ATM version:
* Python version:3.6
* Operating System:windows

### Description

Describe what you were trying to get done.
Tell us what happened, what went wrong, and what you expected to happen.

### What I Did
I am trying to use the atm Cli to get a data file and train it 


```
Paste the command(s) you ran and the output.
If there was a crash, please include the traceback here.

command: 
atm enter_data --train-path path/toTheFile
result:
    import pwd
ModuleNotFoundError: No module named 'pwd'

So i can't run Atm using a virtual machine, so i have to run it purely on windows, pwd is a python module used for Unix like systems,so windows goes crazy, i tried importing winpwd but still to no effect, maybe i am importing winpwd at a non optimum location
```
","@pvk-developer,the Atm cli works with a Linux distribution system, i used Ubuntu, i needed the cli just for testing, but i if someone is looking for a better solution, they should look into Windows Sub System for Linux, it helps you run a Linux environment on you local machine.


""The Windows Subsystem for Linux lets developers run a GNU/Linux environment -- including most command-line tools, utilities, and applications -- directly on Windows, unmodified, without the overhead of a virtual machine.""--https://docs.microsoft.com/en-us/windows/wsl/about ",70934951
372,Support for classifiers accepting sparse matrix formats only,open,2019-09-21T14:43:43Z,2019-09-21T14:43:43Z,,NONE,"Hi @csala!,

Sorry it's been so long!

I've been making progress with my research but hit a snag recently when trying to implement the state of the art SVM [ThunderSVM](https://github.com/Xtra-Computing/thundersvm).
Unfortunately, ThunderSVM only accepts sparse matrix format as the data input.
It is based on a another SVM implementation (LIBSVM). LIBSVM provides a tool to translate between input formats.
My question is, how hard would it be to implement a feature into ATM that would:
1. Use the LIBSVM conversion tool in a 'pre-process' sort of stage in ATM, to convert standard input data to LIBSVM sparse format.
2.  Use the converted data from step 1 when calling the ThunderSVM (or any other similar classifier) classifier.
3. Use the results from step 2 as normal, storing them in the ModelDB hub.

I am looking at alternatives to ThunderSVM in case this is not possible/ really hard.
I think implementing standard input into ThunderSVM itself would be even harder, but I will open an issue on their repo too.

** As a side note, when I do use ThunderSVM in ATM, it  _seems_ that ATM runs it as usual (but it does get stuck sometimes). It's probably just that ATM is interpreting the sparse format as standard input. But it is curious that is isn't breaking. Let me know if I should open a separate issue and fill in all the technical details.

Thanks!",,70934951
373,Fix warnings when running pytest,open,2018-10-11T21:06:45Z,2018-10-11T21:07:15Z,,NONE,"Currently, when I run pytest, I receive a whole heap of warnings. It would be best to fix these before they break.

```
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Iterable, Mapping

/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/dateutil/parser/__init__.py:46: DeprecationWarning: _timelex is a private class and may break without warning, it will be moved and or renamed in future versions.
  warnings.warn(msg, DeprecationWarning)
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/dateutil/parser/__init__.py:46: DeprecationWarning: _timelex is a private class and may break without warning, it will be moved and or renamed in future versions.
  warnings.warn(msg, DeprecationWarning)
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/dateutil/parser/__init__.py:46: DeprecationWarning: _timelex is a private class and may break without warning, it will be moved and or renamed in future versions.
  warnings.warn(msg, DeprecationWarning)
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/dateutil/parser/__init__.py:46: DeprecationWarning: _timelex is a private class and may break without warning, it will be moved and or renamed in future versions.
  warnings.warn(msg, DeprecationWarning)
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/sqlalchemy/sql/sqltypes.py:603: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.
  'storage.' % (dialect.name, dialect.driver))

/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/dateutil/parser/__init__.py:46: DeprecationWarning: _timelex is a private class and may break without warning, it will be moved and or renamed in future versions.
  warnings.warn(msg, DeprecationWarning)
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/dateutil/parser/__init__.py:46: DeprecationWarning: _timelex is a private class and may break without warning, it will be moved and or renamed in future versions.
  warnings.warn(msg, DeprecationWarning)
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/dateutil/parser/__init__.py:46: DeprecationWarning: _timelex is a private class and may break without warning, it will be moved and or renamed in future versions.
  warnings.warn(msg, DeprecationWarning)
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/dateutil/parser/__init__.py:46: DeprecationWarning: _timelex is a private class and may break without warning, it will be moved and or renamed in future versions.
  warnings.warn(msg, DeprecationWarning)
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/sqlalchemy/sql/sqltypes.py:603: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.
  'storage.' % (dialect.name, dialect.driver))
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
/Users/arcarter/.virtualenvs/mashup/lib/python3.7/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
```",This would also probably allow us to unfreeze some of the requirements,70934951
374,Use MLBlocks for methods backend,open,2018-09-30T17:04:55Z,2018-11-12T15:18:39Z,,MEMBER,"Currently, ATM defines learning methods (classification only) in json files within the repo. The MLBlocks/MLPrimitives projects provide similar functionality, but include many more methods, including neural networks, gradient boosted machines, and regression models (#72). We should explore using these projects as the backend for learning methods.","Here are some thoughts on implementation:
- For each method in `atm/methods/*.json`, find a corresponding primitive in `HDI-Project/MLPrimitives/mlblocks_primitives`, such as RandomForestClassifier https://github.com/HDI-Project/MLPrimitives/blob/master/mlblocks_primitives/sklearn.ensemble.RandomForestClassifier.json
- Replace `atm/methods/` dir with `atm.constants.primitives` or similar, a list of strings identifying MLPrimitives
- Use MLBlocks to make a pipeline: https://github.com/HDI-Project/ATM/blob/master/atm/model.py#L95

Ultimately, we want people to be able to include/upload their own primitives for use with MLBlocks.",70934951
375,Add support to record selection information for a datarun,open,2018-07-08T08:28:59Z,2018-09-06T18:00:57Z,,NONE,"I am working on ATM-VIS, a web-based GUI for atm.
Currently, the db only stores classifier information. Users have no clues of how each hyperpartition is chosen at each step, and they have no hints on which selector to use. Providing the selector information at each step might help the users better understand or trust the ATM procedures (or leads to improvements of the algorithm).

There is a need to provide selection information (rewards/arm scores) during (or after) a data run to the users (so that the users can choose a selector that suits their needs).

For example, we would like to plot how the rewards of each hyperpartition (or method) changes as the datarun proceeds","What about the `compute_rewards` API method? https://hdi-project.github.io/BTB/btb.selection.html#btb.selection.selector.Selector.compute_rewards

The ATM code would have to be modified to get the rewards from BTB and log them accordingly at each step.",70934951
376,Isolate unit tests from the outside,open,2018-05-05T09:38:18Z,2019-02-19T21:27:06Z,,MEMBER,"Current unit tests create a lot of tests elements both in the current working directory and in `/tmp/`, and rely on them for the testing.

This should be changed, and mocks should be used to prevent the unit tests from really accessing and using any element from outside the test itself.",,70934951
377,Avoid creating redundant datasets,open,2018-01-31T05:40:24Z,2019-02-19T21:23:26Z,,CONTRIBUTOR,"If `enter_data()` is called with the same `train_path` twice in a row and the data itself hasn't changed, a new Dataset does not need to be created. 

We should add a column which stores some kind of hash of the actual data. When a Dataset would be created, if the metadata and data hash are exactly the same as an existing Dataset, nothing should be added to the ModelHub database and the existing Dataset should be returned instead.",,70934951
378,Support for regressions ,open,2018-01-29T12:14:59Z,2019-02-19T21:22:42Z,,NONE,"Hello, If I understood correctly currently ATM solves only classification tasks. I wonder if there is a plan for adding support for regression problems. Thanks","Yes! The plan is to eventually add support for other kinds of models, starting with regressions. However, this will come a little bit down the road -- if you'd like the functionality soon, you might have to fork the project and build it yourself. If you do, feel free to open up a PR and contribute!",70934951
379,Separating repeated processing from classifier models,open,2018-01-25T16:52:05Z,2019-02-19T21:20:30Z,,CONTRIBUTOR,"In between different runs of the ATM, the outputs of all the steps of the pipeline are ""static,"" except for the input and output to the classifier that is chosen by BTB.  What I mean by this is, for example, suppose PCA is in the pipeline, then every time ATM/BTB chooses a new model to run, it will recompute the PCA for the same dataset.  Unless I'm misunderstanding the flow of data, this seems inefficient.  Although the current pipeline is pretty simple (scaling/PCA), there could be more computationally intensive elements to the pipeline that people may want to add.  

We can separate the pipeline into two pipelines, one that is ""static"" and the outputs stored somewhere to disk such that it can be recalled between runs, and a ""dynamic"" which is essentially the classifier, and any blocks which change based on the ATM/BTB model being run.

If you think this is a good idea, how do we want to go about architecting this from a software perspective?  One approach is to compute the static pipeline before the `test_classifier` method is run and save that to the data directory where the train/test dataset is being saved.",This could be implemented by a feature of MLBLocks (#113) and should wait on the resolution of that issue.,70934951
380,Dockerisation and parallelisations,open,2018-01-22T17:52:41Z,2019-02-19T21:17:48Z,,NONE,Dockerisation of this project would be great. Makes it easy to install and deploy :) It would also be great to incorporate Celery in so that the jobs can be dispatched to different worker instances. ,"Hey @zf109 thanks for the suggestion. Maybe this is too broad for one issue. Can you split dockerization and incorporating celery into two separate issues, and provide some details about how you might go about this? Thanks!",70934951
381,Add database command to remove dataruns,open,2018-01-15T21:14:07Z,2019-10-08T14:06:16Z,,CONTRIBUTOR,"Right now, if you create a datarun with a typo or just decide you don't want to run it, there's no simple way to remove it from the database. We should add it as a subcommand to `enter_data.py`. Maybe:
```
python enter_data.py remove --datarun 1
```
likewise,
```
python enter_data.py remove --dataset 1
```","Doesn't look like this was implemented. Definitely wasn't updated in docs (as far as I can tell).
This feature is actually essential for developers trying to implement custom classifiers, because testing becomes almost impossible when we have to manually kill ATM.

Edit:
To those facing the same problem I get around it by deleting atm.db.
This removes all previously created/halted dataruns.",70934951
382,Add in-memory database option,open,2018-01-15T18:22:50Z,2018-11-28T19:44:20Z,,CONTRIBUTOR,"In general, we would like to better support the use of ATM as a normal library. It should be just as convenient to use pieces of ATM in another python project as it is to use the system from the command line.

To that end, we should add a database option that is purely in-memory -- and does not leave a file system footprint unless requested. For example, we could use pandas `DataFrames` in the back-end instead of tables in a SQL database while maintaining the same interface for the `Database` class.",Another option is sqlite in-memory (`:memory:`),70934951
383,Allow custom evaluation metrics ,open,2018-01-14T20:07:24Z,2019-02-19T21:05:38Z,,CONTRIBUTOR,"Right now, it's only possible to configure a datarun to use methods and metrics that are included with the library. It should be possible to pass JSON files for custom machine-learning methods and Python files/functions for custom metrics. This can be implemented in much the same way that custom tuners/selectors for BTB are handled.",This issue should only pertain to custom metrics. Custom methods are covered in #113.,70934951
384,Update fabfile.py to fix AWS compatibility,open,2017-12-29T19:33:06Z,2019-02-19T21:00:35Z,,CONTRIBUTOR,"The fabfile is currently out of sync with the rest of the codebase, making it impossible to automatically launch an ATM cluster on AWS. This needs to be fixed.
",@csala can you look into this?,70934951
385,"GaussianProcessClassifier errors with ""N-th leading minor is not positive definite""",open,2017-12-24T23:30:17Z,2019-02-19T20:58:29Z,,CONTRIBUTOR,"Appears to only happen when `kernel == 'exp_sine_squared'`. Does not happen every time. More investigation needed.  

```
Error testing classifier: datarun=<ID = 24, dataset ID = 10, strategy = gp__bestk, budget = classifier (100), status: running>
Traceback (most recent call last):
  File ""/home/bcyphers/work/fl/atm/atm/worker.py"", line 401, in run_classifier
    model, performance = self.test_classifier(hyperpartition.method, params)
  File ""/home/bcyphers/work/fl/atm/atm/worker.py"", line 339, in test_classifier
    test_path=test_path)
  File ""/home/bcyphers/work/fl/atm/atm/model.py"", line 195, in train_test
    cv_scores = self.cross_validate(X_train, y_train)
  File ""/home/bcyphers/work/fl/atm/atm/model.py"", line 132, in cross_validate
    n_folds=self.N_FOLDS)
  File ""/home/bcyphers/work/fl/atm/atm/metrics.py"", line 194, in cross_validate_pipeline
    pipeline.fit(X[train_index], y[train_index])
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/pipeline.py"", line 270, in fit
    self._final_estimator.fit(Xt, y, **fit_params)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/gaussian_process/gpc.py"", line 610, in fit
    self.base_estimator_.fit(X, y)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/multiclass.py"", line 216, in fit
    for i, column in enumerate(columns))
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 758, in __call__
    while self.dispatch_one_batch(iterator):
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 608, in dispatch_one_batch
    self._dispatch(tasks)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 571, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 109, in apply_async
    result = ImmediateResult(func)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 326, in __init__
    self.results = batch()
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/multiclass.py"", line 80, in _fit_binary
    estimator.fit(X, y)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/gaussian_process/gpc.py"", line 208, in fit
    self.kernel_.bounds)]
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/gaussian_process/gpc.py"", line 426, in _constrained_optimization
    fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/scipy/optimize/lbfgsb.py"", line 193, in fmin_l_bfgs_b
    **opts)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/scipy/optimize/lbfgsb.py"", line 328, in _minimize_lbfgsb
    f, g = func_and_grad(x)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/scipy/optimize/lbfgsb.py"", line 278, in func_and_grad
    f = fun(x, *args)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/scipy/optimize/optimize.py"", line 292, in function_wrapper
    return function(*(wrapper_args + args))
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/scipy/optimize/optimize.py"", line 63, in __call__
    fg = self.fun(x, *args)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/gaussian_process/gpc.py"", line 200, in obj_func
    theta, eval_gradient=True)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/gaussian_process/gpc.py"", line 344, in log_marginal_likelihood
    self._posterior_mode(K, return_temporaries=True)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/sklearn/gaussian_process/gpc.py"", line 397, in _posterior_mode
    L = cholesky(B, lower=True)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/scipy/linalg/decomp_cholesky.py"", line 81, in cholesky
    check_finite=check_finite)
  File ""/home/bcyphers/work/fl/atm/venv/lib/python2.7/site-packages/scipy/linalg/decomp_cholesky.py"", line 30, in _cholesky
    raise LinAlgError(""%d-th leading minor not positive definite"" % info)
LinAlgError: 31-th leading minor not positive definite
```",Can anyone reproduce this and provide a minimal working example?,70934951
386,Add minimal unit tests,open,2017-12-24T23:23:26Z,2019-02-21T03:31:48Z,,CONTRIBUTOR,"Right now there are just a couple of actual tests in the `test/` folder, and they leave huge portions of the code untouched.

Eventually, we will need a suite of unit tests and comprehensive integration tests that reasonably convince us that a new change won't break anything. We'll also want to integrate with [CircleCI](https://circleci.com/) and github so that we can evaluate pull requests from the web (but that will come later).

I'll try to update this issue with our progress as time goes on, but a good start would be unit tests for:
- each hyperpartition for each classification method
- each `database.py` create/query/update function
- hyperpartition enumeration
- hyperpartition selection/parameter tuning
- metric computation in `metrics.py`
- data loading and encoding with a variety of quirky data types
- serialization/deserialization of models/metrics objects","Ok great I will try to build the environment, any problem I'II post here...thanks!",70934951
387,LHCbCompressedDatasetFix,open,2020-03-27T12:13:41Z,2020-03-27T12:23:13Z,,CONTRIBUTOR,`isEmpty` was backwards.,,43645564
388,Allow for LocalFile in task chain input data,open,2020-03-24T18:45:49Z,2020-03-25T22:09:33Z,,CONTRIBUTOR,Allows for LHCbTransform to chain input data with LocalFile in case of batch running.,"> I think you are right, we should allow for MassStorageFile as well. But I guess we still want to keep the inputdata to a single type?

Is this not part of a larger somewhat unresolved issue in Ganga where we allow inputdata to be a mix of different types of `GangaFile` objects. Often things will fail if that is the case. I do not think this is the place where we want to enforce that restriction (as it implies to a broader area).",43645564
389,Createa permanent server for the report functionality,open,2020-03-15T23:02:56Z,2020-03-27T06:58:25Z,,MEMBER,The `report()` function has been updated but we can no longer use the CERN based server. There is a need to create a new one. See #1587. ,"@egede I have created a repository where I have pushed all the codes and instructions which need to be done to automate the whole process that we have discussed earlier. Below is the repo link:
https://github.com/RajatSingh95/ec2-automation

Please have a look at it and let me know if you encounter any problem.",43645564
390,Condor jobs stop being updated in `jobs` list?,open,2020-03-12T13:23:33Z,2020-03-25T22:24:11Z,,NONE,"I noticed that sometimes jobs with the `Condor` backend stop tracking the status of the actual jobs on Condor. I.e. they have a perpetual state of ""submitted"" instead of transitioning to ""running"" or ""completed"". I suspect that it might have something to do with the jobs being moved around by the actual Condor system. I.e. I have seen this happen when the job is ""held"" by Condor and subsequently ""released"" to a different node. In this case the `actualCE` reported by ganga is also different from the one the Condor job is actually running on.

By the way, should a Condor job with the ""submitted"" status even have a `backend.actualCE` value?","> Just a quick update. I noticed that Condor jobs stop being updated as soon as I `.kill()` and then `.resubmit()` them. They do not have to go through the ""held"" state.

Thanks for this report. Sounds like a clear bug to trace down!",43645564
391,Make backend script uniform for Batch and Local backend,open,2020-02-29T17:37:53Z,2020-03-09T00:41:50Z,,MEMBER,Ref: #1592 #1588,"I still see error in the CI tests as given below. You should be able to reproduce them yourself just using `pytest`. Be aware that the first test just fails with a timeout after a very long time. Compare to the `develop` branch where is succeeds almost straight away. 

```
ganga/GangaCore/test/GPI/TestSubjobs.py
self = <GPI.TestSubjobs.TestSubjobs testMethod=testKilling>

    def testKilling(self):
        """"""
        Create some subjobs and kill them
        """"""
        from GangaCore.GPI import Job, GenericSplitter, Local
        from GangaTest.Framework.utils import sleep_until_state
        j = Job()
        j.application.exe = ""sleep""
        j.splitter = GenericSplitter()
        j.splitter.attribute = 'application.args'
        j.splitter.values = [['400'] for _ in range(0, 5)]
        j.backend = Local()
        j.submit()
    
        sleep_until_state(j, None, 'running')
>       assert j.status == 'running'
E       AssertionError: assert 'completed' == 'running'
E         - completed
E         + running


ganga/GangaCore/test/GPI/Bugs/TestSavannah19123.py
_____________________ TestSavannah19123.test_Savannah19123 _____________________
self = <GPI.Bugs.TestSavannah19123.TestSavannah19123 testMethod=test_Savannah19123>
    def test_Savannah19123(self):
        from GangaCore.GPI import Job, Local
    
        from GangaTest.Framework.utils import sleep_until_state
    
        # check if stdout and stderr exists or not, flag indicates if files are required to exist or not
    
        def check(exists_flag):
            for fn in ['stdout','stderr']:
                fn = os.path.join(j.outputdir,fn)
                file_exists = os.path.exists(fn)
                if exists_flag:
                    self.assertTrue(file_exists, 'file %s should exist but it does not' % fn)
                else:
                    self.assertFalse(file_exists, 'file %s should not exist but it does' % fn)
    
        j = Job()
        j.application.exe = 'bash'
        j.application.args = ['-c', 'for i in `seq 1 30`; do echo $i; sleep 1; done']
        j.backend = Local()
    
        j.submit()
    
        check(False)
    
        if not sleep_until_state(j, 5, 'running'):
            # problem with the test - print out stdout/stderr and assert
            for fn in ['stdout','stderr']:
                fn = os.path.join(j.outputdir,fn)
                print("" ----  Contents of "" + fn)
                if os.path.exists(fn):
                    print(open(fn).read())
                else:
                    print(""NO FILE AVAILABLE"")
    
>           self.assertEqual(j.status, 'running')
E           AssertionError: 'submitted' != 'running'
E           - submitted
E           + running


```",43645564
392,Make backend wrappers work in both python2 and python3,open,2020-02-27T22:57:21Z,2020-03-01T22:53:11Z,,MEMBER,The backend wrapper for the local backend https://github.com/ganga-devs/ganga/blob/develop/ganga/GangaCore/Lib/Localhost/LocalHostExec.py.template explcitly requires python2. The simple script (and the code injected into it) should be agnostic to python2/3 and the shebang changed to just say `/bin/env python`,"The problem is still there, see https://github.com/ganga-devs/ganga/blob/d9d8f56601d83a321d52ad5f8c90bc091c9ccc9d/ganga/GangaCore/Lib/Localhost/LocalHostExec.py.template#L1
However, it will be solved by #1598 ",43645564
393,Make backend wrappers more uniform,open,2020-02-25T09:41:38Z,2020-02-25T11:10:36Z,,MEMBER,"The backend wrapper scripts for different backends are different for no good reason. 

- Local: https://github.com/ganga-devs/ganga/blob/develop/ganga/GangaCore/Lib/Localhost/LocalHostExec.py.template
- Batch: https://github.com/ganga-devs/ganga/blob/develop/ganga/GangaCore/Lib/Batch/BatchScriptTemplate.py.template
- Condor: https://github.com/ganga-devs/ganga/blob/a053267c3e8ffa1c6e4adb685c5ae49a66eed9ea/ganga/GangaCore/Lib/Condor/Condor.py#L396

Some differences may have to remain, but for sure there can be some consolidation. ",I am working on this.,43645564
394,Add database support to ganga,open,2020-02-12T07:56:04Z,2020-02-12T07:56:04Z,,MEMBER,,,43645564
395,Put limit on subjobs that can pull virtualization image from the same server.,open,2019-12-10T03:50:51Z,2020-01-01T22:28:15Z,,MEMBER,,"Returning to this after the holidays.

At least for Singularity, I do not think that `$TMPDIR` is used for anything. The wrapper sets `$SINGULARITY_CACHEDIR` to the working directory, so it should be confined to there. If `$TMPDIR` is not used at all (I will test), should we then reconsider some of the statements below? See my first comments.

- *Never pull containers on worker nodes, always ship them as a input data, regardless of source*
In Ganga, the inputsandbox is uploaded to the Grid before submission (to prevent uploading the same sandbox several thousand times in case of subjobs), so to enforce this rather than having it as a `DiracFile` (i.e a file on a storage element), seems silly. We could rather implement a block on downloading directly from `Gitlab` at CERN to prevent overloading of their server.

- *Prevent users from submitting containers which are larger than 1GB*
I do not really see how we can effectively limit the size of the container prior to submitting, but we can make the jobs fail if it is the case.

- *Prevent too many jobs being submitted that use job.virtualization (~10)*
A limit can be placed on the number of subjobs. If submitted as individual jobs, there is nothing that can be done though (but the warning below will be displayed for every job).

- *Display a prominent warning whenever job.virtualization is used to state that this is experimental and abuse (i.e. avoiding these restrictions) will result in grid access being immediately revoked*
Sure.
",43645564
396,Condor backend is missing worker noed code injection,open,2019-12-05T04:13:32Z,2019-12-05T04:13:49Z,,MEMBER,"As faras I can see, there is no code injection into the worker node script for the `Condor` backend. So if I have the default configuration 

```
MassStorageFile = {'fileExtensions': [''], 'backendPostprocess': {'LSF': 'WN', 'PBS': 'WN', 'Condor': 'WN', 'SGE': 'WN', 'Slurm': 'WN', 'LCG': 'client', 'CREAM': 'client', 'ARC': 'client', 'Local': 'WN', 'Interactive': 'client', 'Dirac': 'client'}, 'uploadOptions': {'mkdir_cmd': 'mkdir', 'cp_cmd': 'cp', 'ls_cmd': 'ls', 'path': '/tmp'}, 'defaultProtocol': 'root://eos.cern.ch'}
```
and then do

```
j.inputfiles = [MassStorageFile(...)]
```
the file will never be copied to the WN (The configuration asks for it to happen on the WN, but the WN code doesn't inject any code for it). See https://github.com/ganga-devs/ganga/blob/b8badbbf36c599cf2345256d08868b624c434f9f/ganga/GangaCore/Lib/Condor/Condor.py#L433",,43645564
397,Create GitHub Actions pythonpackage.yml,open,2019-11-13T16:52:35Z,2019-11-13T16:52:35Z,,CONTRIBUTOR,Testing GitHub Actions,,43645564
398,PEP8 Ganga Codebase,open,2019-11-01T14:08:05Z,2020-02-19T09:42:54Z,,CONTRIBUTOR,,"So the packages to convert are:

- GangaCore
- GangaDirac
- GangaLHCb
- GangaTutorial
- GangaTest
- GangaGaudi
",43645564
399,Misbehaving logger,open,2019-08-07T09:36:34Z,2019-08-30T06:44:25Z,,MEMBER,"Message taken from #1470 by @JacekHoleczek 

Now, when the ""high CPU usage"" problem is confirmed, I would like to return to another problems that I reported here.

I think I can live with the misbehaving logger. Under normal conditions, the amount of ""screen output"" is not that big (just the usual messages about jobs being submitted and completed) so, I think I can enforce _interactive_cache = False (note: --very-quiet is not an option, I really want to see what happens).

However, I would really like to get the misbehaving ganga prompt fixed (pressing any key destroys the last printed ""INFO"" line, or maybe even two lines). It seems to me that the origin of this problem is that the prompt uses two lines. I haven't found any option in the "".gangarc"" file to (re)define my own prompt (e.g. I could imagine having a one line ""[HH:MM:SS] In [...]:"" prompt).","So, the ""misbehaving logger"" problem started in the https://github.com/ganga-devs/ganga/issues/1453 thread for the ganga 7.1.12 version. But I used this version for a short time only as a test (my ""production version"" was 6.0.44). I then switched to ganga 7.1.14 and you can see my full report in point 4. of [my first post](https://github.com/ganga-devs/ganga/issues/1470#issue-448467697) in the https://github.com/ganga-devs/ganga/issues/1470 thread and then also some additional notes in [my second post](https://github.com/ganga-devs/ganga/issues/1470#issuecomment-496496005) there.

Note that the ""prompt"" problem is related but I guess it is a separate issue.",43645564
400,Adds proxy for copy on write,open,2019-06-13T07:14:56Z,2020-02-19T09:20:21Z,,MEMBER,,What is the plan for this?,43645564
401,"Status of master job left in ""submitting"" when some subjobs fail to submit.",open,2019-06-12T03:18:11Z,2020-02-02T23:45:38Z,,MEMBER,"If some subjobs fail to submit (error from the batch system), then they are left in the new state. If I then individually call submit on those subjobs, they start OK, but the master job is left in the ""submitting"" state and therefore will not do any monitoring. Restarting Ganga doesn't help - the only remedy is to do `j._impl.status='submitted'` which is not a solution.","@dilawari2008  If interest is in connection with GSoC https://hepsoftwarefoundation.org/gsoc/projects/2020/project_Ganga.html, then please contact me by using the information on the mentor part of the individual projects.",43645564
402,Condor auto scheduler chooser,open,2019-06-11T10:20:43Z,2019-06-11T10:20:43Z,,CONTRIBUTOR,"We should add a flag that makes ganga choose the least busy available scheduler when submitting condor jobs.

We should also implement parallel_submit for Condor.",,43645564
403,INFO line for a completed task missing (ganga 7.1.12),open,2019-04-11T07:25:03Z,2019-04-11T10:42:51Z,,CONTRIBUTOR,"The good old ganga 6.0.44 informed me that the task was completed (finished).
This is now missing in the new ganga 7.1.12.
I get plenty of ""INFO"" lines for all individual jobs but there is no ""final"" line which says that my whole task completed / finished.
Could you, please, add it back (and please make sure that it automatically ""flushes"" the screen / terminal output so that, when ""everything is ready"", I get it automatically on the screen, without any need to press ""enter"" from time to time in order to see if any new INFO is in the ""screen stream buffer"").",That is rather less feasible.,43645564
404,"move ""output"" to ""debug"" for failed jobs (an enhancement proposal)",open,2019-03-17T08:03:47Z,2019-03-18T12:12:30Z,,CONTRIBUTOR,"When a batch job ""dies"" for some reason, ganga automatically creates a ""ganga_job_id/debug"" subdirectory. However, in all cases (PBS, SGE), this subdirectory is completely empty.
Then everything from the ""ganga_job_id/output"" subdirectory is removed and the job is resubmitted.
As a result, one doesn't have any way to ""inspect"" what happened.

So, my proposal is simple ... after creating the ""ganga_job_id/debug"" (each time anew and so completely empty), ganga should MOVE everything from the ""ganga_job_id/output"" subdirectory there.
Then, one would always have the ""last traceback"" available.

Maybe something like this  could be implemented:
rm -rf ganga_job_id/debug # remove the previous ""debug"" contents, if any
mv ganga_job_id/output ganga_job_id/debug # move the ""output"" into ""debug""
mkdir ganga_job_id/output # create ""output"" anew",,43645564
405,GaudiExec problems with parallel submission,open,2019-01-29T14:28:48Z,2019-01-29T14:28:48Z,,CONTRIBUTOR,"There appears to be a problem with GaudiExec and parallel submission a user reports an error of:

```python
ERROR    GangaException: Target File: /grid_mnt/data__DATA/data.lhcb/fleuret/prod-MC/pNe69GeV/D0_4pi/task/L0Dev/MooreDev_v26r6p1/build.x86_64-slc6-gcc62-opt/ganga/input-sandbox.tgz NOT found! ... reverting job 2848 to the new status
ERROR    Couldn't submit the job. Deactivating unit.
INFO     job 2848 status changed to ""new""
ERROR    GangaException: Target File: /grid_mnt/data__DATA/data.lhcb/fleuret/prod-MC/pNe69GeV/D0_4pi/task/L0Dev/MooreDev_v26r6p1/build.x86_64-slc6-gcc62-opt/ganga/input-sandbox.tgz NOT found!
INFO     Unit 2 of transform 2, Task 32 has aborted the loop
INFO     job 2846 status changed to ""running""
INFO     job 2843 status changed to ""running""
INFO     job 2846 status changed to ""completing""
INFO     job 2845 status changed to ""completing""
INFO     job 2843 status changed to ""completing""
INFO     Job 2845 Running PostProcessor hook
INFO     job 2845 status changed to ""completed""
INFO     Job 2846 Running PostProcessor hook
INFO     job 2846 status changed to ""completed""
INFO     Job 2843 Running PostProcessor hook
INFO     job 2843 status changed to ""completed""
INFO     submitting job 2849
INFO     job 2849 status changed to ""submitting""
INFO     Preparing GaudiExec application.
INFO     Make-ing target 'ganga-input-sandbox'     (This may take a few minutes depending on the size of your project)
ERROR    GangaException: Target File: /grid_mnt/data__DATA/data.lhcb/fleuret/prod-MC/pNe69GeV/D0_4pi/task/L0Dev/MooreDev_v26r6p1/build.x86_64-slc6-gcc62-opt/ganga/input-sandbox.tgz NOT found! ... reverting job 2849 to the new status
ERROR    Couldn't submit the job. Deactivating unit.
INFO     job 2849 status changed to ""new""
ERROR    GangaException: Target File: /grid_mnt/data__DATA/data.lhcb/fleuret/prod-MC/pNe69GeV/D0_4pi/task/L0Dev/MooreDev_v26r6p1/build.x86_64-slc6-gcc62-opt/ganga/input-sandbox.tgz NOT found!
INFO     Unit 3 of transform 2, Task 32 has aborted the loop
INFO     job 2842 status changed to ""running""
INFO     submitting job 2850
INFO     job 2850 status changed to ""submitting""
INFO     Preparing GaudiExec application.
INFO     Make-ing target 'ganga-input-sandbox'     (This may take a few minutes depending on the size of your project)
ERROR    GangaException: Target File: /grid_mnt/data__DATA/data.lhcb/fleuret/prod-MC/pNe69GeV/D0_4pi/task/L0Dev/MooreDev_v26r6p1/build.x86_64-slc6-gcc62-opt/ganga/input-sandbox.tgz NOT found! ... reverting job 2850 to the new status
ERROR    Couldn't submit the job. Deactivating unit.
INFO     job 2850 status changed to ""new""
ERROR    GangaException: Target File: /grid_mnt/data__DATA/data.lhcb/fleuret/prod-MC/pNe69GeV/D0_4pi/task/L0Dev/MooreDev_v26r6p1/build.x86_64-slc6-gcc62-opt/ganga/input-sandbox.tgz NOT found!
INFO     Unit 4 of transform 2, Task 32 has aborted the loop
INFO     submitting job 2851
INFO     job 2851 status changed to ""submitting""
INFO     Preparing GaudiExec application.
INFO     Make-ing target 'ganga-input-sandbox'     (This may take a few minutes depending on the size of your project)
ERROR    GangaException: Target File: /grid_mnt/data__DATA/data.lhcb/fleuret/prod-MC/pNe69GeV/D0_4pi/task/L0Dev/MooreDev_v26r6p1/build.x86_64-slc6-gcc62-opt/ganga/input-sandbox.tgz NOT found! ... reverting job 2851 to the new status
ERROR    Couldn't submit the job. Deactivating unit.
INFO     job 2851 status changed to ""new""
ERROR    GangaException: Target File: /grid_mnt/data__DATA/data.lhcb/fleuret/prod-MC/pNe69GeV/D0_4pi/task/L0Dev/MooreDev_v26r6p1/build.x86_64-slc6-gcc62-opt/ganga/input-sandbox.tgz NOT found!
INFO     Unit 5 of transform 2, Task 32 has aborted the loop
INFO     job 2842 status changed to ""completing""
INFO     Job 2842 Running PostProcessor hook
INFO     job 2842 status changed to ""completed""
INFO     submitting job 2852
INFO     job 2852 status changed to ""submitting""
INFO     Preparing GaudiExec application.
INFO     Make-ing target 'ganga-input-sandbox'     (This may take a few minutes depending on the size of your project)
ERROR    GangaException: Target File: /grid_mnt/data__DATA/data.lhcb/fleuret/prod-MC/pNe69GeV/D0_4pi/task/L0Dev/MooreDev_v26r6p1/build.x86_64-slc6-gcc62-opt/ganga/input-sandbox.tgz NOT found! ... reverting job 2852 to the new status
ERROR    Couldn't submit the job. Deactivating unit.
INFO     job 2852 status changed to ""new""
```

These jobs are being separated by separate units of an LHCbTask. I think the issue is that one attempt at submission is deleting another's version of the target. I had hoped #1393 would fix it but apparently not.

",,43645564
406,DEBUG log level neglected,open,2019-01-17T15:42:17Z,2020-02-25T08:48:49Z,,CONTRIBUTOR,"I've got a strange problem (ganga-7.1.9).
As there are no comments for https://github.com/ganga-devs/ganga/issues/1400 nor https://github.com/ganga-devs/ganga/issues/1401,  I wanted to have a look myself at what the ""DEBUG"" shows.
So, in my ""\$\{HOME\}/.gangarc"", I tried ""Ganga = DEBUG"", ""GangaCore.GPIDev = DEBUG"", ""GangaCore.Utility.logging = DEBUG"" and ""_format = DEBUG"" ... but the screen output and the ""_logfile = ~/.ganga.log"" remain almost completely empty (there are just some 10 ""INFO"" lines printed).

Do I need to add that it does work as expected with the old ganga-6.0.44 or do I not?","> Can I work on this issue?

You are welcome. It will likely take a bit of digging to figure out what the actual problem is.",43645564
407,Parametric submission,open,2018-10-05T16:16:40Z,2019-01-10T13:11:58Z,,CONTRIBUTOR,"Fixes #1350 .

This just collects together all the input data and the input sandboxes from the subjobs. The assumption is that should reflect the splitter output for any splitter/RT handler. We either split by dataset or by arguments which changes the run script in the sandbox.

The GaudiExec RT handler now needs some adjustment as all the run scripts end up together but with different names (i.e. different executables).

There is a lot of other code floating about for parametric submission - do we want to get rid of that (in another PR)?","That the input sandboxes need to be LFNs somewhat limits the usefulness of parametric submission.

If the upload of small sandbox files is quick then I guess we can put the uploading into the submit function. If it is not then we will have to tar them and give each job a different executable script, as the GaudiExec RT handler does. In that case the sandbox doesn't need to be part of the parametric submission as its the same for every subjob.

It may also require some trickery to make sure uploaded files are removed with the job as we do for GaudiExec. Again that is potentially slow.

Edit: At ~1s per file to upload it is not feasible to do the uploading ourselves. We'll therefore upload a tarball.",43645564
408,DIRAC bulk jobs submission (parametric jobs),open,2018-09-25T08:02:57Z,2018-12-14T15:17:22Z,,CONTRIBUTOR,"This issue is about using the parametric jobs submission of DIRAC (i.e. ""bulk submission"") for jobs submission speedup. Maybe there's another issue/task somewhere but I can't find it. If there's one, I am sorry for the duplication.

Since DIRAC v6r20p12, I believe there's in place everything that's needed for ganga to submit parametric jobs to DIRAC, which would greatly improve the submission speed of jobs. Some documentation is here: https://dirac.readthedocs.io/en/latest/UserGuide/Tutorials/JobManagementAdvanced/index.html#parametric-jobs

How to invoke this in a ""hello world"" example:
```
from DIRAC.Interfaces.API.Job import Job
from DIRAC.Interfaces.API.Dirac import Dirac
# or extensions, e.g. from LHCbDIRAC.Interfaces.API.LHCbJob import LHCbJob for LHCb

J = Job()
J.setCPUTime(17800)
J.setInputSandbox('exe-script.py')
J.setParameterSequence(""args"", ['one', 'two', 'three'])
J.setParameterSequence(""iargs"", [1, 2, 3])
J.setExecutable(""exe-script.py"", arguments="": testing %(args)s %(iargs)s"", logFile='helloWorld_%n.log')
Dirac().submit(J)
```

Now, there's a limit to the maximum amount of parametric jobs that can be submitted, and this limit HAVE to be discovered before attempting submitting a parametric job, and if maybe split in multiple chunks:
```
from DIRAC.WorkloadManagementSystem.Client.JobManagerClient import JobManagerClient
JobManagerClient().getMaxParametricJobs()
``` 

","@rob-c Yes you are correct. The splitting itself is not optimal but not too bad. I have identified a few areas of improvements but only gaining tens of seconds (the whole thing only takes about a minute for 200 files).

It is the subsequent creation of the ganga job/dataset objects that is taking the time.",43645564
409,CREAM CE,open,2018-09-03T15:28:26Z,2018-10-22T21:29:43Z,,NONE,"We are moving to ARC so this might not matter to us anymore but on a CentOS7 UI the CREAM backend fails because it cannot find globus-url-copy anymore and cannot download stdout and stderr.

```
WARNING  exit status [127] of command globus-url-copy gsiftp://ce03.tier2.hep.manchester.ac.uk/var/cream_sandbox/northgma/CN_alessandra_forti_L_HEP_OU_Manchester_O_eScience_C_UK_vo_northgrid_ac_uk_manchester_Role_NULL_Capability_NULL_northgma009/70/CREAM706780812/OSB/__jobscript__.log file:/home/aforti/gangadir/workspace/aforti/LocalXML/7/output//__jobscript__.log
WARNING  command globus-url-copy gsiftp://ce03.tier2.hep.manchester.ac.uk/var/cream_sandbox/northgma/CN_alessandra_forti_L_HEP_OU_Manchester_O_eScience_C_UK_vo_northgrid_ac_uk_manchester_Role_NULL_Capability_NULL_northgma009/70/CREAM706780812/OSB/__jobscript__.log file:/home/aforti/gangadir/workspace/aforti/LocalXML/7/output//__jobscript__.log not found
ERROR    /bin/sh: globus-url-copy: command not found
```","@afortiorama Sorry about that, I made a mistake with that change.

I've been through it with Ian and the downloading of output with `arcget` should now work (the change in behaviour was more involved than I thought). It was fixed in #1360 so should be in the DEV version from tomorrow morning.

Let us know if you still experience issues.",43645564
410,Ganga registers failed Athena subjob (non-zero exit code) as complete,open,2018-05-24T22:57:12Z,2020-02-05T02:18:15Z,,NONE,"Athena returns a non-zero exit code but Ganga registers the subjob as complete, when instead it should be registered as failed.

In this example, I am trying to access an element that is out of bounds in my Athena algorithm execute method, for example:
```
std::vector<int> blah(0);
blah.at(1);
```
This throws an exception, as expected, and Athena reports error code 64 or 65:
```
AthenaEventLoopMgr   INFO   ===>>>  done processing event #60000, run #0 60001 events processed so far  <<<===
ZdZdPlottingAlg     FATAL  Standard std::exception is caught                         
ZdZdPlottingAlg     ERROR vector::_M_range_check: __n (which is 0) >= this->size() (which is 0)
ZdZdPlottingAlg     ERROR =======================================================================
ZdZdPlottingAlg     ERROR Error in event in file /mnt/nvme1data/production_20180512/user.ychiu.20180512_mu_2.ZdZd13TeV.data1516_NTUP4L/user.ychiu.14091454.NTUP4L._000184.root
ZdZdPlottingAlg     ERROR [RunNumber,EventNumber] = [280673,0]                                                                                              
ZdZdPlottingAlg     ERROR =================================EXITING===============================
AthAlgSeq            INFO execute of [ZdZdPlottingAlg] did NOT succeed               
AthMasterSeq         INFO execute of [AthAlgSeq] did NOT succeed                                                                                                                         
AthenaEventLoopMgr   INFO Execution of algorithm AthMasterSeq failed with StatusCode::FAILURE
AthenaEventLoopMgr  ERROR Terminating event processing loop due to errors            
ZdZdPlottingAlg      INFO Finalizing ZdZdPlottingAlg... 
```
...
```
AthAlgSeq            INFO Finalizing AthAlgSeq...
ZdZdPlottingAlg     DEBUG Calling destructor
ZdZdPlottingAlg     DEBUG Calling destructor
AthOutSeq            INFO Finalizing AthOutSeq...
AthRegSeq            INFO Finalizing AthRegSeq...
AthMasterSeq         INFO Finalizing AthMasterSeq...
EventSelector        INFO Finalize...
EventSelector        INFO Total events read: 61138
AthDictLoaderSvc     INFO in finalize...
ToolSvc              INFO Removing all tools created by ToolSvc
*****Chrono*****     INFO ****************************************************************************************************                                                                                                               
*****Chrono*****     INFO  The Final CPU consumption ( Chrono ) Table (ordered)
*****Chrono*****     INFO ****************************************************************************************************                                                                                                               
ChronoStatSvc        INFO Time User   : Tot= 18.5  [s]                                             #=  1
*****Chrono*****     INFO ****************************************************************************************************                                                                                                               
ChronoStatSvc.f...   INFO  Service finalized successfully
Py:Athena            INFO leaving with code 65: ""failure in an algorithm execute""

```

but when I check Ganga `jobs` and `jobs.subjobs`, the jobs appear to have completed successfully:

```
      43 | completed |           |       1 |         Athena |      Localhost |                                              |                                                                                                                
```
```
    43.0 | completed |        _0 |         |         Athena |      Localhost |                 seneca-bridge.baxter.chiu.io |                                                                                                                
```

I am launching the job with `ganga athena --local --inPfnListFile=in.txt --split=2 --outputlocation=`pwd` testGanga/testGangaAlgJobOptions.py`.",,43645564
411,Max # of concurrent subjobs in ganga athena --local mode,open,2018-04-26T08:45:03Z,2020-02-05T02:18:15Z,,NONE,"If I submit a --local job with N number of subjobs, ganga athena launches all N subjobs at once. This could result in sub-optimal performance if N is greater than the # of cores on the machine and/or results in too much stress on the storage subsystem.

Can a setting that limits the max # of concurrent subjobs be added for --local operation? It could start (max #), then once one subjob finishes, another subjob is started in its place.

For example, I run over many datasets, with multiple text files listing the files of each dataset:
``` ganga athena --local --inPfnListFile='1.txt,2.txt,...25.txt' --split=8 ...```

Ganga warns:
```
WARNING  Number of requested subjobs smaller than number of output dirs found - changing j.splitter.numsubjobs = 25
INFO     submitting 25 subjobs
```
Then it starts 25 subjobs:
```jobs(8).subjobs```
```Registry Slice: jobs(8).subjobs (25 objects)```","I'll just add that I think it would be ok if the ""ganga athena"" process sits there waiting for the running processes to finish so that it can spawn the next batch. 

This functionality is only requested for the --local mode, it's probably not relevant for batch",43645564
412,"""insecure string pickle"" error preventing successful jobs reaching completed status when DiracFile used in output",open,2018-02-01T11:39:20Z,2018-02-02T14:36:44Z,,NONE,"Hello,

I'm running Ganga from the `development` branch (checked out yesterday) and trying to put my output from the job(s) onto a storage element with `DiracFile` (my output will ultimately be too large for using the sandbox).

The job runs and completes fine, and the output is written to the SE (I can check all of this from the DIRAC web portal). However, my Ganga instance can't seem to do the post-processing on the jobs and so reports them as failed, throwing the following error:

```
WARNING  An error occured finalising job: 63
WARNING  Attemting again (4 of 5) after 2.5-sec delay
INFO     job 63 status changed to ""submitted""
INFO     job 63 status changed to ""running""
INFO     job 63 status changed to ""completing""
INFO     Contacting DIRAC for job: 63
WARNING  An error occured finalising job: 63
WARNING  Attemting again (5 of 5) after 2.5-sec delay
ERROR    Unable to finalise job after 63 retries due to error:
insecure string pickle
INFO     Forcing job 63 to status ""failed""
INFO     Job 63 PostProcessor Failed
INFO     job 63 status changed to ""failed""
ERROR    Exception raised executing 'job_finalisation' in Thread 'Job 63 Finalizing':
Traceback (most recent call last):
  File ""/home/gridpp/sciencing27/lib/python2.7/site-packages/ganga-6.7.4-py2.7.egg/GangaCore/Core/GangaThread/WorkerThreads/WorkerThreadPool.py"", line 115, in __worker_thread
    result = item.command_input.function(*these_args, **item.command_input.kwargs)
  File ""/home/gridpp/sciencing27/lib/python2.7/site-packages/ganga-6.7.4-py2.7.egg/GangaDirac/Lib/Backends/DiracBase.py"", line 1047, in job_finalisation
    DiracBase._internal_job_finalisation(job, updated_dirac_status)
  File ""/home/gridpp/sciencing27/lib/python2.7/site-packages/ganga-6.7.4-py2.7.egg/GangaDirac/Lib/Backends/DiracBase.py"", line 919, in _internal_job_finalisation
    job.backend.normCPUTime, getSandboxResult, file_info_dict, completeTimeResult = execute(""finished_job(%d, '%s')"" % (job.backend.id, output_path), cred_req=job.backend.credential_requirements)
  File ""/home/gridpp/sciencing27/lib/python2.7/site-packages/ganga-6.7.4-py2.7.egg/GangaDirac/Lib/Utilities/DiracUtilities.py"", line 245, in execute
    update_env=update_env)
  File ""/home/gridpp/sciencing27/lib/python2.7/site-packages/ganga-6.7.4-py2.7.egg/GangaCore/Utility/execute.py"", line 280, in execute
    stdout_temp = pickle.loads(stdout)
ValueError: insecure string pickle
```

Googling the `ValueError: insecure string pickle` error throws up a number of possible reasons, but might you have any thoughts on the problem? It only happens when a DiracFile is used in the `outputfiles` - if the output file is a `LocalFile` there's no error and Ganga reports the job as `completed`.

Thanks, @twhyntie

PS: I used the `development` branch after the problems with using the CVMFS version before - is this the right thing to do for now?","@twhyntie  OK, great to hear this could be solved externally,

I think it's worth keeping this open and wrapping the sensitive code to return a more sensible error to help users in future. I'm going through doing some polish in my spare time and this is the sort of thing that could do with a sensible fix.",43645564
413,locations attribute not updated for LCGSEFile,open,2018-01-10T18:11:56Z,2018-01-15T07:54:13Z,,NONE,"Whenever I run a job with `j.outputfiles` set to some `LCGSEFile` the `LCGSEFile.locations` attribute is not updated when the job is finished: it stays empty.
I then have to manually go check the guid of the files in the `__postprocesslocations__` file in the output folder. This is quite tedious and don't think it's the intended behavior if I look at the `setLocation` function in [`LCGSEFile.py`](https://github.com/ganga-devs/ganga/blob/6.7.4/python/Ganga/GPIDev/Lib/File/LCGSEFile.py): there is ` lcgse_file.locations = guid` instruction that seems to never be called.

>**EDIT**: If I pass the exact file name in the `namePattern` field the `locations` is properly set with the guid of the file. It's not working when using a wildcard.

Another question somewhat related : Is it possible to define our own lfn for the `LCGSEFile`?
In my case it defaults to `/grid/$voname/generated/$date/file-$some_random_number`
I didn't find a way to change that in the `config` or using the various attribute of LCGSEFile.
I would be interested in defining it to something like `se_rpath` + `namePattern` for example.
`se_rpath` and `namePattern` seems to only impact the srm name. ","Nearly all job submission to the grid go through a workload management system such as DIRAC, and as far as I know, submission directly to grid sites through cream was discouraged. 

Nevertheless, we are happy to fix the bug you reported and I am pleased that the rest is still working. ",43645564
414,Appending (or renaming) output file with strings from input file,open,2017-11-21T21:49:14Z,2018-03-21T15:28:28Z,,NONE,"It seems that ganga appends `.X` to the output filename of each (athena) subjob, where .X is the subjob number. Is it possible to append more information to the output filename? E.g. extract ""NthField"" from the filename of the input ROOT file or of the input list?

E.g.

`mc16_12345.top.stuff.txt` results in outputs named `my.output.root.12345.top.<subjob#>`

in a similar way to how `-NthFieldAsOutDir` works:

```
ganga athena --help:

  --NthFieldAsOutDir=NTH_FIELD_AS_OUTDIR
                        If multiple input list files given, store the output
                        in a dir given by the field of the list file name,
                        e.g. =2,3 data.201608.top.stuff.txt ->
                        201608.top/<output>
```",@cjustin88 I'll try to look into this in the next couple of days - very sorry for the delay!,43645564
415,Accept wildcard for --inPfnListFile,open,2017-11-21T21:41:08Z,2020-02-05T02:18:15Z,,NONE,"It would be convenient to be able to wildcard expand the argument given to --inPfnListFile, e.g:

```ganga athena --local --inPfnListFile=""./*"" --split=12 --outputlocation=""$HOME/ZdZd/ganga"" ZdZdAnalysis/analysisJobOptions_r21.py```

where the current directory contains many text files, each corresponding to a dataset directory (e.g. `mc16_12345.txt` contains the absolute paths to the files `mc16_12345_1.root`, `mc16_12345_2.root`, etc... inside the directory `mc16_12345`).

Currently, this has to be done explicitly:

```ganga athena --local --inPfnListFile=""mc16_12345.txt,mc16_67890.txt,etc..."" --split=12 --outputlocation=""$HOME/ZdZd/ganga"" ZdZdAnalysis/analysisJobOptions_r21.py```

This is very cumbersome if I have to supply many text files (for instance, 15 text files corresponding to 15 different MC datasets and their folders).

I don't think this can be done with bash or zsh globbing either, since `*` expands to 

`mc16_12345.txt mc16_67890.txt`

instead of

`mc16_12345.txt,mc16_67890.txt`, which is the required argument format.

Thanks,
Justin",,43645564
416,Output files created in a subdirectory are not successfully copied at end of job,open,2017-11-07T16:36:25Z,2017-11-07T16:36:25Z,,CONTRIBUTOR,"Hi,
I've discovered that if I do:

```
ganga athena --condor .... --outputlocation=""$HOME/myOutput"" --outputdata ""myDir/myfile.root"" ....
```
I think my job creates ```myDir/myfile.root``` successfully, but the ganga steering script tries to copy the output to ```$HOME/myOutput/myDir/myfile.33.0.root``` (33 is the job number, 0 is the subjob number) and that fails because the directory ```$HOME/myOutput/myDir``` doesn't exist.

I think this should be a simple fix ... the ganga steering script needs to ensure the directories get created, for example by using ```--parents``` option in the cp command, or just ensuring that the output files dont go into subdirectories

Thanks
Will
",,43645564
417,Batch submission with LHCbDataset is too slow,open,2017-10-25T08:32:23Z,2017-10-25T10:30:03Z,,CONTRIBUTOR,"Batch submission with LHCbDataset is extremely slow compared to using Dirac. This is likely the case for any backend other than Dirac.

Splitting is fast, but then for each subjob time is wasted in two dirac calls per input file in the LHCbDataset. You can see that from a stack trace below.

The following diff speeds things up by overriding the generic `GangaDataset` method and making a single call. Actually, I am quite sure that this call shouldn't be made in the first place as the `catalog.xml`, which is generated before and shipped to the WN, has all the LFN->PFN mappings. However, fixing this is way beyond my understanding of how ganga works internally.

```diff
diff --git a/python/GangaLHCb/Lib/LHCbDataset/LHCbDataset.py b/python/GangaLHCb/Lib/LHCbDataset/LHCbDataset.py
index 46ff131..9099746 100755
--- a/python/GangaLHCb/Lib/LHCbDataset/LHCbDataset.py
+++ b/python/GangaLHCb/Lib/LHCbDataset/LHCbDataset.py
@@ -12,7 +12,7 @@ import Ganga.Utility.logging
 from LHCbDatasetUtils import isLFN, isPFN, isDiracFile, strToDataFile, getDataFile
 from Ganga.GPIDev.Base.Proxy import isType, stripProxy, getName
 from Ganga.GPIDev.Lib.Job.Job import Job, JobTemplate
-from GangaDirac.Lib.Backends.DiracUtils import get_result
+from GangaDirac.Lib.Backends.DiracUtils import get_result, getAccessURLs
 from Ganga.GPIDev.Lib.GangaList.GangaList import GangaList, makeGangaListByRef
 from Ganga.GPIDev.Adapters.IGangaFile import IGangaFile
 logger = Ganga.Utility.logging.getLogger()
@@ -277,6 +277,17 @@ class LHCbDataset(GangaDataset):
                     raise GangaException(""Cannot Get File Name"")
         return names

+    def getFilenameList(self):
+        ""return a list of filenames to be created as input.txt on the WN""
+        lfns = []
+        for f in self.files:
+            if len(f.subfiles) == 0:
+                lfns.append(f.lfn)
+            else:
+                for i in f.subfiles:
+                    lfns.append(i.lfn)
+        return getAccessURLs(lfns)
+
     def getCatalog(self, site=''):
         '''Generates an XML catalog from the dataset (returns the XML string).
         Note: site defaults to config.LHCb.LocalSite
```


```
  File ""<ipython-input-1-26cd9c8b2d25>"", line 1, in <module>
    jobs(603).subjobs[13].submit()
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/Ganga/GPIDev/Base/Proxy.py"", line 708, in proxy_wrapped
    r = f(*s_args, **s_kwargs)
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/Ganga/GPIDev/Lib/Job/Job.py"", line 1523, in submit
    r = self.backend.master_submit( rjobs, jobsubconfig, jobmasterconfig, keep_going, self.parallel_submit)
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/Ganga/GPIDev/Adapters/IBackend.py"", line 196, in master_submit
    if b.submit(sc, master_input_sandbox):
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/Ganga/Lib/Batch/Batch.py"", line 130, in submit
    scriptpath = self.preparejob(jobconfig, master_input_sandbox)
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/Ganga/Lib/Batch/Batch.py"", line 432, in preparejob
    '###CREATEINPUTDATALIST###' : getWNCodeForInputdataListCreation(job, ''),
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/Ganga/GPIDev/Lib/File/OutputFileManager.py"", line 348, in getWNCodeForInputdataListCreation
    '###FILELIST###', ""%s"" % job.inputdata.getFilenameList())
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/Ganga/GPIDev/Lib/Dataset/GangaDataset.py"", line 96, in getFilenameList
    filelist += f.accessURL()
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/Ganga/GPIDev/Credentials/__init__.py"", line 44, in cred_wrapped_method
    return method(self, *args, **kwargs)
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/GangaDirac/Lib/Files/DiracFile.py"", line 574, in accessURL
    return getAccessURLs(lfns, thisSE, protocol, self.credential_requirements)
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/GangaDirac/Lib/Backends/DiracUtils.py"", line 173, in getAccessURLs
    thisSEFiles = execute('getAccessURL(%s, ""%s"", %s)' % (lfns, SE, protocol), cred_req=credential_requirements)['Successful']
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/GangaDirac/Lib/Utilities/DiracUtilities.py"", line 229, in execute
    update_env=update_env)
  File ""/afs/cern.ch/user/r/rmatev/ganga/python/Ganga/Utility/execute.py"", line 241, in execute
    logger.debug(''.join(traceback.format_stack()))
```","Hmm, any additional calls here are likely due to the fact that the handling code is written to pass the PFN addresses to the job in some manner. I suspect (but it's been a long time since I looked at this code) there is some `data.txt` or equivalent made which contains a flat list of PFN of the files the job is written to access.
It should be possible to disable this being generated as I suspect this functionality is needed for grid users who expect to be able to test locally from the vanilla dirac file catalog. Failing that what happens when you change the above method you made to return an empty string? I suspect the problem goes away and you get an empty text file with the job which isn't needed for LHCb apps.",43645564
418,Problem with submitting jobs at grid,open,2017-10-18T09:02:59Z,2017-10-26T08:32:14Z,,NONE,"Dear colleagues, 

I have the following problem: starting from 30.6.2017, I am not able to submit jobs at grid from lxplus(for the analysis of 2012 8 TeV data I am using Root Core in AnalysisBase 1.5.13 using EventLoop).

   It looks like that the problem was triggered by the installation of new versions of ganga at lxplus. The error messages are very complex.

The submission at grid was working last time with ganga version 6.2.3 , which is not available any more
at /cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/Ganga/ .
   Would it be possible to put ganga 6.2.3 back at this place - until the end of this year? 

                                                                                                          Thank you,
                                                                                                          Pavel","Hello,
I am not an ATLAS expert but the grid utilities for ATLAS were removed from Ganga 6.7.0 so any version older than that should be fine.

I don't know about the ATLAS releases but from our side we have not removed anything. @drmarkwslater will know more.

Edit: checking I released we did in fact remove everything from 6.3.1 backwards from CVMFS.  However 6.6.4 I think should still work for you.",43645564
419,How to add an extra attribute to a job?,open,2017-08-05T15:47:10Z,2017-08-07T15:13:55Z,,NONE,"Hi,

I'm currently trying to run a GPU based CREAM job using Ganga. In the JDL file one would normally add GPUNumber = 1, however I can't see how to do this using Ganga with the Job or backend- 

```
Ganga In [2]: j.GPUNumber = 1
ERROR    GangaAttributeError: Can't assign 'GPUNumber' as it does NOT appear in the object schema for class 'Job'
```
```
Ganga In [8]: j.backend.GPUNumber = 1
ERROR    GangaAttributeError: Can't assign 'GPUNumber' as it does NOT appear in the object schema for class 'CREAM'
```
Is there a way I can achieve this?
Thanks,
Will

Edit: I'm using the latest installation in /cvmfs/ganga.cern.ch
","Unfortutunately this doesn't work and prevents the job from being submitted.
Here's an example JDL.
```
[
Executable = ""script.sh"";
inputSandbox = { ""script.sh""};
StdOutput = ""hello.out"";
StdError = ""hello.err"";
OutputSandbox = {""hello.out"",""hello.err""};
outputsandboxbasedesturi = ""gsiftp://localhost"";
GPUNumber=1;
VirtualOrganisation = ""cernatschool.org"";
]
```
Thanks.
",43645564
420,Migrate tests in GangaCore/Utility/Config/tests.py to the testing framework,open,2017-07-27T10:50:57Z,2020-03-10T23:40:01Z,,CONTRIBUTOR,"```
Ganga/Utility/Config/tests.py
Ganga/Utility/logging/message_caching_test.py
Ganga/Utility/logging/test.py
```",#1609 did some of this,43645564
421,GangaRepositoryXML can't parse complex dictionaries and doesn't warn of this,open,2017-07-19T15:21:05Z,2017-08-21T11:20:23Z,,MEMBER,"Dictionaries such as in the GenericSplitter don't correctly support GangaObject derrived classes being stored in them when they're flattened into the job description XML.

The result of this is that the user will generate a job which will run and correctly submit but when ganga re-launches it will fail to parse the string representation of a proxied object into a meaningful class.

This throws an XML error and marks the job as incomplete and inaccessible to prevent potential further data loss.

The most complete fix for this is to implement a GangaDictionary class to have a much better handle on the object being parsed into and back from XML.

For the time being I think a fix can be coded up just within VStreamer but this is a strong limitation on the existing workflows for users wanting to make extensive use of simpler class structures.

A work-around may be to inform the user that the dictionary object can't be saved if it contains class objects and to store an empty string. This minor amount of data loss would allow for complex jobs to be constructed and submitted although the exact configuration of the job splitter may be lost.",@egede After fixing #1103 We may just need to add XML support for a dict type which may already be there. I'll have a look at some point soon but after fixing the proxy to correctly strip itself everywhere this may be a much less technical thing to `fix` to allow for putting Ganga classes within a dict in the XML. (If it turns into too mich work this may drop to a side project).,43645564
422,GangaDataset doesn't work with treat_as_inputfile on vanilla DIRAC,open,2017-07-12T15:39:41Z,2017-07-12T15:39:41Z,,MEMBER,"I've seen some evidence for GangaDataset not respecting the `treat_as_inputfile` attribute when a job is being submitted to the GridPP dirac.

I've not had a chance to go digging about this but it's something that I expected to _just work_ and instead I got a DIRAC error on job submission.",,43645564
423,Add option to command line to specify a steering script to execute,open,2017-07-07T11:22:38Z,2017-07-07T11:22:38Z,,CONTRIBUTOR,"While trying to debug problems with ganga, it would be *very* helpful to be able to override the steering script that ganga uses on the worker nodes to setup and execute the job.

For example, I would like to make modifications to https://github.com/ganga-devs/ganga/blob/develop/python/GangaAtlas/Lib/Athena/run-athena-new.sh ... it would be very helpful if I could take that file, make my changes, and then tell ganga to use that script instead when it executes on the worker, e.g.:

```--steeringScript my-athena-script.sh```

This should be sent to the workers inside the source tarball, and all ganga will tell the worker to do is execute this script. 

That would be very good feature to have. Thank you
Will",,43645564
424,Problem submitting (very) large jobs,open,2017-07-07T09:44:20Z,2017-07-07T09:44:20Z,,MEMBER,"An LHCb user is reporting that some of their subjobs (after 500 or so) are losing their master input sandbox during submission.
(Submitting many subjobs using GaudiPython on DIRAC, I've asked them to re-consider their granularity as >2k subjobs is a bit unwieldy even after ganga is taken away as a potential bottleneck)

I plan to make some tests to submit and check 1000 jobs locally and then I plan to construct and submit (but nop the submission command) so that we can pretend to submit 1000 subjobs to DIRAC to check that the ganga mechanism is working without problems.",,43645564
425,Missing a splitter for Vanilla DIRAC users,open,2017-06-14T10:55:01Z,2017-06-14T10:55:01Z,,MEMBER,"After trying to help some LSST users in Edinburgh I think we're missing a Splitter which is capable of splitting files on Vanilla DIRAC based upon the location of files at different sites.

It should be possible to make a direct port of `SplitByFiles` from LHCb as it uses sthe `OfflineDiracSplitter` which _should_ be using vanilla DIRAC methods.

I'll try and get around to rebasing this at the end of the month as it should be a relatively quick thing to migrate some functionality to Vanilla DIRAC.

If there is a working splitter for Vanilla DIRAC that I've overlooked feel free to close this.",,43645564
426,report function in API not working,open,2017-06-09T09:34:07Z,2020-03-15T23:01:01Z,,CONTRIBUTOR,"The report function is giving an error:
```python
Ganga In [16]: report(jobs(1916))
ERROR    No module named config
INFO     job 1918 status changed to ""running""
ERROR    !!Unknown/Unexpected ERROR!!
ERROR    If you're able to reproduce this please report this to the Ganga developers!
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
/afs/cern.ch/user/m/masmith/cmtuser/GANGA/GANGA_HEAD/install/ganga/python/Ganga/GPI/__init__.pyc in <module>()
----> 1 report(jobs(1916))

/afs/cern.ch/user/m/masmith/cmtuser/GANGA/GANGA_HEAD/install/ganga/python/Ganga/Utility/feedback_report.pyc in report(job)
     37     import platform
     38 
---> 39     import Ganga.GPIDev.Lib.Config.config as config
     40     from Ganga.GPIDev.Base.VPrinter import full_print
     41 

ImportError: No module named config
```",Fixed by #1587 ,43645564
427,Tests for the BenderBox application,open,2017-03-06T14:16:08Z,2018-05-25T18:10:53Z,,CONTRIBUTOR,A reminder to add some tests.,,43645564
428,Eliminate the ability of users to ban DIRAC sites,open,2017-03-01T14:51:40Z,2017-03-17T15:59:44Z,,MEMBER,Consider if this should be done. Consultation required??,@mesmith75 The ability will still be there as we can inject commands directly to the Dirac script that is executed. It will just be more hidden.,43645564
429,Create Jenkins slot for testing against LHCb Dirac certification server,open,2017-03-01T14:41:09Z,2017-03-20T09:02:25Z,,MEMBER,This should not run automatically but should be able to be started on demand. Permission should be given to @fstagni.,"It should be OK now:
```
[LHCbDirac prod] ~ $ dirac-dms-list-directory /lhcb/user/g/gangage
/lhcb/user/g/gangage/: 0 files, 0 sub-directories
```",43645564
430,"Change the default BkQuery dqflag to be ['OK',UNCHECKED']",open,2017-03-01T14:16:14Z,2017-03-13T15:17:34Z,,MEMBER,,Agreed that this change should *not* be made. Would be good to keep new test cases though.,43645564
431,Race condition in job submission,open,2017-01-25T12:05:18Z,2017-01-26T17:07:05Z,,CONTRIBUTOR,"If two jobs share the same prepared application (i.e. same conf-XXX string) then they interfere with one another if they are submitted in parallel.
This is because both jobs use the same temporary `/tmp/username/conf-XXX.tgz` path for uploading to the grid.

In addition, the jobs pollute the `shared/conf-XXX/` directory with their `inputfiles`, so one job ends up with files from the other in addition to its own (this is presumably dangerous if two different jobs have the same prepared application but different input files with the same name (e.g. `job1/blah.txt` and `job2/blah.txt`))","> We can still store the uploaded information for a submitted job in the application as this is stored once per job object and the application is duplicated and unique upon assignment. It would be nice to move this information to the prepared state but it doesn't apply as the RTHandler intentionally uploads all files for the job in 1 upload to reduce the amount of DIRAC interaction we have.
i.e. There is no need to lock the uploading mechanism as this should be uploading a unique file to the grid and storing unique information within each job object.

@rob-c Does that mean that we do not want the file releasearea tarball uploaded to Dirac to be shared between different jobs with the same prepared application object, but only to be shared between subjobs. That will make things easier, but at the moment the situation is inconsistent.

```
a.prepare()
j1 = Job(application=a)
j2 = Job(application=a)
j1.submit()
j2.submit()
```

results in the releasearea tarball getting uploaded twice. Whereas 

```
a.prepare()
j1 = Job(application=a)
j1.submit()
j2 = j1.copy()
j2.submit()
```

results in only one. The behaviour here needs to be made consistent. To not share the uploaded files between different jobs is obviously easier but it will be less efficient in the usecase from @olupton.",43645564
432,Slow GaudiExec submission,open,2017-01-24T10:16:47Z,2017-08-03T09:40:25Z,,CONTRIBUTOR,"Hi,

I'm submitting several jobs that all use the same local `DaVinciDev_v42r1/` area.
It seems that every job builds, copies, uploads and removes the `cmake-input-sandbox.tgz` file.
This seems to be rather slow, and unnecessary (I don't think this file contains the job options?).
Couldn't `make` be trusted to update this archive if needed?

Thanks,
Olli",I think this will be fixed largely by #917 hopefully in Ganga 6.7,43645564
433,Change *.root file default from LCGSEFile to LocalFile,open,2017-01-06T11:46:58Z,2017-01-09T10:49:02Z,,CONTRIBUTOR,"Probably more likely people are using `LocalFile`s than `LCGSEFile`s these days.

Thanks,

Mark","@JacekHoleczek Changing this breaks a _lot_ of higher level features for a _large_ number of existing users.
I think that the `LCGSEFile` is maybe overstepping what is 'sensible' for a default configuration but there are good reasons for having this default behaviour such as handling of sandboxes and storage limitations for many users.",43645564
434,Dirac OfflineSplitter needs to check for failed splitting,open,2016-12-13T10:52:21Z,2016-12-13T10:52:21Z,,MEMBER,"The offline splitting algorithm needs to check for a failed splitting or for 0 groups of filed being created and to throw a more appropriate exception than a ""can't divide by 0 errror"".",,43645564
435,Missing configuration option in default .gangarc file - [defaults_DiracProxy],open,2016-12-11T12:56:36Z,2017-01-23T17:19:45Z,,NONE,"Hello,

It looks like with the roll-out of 6.3.0 a configuration option has been missed in the default `.gangarc` file, so running via CVMFS throws an error:

```bash
ERROR    problems with loading Plugin /cvmfs/ganga.cern.ch/Ganga/install/LATEST/python/GangaDirac
ERROR    Reason: GangaValueError:  ('DIRAC Proxy `group` is not set. Set this in ~/.gangarc in `[defaults_DiracProxy]/group`',)
```

The culprit looks to be `[defaults_DiracProxy]`, which for me had no value assigned when I ran 6.3.0 for the first time.

Is this is intentional, as you can't know by default which user group a user will be using? If so, could there be a more graceful fail if no value is supplied, or a user prompt to set it?

Many thanks as ever, @twhyntie

PS: Thanks to @willfurnell and Giuseppe (EUCLID) for pointing this out :smile:",Can we close this?,43645564
436,ganga athena --local not executing joboptions in 6.2.3,open,2016-12-07T09:45:21Z,2017-08-03T10:02:25Z,,CONTRIBUTOR,"Hello,

I just tried using the --local option in ganga athena in the version I get from cvmfs by default (6.2.3 apparently). The job fails, the issue appears to be because the joboptions are not executed. 

It would be useful if in the log file ganga would print the exact athena command it is executing, as it can be hard to work out what is at fault here. You can reproduce the problem by putting, e.g. just ""import AthenaRootComps.ReadAthenaxAODHybrid"" in a joboption file and then do:

ganga athena --local --inputtype Local --locallocation '/some/input/files' '*.root*' --split=4 --outputlocation=""/path/to/somewhere"" --outputtype 'ATLAS' --extOutFile 'blah.txt' jobo.py

Note I had to specify an output file just to get ganga to run this. That is another thing, ganga should surely allow us to submit jobs that produce no output, no?

Anyway, my log files from the above suggests jobo.py doesn't run, just the preexec and the input.py, and the latter causes the failure because the EventSelector isnt set up.

Sorry for reporting all these problems in one go, I hope this only affects the --local option and not the other options??

Thanks
Will
",Can we close this?,43645564
437,Overly verbose error message when LFN not available,open,2016-12-01T13:56:44Z,2016-12-02T09:53:36Z,,NONE,"I had some ganga jobs fail this morning due to the unavailability of input files (they were on a site that is having planned down time today). The message I see includes a long stack trace that obfuscates the original reason for the failure. Perhaps this should not be shown. This issue was created on the suggestion of Robert Currie.

I am running on the dirac back-end, and tested both boole jobs and brunel jobs, both show similar errors. 

Here is the full message dumped to my screen:

> Ganga In [3]: ganga boole/boole-ganga_planB.py 1771
> INFO     Job 1783: Preparing Boole application.
> INFO     submitting job 1783
> INFO     job 1783 status changed to ""submitting""
> INFO     Job 1783's application has already been prepared.
> INFO     Splitting Job: 1783
> INFO     Constructing subjobs
> INFO     Requesting LFN replica info
> INFO     Got Replica Info: [0:250] of 438
> INFO     Got Replica Info: [250:438] of 438
> WARNING  LFN:
> /lhcb/user/m/mrwillia/2016_11/146305/146305859/Gauss_MinBias_11102003.sim
> was either unknown to DIRAC or unavailable, Ganga is ignoring it!
> WARNING  LFN:
> /lhcb/user/m/mrwillia/2016_11/146307/146307131/Gauss_MinBias_11102003.sim
> was either unknown to DIRAC or unavailable, Ganga is ignoring it!
> WARNING  LFN:
> /lhcb/user/m/mrwillia/2016_11/146307/146307419/Gauss_MinBias_11102003.sim
> was either unknown to DIRAC or unavailable, Ganga is ignoring it!
> WARNING  LFN:
> /lhcb/user/m/mrwillia/2016_11/146305/146305859/Gauss_MinBias_11102003.sim
> was either unknown to DIRAC or unavailable, Ganga is ignoring it!
> WARNING  LFN:
> /lhcb/user/m/mrwillia/2016_11/146307/146307131/Gauss_MinBias_11102003.sim
> was either unknown to DIRAC or unavailable, Ganga is ignoring it!
> WARNING  LFN:
> /lhcb/user/m/mrwillia/2016_11/146307/146307419/Gauss_MinBias_11102003.sim
> was either unknown to DIRAC or unavailable, Ganga is ignoring it!
> WARNING  LFN:
> /lhcb/user/m/mrwillia/2016_11/146307/146307461/Gauss_MinBias_11102003.sim
> was either unknown to DIRAC or unavailable, Ganga is ignoring it!
> ERROR    Errors found getting LFNs:
> []
> WARNING  ---------- error in user/extension code ----------
> WARNING  Traceback (most recent call last):
> WARNING    File
> ""/afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/Ganga/GPIDev/Lib/Job/Job.py"",
> line 1472, in submit
> WARNING      rjobs = self._doSplitting()
> WARNING    File
> ""/afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/Ganga/GPIDev/Lib/Job/Job.py"",
> line 1361, in _doSplitting
> WARNING      subjobs = self.splitter.validatedSplit(self)
> WARNING    File
> ""/afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/Ganga/GPIDev/Adapters/ISplitter.py"",
> line 70, in validatedSplit
> WARNING      subjobs = self.split(stripProxy(job))
> WARNING    File
> ""/afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/GangaLHCb/Lib/Splitters/SplitByFiles.py"",
> line 195, in split
> WARNING      split_return = super(SplitByFiles, self).split(job)
> WARNING    File
> ""/afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/GangaGaudi/Lib/Splitters/GaudiInputDataSplitter.py"",
> line 70, in split
> WARNING      for dataset in all_jobs:
> WARNING    File
> ""/afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/GangaDirac/Lib/Splitters/OfflineGangaDiracSplitter.py"",
> line 349, in OfflineGangaDiracSplitter
> WARNING      raise SplittingError(""Error trying to split dataset with
> invalid LFN and ignoremissing = False"")
> WARNING  SplittingError: Error trying to split dataset with invalid
> LFN and ignoremissing = False
> WARNING  --------------------------------------------------
> ERROR    Error trying to split dataset with invalid LFN and
> ignoremissing = False ... reverting job 1783 to the new status
> INFO     job 1783 status changed to ""new""

> JobError                                  Traceback (most recent call last)
> /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/external/ipython/1.2.1/noarch/lib/python/IPython/utils/py3compat.py
> in execfile(fname, *where)
>     202             else:
>     203                 filename = fname
> --> 204             __builtin__.execfile(filename, *where)
> 
> /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/bin/ganga
> in <module>()
>      61 j.do_auto_resubmit = True
>      62 j.prepare()
> ---> 63 j.submit()
>      64
> 
> /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/Ganga/GPIDev/Base/Proxy.py
> in proxy_wrapped(*args, **kwargs)
>     666         s_args = [stripProxy(a) for a in args]
>     667         s_kwargs = dict((name, stripProxy(a)) for name, a in
> kwargs.items())
> --> 668         r = f(*s_args, **s_kwargs)
>     669         return addProxy(r)
>     670
> 
> /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/Ganga/GPIDev/Lib/Job/Job.py
> in submit(self, keep_going, keep_on_fail, prepare)
>    1470             logger.debug(""Checking Job: %s for splitting"" %
> self.getFQID('.'))
>    1471             # split into subjobs
> -> 1472             rjobs = self._doSplitting()
>    1473
>    1474             # Some old jobs may still contain None assigned to
> the self.subjobs so be careful when checking length
> 
> /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/Ganga/GPIDev/Lib/Job/Job.py
> in _doSplitting(self)
>    1359             logger.info(""Splitting Job: %s"" % fqid)
>    1360
> -> 1361             subjobs = self.splitter.validatedSplit(self)
>    1362             if subjobs:
>    1363                 if not isType(self.subjobs, (list, GangaList)):
> 
> /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/Ganga/GPIDev/Adapters/ISplitter.py
> in validatedSplit(self, job)
>      68
>      69         # try:
> ---> 70         subjobs = self.split(stripProxy(job))
>      71         # except Exception,x:
>      72         #raise SplittingError(x)
> 
> /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/GangaLHCb/Lib/Splitters/SplitByFiles.py
> in split(self, job)
>     193                 logger.debug(""Returning []"")
>     194                 return []
> --> 195         split_return = super(SplitByFiles, self).split(job)
>     196         logger.debug(""split_return: %s"" % split_return)
>     197         return split_return
> 
> /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/GangaGaudi/Lib/Splitters/GaudiInputDataSplitter.py
> in split(self, job)
>      68         logger.info(""Constructing subjobs"")
>      69         logger.debug(""Filling DataSet"")
> ---> 70         for dataset in all_jobs:
>      71             logger.debug(""Creating Subjobs with dataset of
> size: %s"" % str(len(dataset)))
>      72             #logger.debug( ""Creating Subjobs with dataset: %s""
> % str(dataset) )
> 
> /afs/cern.ch/lhcb/software/releases/GANGA/GANGA_v602r3/install/ganga/python/GangaDirac/Lib/Splitters/OfflineGangaDiracSplitter.py
> in OfflineGangaDiracSplitter(_inputs, filesPerJob, maxFiles,
> ignoremissing)
>     347         if ignoremissing is False:
>     348             logger.error(""Errors found getting LFNs:\n%s"" % str(errors))
> --> 349             raise SplittingError(""Error trying to split
> dataset with invalid LFN and ignoremissing = False"")
>     350
>     351     # This finds all replicas for all LFNs...
> 
> JobError: JobError: Error: Error trying to split dataset with invalid
> LFN and ignoremissing = False
> 
> [12:19:00] ","@mesmith75 Potentially yes, but this would likely be a very expensive op. It is essentially also done in the splitter, hence why it raises an exception. The problem is that the user gets far too much verbosity to the screen when the job fails to submit due to expected reasons. This needs looking into.",43645564
438,Add transform's unit's subjob id to PBS jobs' names,open,2016-11-26T19:34:54Z,2016-11-28T12:13:07Z,,CONTRIBUTOR,"I ran my simple ""task"" and the splitter seems to work fine. There is a (small?) inconvenience, though. In the output of ""qstat"", jobs are named as ""T0:0_U0"", ""T0:0_U1"", and so on. Unfortunately, when a transform's unit is additionally split into subjobs, all its subjobs have the same name in the output of ""qstat"". Could you, please, create meaningful names for all PBS subjobs. For example something like ""T0:0_U0.0"", ""T0:0_U0.1"", ""T0:0_U0.2"", ..., ""T0:0_U1.0"", ""T0:0_U1.1"", ""T0:0_U1.2"", ..., and so on.

Original : #870 ","@drmarkwslater Can you look at this one, ",43645564
439,"Add ""fqid"" to the PBS (sub)jobs' names",open,2016-11-26T19:32:42Z,2016-11-26T19:32:42Z,,CONTRIBUTOR,"I ran my simple ""job"" and the splitter seems to work fine. There is a (small?) inconvenience, though. In the output of ""jobs"" and then also ""jobs[0]"", none of the (sub)jobs has a name. Then in the output of ""qstat"", all jobs have the same name ""\_\_jobscript\_\_"". Could you, please, create meaningful names for all subjobs, for example adding the internal ""fqid"" to the PBS (sub)jobs' names.

Original : #870 ",,43645564
440,popen2 module is deprecated,open,2016-11-26T19:31:21Z,2016-11-26T19:31:21Z,,CONTRIBUTOR,"For each PBS job, I get a warning (could one get rid of it?):
######################################################################
/var/spool/torque/...: DeprecationWarning: The popen2 module is deprecated. Use the subprocess module.
import popen2
######################################################################

Original : #870 ",,43645564
441,Pollution of stdout in execute causes test failures,open,2016-10-27T09:50:46Z,2016-10-27T09:50:46Z,,MEMBER,"The use of stdout in execute can cause test failures as below:

```
def test_output():
        """"""Test that ``print`` and ``output`` both work as expected""""""
>       assert execute('print(""foo"")').strip() == 'foo'
E       assert 'URL banned d...n/Server\nfoo' == 'foo'
E         - URL banned dips://dirac01.grid.hep.ph.ic.ac.uk:9135/Configuration/Server
E           foo

python/GangaDirac/test/Unit/Utilities/TestDiracUtilities.py:28: AssertionError
```

I suggest the best solution to this is to intentionally add a line which is something like `---Ganga-Execute-Cut-Here---` which would allow for any output bleeding in from initializing the environment to be correctly removed from the result which is an object we want. This can probably be done within Ganga as the 2 steps of initialize and execute are well separated in the code so this should just be a matter of writing to a stream correctly.
",,43645564
442,Multiple sessions fails to update Repository correctly,open,2016-10-21T10:49:19Z,2016-10-21T10:49:19Z,,MEMBER,"I've noticed that when running multiple sessions now I don't see jobs appear in one session that were newly created in another.
Nothing is falling over itself atm but at a glance I think the code to re-scan the repository for underlying changes had been removed or has moved.
It isn't insane to declare that new objects aren't safe to access between sessions and that accessing old objects which have been deleted would lead to problems but having some re-load functionality back in the registry would help keep the `jobs` summary consistent between sessions which may confuse users as it gets out of sync.
(I've confirmed no jobs are lost on disk due to not being present in the database on shutdown but it may cause some confusion, potentially with the master index is my only worry).
",,43645564
443,Add test for bookkeeping with LHCb Dirac,open,2016-10-18T22:35:59Z,2018-05-25T18:10:45Z,,CONTRIBUTOR,"We need to add a test for the LHCb Dirac bookkeeping functions.
",This will be taken care of by #982 ,43645564
444,Class not propagated using execfile,open,2016-10-17T08:58:42Z,2016-11-14T13:15:19Z,,NONE,"Hi all,
I tried to update to the newest ganga version on lxplus with the atlas setup on cvmfs but I got some problems with classes when using execfile.
What I do is setting up the ATLAS environment: 
`source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh`
then setting up the ganga verison (which is currently 6.2.2): 
`localSetupGanga`
setting up python over cvmfs (which is currently python 2.7.4-x86_64-slc6-gcc48) : 
localSetupPython
and then starting ganga.
I did a minimal setup where the problem can be seen. The files needed are saved here: 
https://cernbox.cern.ch/index.php/s/bCeCYcEmhR0XBSl
When doing
`execfile(""userCode.py"")`
The TestUtils.py are executed but when calling 
`methodCall()`
the class is not known. So it seems that the class definition is not propagated when using execfile. 
This worked for version Ganga-6-1-2
Is it intended that this will not work in newer versions of ganga ? 

Cheers,
Maurice
","HI Maurice,

Apologies for not being in touch - I've been swamped with teaching and illness in the last month :disappointed: I'll make every effort to get back on the Ganga horse in the next few days!
",43645564
445,Need to stop monitoring threads from within a thread,open,2016-09-23T12:39:40Z,2016-09-23T17:21:11Z,,MEMBER,"I spend some time yesterday evening playing with getting the disk checking working again in the monitoring system in Ganga.
This took only 2-3min of work to have it checking my AFS space so that it doen't try to download a file when I've `<2%` storage left on my account.

The problem I have spotted here is that from within this thread there is no nice way of communicating back up to the monitoring loop that I want to stop the monitoring entirely.
Trying to call something like `disableMonitoring` fails due to the thread-lock caused by the worker thread waiting for itself to finish.

Does anyone have a good idea of the best way of communicating back up to the monitoring system that it needs to stop running because of an event such as the disk is now _dangerously_ full?

I'm tempted to add a class level boolean to the main monitoring loop and to flip it within a thread that is calling for the monitoring loop to stop and the queues to be emptied. Does anyone have a better suggestion for how to go about this?
","At the moment we don't as far as I know. Of we have sure mechanism for communicating a stop or exit from worker threads then we could do something sensible. The only thing we can't do is ever exit from a non master thread so we need to the master to exit and try to shutdown cleanly. I think the system is robust enough that I could write something to give us this behavior atm so I'll give it some spare time next week to see if I can formalise something. 
",43645564
446,More checks needed in DIRACBase,open,2016-09-12T13:03:59Z,2016-09-13T08:47:04Z,,MEMBER,"Looking through the DIRAC Base I think we should look into checking the output from DIRAC a lot more. The monitoring code in particular still falls over in bad ways when DIRAC returns something unexpected due to various reasons. This would help for people with potentially flaky connections or when DIRAC is under heavy load for instance.
","All DIRAC API calls should return a dictionary created by either [S_OK](http://diracgrid.org/files/docs/CodeDocumentation/Core/Utilities/ReturnValues.html#DIRAC.Core.Utilities.ReturnValues.S_OK) or [S_ERROR](http://diracgrid.org/files/docs/CodeDocumentation/Core/Utilities/ReturnValues.html#DIRAC.Core.Utilities.ReturnValues.S_ERROR). These will all have the key `'OK'` which maps to a bool. We should make sure that we are always checking this in `DiracCommands.py` with something like:

``` python
result = deleteJob(257324)
if not result['OK']:
    raise GangaDiracError(result['Message'], result['Errno'])
...
carry on as usual
```

We then have to make sure that `execute()` propagates these exceptions correctly and then make sure that any call to execute is wrapped in a `try`/`except` at some level to make sure that these errors never make it to the user.
",43645564
447,Problems with config in unittests,open,2016-09-06T14:31:44Z,2016-09-06T16:57:49Z,,MEMBER,"I've introduced the following:

```
import pytest
from Ganga.Utility.Config import getConfig

glite_status = getConfig('LCG')['GLITE_ENABLE']

@pytest.mark.skipif(glite_status == False, reason=""Cannot run test GLITE not enabled"")
```

into a short unittest the intention is to never run the test when I know it will fail. Standard stuff when supporting multiple platforms and multiple configs.

However. I find that the glite_status is being set to the coded default value regardless of what is in my `.gangarc`, `LHCB.ini` or other configuration sources.

If there some special treatment of the LCG config within the unittests I'm not aware of?
","This adds <0.3sec to test discovery on an AFS system from testing. On my SSD I don't notice accessing the 1 file during test discovery, is there anything else contributing to this being slow?
",43645564
448,GaudiExec enhancements,open,2016-09-02T07:36:14Z,2018-03-06T17:01:14Z,,MEMBER,"GaudiExec loses a few features which it would be good to re-implement:
- [x] Post-processing metadata per subjob
- [ ] Pickling payload before submitting several hundred subjobs
- [x] XML lumi data could be added back in

Edit:
- [ ] Inspect the submitted scripts for `DaVinci().TupleFile`

This list may change but these are the biggest todo items for best support I can think of.
","@egede OK I'll code up some something to make use of the correct environment from the unpacked tarball. This should be relatively trivial but will take a little time to check it's working correctly.
",43645564
449,Version number in session/job monitoring,open,2016-08-30T22:57:54Z,2016-08-31T14:05:34Z,,MEMBER,"I should probably open this against the monitoring system as this is the bit most effected but there was a change (at some point in 6.1.x) which caused the version string reported to change dramatically somewhere from `Ganga-x-y-z` to simply `x.y.z`.

I'm leaving an issue open here as it's only prudent to go back and check that nothing else has changed whenever this happened. This took me a few hours to fix the monitoring code to deal with the fact the schema effectively changed in the database at some un-announced interval.
I plan to get a gangamon instance ready to be publicly viewable in the near future so I'm keen that the information be a bit more correct than it was displaying. It looked like 60% of our userbase had shifted to start using `ganga-svn` which thankfully isn't true.

I am seeing a large consistent usergroup using `6.1.2-hotfix1` I'm assuming Atlas (hammercloud?) Does anyone know why they are lagging so far behind since we're now looking at 6.2.0?
","@rob-c Basically yes - If they have any problems (almost always with Atlas specific stuff) I'll put the patch into Ganga and they'll just manually patch the version they're running. as you say, I don't believe there should be any problems when they eventually decide to shift but we'll have to see.
",43645564
450,Check stdout/stderr for old LSF jobs that have gone from the batch system,open,2016-08-23T11:04:57Z,2017-02-03T21:20:56Z,,CONTRIBUTOR,"If you submit an LSF job and then don't monitor if for a long time, Ganga will think the job has failed because LSF cleans up the job. It should probably just check the the stdout/stderr if LSF reports the job as finished or isn't present any more. This is what Condor does.
",Just looking at this one now. @drmarkwslater are you sure this is true. There is a status file written with START and STOP tags that is used by the monitoring. I do not think we use the `bjobs` command at all. ,43645564
451,Feature request: tail stdout of condor job,open,2016-08-18T09:54:20Z,2016-08-19T14:50:23Z,,CONTRIBUTOR,"Hi,

Not sure if this is already a feature of ganga, but it would be great if there was an easy way from to invoke 'condor_tail -f <jid>' from within the ganga prompt for a running condor job. Ideally doing ""job.peek('stdout')"" for a running condor job would execute this tailing the stdout!

Cheers
Will
",,43645564
452,Proposal for a new `internal` attribute for SharedDir,open,2016-07-22T10:07:50Z,2016-07-28T07:55:39Z,,MEMBER,"Hi all,

I'm considering a hidden attribute to the Job schema (with default of False) to mark a file as internal. I propose the name `internalFiles` unless anyone has a better naming convention.

The intention of this is that if an application (`GaudiRun`) generates a file on a remote fs (`DIRAC`) then the file can be placed in this list without the user being aware of this file. And then we can add some hook into the remove methods to remove all ganga-internal files from storage which were auto-generated by the job during submission.

These files are intended to be different to the prepared state files which are stored in the shared directory of Ganga.

At the moment the `GaudiRun` application is generating `DiracFile` objets and storing them in the application as hidden attributes. This is fine but the objects aren't removed unless the application is unprepared and so it's not 100% clear when they will be removed from Dirac (if ever).
I feel it would be much better to store these files in the Job object rather than having to fudge in some sort of application clean-up logic in the Job remove section.

I won't be looking to add this until likely after 6.2.0 but it would make managing files generated by `GaudiRun` and other apps easier I feel.

Does anyone have any thoughts about this?

Thanks,

Rob
","@drmarkwslater At the moment these files are also placed in a sensible location so I could add a howto in the `GaudiExec` doc on how to remove the parent folder and clean up their Dirac storage.
It would be nice to tie an LFN into the dirac job cleanup service but I imagine this to be difficult and beyond the scope of ganga.
",43645564
453,Generic support for preparable IGangaFile objects,open,2016-07-21T08:37:54Z,2016-07-21T08:37:54Z,,MEMBER,"I think we should/could add support for IGangaFile type objects which would allow for preparable attributes to be copied to the prepared folder automatically.

I think this should be a relatively minor task to modify the underlying methods but would reduce some duplicated code higher up if this is implemented correctly.

In doing this we may want to make some policy about whether remote files can be regarded as 'preparable' and how we should handle them.
",,43645564
454,Subjobs in DIRAC tests need to be careful,open,2016-07-14T15:15:25Z,2016-07-14T15:15:25Z,,MEMBER,"Assigning subjobs behind the proxy when there is a getter could cause issues and strange behaviour as has been caught by #669. The DIRAC tests need to be more careful in how they store the subjobs that have been created for testing.
",,43645564
455,Testing backends (jobs not completing),open,2016-07-13T14:58:41Z,2016-07-28T07:37:01Z,,MEMBER,"Hi all,

I'm wondering if we can make a sensible policy here about how we test the backends?

The LCG test on nightly is failing when we attempt to wait for a job to be picked up and reach a completed state. I would argue we should **never** be testing for this in an automated way. We cannot.

I think we can safely attempt to submit jobs on each backend and attempt to update the information for a submitted job but we can never construct a test to rely on a backend propagating a job to a completed state.

I am proposing removing the `Ganga.test.GPI.LCG.TestLCG.test_job_complete` test or making it throw some error that is allowed relating to the test not reaching the final completed/failed state.

I would argue we can only ever test for a job completing against a _fully_ mock-ed backend for backends such a Batch, LCG, DIRAC (and others).
There we explicitly know that some information(files) are available and we can test that Ganga parses them correctly. We never know when running the tests that the backend will complete the job or not.

I only mention this as I'll likely start ignoring a set of tests which fails every single night and we end up back with a broken test system. I'm already getting emails in a thread which is 10+ emails deep (mostly unread), all to do with this job failing, and this will hide problems such as a nightly test having a more serious error.

Are there any objections to removing this test until we have a fully mocked LCG backend which will _always_ provide the expected output data for a completed LCG job?

Thanks,

Rob
","I'm still getting emails about LCG jobs failing in nightlies... any idea on an eta or should I patch the test so that it stops failing for now?
",43645564
456,Cross plugin testing,open,2016-07-06T13:13:19Z,2016-07-06T13:13:19Z,,MEMBER,"I've recently started running the Core tests against the various plugins and environments associated with them.

It may be worth designing some system to allow us to run tests across multiple plugins i.e. the DIRAC tests under LHCb.

This would depend on what backend and such different plugins intend to use/support.
",,43645564
457,move fqid file naming logic out of MassStorageFile,open,2016-07-05T11:42:59Z,2016-07-05T11:42:59Z,,MEMBER,"I'm planning to make 2 PR (maybe in 6.1.22 but probably later on due to other work).

The first of these will abstract the fqid file-naming logic out of MassStorageFile and will place it into a common utils file somewhere.

The second will try to take this and implement the same feature on the LocalFile as well as the DiracFile. (and possibly document how it's to be done for others).
I plan to add `outputfilenameformat` to the schema of both file types in order to do this.

Does anyone have any thoughts on this?
",,43645564
458,Copying a Task doesn't copy the Transforms,open,2016-07-01T10:12:19Z,2017-01-20T22:16:51Z,,CONTRIBUTOR,"This doesn't seem sensible to me...
",Clear milestone as nothing is happening.,43645564
459,Fix the execute class to return a tuple,open,2016-06-30T12:26:44Z,2016-06-30T12:26:44Z,,MEMBER,"If the execute class returns a tuple we can use it to replace the `Shell()` instance in `MassStorageFile` and `_exec_cmd` in `GaudiRun` with little work. (As well as a few other places)

I'd rather migrate to this over the older Shell code as this has been developed more and offers more features so I'd opt to move to this almost entirely and drop the other implementations of `Popen`/`os.system` wrappers.
",,43645564
460,Fix master index flushing,open,2016-06-29T13:13:45Z,2016-06-29T13:25:38Z,,MEMBER,"Some users are ending up in a very bad situation with many, many jobs causing Ganga to be unusable due to extremely long startup times caused by having to load many index files from disk.
This is exacerbated by events such as user hitting Ctrl+C or dropped connections and the master index not being re-generated correctly until shutdown.

At the moment all we do is remove it upon startup but it would be good to update the master index once every few min or so at least or update a version in memory to flush to disk with the jobs synchronisation so that future startups aren't effected by this bug.
",,43645564
461,GaudiRun add support for streaming MassStorageFile to the Local and Batch backends,open,2016-06-28T10:28:38Z,2016-06-28T11:01:51Z,,MEMBER,"I plan to add some method to construct the `data.py` file which is run with the job to have the appropriate streaming protocol and location for MassStorageFiles to allow them to be used with Local/Batch jobs.

I'll need to add a check to make sure users don't submit grid jobs with this filetype as they should make use of DiracFile to get their data on these WN.
","> I'm not planning to stop users doing this if they want but a 1 line warning that they might be doing things in a sub-optimal way would probably be better imo. It would hopefully stop people accidentally submitting jobs which worked at CERN across to different sites on the grid without access to the data.

@rob-c I agree.
",43645564
462,Review getter/setter methods on Schema Items,open,2016-06-27T09:25:04Z,2016-07-28T08:58:35Z,,CONTRIBUTOR,"We seem to have at least 2 ways of applying getters and setters or callbacks to schema items, the `attribute_filter_blahblah` stuff and the `checkset` value (which is broken and hardly used either). This should be reviewed properly and a single solution settled on and documented.
","Hi All,
These changes I mostly have already still waiting for me to commit them as part of my rewriting of the object/schema/proxy objects. perhaps it's time I speed up their introduction :wink:
",43645564
463,DIRAC execute running in a random temp directory,open,2016-06-24T09:48:46Z,2016-06-24T09:55:25Z,,CONTRIBUTOR,"There's an issue in `DiracFile` caused by the recent change to run DIRAC `execute()` commands in a random temporary directory. DIRAC API calls such as `addFile()` are `cwd`-aware and will look in the the current directory for the source files.

This means that `put`ting a file like:

``` python
Ganga In [0]: !touch test.txt

Ganga In [1]: d = DiracFile('test.txt', localDir='.', defaultSE='UKI-SOUTHGRID-RALPP-disk')

Ganga In [2]: d.put()
INFO     Uploading file './test.txt' to 'UKI-SOUTHGRID-RALPP-disk' as '/gridpp/user/r/robot.general.gangaproject/GangaFiles_13.30_Wednesday_22_June_2016/test.txt'
ERROR    Error in uploading file test.txt : {'UKI-SOUTHGRID-RALPP-disk': {'Message': 'Local file ./test.txt does not exist', 'OK': False}}
Ganga Out [10]: ""{'UKI-SOUTHGRID-RALPP-disk': {'Message': 'Local file ./test.txt does not exist', 'OK': False}}""
```

will fail since the `cwd` is now some `/tmp/...` directory.

I'm not sure that the benefit of running all DIRAC commands in a temporary directory is worth having to manually go through and take into account all the places where the DIRAC command is expecting to be run in a certain place.

My preferred solution would be to remove the temp directory code from DIRAC `execute()` unless it can be shown that there is a real performance impact.
","You can pass the cwd to the command from the appropriate method in the DiracFile. If this is the preferred way of working then we should be explicit about this within DiracFile, it's not the main caller of the execute function in DiracUtils which is called extensively from elsewhere.
",43645564
464,DiracFile subfiles not populated,open,2016-06-22T17:20:35Z,2016-08-23T11:07:38Z,,MEMBER,"subfiles for master job not correctly populated from subjobs on develop
",,43645564
465,Ganga doesn't cope well with >> 1000 jobs,open,2016-06-22T16:39:03Z,2016-08-23T11:07:38Z,,MEMBER,"We should do something better than stalling on startup when a user has >> 1,000 jobs.

Maybe only load the last 1000 jobs into memory and put some logic into the XMLRepo to generate jobs older than this when required.
","No, this is users don't use splitters (not usually for good reasons) but I've worked with a few users wondering why we're so slow at loading 10k+ index files from disk on startup from AFS.
I'm keen to move to fix the master index so that it never gets out of sync tbh, that should be a relatively trivial fix and doesn't expose these users to the problem of starting up with lots of initial disk I/O.

This still has the issue that creating this many empty jobs is still an extremely expensive operation but that's another bug for another issue
",43645564
466,Reduced XML Feature,open,2016-06-20T11:35:33Z,2016-07-01T10:18:10Z,,MEMBER,"Hi all,

I've been doing some looking at reducing the XML files which we're writing to disk.

Do we explicitly want to write all default Schema attributes to disk?

I suppose the answer for the moment is yes we do because we could add a Schema attribute to change the behaviour of the class.

If we are careful going forward to not change the Schema in a backwards incompatible way we can explore only writing the objects to disk which we have changed from the default Schema value.
This makes loading objects from disk much quicker in addition to making the XML sizes on disk much smaller.

I'm not proposing merging any code to do this but I've played with a private patch to do this in some form and the XML size for a large number of DiracFiles reduces by about 60-70% and the loading of the objects into memory is much quicker.

Do people have a strong opinion about possibly looking into implementing this?

(This is already backwards compatible aka, XML with a very reduced schema on disk can be loaded without issue by Ganga already)

Thanks,

Rob
","This is related to constructing 100k \* number of schema entries in loading them from disk. The time taken to construct an object at the moment is extremely high but if we know this corresponds to the present Schema then we can simply load the changes atop the Schema and things move much more quickly.

This isn't related to the xml file size although it does go down when not repeatedly writing empty lists and dicts throughout.

I'll come back to this in a bit (other larger tasks to do atm) with a more detailed explanation of what I'm considering but I think there's enough weariness that I'm not going to invest much time into attempting to demonstrate this.
",43645564
467,Common way to interact with python prompt,open,2016-06-13T09:28:44Z,2016-06-13T09:28:44Z,,MEMBER,"Building on #549 I think it would be sensible to plan some common framework for how we display some important messages to the user.
i.e. credential running out, flushing the in-memory logging, something of interest to user such as a large task reaching completion etc.

This could be done correctly in a Python/IPython agnostic way if we're clever I think but there is a call for presenting messages of this type to the user in a common way rather than pushing everything to the logging from another background thread.

This came up in discussion of this PR but is worth looking at again once that has been merged.
",,43645564
468,Allow Tasks to combine the output data of several transforms to a single unit/job,open,2016-06-09T10:48:15Z,2016-06-09T12:47:12Z,,CONTRIBUTOR,"This may not be possible in the current implementation but it would nice to allow a single unit process all the output from several transforms. What this boils down to is allowing `TaskChainInput` datasets to have more than one Transform as input.
","I remember I had this in the original implementation of Tasks but I **think** I dropped it because of reasons that now escape me. I will investigate!
",43645564
469,Shutdown should cleanup config system after itself,open,2016-06-08T11:59:25Z,2016-06-08T12:13:15Z,,MEMBER,"We should add some code to cleanup the config system and any other remaining globals upon shutdown.
This would improve the way that the config is handled between tests, i.e. we cleanly and correctly start from scratch each time rather than any horrible hacks specific to the test system.
","Scratch that. Why are we modifying the code after bootstrap. I thought this was a test thing. Never looked at it myself. 
",43645564
470,Potential performance fix,open,2016-06-07T19:59:40Z,2016-06-08T08:59:30Z,,MEMBER,"I'm trying to do some dev work/testing on AFS and handling large objects is extremely expensive.

In paticular I'm talking about XML files we generate which are ~90Mb or so in size.

I suspect that a lot of the bottleneck in automatic flushes come from the pause of waiting for the file transactions to complete.

Would it make sense to introduce some locking queue of 1 thread which allows us to queue writing the XML string to disk and then release objects to carry on doing whatever the user/monitoring system wants to do with it?

Locking large jobs for a very long transaction like this on AFS is slightly annoying from an end-user perspective.
","@alexanderrichards Yes but it changes the Schema and so we need to be able to parse old objects somehow into the new ones which requires a bit of being careful.
The object would be constructed via modifying it's internal attributes once it's read from XML but then the class has no way to know it needs to re-analyze these attributes to convert them into the new format.
Additionally users or other code may construct a Dataset from a list of input DiracFiles which would require a step to parse (sanitise) data like this on input into the new format.
",43645564
471,Running tests in different envs,open,2016-06-06T10:02:08Z,2016-06-22T17:33:09Z,,MEMBER,"Hi all,

Recently a minor(ish) bug had cropped up which effected users not running the bash shell as their environment.
I think the actual bug was due to different behaviour of bash when executed atop another bash session vs bash being executed atop a different shell. (I've not confirmed this but it seem to fit)

I think it would be a good idea to run the tests at least nightly atop a different shell such as 'zsh' or 'tclsh' to avoid the possibility of us being effected by assumptions in the code which may be not be valid.
e.g. executing a bash script within the same (non-bash) shell as the environment.

What are people's thoughts on this?

Thanks,

Rob
","I think it makes sense to have multiple nightly tests if this starts taking too long with each test running in a different env.
The main difficulty I imagine is if plugins support different envs. I can think of a few solutions but they all require a little bit of work.

I'm tempted to run the tests myself locally on zfs just to be certain nothing crops up.
",43645564
472,Allow athena job submission with no output,open,2016-05-27T15:32:21Z,2016-08-19T08:29:45Z,,CONTRIBUTOR,"Trying to submit an athena job to a batch system when there is no output fails:

ERROR    ApplicationConfigurationError: j.outputdata.outputdata is empty - Please specify output filename(s). 
ERROR    ApplicationConfigurationError: j.outputdata.outputdata is empty - Please specify output filename(s).  ... reverting job 1 to the new status

This came from the following submission, which is a job that produces no output (other than the log files):

ganga athena --lsf --nocompile --inPfnListFile 'datasets.txt' --split=2 test.py

I think if the user doesnt specify an output then no outputdata object should be created. Even if I added: 

--outputlocation=""~will/mygangaoutput"" --outputtype 'ATLAS'

to the submission command, it still failed with same thing. But I think best thing would be if the job has no output, then ganga should just submit, whether you specify an output location or not. 

A ""--noOutput"" option would also be ok. I didn't see any for ganga athena but I'm sorry if I missed it

Cheers
Will
","As an extension to this, could it be possible to also submit jobs to a batch system that have no input. In this case, the same job should be replicated as many times as 'split' is specified, but with a ""jobNumber"" variable defined in the python preexec joboptions specifying which subjob number it is.

Ganga seems to correctly detect 'noInput' as in the configuration detection step, so in this case it should ignore any requirement for --locallocation etc.

Thanks
Will
",43645564
473,Have Tasks resubmit jobs that are hanging,open,2016-05-27T12:25:24Z,2016-05-27T12:25:24Z,,CONTRIBUTOR,"It would be good for Tasks to resubmit jobs that have been hanging in submitted/running for a long time. 
",,43645564
474,Still loading jobs for remove,open,2016-05-26T11:27:14Z,2016-05-26T11:55:00Z,,MEMBER,"I know this is a minor bug but we're still loading jobs un-necessarily for removal.

If a job is 'new' I wouldn't expect to need to load it from disk to remove it. (I would argue the same for 'completed' and 'failed').

If a job is in a 'submitted', or 'running' state then it's probably a good idea to allow the load due to needing to tell the backend to kill/remove the job as well.

I only mention this as I have new jobs I can't remove because I can't load them if I don't load the correct plugin. It's annoying but not the end of the world
","Not sure. No more than we did previously but I think the [Configuration] section might be fixed...
",43645564
475,Make use of ids in Dirac splitting,open,2016-05-23T09:24:41Z,2016-05-23T09:24:44Z,,MEMBER,"This is a small issue to remind me to look into some sections of the LHCb/Dirac codebase to do with job submission/splitting as well as datasets and such.

It may be possible to get some performance improvements by checking object instances vs making full object comparisons especially when iterating over large lists.

This is somehow a nicety but there is the potential for some nice performance improvements as a result in some areas if this is done correctly.
",,43645564
476,No lcg* tools for CC7/SL7,open,2016-05-20T06:13:35Z,2016-05-20T08:57:18Z,,CONTRIBUTOR,"I've just learnt (this might have been common knowledge but I missed it) that there will be no `lcg-*` tools available for CC7/SL7 and so we should go through the code base and exchange these commands for the `gfal-*` ones instead.
",,43645564
477,IGangaFile and copies of jobs,open,2016-05-12T12:51:07Z,2016-05-12T14:41:08Z,,MEMBER,"The other day I had a discussion with @alexanderrichards  about what we do with `IGangaFile` objects when a file is copied. We in general have (at least two problems).
1. Wildcards can be used in our name patterns. For outputfiles, these 
   might be populated when the job has finished leading to that the 
   wildcard is ""resolved"". However, when the job is copied, we want to 
   restore the wildcard. We solve this at the moment by the use of a list that contains the ""real"" files and then some special logic when the job is copied.
2. We (as in `Ganga` behind the scenes) sometimes add files to the list of `inputfiles` during the submit process. Examples are histogram files requested in `Gaudi` jobs within LHCb and the `exe` file in the current `Executable` application. This will come up for the new `Program` application as well.  At the moment this is solved through a mixture of ignoring the problem and creating special lists with these automatically added objects.

I wonder if this could be solved in a more elegant way by using the `hidden` and `copyable` schema attributes of `GangaObject`s. So for the two cases
1. The name with the wildcard is initially copyable and not hidden. When the output is resolved, the object with the wildcard is turned hidden but stay copyable. The resolved names on the other hand are not hidden and not copyable. When copied, we would have to ""unhide""  the wildcard `IGangaFile`.
2. These objects we could just add to the list of inputfiles but they will be both hidden and not copyable. 

I do not know if it is possible to change the `copyable` and `hidden` attributes on an object-by-object level. The advantage of thjis idea is that the whole business turns into properties of the individual `IGangaFile` objects rather than relying on some external lists to manage the who thing.

Comments?
","I think something similar to `is_static_object` in the schema would be useful to represent to the parent object that we shouldn't copy it when making a copying of an object from in front of the Proxy.

(I'm tempted to say we can go as far as managing the prepared state file/directory this way when we look to remove File support and maybe improve SharedDir at some point in the future).
",43645564
478,Issues with GangaAtlas shutdown,open,2016-05-10T09:20:30Z,2016-05-10T09:55:35Z,,CONTRIBUTOR,"It seems there's some problems with GangaAtlas shutdown in (at least) 6.1.19. There are quite often `SessionLock` issues and almost always:

```
[10:12:44]
Ganga In [1]: 
Do you really want to exit ([y]/n)? y

INFO     Stopping running tasks before shutting down Repositories
Exception TypeError: ""'NoneType' object is not callable"" in <function _removeHandlerRef at 0x7f014ea79f50> ignored
```

I suspect that there is something getting screwed up when DQ2/Rucio is imported and the `atexit` handlers are somehow going wrong.
","OK so this may explain this then. I see it after setting up an Athena release which points me at Python 2.7.9 but NOT on Python 2.6 so I guess it's just the version of Athena/Python that I'm using that's causing the problem.

There may still be issues with the `SessionLock` shutdown which I personally haven't seen but has been mentioned. It looks like on some systems the `SessionLockRefresher` is taking too long to shutdown/hanging:

```
Job status update or output download still in progress (shutdown not completed after 6 seconds).
1 background thread(s) still running: ['SessionLockRefresher'].
Do you want to force the exit (y/[n])? y
INFO     Stopping Job processing before shutting down Repositories
WARNING  Shutdown forced. 1 background thread(s) still running: ['SessionLockRefresher']
INFO     Shutting Down Ganga Repositories
INFO     Registry Shutdown
Exception in thread GANGA_Update_Thread_SessionLockRefresher (most likely raised during interpreter shutdown):
Traceback (most recent call last):
  File ""/usr/lib64/python2.6/threading.py"", line 532, in __bootstrap_inner
  File ""/ALRB/atlasadmin/atlas-sl6x64.triumf.ca/ATLASLocalRootBase/x86_64/Ganga/6.1.19/install/6.1.19/python/Ganga/Core/GangaRepository/SessionLock.py"", line 165, in run
  File ""/ALRB/atlasadmin/atlas-sl6x64.triumf.ca/ATLASLocalRootBase/x86_64/Ganga/6.1.19/install/6.1.19/python/Ganga/Core/GangaThread/GangaThread.py"", line 48, in unregister
<type 'exceptions.AttributeError'>: 'NoneType' object has no attribute 'getInstance'
```

Any thoughts on what could take the `SessionLockRefresher` so long to shutdown?
",43645564
479,Standardise positional arguments for GangaObject constructors,open,2016-05-09T14:20:46Z,2016-05-09T14:25:26Z,,CONTRIBUTOR,"As mentioned in #449 (and was a potential stopper for #206) it's often useful to be able to construct certain `GangaObject`s with a single positional argument. The most obvious examples are the `IGangaFile` types which, at the GPI are useable as:

``` python
f = LocalFile('test.sh')
```

it also works for some types at the raw level:

``` python
f = stripProxy(LocalFile)('test.sh')
```

The former works because of `__construct__` and the latter works because of the overridden `__init__`. These two implementations are separate and have no enforced consistency.

I think that probably the canonical implementation should be in `__init__` which the proxy `_init` function will call with `*args`. The flow is then:
- If it's a raw `GangaObject` then `__init__` does as expected in the usual Python way
- If it's a proxy object then the proxy `_init` will call `__init__` with `instance = pluginclass([stripProxy(arg) for arg in args])`

There are some questions/caveats to this though:
- `GangaObject`s must remain constructible with no arguments (and most will continue to take none).
- Current default behaviour is to do a `copyFrom` (`GangaObject.__construct__`) which this will break. I think the only place this is really used is for `Job(j)` which could be accounted for in `Job.__init__`.
- This is orthogonal but related to `_auto__init__` which is called argument-less for all proxy objects.
","This is compatible with removing the `__construct__` imo in favor of fixing the `__init__` methods.

I would argue that `_auto__init__` should remain with no argument as it's a nice hook I like
",43645564
480,add WN dir to PATH,open,2016-05-09T09:10:44Z,2016-05-10T15:16:52Z,,MEMBER,"Hi all,

I'm constantly hitting this as a common thing (hence PR such as the one against the Localhost which was ultimately dropped)

Do we have a policy about whether the WN dir where a command is running from should be in the PATH or not?

We already change the env such that we pick up supporting code we ship with jobs to some backends.

Would it be sensible to say that it's a logical extension to include the dir of the WN to the PATH of a job?
(and possibly the LD_LIBRARY_PATH?)

This is would require a relatively minor PR but would remove the amount of times I've been tripped up by having to do this by hand in a job I'm executing.

Does anyone have any thoughts on this?

Rob
","@egede I quite liked `Program` actually 😄 
",43645564
481,Get rid of File,open,2016-04-26T14:45:23Z,2016-04-27T13:53:57Z,,CONTRIBUTOR,"Currently in a number of Ganga APIs we have the option of using the `File` type. This is completely separate and unrelated to `IGangaFile` (and therefore `LocalFile`, `DiracFile` etc.). I believe that historically (before we had an inputfiles/outputfiles system) it was used to signal that, for example, the file should be copied into the user's sandbox.

It is currently usable in a number of places, for example in `Executable.exe` and `CustomChecker.module`. The problem is that is used very inconsistently. The API they seem to _want_ to provide is the ability to seamlessly do either of the following:

``` python
j.application.exe = 'myscript.sh'
j.application.exe = File('myscript.sh')
```

But there is a fundamental problem that `File` is a `GangaObject` and therefore needs to be recorded as a `ComponentItem` in the schema but a string is not and must be recorded as a `SimpleItem`. This means that we've ended up in a bunch of different 'solutions' in the code, the main two that I've seen are:

``` python
ComponentItem(category='files')  # e.g. CustomChecker.module
SimpleItem(defvalue='echo', typelist=[str, File])  # e.g. Executable.exe
```

Due to Ganga's currently lax and inconsistent type checking they are both able to accept both `str` and `File` objects via assignment but what happens afterwards only seems to work (where it _does_ work) by chance.
# Serialisation

Consider the following 4 variants:

``` python
box.add(Executable(exe='~/mychecker.py'), 'e')  # 1. SimpleItem str
box.add(Executable(exe=File('~/mychecker.py')), 'e2')  # 2. SimpleItem File
box.add(CustomChecker(module='~/mychecker.py'), 'cc')  # 3. ComponentItem str
box.add(CustomChecker(module=File('~/mychecker.py')), 'cc2')  # 4. ComponentItem File
```

When they are serialised they end up with very different XML outputs (snipped):

``` xml
1.
<attribute name=""exe""> 
  <value>'~/mychecker.py'</value>
</attribute>

2.
<attribute name=""exe""> 
  <value>File(name='/afs/cern.ch/user/m/mwilliam/mychecker.py',subdir='.')</value>
</attribute>

3.
<attribute name=""module"">
  <value>'~/mychecker.py'</value>
</attribute>

4.
<attribute name=""module"">
  <class name=""File"" version=""1.1"" category=""files"">
    <attribute name=""name""> 
      <value>'/afs/cern.ch/user/m/mwilliam/mychecker.py'</value>
    </attribute>
    <attribute name=""subdir""> 
      <value>'.'</value>
    </attribute>
  </class>
</attribute>
```

`SimpleItem`/`str` (1) and `ComponentItem`/`File` (4) both work as expected and round-trip correctly but 2 and 3 do not. 2 streams out 'successfully' as `SimpleItem`s use the object's `repr`. 3 streams out 'successfully' due to an unassuming (and unexplained) `if isType(s, str)` in `acceptOptional` (added by @rob-c in 717775211e84ea). The latter is not present in `VPrinter` so that a plain `CustomChecker(module='~/mychecker.py')` will raise an exception.

As for reading (after a Ganga restart), `box['e']` works and `exe` is set to a string, `box['e2']` works with `exe` set to `File` (which works due to the stored XML data being passed through `eval`). `box['cc']` fails due to the inconsistent types and `box['cc2']` works with `module` being `File`.
# Usage

`File` has two public attributes, `name` and `subdir`. On the whole, only `name` is ever used (and people will just put the full path in `name`). For example, `Executable`, `CustomChecker`, `CustomMerger`, `Gaudi` and co., `Bender`, and `Athena` and co. just use `name` and never `subdir`. Only `Root` and `GaudiPython` use `subdir`.
# Partial proposal

I suggest that we find a way to get rid of `File` as it does not add any _functionality_ over a simple string. There is an open question about a safe transition mechanism (both from the point-of-view of the XML repository and also and user scripts) about which I am open to discussion.
","This is another issue which would flow a lot quicker with a blackboard of some sort and a round table I think.
",43645564
482,file types aren't checked when appending inputfiles,open,2016-04-21T14:42:01Z,2016-04-21T14:42:01Z,,MEMBER,"Simply put users can still do this:

```
j.inputfiles.append(File(""/afs/cern.ch/user/l/lohenry/Fitter/x86_64-slc6-gcc48-opt/libV0hhFitterLib.so""))
```

in LHCb for 6.1.18.

This should not be allowed as inputfiles should only contain IGangaFile type objects. (I guess this is being translated into a LocalFile but it should raise an exception and it's not)

I don't know how to fix this, but I suspect that the GangaList which is in inputfiles should make use of the TypeCheck to make sure it is allowed to append/extend an object.

We can warn users not to do it in 6.1.19 but this would be good to fix.
",,43645564
483,Possible problem with wildcards on LHCb,open,2016-04-21T11:11:17Z,2016-04-21T11:11:17Z,,MEMBER,"Hi, as mentioned in the meeting this-morning:

An LHCb user has reported problems with a job submitted which looks close to the following:

```
******
j = Job(application=Gauss(version=""v49r0"",   optsfile=local_dir +
""/gauss-job.py"",  extraopts=""execute(30000000)""))

j.name = ""Gauss-MW_privateDB""
j.outputfiles = [DiracFile(""*.root""), DiracFile(""*.sim"")]
j.inputfiles =
[""/afs/cern.ch/user/m/mrwillia/cmtuser/VELO/software/xml/dddb-20150424_longpixel-137.5mu_X.db""
#+other similar files]
j.backend=Dirac()

events = 20000
eventsperjob = 200
j.splitter =
GaussSplitter(numberOfJobs=int(round(events*1.1/eventsperjob)),
eventsPerJob=eventsperjob)
j.prepare()
j.submit()
******
```

The job was submitted under v600r44 (6.0.44) but the problem at least persisted into 6.1.18 as the user appeared to get a cascade of the amount of files which are returned:

```
Ganga In [3]: jobs(1685).subjobs(0).outputfiles
Ganga Out [3]:
[ DiracFile (
   defaultSE = '',
   namePattern = 'Gauss_MinBias_30000000-GaussHistos.root',
   guid = 'D3FA1E16-D50E-EA24-C070-2813B3C40DA8',
   remoteDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180278',
   compressed = False,
   localDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180278/Gauss_MinBias_30000000-GaussHistos.root',
   lfn = '/lhcb/user/m/mrwillia/2016_04/130180/130180278/Gauss_MinBias_30000000-GaussHistos.root',
   failureReason = '',
   locations = ['CERN-USER'],
   subfiles =[]
 ),  DiracFile (
   defaultSE = '',
   namePattern = 'Gauss_MinBias_30000000.sim',
   guid = 'BAF55220-1F03-E611-9AA4-FA163E3C2FC6',
   remoteDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180278',
   compressed = False,
   localDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180278/Gauss_MinBias_30000000.sim',
   lfn = '/lhcb/user/m/mrwillia/2016_04/130180/130180278/Gauss_MinBias_30000000.sim',
   failureReason = '',
   locations = ['CERN-USER'],
   subfiles =[]
 ),  LocalFile (
   namePattern = 'summary.xml',
   compressed = False,
   localDir = ''
 )]




Ganga In [4]: jobs(1685).subjobs(1).outputfiles
Ganga Out [4]:
[ DiracFile (
   defaultSE = '',
   namePattern = 'Gauss_MinBias_30000000-GaussHistos.root',
   guid = '7F5064A6-E04B-4E11-0A5C-6EFD8B08554C',
   remoteDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180279',
   compressed = False,
   localDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180279/Gauss_MinBias_30000000-GaussHistos.root',
   lfn = '/lhcb/user/m/mrwillia/2016_04/130180/130180279/Gauss_MinBias_30000000-GaussHistos.root',
   failureReason = '',
   locations = ['CERN-USER'],
   subfiles =[]
 ),  DiracFile (
   defaultSE = '',
   namePattern = 'Gauss_MinBias_30000000-GaussHistos.root',
   guid = '7F5064A6-E04B-4E11-0A5C-6EFD8B08554C',
   remoteDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180279',
   compressed = False,
   localDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180279/Gauss_MinBias_30000000-GaussHistos.root',
   lfn = '/lhcb/user/m/mrwillia/2016_04/130180/130180279/Gauss_MinBias_30000000-GaussHistos.root',
   failureReason = '',
   locations = ['CERN-USER'],
   subfiles =[]
 ),  DiracFile (
   defaultSE = '',
   namePattern = 'Gauss_MinBias_30000000.sim',
   guid = '66C2F7FE-1D03-E611-A74E-525435315604',
   remoteDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180279',
   compressed = False,
   localDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180279/Gauss_MinBias_30000000.sim',
   lfn = '/lhcb/user/m/mrwillia/2016_04/130180/130180279/Gauss_MinBias_30000000.sim',
   failureReason = '',
   locations = ['CERN-USER'],
   subfiles =[]
 ),  DiracFile (
   defaultSE = '',
   namePattern = 'Gauss_MinBias_30000000.sim',
   guid = '66C2F7FE-1D03-E611-A74E-525435315604',
   remoteDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180279',
   compressed = False,
   localDir = '/lhcb/user/m/mrwillia/2016_04/130180/130180279/Gauss_MinBias_30000000.sim',
   lfn = '/lhcb/user/m/mrwillia/2016_04/130180/130180279/Gauss_MinBias_30000000.sim',
   failureReason = '',
   locations = ['CERN-USER'],
   subfiles =[]
 ),  LocalFile (
   namePattern = 'summary.xml',
   compressed = False,
   localDir = ''
 )]

```

The problem appears to be isolated on a per-subjob basis, my original fear that LFN were bleeding between subjobs doesn't appear to be correct. However the user is seeing the same file added more than once to the outputfiles each time. This is likely due to a problem either in 6.0.44 or 6.1.18 to do with wildcards matching the 'automatic' rules for fileypes on LHCb.
(Tagging Core/LHCb/Dirac as it's not known yet where the error is, if there is one)

I propose that if the problem is not reproducible with 6.1.18 then we don't have to worry too much and can probably add something to the twiki for people migrating from 6.0.44, however if it is a bug in 6.1.18 it would be nice to find/fix it.
",,43645564
484,LHCbTask fails without credentials in a bad way,open,2016-04-19T15:49:56Z,2016-04-20T07:53:59Z,,MEMBER,"Running the example from #173 I see the following error crop up when I try and run a task with a Dirac backend without the correct credentials:

```
2016-04-19 17:47:51,782 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Adapters              :192 ERROR   : GangaException:  ('Can not execute DIRAC API code w/o a valid grid proxy.',)
2016-04-19 17:47:49,286 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Lib.Job               :1432 INFO    : Splitting Job: 23
2016-04-19 17:47:49,711 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Lib.Job               :1461 INFO    : submitting 2 subjobs
2016-04-19 17:47:49,718 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Lib.Job               :1296 INFO    : Preparing subjobs
2016-04-19 17:47:49,740 GANGA_Update_Thread_GangaTasks Ganga.GangaDirac.Lib.RTHandlers    :123 INFO    : Job has no inputdata (T1 sites will be banned to help avoid overloading them).
2016-04-19 17:47:49,787 GANGA_Update_Thread_GangaTasks Ganga.GangaDirac.Lib.RTHandlers    :123 INFO    : Job has no inputdata (T1 sites will be banned to help avoid overloading them).
2016-04-19 17:47:49,841 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Adapters              :177 INFO    : submitting job 23.0 to Dirac backend
2016-04-19 17:47:51,136 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Credentials           :414 WARNING : Grid proxy has expired!
2016-04-19 17:47:51,137 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Credentials           :308 WARNING : Renew by typing 'gridProxy.renew()' at the prompt.
2016-04-19 17:47:51,784 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Lib.Job               :1648 ERROR   : JobManagerError: error during submit
2016-04-19 17:47:51,784 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Lib.Job               :1656 ERROR   : JobManagerError: error during submit ... reverting job 23 to the new status
2016-04-19 17:47:51,939 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Lib.Tasks             :212 ERROR   : Couldn't submit the job. Deactivating unit.
2016-04-19 17:47:51,937 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Lib.Job               :640 INFO    : job 23 status changed to ""new""

2016-04-19 17:47:51,940 GANGA_Update_Thread_GangaTasks Ganga.GPIDev.Lib.Tasks             :248 INFO    : Unit 0 of transform 0, Task 4 has aborted the loop

```

The number before INFO/ERROR here is the line number of the error which may help track this down a little faster.

Also copying tasks seems to keep the status of the old task despite being a new task I think... Is copying of tasks supported?
I don't know if this was the displayed information of if the task is corrupt, I haven't debugged this yet.
",,43645564
485,Get/Check Config options are loaded correctly when using little ganga,open,2016-04-08T12:33:13Z,2016-04-08T12:33:13Z,,CONTRIBUTOR,"We need to make sure that the config options from the environment, .ini files and .gangarc are loaded, checked for errors and frozen correctly across plugins.
",,43645564
486,Clean up failed job submission when using threads in Tasks,open,2016-04-06T09:18:19Z,2016-05-05T09:33:03Z,,CONTRIBUTOR,"At present, if you use `submit_with_threads` in Tasks and there's an exception, it will never try to resubmit. This is a bad thing.
",,43645564
487,Change default way that `hadd` is used for merging `root` files,open,2016-04-05T10:03:21Z,2016-04-05T10:03:21Z,,MEMBER,"I just thought I should let you know about some new options I submitted to ROOT for their hadd application, that have been available since the 6.06.00 release, that I think should be utilized by the merging in ganga.

The release notes are here

https://root.cern.ch/doc/v606/release-notes.html#hadd

but in short it was to
- Add support for the newer LZMA compression
- Add an option to automatically set the merged file compression level, based on what the input files used.

Now, one other thing you should take into account is by default the ROOT tuple files created by DaVinci etc., now use the LZMA level 6 compression level, not the ROOT default which is ZLIB level 1. This was done as it makes quite a nice reduction in the size of the ROOT files the users get back (There is an option in DaVinci to change this if a user really wants, but I guess most do not know it exists, which was partly the idea ;))

The problem is if you just merge files with

> hadd merged.root in1.root in2.root

then merged.root is forced to be ZLIB:1, even if the input is different. This is a problem because it forces ROOT into a much (orders of magnitude) slower mode of operation, as instead of just piping the compressed data stream directly from the input to output, it has to uncompress and recompress it.

To get around this you should use the -ff option, so

> hadd -ff merged.root in1.root in2.root

( I tried my best to argue that this should be the default mode, but the ROOT devs wouldn't go that far...)

So... The point I am trying to get to is what version of ROOT does ganga now internally use for merging ? If it is 6.06.00 or newer, I would strongly recommend using the -ff option for any hadd merging you do internally.

cheers Chris 
",,43645564
488,workerDir feature for IGangaFile,open,2016-03-30T13:21:52Z,2017-10-11T10:04:04Z,,MEMBER,"I would like to see a `remoteSubDir` attribute (or alternative) added to `IGangaFile`, for `LocalFile`, `MassStorageFile` and `DiracFile`.

If defined this should be used to move files into a relative path on the worker node at the start of jobs.
In addition it should be used as part of the pattern matching for files when the job has finished.

This has been requested a few times (and may even be an open issue on github that I can't find) and should be less than 50 of lines of code in the correct few places.

Adding this feature removes one major sticking point that prevents some power-users from migrating from 6.0.44, hence why I would very much like to see this in 6.1.19.
","@rmatev The best work-around I know is to submit a bash job which before the user-code runs extracts a tarball which has been in the input-sandbox or an input-LFN. It's a bit of an ugly hack but it works.

(aside)
I had planned to implement this after fixing adding proper support for wildcards and automatic DiracFile name support (which is now in).
Most of the difficult legwork has been done for this, the interfaces should pass an IGangaFile to the backend which generates the JDL. This means JDL can be expanded based upon the IGangaFile but there needs to be a common way to manage moving files on the WN from the IGangaFile interface. There is almost enough pieces in place to implement this but the various classes which inherit this all need to be expanded to support this.

If someone has a free week or a PhD/summer student to throw at this it's quite a nice little feature to implement and straight-forward as part of a wider project.",43645564
489,Possible Schema bug/shortcoming exposed by lsst hacking,open,2016-03-24T14:27:15Z,2016-03-29T12:21:24Z,,MEMBER,"Hi all,

I think I've discovered a bug or at least a shortcoming of the Schema system as it stands.

I would like to create an application with a fully qualified IGangaFile object in the schema.
This works however when I create an instance of this application interactively I see that the schema attribute is populated with an empty IGangaFile of the correct type.

e.g I define in my class

```
...
class myClass(...)
   _schema = Schema(... { someAttr : GangaFileItem(defvalue=DiracFile(lfn='myDefaultValue'))})
...
```

however I see:

```
Ganga In [1]: myClass()
Ganga In [2]: myClass.someAttr.lfn
''
Ganga In [3]:
```

I would like to have e.g. a DiracFile pointing to a default location which can be user configurable, however this is something I'm curious to know if people agree with my expectations that the Schema be able to deliver this?

Thanks,

Rob
","indeed what your describing is the named template system that I implemented some 2 years ago. Note this is not the same as the regular templates in Ganga which are mutable. As I recall you need to merely create a sub directory called templates within your extension package e.g. LHCb and then you will get a new repo of templates which can be accessed for in the LHCb case as:

``` python
lhcbtemplates
```

something like this. I'll have a check and get back to you.
",43645564
490,Input Output on shared storage,open,2016-03-21T19:44:36Z,2016-03-23T10:01:58Z,,NONE,"Hi guys,

Apologies if this question should be asked elsewhere but I tried the mailing list and got the message back undelivered https://github.com/ganga-devs/ganga/issues/222#issuecomment-199442925 .

I'm currently looking at ganga's Input/Output model but I'm getting a little confused with how LocalFile works and I don't get how SandboxFile is meant to be used.

The cluster where I'm using ganga has shares using NFS and a BeeGFS cluster for high speed access as well as /scratch.

Assuming this:
a) NFS (data location - variable between jobs)
b) BeeGFS (computing location - lets assume /scratch)

I would like to setup jobs for the following scenarios:

I) Job copies data from a) to b), performs computation, job finishes and moves results from b) to a)
II) Job reads data from a), computes using /scratch for temporary files, job writes final output to a) or moves from b)
III) Job reads data from a), computes (without using /scratch), job writes final output to a).

The ones I'll use more often are I) and II).
Can I actually setup ganga to perform these actions automatically or do I need to take care of the moving/copying/linking files by some other way?
","Hi, it certainly matches wildcards on outputfiles. We always intended for it to also match wildcard on input files but this was more difficult (especially with DiracFile as we need to do a wildcard DIRAC query which didn't properly exist in the API). I'm not sure if we got as far as making at least the other file types work with wildcard input. It would be easy at least to try with a simple LocalFile hello world type job.
",43645564
491,Storing sequences in Ganga,open,2016-03-21T12:24:41Z,2016-04-20T13:39:46Z,,CONTRIBUTOR,"I've been thinking about the overall requirements for storing sequences of `GangaObject`s and I've mostly narrowed it down to the following few requirements:
1. In the GPI, proxy-wrap objects as they are extracted from lists and strip proxies as they are added.
2. If the sequence is a schema attribute value:
   - set parent correctly when objects are added to a list and when a list itself is re-parented.
   - check `_readonly` (for locking submitted jobs).
   - set the dirty flag correctly.

If a sequence is not set as a value of a schema attribute then it has no parent, no need to track dirty and no need to track if it's read-only and thus special treatment is not needed (`list` is sufficient).

Currently `GangaList` is designed to provide these features but I feel it has some design flaws which make it a little tricky to use. The main problem I see with it is that it was designed as a `GangaObject` so that it could make use of the automatic proxy-generation but in the process a lot of code has needed to be added across the project to account for this. For example, it makes it difficult to work out the module dependency tree since `GangaObject` relies on `GangaList` (in getters and setters) but `GangaList` _is a_ `GangaObject`.

_As an aside before I continue, I've come to appreciate the distinction between `Node` and `GangaObject`. The former is for managing the hierarchy of parents (and serialisation) while the latter is for auto-proxying and schema-having._

My plan is a simpler replacement for `GangaList` which breaks it into two distinct parts:
- `List` is a subclass of `list` and `Node` which records the parent of all its elements and allows any new elements added to be parented correctly (point 2 above). It does not have a `_schema`. It should be used as an implementation detail of sequence schema attributes.
- `ProxyList` is a GPI-layer wrapper of this which does the proxy-wrapping of its elements (point 1 above).

I have an implementation of the above written which is about 100 lines of code. I believe that is is already in a state to be able to replace `GangaList` and would allow a lot of other code across the project to be simplified. For example it would make a lot of sense to set `SubJobXMLList` to be a subclass of `List`.

This should cover all the use cases discussed in #184 except I have not yet written any of the code to deal specifically with copying `GangaObject`s or assigning via the schema.

Thoughts?
","Yes, your summary of `List` is correct.

I believe @drmarkwslater has been looking at `GangaList`'s various constructor functions with an aim to do more-or-less what you say. His investigations will still be very useful if we switch to `List`/`ProxyList` as we need to have a good handle on the semantics of this all.

On your last point, this should be already possible but we're going to need to make sure we have _very_ good test coverage (and documentation) of these classes before merging, If you have use-cases like these I'd be very interested in hearing about them so post them somewhere (here or on a [gist](https://gist.github.com/)) and I will add them to the test suite.
",43645564
492,Local wrapper script exiting with code 9,open,2016-03-14T09:48:43Z,2016-06-14T14:03:44Z,,CONTRIBUTOR,"We've seen a few sporadic failures of test [Savannah19123](https://github.com/ganga-devs/ganga/blob/2939a9d04bac16cf9f6a25b90dbc3feb8cfc4d9f/python/Ganga/new_tests/GPI/Bugs/TestSavannah19123.py). The symptom is that the test is checking whether certain file exist or don't exist at the right times and at the end of the test it calls `j.kill()` which errors since the job is in the `failed` state (which it should not be). We couldn't work out why the job was failing so Mark added some debug to it. The debug hasn't helped directly but it's now occurred twice in a row on the Nightly tests.

You can see the output at [ganga-ci](https://ganga-ci.cern.ch/jenkins/job/Nightly/208/consoleText) but the important part is the logging output which has:

```
Ganga.Lib.Localhost : CRITICAL wrapper script for job 0 exit with code 9
Ganga.Lib.Localhost : CRITICAL report this as a bug at https://github.com/ganga-devs/ganga/issues/
```

so it seems that `__jobscript__` is failing in some way and is returning a `9` (whatever that means). I've seen this test fail on my local computer before so I don't think it's directly related to the canga-ci machine.
","@milliams I don't think we see this anymore, are we ok to close this?
",43645564
493,Split up the Ganga core package,open,2016-03-11T12:38:20Z,2016-03-11T12:38:20Z,,MEMBER,"The Ganga core package should be split up in a package that has the true core functionality and one or more other ones that contains the different GPI objects. See #297 for discussion. Depends on #302 to be done first.
",,43645564
494,Make Ganga terminate gracefully when receiving SIGTERM (kill),open,2016-03-11T10:20:02Z,2016-03-11T11:11:51Z,,MEMBER,"In a very common scenario for users, I suspended my laptop with a Ganga session running in a terminal window. The session was dead when I opened the laptop again. I was surprised to see that the Ganga process was in fact still running. Running strace on the process, seems to indiocate that Ganga is in fact still healthy.

```
[pid 22805] <... open resumed> )        = 14
[pid 22805] getdents(14, /* 13 entries */, 32768) = 1000
[pid 22805] getdents(14, /* 0 entries */, 32768) = 0
[pid 22805] close(14)                   = 0
[pid 22805] open(""/home/egede/gangadir/repository/egede/LocalXML/6.0/sessions"", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 14
[pid 22805] getdents(14, /* 13 entries */, 32768) = 1000
[pid 22805] getdents(14, /* 0 entries */, 32768) = 0
[pid 22805] close(14)                   = 0
[pid 22807] open(""/home/egede/gangadir/repository/egede/LocalXML/6.0/templates"", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 14
[pid 22807] getdents(14, /* 4 entries */, 32768) = 104
[pid 22807] getdents(14, /* 0 entries */, 32768) = 0
[pid 22807] close(14)                   = 0
[pid 22807] open(""/home/egede/gangadir/repository/egede/LocalXML/6.0/templates.metadata"", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 14
[pid 22807] getdents(14, /* 5 entries */, 32768) = 128
[pid 22807] getdents(14, /* 0 entries */, 32768) = 0
[pid 22807] close(14)                   = 0
```

Is this actually the intention? Also do we try to do something sensible if the Ganga process receives a kill signal (rather than kill -9)? Would it be possible to make Ganga shutdown nicely in this case  without producing any stdout/stderr which would cause the process to die as no terminal is attached. At the moment, a kill of the process gives the strace below.

```
[pid 22805] <... rt_sigaction resumed> NULL, 8) = 0
[pid 22805] rt_sigaction(SIGQUIT, {SIG_DFL, [], SA_RESTORER, 0x30160326a0}, NULL, 8) = 0
[pid 22805] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
[pid 22805] stat(""/home/egede/gangadir/repository/egede/LocalXML/6.0/sessions/localhost.09.30_Friday_11_March_2016.PID.22782.session.box.metadata.locks"", {st_mode=S_IFREG|0775, st_size=29, ...}) = 0
[pid 22805] stat(""/home/egede/gangadir/repository/egede/LocalXML/6.0/sessions/localhost.09.30_Friday_11_March_2016.PID.22782.session.box.locks"", {st_mode=S_IFREG|0775, st_size=29, ...}) = 0
[pid 22805] rt_sigaction(SIGINT, {SIG_IGN, [], SA_RESTORER, 0x30160326a0}, {0x7f9b5b0837e0, [], SA_RESTORER, 0x30160326a0}, 8) = 0
[pid 22805] rt_sigaction(SIGQUIT, {SIG_IGN, [], SA_RESTORER, 0x30160326a0}, {SIG_DFL, [], SA_RESTORER, 0x30160326a0}, 8) = 0
[pid 22805] rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0
[pid 22805] clone(child_stack=0, flags=CLONE_PARENT_SETTID|SIGCHLD, parent_tidptr=0x7f9b41ff9e78) = 3557
[pid 22782] <... select resumed> )      = ? ERESTARTNOHAND (To be restarted)
[pid 22782] --- SIGTERM (Terminated) @ 0 (0) ---
Process 22782 detached
[pid 22822] +++ killed by SIGTERM +++
[pid 22821] +++ killed by SIGTERM +++
[pid 22820] +++ killed by SIGTERM +++
[pid 22819] +++ killed by SIGTERM +++
[pid 22818] +++ killed by SIGTERM +++
[pid 22814] +++ killed by SIGTERM +++
[pid 22808] +++ killed by SIGTERM +++
[pid 22807] +++ killed by SIGTERM +++
[pid 22805] +++ killed by SIGTERM +++
[pid 22802] +++ killed by SIGTERM +++
[pid 22801] +++ killed by SIGTERM +++
[pid 22800] +++ killed by SIGTERM +++
[pid 22799] +++ killed by SIGTERM +++
[pid 22798] +++ killed by SIGTERM +++
[pid 22797] +++ killed by SIGTERM +++
[pid 22796] +++ killed by SIGTERM +++
[pid 22816] +++ killed by SIGTERM +++
[egede@localhost ~]$ 
```
","@rob-c To switch to the headless system indeed sounds like a good idea. This might actually make disconnected processes in general shut down in a much nicer way, Changed to an ""enhancement"".
",43645564
495,Integration to Jupyter,open,2016-03-04T15:52:51Z,2016-03-04T15:58:30Z,,MEMBER,"We should integrate Ganga with Jupyter. This requires several things for working
- [ ] import ganga such that jobs can be accessed from within a Pythn Notebook.
- [ ] A modified `terminal` in Julyter where an interactive Ganga session with monitoring can run.
- [ ] Potentially job monitoring running in a separate server like process to allow different Python 
    Notebooks to access jobs.
","@alexanderrichards Sounds great. Just open as a separate PR.
",43645564
496,"Investigate possible removal of ""filter"" subsystem",open,2016-03-01T09:58:12Z,2016-03-01T14:19:10Z,,CONTRIBUTOR,"In Ganga we have a system called 'filter' ([Filters.py](https://github.com/ganga-devs/ganga/blob/develop/python/Ganga/GPIDev/Base/Filters.py)) which seem to be a way of injecting code before assignments happen on schema items.

For example, setting

``` python
j.postprocessors = FileChecker()
```

will have a filter (defined in IPostProcessors.py) intercede, and wrap the `FileChecker` object in a `MultiPostProcessor` so that doing

``` python
type(j.postprocessors)
```

gives `Ganga.GPIDev.Base.Proxy.MultiPostProcessor`.

This is work which would make so much more sense being implemented in a standard Python `@property` 'setter' inside the `Job` class.

Apart from that, filters seem to provide the possibility of doing

``` python
j.backend = 'Remote'
```

and have it look up the plugin automatically but I'm unsure that anyone uses this in reality.

In summary, I feel that the filter system is too magical and scatters the code that is doing the interception throughout the code base. We need to consider whether the feature is worth the complexity and perhaps consider it for removal in the future.
","this is what ive done in the rewrite so it makes sense to me
",43645564
497,Fix GUID of Gaudi output files uploaded to SE for LHCb,open,2016-02-23T15:24:25Z,2016-04-28T09:06:45Z,,MEMBER,"If an output file (like a DST) is produced in LHCb outside the Grid environment, and subsequently upoloded to an SE, it gets assigned the wrong GUID and a subsequent Gaudi job can't read it. So the following workflow
1. Brunel job running on a batch system produce a DST
2. The output DST is declared a DiracFile and uploaded to an SE.
3. DaVinci job reading this DST directly from SE attempts to run.

At the moment, step 3 will fail unless the script `dirac-lhcb-fix-file-guid` has been executed. Looking inside the script, it sets the GUID at upload from the result of a call to `getRootFileGUID` from `LHCbDIRAC/Core/Utilities/File.py`. The solution is to insert this call for LHCb users before the `put` method is used in `DiracFile`.
","I don't know off the top of my head what functions we're using. This may be possible to fix by simply changing the LHCbDirac command to point to the correct bit of code. (Or fix DiracFile to use the abstracted commands we define in Ganga)
",43645564
498,User defined output LFN for DiracFile,open,2016-02-16T12:24:45Z,2016-05-05T09:33:44Z,,MEMBER,"A user should be able to define the LFN for a DiracFile. The support within the Dirac API seems to be in place.

https://its.cern.ch/jira/browse/LHCBDIRAC-299
https://its.cern.ch/jira/browse/GANGA-1921
","I think I'll look into this for 6.1.20 as it would be nice to have for both LHCb, LSST and others
",43645564
499,CREAM jobs stuck with old status,open,2016-02-15T11:55:01Z,2016-03-01T15:18:57Z,,NONE,"After some kind of update failure in checking the status of my jobs (I think this was a timeout, though apologies, I don't have the error message I got), their states have not continued updating.  When I check them directly with glite-ce-job-status it shows them as having finished (though failed).  

Apologies if this is not strictly a ganga problem but rather with another part of the system, or if I have misunderstood some other aspect of this - I'm very new to grid computing in general.  

But if this is a ganga issue might there be some way to restart the monitoring of a job perhaps? Or something else I should do?

See below for an illustration

```
[11:46:01]
Ganga In [30]: j=jobs((266,2))  #I have a number of jobs like this, some stuck as ""running"" some as ""submitted""

[11:46:06]
Ganga In [31]: j.status
Ganga Out [31]: 'running'

[11:46:10]
Ganga In [32]: j.time
Ganga Out [32]: 

         Time (UTC)   Status
- - - - - - - - - - - - - - - - - - - - - 
2016/02/10 14:17:58 - new
2016/02/10 14:18:01 - submitting
2016/02/10 14:18:17 - submitted
2016/02/10 14:18:34 - running


[11:46:16]
Ganga In [33]: !date  #To show how long they have been like this
Mon Feb 15 11:46:20 GMT 2016

[11:46:20]
Ganga In [34]: j.backend.id  #Will use this below to check manually
Ganga Out [34]: 'https://lcgce03.phy.bris.ac.uk:8443/CREAM734329072'

[11:46:46]
Ganga In [35]: quit()

Ganga.Core.InternalServices        : INFO     Stopping Job processing before shutting down Repositories
Ganga.Core.InternalServices        : INFO     Shutting Down Ganga Repositories
Ganga.Runtime                      : INFO     Registry Shutdown



# Now looking at the status manually
-bash-4.1$ glite-ce-job-status https://lcgce03.phy.bris.ac.uk:8443/CREAM734329072

******  JobID=[https://lcgce03.phy.bris.ac.uk:8443/CREAM734329072]
    Status        = [DONE-FAILED]
    ExitCode      = [N/A]
    FailureReason = [Job has been terminated (got SIGTERM); reason=-10]

```
","@joezuntz 6.1.16 is now available on cvmfs and `pip install` if you're able to try this version.
",43645564
500,Clean up import ganga code and start using this in tests,open,2016-02-08T14:39:01Z,2016-02-09T09:27:25Z,,CONTRIBUTOR,"After discussions with @milliams we think it would be good to get the little ganga stuff properly cleaned up and understood so we can then know exactly what's done at startup. In addition, the exit handlers should be able to be run independently of the atexit versions. We can then have tests such as:

``` python
class TestStartup(unittest.TestCase):
   @safe_shutdown    # decorator to run proper shutdown code
   def test_startup_submit(self):
      from ganga import *
      j = Job()
      j.submit()
      monitorUntilCompleted(j)    # Or something similar

   # Note: Don't care about the shutdown here so no decorator
   def test_checkjob_persist(self):
      from ganga import *
      assertEqual(len(jobs), 1)
```

This will do away with all the GangaTestCase stuff, be much closer (~the same) as what user will actually do and as a by product, I may be able to clean up the bootstrap code :)
","> and is currently about 120 lines of code

:+1: 

> reduces by a factor of 10 easily

:+1: 
All good!
",43645564
501,Parsing arguments with spaces,open,2016-02-02T12:23:48Z,2016-02-24T11:53:27Z,,NONE,"Ganga does not handle properly arguments with blank spaces in between, as in the case of optional arguments. For instance, in a Gaudi Python application:

```
application.args = [""arg"", ""-d"", ""-t optarg""]
application.script = [""script.py""]
```

is interpreted as:

```
script.py ""arg"" -d -t "" optarg""
```

leading to a blank extra space at the beginning of string optional arguments.

While `application.args = [""arg"", ""-d"", ""-t"", ""optarg""]` seems to do the job, it would be helpful to handle the previous usage properly or at least to have a warning message when using blank spaces. See this [discussion](https://groups.cern.ch/group/lhcb-distributed-analysis/Lists/Archive/Flat.aspx?RootFolder=%2Fgroup%2Flhcb-distributed-analysis%2FLists%2FArchive%2FGaudiPython%20with%20private%20package%20files&FolderCTID=0x01200200AD716970E9BBD0488B2362305E8E167D) in the mailing-list for an example of a use case. This issue can be found in one of the latest e-mails, E-mailed: 01/02/2016 17:13, the previous ones are an example of the consequences, which were difficult to connect to this issue.
","I think a harmless warning to the user when setting the parameter should be all that is required here. If they know better they can ignore it, if there is a problem elsewhere in the code then we should deal with that as a separate issue imo
",43645564
502,What is the prepared state doing?,open,2016-01-28T10:55:59Z,2016-01-28T11:20:48Z,,CONTRIBUTOR,"Looking at the hashes from the prepared state we realised we don't know how it works. Some concerns are:
1. If the hash is made from the Job `__repr__` then it would change whenever the state changes.
2. If the hash is made from the Application `__repr__` then nothing prevents me making a second prepared app with the same hash and hence we have a collision and an identity issue when choosing which prepared state to take from disk

If the intention of the prepared state hash is solely as a unique identifier to pick up the correct disk resident files then the python uuid package should probably be used instead as below:

``` python
import uuid
ident = uuid.uuid4()
```

If however the point is as above but includes the ability to detect if anything has changed then the above method wont work.

**However**: Off the top of my head I can't imagine why we would be tracking if a prepared state for an already prepared application had been changed. Surely they would just unprepare and reprepare thus creating a new uuid and the old files would be deleted when the counter for the old prepared state uuid hit zero.

In short we (or at least I :wink:) have no idea what's going on
","OK, the app isn't prepared if the hash changes. (or is removed?) when they go to an older version and we've changed it on disk.
At the very least there are a LOT of warnings which shouldn't be exposed to the user imo.
(several lines per write operation in the XML!)

I don't think we trigger a mas unprepare any more as there is no cleanup called on the application prepared repo.

Edit: Apologies I had to look this up, I thought there was an unprepare(or re-prepare) in the XML layer but there is not.
",43645564
503,"When importing in python, pretty printing doesn't work",open,2016-01-26T16:20:43Z,2016-07-04T07:36:38Z,,CONTRIBUTOR,"On the little ganga branch, I can now import everything, submit jobs, etc. However, the pretty printing just doesn't work. I'm guessing this is because it's done in an IPython way with `_repr_pretty_` but if you call `__str__()` then that (sort of) works in vanilla python. What would be the best way to get this working in normal python?
","I'm afraid not:

```
>>> from ganga import jobs
>>> jobs
<Ganga.GPIDev.Lib.Registry.JobRegistry.JobRegistrySliceProxy object at 0xe92d50>
>>> jobs(0)
Job#0
>>> 
```

we need to do something more but I honestly can't remember the details anymore - I'll need to come back to it I guess...
",43645564
504,Have the .gangarc loaded and checked for each package on module imported,open,2016-01-12T09:41:33Z,2016-02-17T15:51:08Z,,CONTRIBUTOR,"When the config system is initially setup (now in `__init__.py` - see #87), the .gangarc file should be loaded, cached and then each option checked for the package. This should then allow:

```
import Ganga    # config system setup, all config options created are checked in .gangarc
import GangaDirac   # additional config options are created and checked against the cached .gangrc
```

This can then be mostly removed from bootstrap.py.
",,43645564
505,Move config setup to __init__.py for all packages (not just Core),open,2016-01-12T09:38:32Z,2016-01-12T09:41:59Z,,CONTRIBUTOR,"See #87 but it needs to be extended for all packages.
",,43645564
506,Migrate the old prepared job hashes to new mechanism,open,2016-01-08T13:36:05Z,2016-01-08T13:36:05Z,,MEMBER,"The prepared state stores the hash of the XML of the application object which is generated in order to prevent the protected object from being modified whilst the job is in a prepared state.

This isn't too bad, but in practice the XML generated prior to 6.1.14 has formatting issues (and missing list support) which I've since fixed. As a result of this fix I've had to keep around the 'broken' code which is used solely for hash validation of the prepared state objects.

I'm considering a few alternative things as a consequence of this:
1. Is this still needed? (This is putting prepared state code into both the repository and elsewhere)
2. Are we wanting to keep checking the prepared state when writing to disk in future if we trust the object is marked read-only in the Schema?
3. Would we be happy breaking forwards compatibility? (this would lead to errors when loading the same job with older versions of Ganga)

This is relatively low priority as we can safely load old jobs and create backwards compatible new jobs without any errors being exposed to the user.
",,43645564
507,Add multicore options to backend attribute,open,2015-12-17T12:17:18Z,2019-10-08T12:10:24Z,,MEMBER,"For all backends, the ability to request multicore processing should be added.

We discussed at the meeting if this should be one or two attributes to allow to distinguish between _as many cores as possible_ and _give me X cores_. As a request for _as many as possible_ might anyway not give access to a full worker node, then I think it is best left as a single attribute.
## Attribute

The attribute should be called _ncore_. The default value is 1. A value of -1 means as many as possible, a positive value means that you request this number of cores. Any other number (0 or below -1) results in an error at submission time.
## Backend implementation

Each backend will have to transmit this request onwards as part of the submission. If the backend does not support it, any ncore number different from 1 should result in a failed submission.
## Worker node implementation

On the worker node, an environment variable should be set that tells the number of cores accessible to the application. This should in the wrapper script be set in an environment variable `GANGA_NCORE`. For specific application objects in Ganga (i.e. Gaudi), we can just start up the application with the correct number of cores requested.
","> BTW, please also consider previous discussions in https://its.cern.ch/jira/browse/LHCBDIRAC-496

I seemingly dont have access to view this. maybe @mesmith75 can you take a look",43645564
508,New monitoring system,open,2015-12-15T15:33:34Z,2016-01-25T20:14:50Z,,CONTRIBUTOR,"The current monitoring is a mire of threads, queues and locks which has grown to the point where it is extremely fragile. Long-term we have expressed a wish for a system which looks like the following:

> An external monitoring dæmon, written in Python 3 and using asyncio which, given a repository on disk will make all the calls to external services to check job status, download output (and do the submission?)

Of course, this doesn't solve the problem of having more than one agent needing to be able to write to a repository so we would still need lock files or make the dæmon act as a server too which all Ganga clients talk to (the dæmon would then maintain a queue of incoming new jobs and change requests).

Since we cannot rely on Python 3 yet, we cannot go straight to this system but an intermediate system which has the same interface characteristics could aid the transition.

The aim of this issue is to log our discussions on this topic to get an understanding of how a final system could look as well as the best way to progress towards it.
","There is some interesting discussion about the current system at #139.
",43645564
509,Schema version checking,open,2015-12-15T15:17:54Z,2015-12-16T13:07:31Z,,CONTRIBUTOR,"Whenever we define a new `GangaObject` subclass we have to also give it a `_schema` data member with an appropriate `Version` argument. As far as I can tell, the only place this version is used is in the VStreamer parsing at https://github.com/ganga-devs/ganga/blob/0b796886d15f843d38e1a85bbb6078f0677dca93/python/Ganga/Core/GangaRepository/VStreamer.py#L311 where it will create a dummy object if the versions don't match. I'm wondering how much we _actually_ use this feature and whether it's worth the effort of having to define a `Version` in every `GangaObject`? I assume the idea is supposed to be that the minor version is increased for every addition to the schema and the major version is increased for every removal.

Looking at `Job` however shows that in 5 years, between [here](https://github.com/ganga-devs/ganga/blob/9c227446c18742684e2abd8fc9160e22c650e2eb/python/Ganga/GPIDev/Lib/Job/Job.py#L104) and [here](https://github.com/ganga-devs/ganga/blob/develop/python/Ganga/GPIDev/Lib/Job/Job.py#L172) there were lots of additions made but the version did not increase once. If we _had_ have increased the version number then loading a new job in an old Ganga would fail.

At present, if we load a new job with an old Ganga and then again with a new Ganga then information is lost (e.g. setting `parallel_submit=True` on a `Job` in 6.1.13, loading it with 6.0.44 and then again with 6.1.13 resets it to `False`).

I'm not sure how to fix this or even if we need to? If we remove a schema attribute between version then it seems that it is just silently ignored when loading and if we add a new one then it seems to just use the default value. If this is the case and we're ok with it then I suggest removing the version checking entirely. Perhaps we print a warning if loading a job with new/removed attributes?
","OK, if we're loading objects from disk with extra information we need a mechanism for preserving this data in a read-only manner.
This should be something we agree on so I'll assume this to be true.

As for the safety, major != safe, minor == safe, I don't know why this is so bad, tbh, we can even drop the minor here if we build in a backwards compatible way of preserving data.
'
'
In fact I'd like to propose we fix preserving information not known to the current schema and keep the major version information to resolve compatibility issues and drop the minor version support as this serves no useful purpose if the job is compatible. As controversial as this may be.

'
'
I'll explain my concerns wrt compatibility below:

My problem with dropping all version checking is that that the only check that is performed when loading an attribute into memory is to test the type against the allowed types in the schema def.
If the class wants/needs to change the contents of the object which is stored then there is no sensible way to add support for this unless we have every class do the checking itself which largely does away with the whole point of schema system.

I know that statement is probably a bit strong but if the class can't write 'supercali...' to where it used to write 'foo:bar' then there is no easy way to convey that the expectations of the class have changed.

Yes the class can add a new attribute then we end up with 'stale' schema attributes which aren't being used or are only required for loading old jobs through ganga.
This leaves the schema cluttered and confusing which I'm not in favour of.

Also when an older object is loaded and manipulated it is always written with the version of the schema used at runtime so future Ganga versions would always upgrade an object potentially into a format that cannot be read by older versions.

eg:

Ganga v7r1 stores the attribute {myData, myGangaClassv1}
Ganga v7r2 can read myGangaClassv1 but now has myGangaClassv2. It can also translate the classes to the newer and better myGangaClassv2 which is largely a superset of myGangaClassv2 but is different enough to not be backwards compatible.

user now creates a job in v7r1, tries out v7r2 and doesn't like it and now attempts to go back to v7r1.

The job they created in 7r1 now has the attribute {myData, myGangaClassv2} which causes all jobs to now not be accessible.
The user cannot use v7r2 due to a new bug or other incompatibility and cannot use v7r1 due to the fact that their jobs have now been mangled by Ganga v7r2 which could read them.
The problem is that Ganga v7r2 has a very good reason to be using myGangaClassv2 because it addresses an issue.

I would like a major version number to allow us to ask the user if they want the object to be upgraded/loaded to prevent occurrences such as this.
",43645564
510,Make jobs.select behave in a more sane way,open,2015-11-20T15:52:19Z,2016-03-10T13:58:48Z,,MEMBER,"Hi all,

This is an issue to follow on from a discussion which was on the ganag-developers list to change the interface to jobs.select to be a bit more sane and look something similar to job.select(ids=[5]).
as suggested by @alexanderrichards.

I've been caught out by the 'strangeness' of the behaviour of this and as it's exposed to the users it would be nice to make this behave in a more standard way.

I think this is just a case of fixing the actual method as I don't think we make too much use of this method internally but it would probably require an entry in the changelog when it's changed.

Thanks,

Rob
","It may be worth looking at this for #286 to see if there is much overlap in the code (I suspect there is)
",43645564
511,GangaFile put and remove statements should return True/False,open,2015-10-30T10:38:02Z,2016-09-29T11:00:33Z,,MEMBER,"It has been requested that GangaFile objects return True/False for put and remove commands so that scripts and monitoring tools can determine if an action has completed successfully.

We already do this for setLocation and I think this should only be a few lines of code to change but I'[d prefer to leave it until 6.1.13 to implement this.
","No, we're nowhere near properly supporting this. There is still work to be done to support this way of working which I may get around to over the next few weeks.
",43645564
512,feat(hive_sync_manually),open,2020-03-26T16:52:16Z,2020-03-26T17:58:09Z,,CONTRIBUTOR,,,106382269
513,how to read from a hudi input?,open,2020-03-25T17:20:36Z,2020-03-25T17:27:35Z,,NONE,"i have
```
inputs:
  mydf:
    file:
      path: s3a://xx/a/b/c/
```
there are partition folders under s3a://xx/a/b/c/ path . and there are hudi parquets under them

i want the mydf to get the partition columns in the df too.

i get 

```
2020-03-25 16:20:27,070 [main] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: load at FilesInput.scala:29, took 0.073979 s
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.;
        at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$9.apply(DataSource.scala:208)
        at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$9.apply(DataSource.scala:208)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)
        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
        at com.yotpo.metorikku.input.readers.file.FilesInput.read(FilesInput.scala:29)
        at com.yotpo.metorikku.input.readers.file.FileInput.read(FileInput.scala:15)
        at com.yotpo.metorikku.Job$$anonfun$registerDataframes$1.apply(Job.scala:68)
        at com.yotpo.metorikku.Job$$anonfun$registerDataframes$1.apply(Job.scala:66)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at com.yotpo.metorikku.Job.registerDataframes(Job.scala:66)
        at com.yotpo.metorikku.Job.<init>(Job.scala:48)
        at com.yotpo.metorikku.Metorikku$.delayedEndpoint$com$yotpo$metorikku$Metorikku$1(Metorikku.scala:10)
        at com.yotpo.metorikku.Metorikku$delayedInit$body.apply(Metorikku.scala:7)
        at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
        at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
        at scala.App$$anonfun$main$1.apply(App.scala:76)
        at scala.App$$anonfun$main$1.apply(App.scala:76)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
        at scala.App$class.main(App.scala:76)
        at com.yotpo.metorikku.Metorikku$.main(Metorikku.scala:7)
        at com.yotpo.metorikku.Metorikku.main(Metorikku.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:890)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:192)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:217)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2020-03-25 16:20:27,077 [pool-1-thread-1] INFO  org.apache.spark.SparkContext - Invoking stop() from shutdown hook
```


also tried format: com.uber.hoodie
```
2020-03-25 16:36:07,067 [main] INFO  com.yotpo.metorikku.Job - Registering mydf table
Exception in thread ""main"" com.uber.hoodie.exception.HoodieException: 'path' must be specified.
        at com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:57)
        at com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:46)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:341)
        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
        at com.yotpo.metorikku.input.readers.file.FilesInput.read(FilesInput.scala:29)
        at com.yotpo.metorikku.input.readers.file.FileInput.read(FileInput.scala:15)
        at com.yotpo.metorikku.Job$$anonfun$registerDataframes$1.apply(Job.scala:68)
        at com.yotpo.metorikku.Job$$anonfun$registerDataframes$1.apply(Job.scala:66)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at com.yotpo.metorikku.Job.registerDataframes(Job.scala:66)
        at com.yotpo.metorikku.Job.<init>(Job.scala:48)
        at com.yotpo.metorikku.Metorikku$.delayedEndpoint$com$yotpo$metorikku$Metorikku$1(Metorikku.scala:10)
        at com.yotpo.metorikku.Metorikku$delayedInit$body.apply(Metorikku.scala:7)
        at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
        at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
        at scala.App$$anonfun$main$1.apply(App.scala:76)
        at scala.App$$anonfun$main$1.apply(App.scala:76)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
        at scala.App$class.main(App.scala:76)
        at com.yotpo.metorikku.Metorikku$.main(Metorikku.scala:7)
        at com.yotpo.metorikku.Metorikku.main(Metorikku.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:890)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:192)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:217)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```

also tried --conf spark.sql.hive.convertMetastoreParquet=false  same error result.

I have --jars ""/home/ec2-user/hoodie-spark-bundle-0.4.6.jar""

these are COW tables, https://hudi.incubator.apache.org/docs/querying_data.html mentions: 
spark.sparkContext.hadoopConfiguration.setClass(""mapreduce.input.pathFilter.class"", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);

^^not sure how to set that in metorikku?

if i do ```s3a://xx/a/b/c/*/*/*.parquet``` it seems to get further but not sure if? a) right approach, b) will have the partition columns, c) will have dupes in the data?

",,106382269
514,Spark Dynamic Allocation not working,open,2020-03-23T21:54:09Z,2020-03-23T21:54:09Z,,NONE,"Hello,

I have enabled dynamic allocation on the cluster but even when there is no data to process, the executors don't get released by dynamic allocation. Is there any specific configuration or is this a Spark issue?
Please help.

Thank you",,106382269
515,feat(Hudi): remove the usage of OverwriteWithLatestAvroPayloadWithDelete,open,2020-03-23T20:05:59Z,2020-03-23T21:29:49Z,,MEMBER,,,106382269
516,performance tuning with partitionby?,open,2020-03-21T11:36:28Z,2020-03-21T11:36:28Z,,NONE,"I have an input.yaml that defines 5 different data frames (all from various parquet paths on s3), the total size of the data across all 5 combined is 35GB. Then in the metric yaml I have single step that is a SQL joining together the 5 data frames and then finally outputting to parquet on s3 with a partitionBy defined on 2 columns (leading to a total of 80 partitions). Right now it takes 5 hours to run this (using executor memory of 90GB, 16 cores per executor and total-executor-cores of 80). Can you recommend any options to make it run faster? I was thinking maybe there is some shuffle/cache/repartition type options I should use?",,106382269
517,bump hudi to 0.5.0 with hive 1 support,open,2020-03-19T14:37:57Z,2020-03-22T06:12:39Z,,CONTRIBUTOR,,,106382269
518,feat(DATA-2844_Hudi_Auto_Healing),open,2020-03-18T20:13:43Z,2020-03-18T21:44:30Z,,CONTRIBUTOR,,,106382269
519,feat(metorikku_spark): add exec commad to master and worker entry point,open,2020-02-24T09:41:00Z,2020-02-24T09:41:00Z,,CONTRIBUTOR, to allow sigint when terminating,,106382269
520,fix(travis): upload snapshot to gtihub releases,open,2020-02-24T08:53:46Z,2020-02-25T20:20:23Z,,CONTRIBUTOR,,,106382269
521,is there a way to ensure csv in input.yaml is read by multiple executors?,open,2020-02-20T08:13:31Z,2020-02-20T13:21:22Z,,NONE,in input.yaml we can define source files ie abc.csv that get loaded into a dataframe. If this is a big file like 300GB will it be read all on a single executor? or is there a way to set shuffle/partitions.etc,"If you have only a single 300GB file that you load in metorikku you can set up first a step in the metric file that uses ```DISTRIBUTE BY``` or ```CLSUTER BY```.
Check out the documentation here:
https://docs.databricks.com/spark/latest/spark-sql/language-manual/select.html

But in any case, the first read will be done by a single executor.",106382269
522,Writing to HBase,open,2020-01-23T04:43:47Z,2020-02-03T22:13:24Z,,NONE,Is there any I can write to HBase Structure.,no https://github.com/YotpoLtd/metorikku/issues/151,106382269
523,feat(spark)L upgrade aws s3 sdk,open,2020-01-21T21:17:13Z,2020-01-21T21:23:14Z,,MEMBER,,,106382269
524,variable interpolation randomly not working (setting None) on first execution,open,2020-01-09T17:14:47Z,2020-01-15T16:27:50Z,,NONE,"spark 2.3.4 in spark standalone cluster running on EC2s (no k8s), java8, metorikku v0.43, amazonlinux1

2 job logs attached (from separate spark submit runs, both same command as below), you can see first run wrote a None partition! while second run wrote proper date partition. Run number 3+ (onwards) also worked well like run 2.

`/home/ec2-user/spark_home/bin/spark-submit --master spark://domain:6066 --conf spark.driver.extraJavaOptions='-Dinput_wildcard=20200107 -Ddatestr=20200107 -Dhourstr= -Dmonthstr=202001' --conf spark.sql.parquet.writeLegacyFormat=true --conf spark.executor.extraJavaOptions='-Dinput_wildcard=20200107 -Ddatestr=20200107 -Dhourstr= -Dmonthstr=202001' --executor-cores 2 --executor-memory 8g --driver-memory 2g --name tesla_catalyst_rawcsv_tesla_uk__tesla_uk_raw_INGEST_20200107 --class com.yotpo.metorikku.Metorikku --verbose --deploy-mode cluster s3a://buck/metorikku.jar --config /home/ec2-user/jsonconfigs/si/job/tesla_catalyst/metorikku/input/rawcsvtesla_uk_tesla_uk_raw_input.yaml`


",this seems to happen most commonly when using 'Append' mode instead of 'Overwrite',106382269
525,testing merge on read,open,2019-12-28T10:08:49Z,2019-12-28T10:08:49Z,,MEMBER,,,106382269
526,instrumentationClient.close() will be invoked multiple times,open,2019-12-20T06:47:21Z,2019-12-20T06:51:14Z,,CONTRIBUTOR,"I implemented a ElasticsearchInstrumentation which will send metrics to Elasticsearch. However, ElasticsearchClient is always closed before sending metrics. The cause is from following code,
line 24 at Job.scala :
```
  val instrumentationClient = instrumentationFactory.create()
  sparkContext.addSparkListener(new SparkListener() {
    override def onJobEnd(taskEnd: SparkListenerJobEnd): Unit = {
      instrumentationClient.close()
    }
  })
```
A spark application usually  creates several jobs (each operation action will create a job). And instrumentationClient will be closed multiple times when these jobs finished. 
I think the instrumentationClient can be closed only when the application ends.",,106382269
527,WIP: Support test expected output csv mock,open,2019-12-19T20:19:46Z,2019-12-24T20:13:23Z,,CONTRIBUTOR,,,106382269
528,fix(Tester):one subtable for all errored rows to be printed together,open,2019-12-12T15:05:34Z,2019-12-12T15:40:52Z,,CONTRIBUTOR,,,106382269
529,Add jdbc upsert writer,open,2019-12-12T03:31:10Z,2019-12-14T10:11:53Z,,CONTRIBUTOR,"Hi, Spark dataframe does only support append or overwrite data. I implement a  simple JdbcUpsertWriter to upsert data in relation database. It will check whether the  dataframe row exists in database. If the row exists,generates an UPDATE statement and if row does not exists, creates an INSERT statement.","The idea of extending JdbcQueryWriter can be summarized:
1. If  only specify query parameter, it will executes query on each dataFrame row.
2. If specify query and update paramter, it will execute query to check dataFrame row exists in database. If row exists in database, generate updateStatement.
3. If specify query and insert paramter, it will execute query to check dataFrame row exists in database. If row does not exist in database, generate insertStatement.
3. If specify query, update and insert paramter, it will execute query to check dataFrame row exists in database. If row exists in database, generate updateStatement, and if row does not exist in database, generate insertStatement.
Do you think it would be ok? @RonBarabash ",106382269
530,overwrite at partitionby folder level,open,2019-09-11T21:43:06Z,2020-01-19T08:15:42Z,,NONE,"output dir: x/y
partition by colTicker

new source data for colTicker can appear everyday, sometimes new values sometimes existing values.

example:
day1 had 2 files under below folder structure:
colTicker=AMZN
colTicker=MSFT (lets say 150 rows in the file)

day2 had 1 file:

colTicker=GOOG

day3 had 3 files:
colTicker=MSFT (lets say 120 rows in the file)
colTicker=IBM
colTicker=LYFT

i want to replace data (so not storing 2 days versions/duplicates) for same colTicker but keep data for all distinct colTicker partitions

so my expected output data after each day:

day1 output data under outputdir:
colTicker=AMZN
colTicker=MSFT (150)

day2 output data under outputdir:
colTicker=AMZN (existing data should be untouched, was loaded in day1 only, never removed)
colTicker=MSFT (150)
colTicker=GOOG

day3 output data under outputdir:
colTicker=AMZN (existing data should be untouched, was loaded in day1 only, never removed)
colTicker=MSFT (120, previous 150 should be overwritten)
colTicker=GOOG (existing data should be untouched, was loaded in day1 only, never removed)
colTicker=IBM
colTicker=LYFT

issue is:
 if i try savemode=append then after day3 MSFT shows 270 rows
 if i try savemode=overwrite then after day3 AMZN/GOOG data is gone

",Definately simply send it as part of the spark-submit --conf spark.sql.sources.partitionOverwriteMode= dynamic or if you're running the standalone use -Dspark.sql.sources.partitionOverwriteMode= dynamic,106382269
531,how to use schemaJson and get partition columns from s3?,open,2019-09-03T20:04:00Z,2019-09-05T20:22:42Z,,NONE,"i have s3 paths like:
s3://mybucket/a/b/c/d/e/col1=x/col2=y/some1.csv
s3://mybucket/a/b/c/d/e/col1=a/col2=b/some2.csv
s3://mybucket/a/b/c/d/e/col1=i/col2=k/some3.csv
s3://mybucket/a/b/c/d/e/col1=t/col2=p/some4.csv

in my input yaml i put s3://mybucket/a/b/c/d/e/ as i want col1 and col2 values to be part of the dataframe and i want to read/select from all 4 files in one go. also i put a schemaJson as i want to rename cols in the dataframe. But my metric yaml complains about can't find column col1, col2. how to solve this?

","@lyogev 

{
""$schema"": ""smallTestSchema"",
""id"": ""smallTestSchema"",
""type"": ""object"",
""name"": ""/"",
""properties"": {
""row_no"": { ""id"": ""smallTestSchema/row_no/"", ""type"": ""string"", ""name"": ""row_no"" }
,""start_date"": { ""id"": ""smallTestSchema/start_date/"", ""type"": ""string"", ""name"": ""start_date"" }
,""end_date"": { ""id"": ""smallTestSchema/end_date/"", ""type"": ""string"", ""name"": ""end_date"" }
,""unit_name"": { ""id"": ""smallTestSchema/unit_name/"", ""type"": ""string"", ""name"": ""unit_name"" }
}",106382269
532,hudi - how to use no partitions or non date partition?,open,2019-08-19T15:14:57Z,2019-08-28T12:45:13Z,,NONE,"how to define no partitions, or an 'unknown' column partition or a partition that is a string?

Caused by: java.lang.IllegalArgumentException: Partition path default is not in the form yyyy/mm/dd
        at com.uber.hoodie.hive.SlashEncodedDayPartitionValueExtractor.extractPartitionValuesInPath(SlashEncodedDayPartitionValueExtractor.java:54)
        at com.uber.hoodie.hive.HoodieHiveClient.getPartitionEvents(HoodieHiveClient.java:216)
        at com.uber.hoodie.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:160)
        ... 62 more","We have 2 options either not sending a partitionBy config and then partitions are ignored and com.uber.hoodie.NonpartitionedKeyGenerator is used to partition. Or write a simple unknown partition:
```
steps:
  - dataFrameName: table_data
    sql:
      SELECT *, 'default' as default
      FROM table

output:
  - outputType: Hudi
    dataFrameName: table_data
    outputOptions:
      path: path.parquet
      keyColumn: id
      timeColumn: date
      saveMode: Append
      hivePartitions: default
      partitionBy: default
      tableName: hive_table
```",106382269
533,how to execute metorikku in a loop from a single spark-submit?,open,2019-08-19T12:39:38Z,2019-08-28T12:43:17Z,,NONE,"use-case: near-real time sync from MSSQL db to s3.
I know I can do a while loop calling spark-submit each time but this will be slow as JVM needs to startup each time. Is there a way to make metorikku keep on looping through the same input/metric.yaml within a single spark-submit?

note: i know should have kafka..etc but not an option at this stage","We don't have such a feature, but it's a cool idea to create fake micro batches in metorikku to stream on non streaming sources",106382269
534,hudi - java.lang.ClassNotFoundException: com.uber.hoodie.SimpleKeyGenerator error,open,2019-08-19T11:53:29Z,2019-08-28T12:41:51Z,,NONE,"Using spark 2.3, v0.0.51/metorikku-standalone.jar

/home/ec2-user/spark_home/bin/spark-submit --master local[*] --conf ""spark.sql.parquet.writeLegacyFormat=true"" --class com.yotpo.metorikku.Metorikku --jars ""/usr/lib/apache-hive-2.3.4-bin/lib/mysql-connector-java.jar,/home/ec2-user/hoodie-spark-bundle-0.4.7.jar"" --conf ""spark.serializer=org.apache.spark.serializer.KryoSerializer"" /home/ec2-user/metorikku-standalone.jar -c /home/ec2-user/hi.yaml

Exception in thread ""main"" com.yotpo.metorikku.exceptions.MetorikkuWriteFailedException: Failed to write dataFrame: dac_curr to output: Hudi on metric: hm
        at com.yotpo.metorikku.metric.Metric.com$yotpo$metorikku$metric$Metric$$writeBatch(Metric.scala:101)
        at com.yotpo.metorikku.metric.Metric$$anonfun$write$1.apply(Metric.scala:136)
        at com.yotpo.metorikku.metric.Metric$$anonfun$write$1.apply(Metric.scala:125)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at com.yotpo.metorikku.metric.Metric.write(Metric.scala:125)
        at com.yotpo.metorikku.metric.MetricSet$$anonfun$run$1.apply(MetricSet.scala:44)
        at com.yotpo.metorikku.metric.MetricSet$$anonfun$run$1.apply(MetricSet.scala:39)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at com.yotpo.metorikku.metric.MetricSet.run(MetricSet.scala:39)
        at com.yotpo.metorikku.Metorikku$$anonfun$runMetrics$1.apply(Metorikku.scala:17)
        at com.yotpo.metorikku.Metorikku$$anonfun$runMetrics$1.apply(Metorikku.scala:15)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at com.yotpo.metorikku.Metorikku$.runMetrics(Metorikku.scala:15)
        at com.yotpo.metorikku.Metorikku$.delayedEndpoint$com$yotpo$metorikku$Metorikku$1(Metorikku.scala:11)
        at com.yotpo.metorikku.Metorikku$delayedInit$body.apply(Metorikku.scala:7)
        at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
        at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
        at scala.App$$anonfun$main$1.apply(App.scala:76)
        at scala.App$$anonfun$main$1.apply(App.scala:76)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
        at scala.App$class.main(App.scala:76)
        at com.yotpo.metorikku.Metorikku$.main(Metorikku.scala:7)
        at com.yotpo.metorikku.Metorikku.main(Metorikku.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: Could not load key generator class com.uber.hoodie.SimpleKeyGenerator
        at com.uber.hoodie.DataSourceUtils.createKeyGenerator(DataSourceUtils.java:98)
        at com.uber.hoodie.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:87)
        at com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:91)
        at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)
        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)
        at com.yotpo.metorikku.output.writers.file.HudiOutputWriter.write(HudiOutputWriter.scala:115)
        at com.yotpo.metorikku.metric.Metric.com$yotpo$metorikku$metric$Metric$$writeBatch(Metric.scala:97)
        ... 33 more
Caused by: com.uber.hoodie.exception.HoodieException: Unable to load class
        at com.uber.hoodie.common.util.ReflectionUtils.getClass(ReflectionUtils.java:40)
        at com.uber.hoodie.common.util.ReflectionUtils.loadClass(ReflectionUtils.java:74)
        at com.uber.hoodie.common.util.ReflectionUtils.loadClass(ReflectionUtils.java:87)
        at com.uber.hoodie.DataSourceUtils.createKeyGenerator(DataSourceUtils.java:96)
        ... 55 more
Caused by: java.lang.ClassNotFoundException: com.uber.hoodie.SimpleKeyGenerator
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at com.uber.hoodie.common.util.ReflectionUtils.getClass(ReflectionUtils.java:37)
        ... 58 more
","This is the one we're using:
https://repo1.maven.org/maven2/com/uber/hoodie/hoodie-spark-bundle/0.4.7/hoodie-spark-bundle-0.4.7.jar
",106382269
535,Need a sample test metrics config ,open,2019-08-02T20:26:42Z,2019-08-28T17:27:35Z,,NONE,"Is it possible to have a testmetrics configuration which contain all the configuration . 

We have currently limited documentation for the testing of the metric files . 

- The documentation has one example config file of job and metric",thank you so much @lyogev . that helps  :),106382269
536,feat(docker/spark/log4j.json.properties): added info level for metori…,open,2019-07-14T06:52:31Z,2019-07-14T06:52:59Z,,MEMBER,…kku classes,,106382269
537,SBT assembly is not building the standalone jar ,open,2019-07-02T21:49:41Z,2019-07-03T09:37:44Z,,NONE,"Hello 

When I am trying to build the standalone jar with sbt assembly , it is not creating the jar . It is geting halted after the test is completed 

but sbt package is somehow working .

19/07/02 16:29:52 WARN CacheManager: Asked to cache already cached data.
[info] - Test Metorikku should not fail on invalid query when ignoreOnFailures is set to true
[info] ScalaTest
[info] Run completed in 1 minute, 12 seconds.
[info] Total number of tests run: 6
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 6, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 6, Failed 0, Errors 0, Passed 6
[success] Total time: 299 s, completed 2/07/2019 04:33:34 PM
[IJ]>","I don't think it halts, it just takes a very long time depending on your computer.
Try with some memory configurations:
```
-Xms2048M
-Xmx2048M
-Xss6M
-XX:MaxPermSize=512M
-XX:ReservedCodeCacheSize=256M
```
",106382269
538,WIP fix(build): speedup build,open,2019-07-01T22:25:32Z,2019-07-12T13:03:00Z,,CONTRIBUTOR,,,106382269
539,feature req - 'spline' (data lineage) integration,open,2019-06-05T11:31:09Z,2020-01-26T20:27:41Z,,NONE,"https://absaoss.github.io/spline/

This feature is about new config flags in yaml so metorikku can integrate with spline to output lineage information between input->metric(s) and metric->metric(s)

ie columnA * columnB from inputX   ---> columnC in metricY","@tooptoop4 we have added apache atlas integration which also support data lineage,
check it out to see if it fits your needs,",106382269
540,feature req - 'deequ' (data quality) integration,open,2019-06-05T11:27:01Z,2020-02-25T14:36:17Z,,NONE,"https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/
https://github.com/awslabs/deequ

This feature is about new config flags in yaml so metorikku can integrate with deequ to:
1. output Data analysis metrics like ApproxCountDistinct, Mean.etc
2. run 'unit tests for data' .ie ComplianceConstraint",Wanna work with this.,106382269
541,read hive data problem,open,2019-05-30T10:02:57Z,2019-07-01T16:41:20Z,,NONE,"I use metorikku reading data from hive, then write to hdfs with parquet format. But the out is always empty. I can not figure out what is wrong,  can someone give me some advice? Thanks.
The job conf:
metrics:
  - test_metric.yml
output:
    file:
        dir: /tmp

test_metric conf:
steps:
- dataFrameName: df1
  sql: 
    SELECT * FROM employee
    
output:
- dataFrameName: df1
  outputType: parquet
  outputOptions:
    saveMode: overwrite
    path: df1.parquet","Please check if the files are created in this directory     path: df1.parquet
For me it generated files inside this directory . Previously I  thought this is a file .",106382269
542,Instrumentation example link in readme is wrong,open,2019-05-03T08:16:55Z,2019-05-03T08:16:55Z,,NONE,Can't find examples of instrumentation like how to access number of failed steps in a metric,,106382269
543,feature - defining failure criteria,open,2019-04-10T12:14:29Z,2019-04-29T06:49:41Z,,NONE,"would be nice to have a step where you could define failure/continue rules:

step1:
sql: select count(*) count from df where (length(run_date) > 0 and to_timestamp(run_date,'yyyy-MM-dd HH:mm:ss') is null )
failCriteria: count > 0
step2:
sql: whatever next step was.....

metorikku would exit out with failure (returncode 1) after step1 if this sql returned count > 0, otherwise if count is 0 then metorikku can proceed to step2
","workaround:
add this where to step2 sql --> where 1 = (select 1 from step1 where count > 0 union all select 1 from step1 where count > 0 union all select 1 from step1 where count = 0 ) so it will force error of multi rows returned if count > 0 only",106382269
544,feature - Hive 'MSCK REPAIR/COMPUTE STATISTICS' support,open,2019-04-02T10:36:56Z,2019-04-13T18:33:46Z,,NONE,"Logs shows:
2019-04-02 10:30:40,289 [main] INFO  org.apache.spark.sql.internal.SharedState - loading hive config file: file:/mysparkpath/hive-site.xml
2019-04-02 10:30:40,344 [main] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('file:///home/ec2-user/hive/warehouse') to the value of spark.sql.warehouse.dir ('/apps/hive/warehouse').
2019-04-02 10:30:40,345 [main] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is '/apps/hive/warehouse'.

Since pointing at hive-site.xml is enough to enable a Hive Context (assuming metastore host/port is in the hive-site) would be great to have a step that allows execution of MSCK REPAIR dataframe name.
",haven't tried it yet,106382269
545,'partition by' same values as input filenames (not folders),open,2019-03-20T22:05:17Z,2019-04-02T06:50:02Z,,NONE,"input:
folderA/20180201.csv
folderA/20180202.csv
folderA/20180203.csv

with single metorikku jar run want to get output in parquet of:
date=20180201/a.parquet
date=20180202/a.parquet
date=20180203/a.parquet
",dont have :( only the filename,106382269
546,log df.count of input/metric/output dataframes,open,2019-03-16T22:23:36Z,2019-03-21T05:18:41Z,,NONE,"For each input, metric, output would be great to log the count of rows processed. ie if input csv had 100 rows then spark log to show input<abc name> 100 rows, then if metric had some where filter log metric<xyz name> 47 rows..eetc","We report counts for processed outputs with instrumentation and we can add logs of course.
For all other use cases I wouldn't want to burden the process with counts that I don't need as these are very expensive.",106382269
547,CSV input with timestamp schema support,open,2019-03-14T19:01:13Z,2019-03-21T05:26:04Z,,NONE,"I am trying to load in a csv input and have put following json schema but i can't see anyway for metorikku to interpret the timestamp format (ie yyyy-mm-dd hh:   ..etc) of the csv column in order to convert to spark timestamp 


{
  ""$schema"": ""smallTestSchema"",
  ""id"": ""smallTestSchema"",
  ""type"": ""object"",
  ""name"": ""/"",
  ""properties"": {
    ""row_no"": {      ""id"": ""smallTestSchema/row_no/"",      ""type"": ""string"",      ""name"": ""row_no""    }
    ,""start_date"": {      ""id"": ""smallTestSchema/start_date/"",      ""type"": ""string"",      ""name"": ""start_date""    }
    ,""end_date"": {      ""id"": ""smallTestSchema/end_date/"",      ""type"": ""string"",      ""name"": ""end_date""    }
    ,""unit_name"": {      ""id"": ""smallTestSchema/unit_name/"",      ""type"": ""timestamp"",      ""name"": ""unit_name""    }
}","You are right dates are not supported currently in our Json schema implementation.
As a workaround load it as string and use SQL function to_timestamp or to_date as documented here:
https://spark.apache.org/docs/2.4.0/api/sql/",106382269
548,Fixed width file input support,open,2019-02-22T23:43:27Z,2019-02-23T20:03:03Z,,NONE,"Rather than CSV, would be nice to to have a fixed width (non delimited) input file support. I guess an accompanying spec would need to be in the input yaml to say column name and char start-end position ie {columnA:""1-20"",columnB:""21-25"",columnC:""26-50""}


","Not planned at the moment, if anyone wishes to help write this it can be very helpful.",106382269
549,api get/post input support,open,2019-02-22T23:30:25Z,2019-02-24T00:04:22Z,,NONE,would be great to be able to specify something like a curl request to a simple GET http endpoint (and later POST params) and the results feeding into a metorikku input,"no, i mean specify input value in the yaml as GET 'https://jsonplaceholder.typicode.com/posts?userId=1' then in a metric i could export to any normal output like csv,parqueet,jdbc.etc Key thing is the input is the results from a API url. I guess at first can make requirement that the API results must be in json format.",106382269
550,mongo input support,open,2019-02-22T23:27:30Z,2019-12-25T18:13:14Z,,NONE,any plans to add mongo input support? ,https://github.com/YotpoLtd/metorikku/pull/239,106382269
551,Support loading config files from HDFS,open,2019-01-31T10:04:15Z,2019-01-31T10:04:15Z,,CONTRIBUTOR,"This is a very simple fix that can help with some use cases.
Just switch loading the files to use org.apache.hadoop.fs classes.",,106382269
552,Support watermark in Kafka output/input,open,2018-11-18T07:44:50Z,2018-11-18T07:44:50Z,,MEMBER,,,106382269
553,Kafka input to support Multiple outputs,open,2018-11-16T13:11:46Z,2018-11-17T06:15:21Z,,MEMBER,"add the ability to write to multiple outputs while using kafka input, currently supports only parquet and other kafka topics and just one of them.","We need to use this:
https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch",106382269
554,JDBC writer does not support column ordering,open,2018-08-19T14:55:55Z,2018-08-19T14:55:55Z,,CONTRIBUTOR,"From the README, we can use:
```INSERT INTO table_name (column1, column2, column3, ...) VALUES ($1, $2, $3, ...);```
The column ordering does not work and instead we need to use 
```INSERT INTO table_name (column1, column2, column3, ...) VALUES (?, ?, ?, ...);```",,106382269
555,No indication of passed test\pass rate,open,2018-04-15T10:02:32Z,2018-04-15T10:02:32Z,,NONE,"Metrikku Test runner:
There is no indication which tests have passed and which ones failed and for what reason - at the end of a run",,106382269
556,"Document samples of all configuration files (test, metric)",open,2018-01-29T20:46:01Z,2019-02-20T14:36:10Z,,CONTRIBUTOR,,,106382269
557,Document how to download latest version from github by script,open,2018-01-29T20:45:41Z,2019-02-20T14:36:55Z,,CONTRIBUTOR,,,106382269
558,Add the ability to deactivate the caching and count in metorikku write,open,2018-01-24T16:36:05Z,2018-01-24T16:36:05Z,,CONTRIBUTOR,"When using the write function in metorikku a cache and count are performed.
These actions should not be mandatory but triggered.",,106382269
559,Metorikku Tester - Comparison of nested arrays,open,2017-11-28T08:24:29Z,2017-11-28T08:24:29Z,,MEMBER,"Currently in Metorikku tester compares nested arrays using string comparison.
We need to use deep equals.
",,106382269
560,Add the ability to override configuration using cli params,open,2017-11-27T14:33:42Z,2019-03-30T19:38:16Z,,MEMBER,"currently Metorikku is using a simple YAML config file as input.
we need to be able to override this configuration using CLI params","@akarsh3007 Metorikku defines a job and its corresponding configuration, u can take a look here:
[Configuration](https://github.com/YotpoLtd/metorikku/blob/master/src/main/scala/com/yotpo/metorikku/configuration/job/Configuration.scala)
[Configuration Parser](https://github.com/YotpoLtd/metorikku/blob/master/src/main/scala/com/yotpo/metorikku/configuration/job/ConfigurationParser.scala)

When instantiating a job we parse the relevant config supplied by the yaml file, after we do this we probably want to override certain members of this class supplied as CLI params to the job,
You would need to add a new flag to the cli parser here:
[CLIParser](https://github.com/YotpoLtd/metorikku/blob/87ff2027b23cd6f0e95b15bc2b6f12c0cca0ae15/src/main/scala/com/yotpo/metorikku/configuration/job/ConfigurationParser.scala#L16)\
something like `--configParameters` and than key value pairs of the values.
make senes?


",106382269
561,Cannot resume GPU operation from the manager,open,2020-03-27T11:39:22Z,2020-03-27T11:52:45Z,,NONE,"**Describe the bug**

When clicking the ""Resume GPU"" options it does nothing. GPU does not start to spin the fans or make any useful load and the button remains the same ""Resume GPU""

<img width=""259"" alt=""Screenshot 2020-03-27 at 13 35 58"" src=""https://user-images.githubusercontent.com/3625244/77752272-ecfd0480-702f-11ea-86ca-74887f1e52c2.png"">

**Steps To Reproduce**
Not sure, it works before on the previous version, but today I got an update to the 7.14.3 and it does not work anymore.

**Expected behavior**
When I click ""Resume GPU"" I expect to see that GPU is starting to work

**System Information**
 - OS: MacOS 10.15.4 (19E266)
 - BOINC Version: 7.14.3",Also it's possible that you have more prioritized CPU tasks that should be finished first and no CPU resources to run additional GPU task,30260406
562,Test fix codecov,open,2020-03-27T11:13:49Z,2020-03-27T12:20:00Z,,MEMBER,"Signed-off-by: Vitalii Koshura <lestat.de.lionkur@gmail.com>
","@BOINC/contributors, please review and merge",30260406
563,boincgui.sh disconneced ... from what?,open,2020-03-27T01:37:46Z,2020-03-27T01:37:46Z,,NONE,"**Describe the bug**
A clear and concise description of what the bug is.
When I start boincgui.sh, only the ""File"" and ""Help"" menus are active, and the diagnostic message at the bottom-right corner says ""Disconnected"". So **what is disconnected from what?**

Is it boincgui.sh disconnected from the boinc daemon? Is it the boinc daemon disconnected from the boinc server? Or does it have something to do with something called ""account manager""?
**Steps To Reproduce**
1. start boinc daemon
2. start boincgui.sh

**Expected behavior**
Normal functioning, or at least some amount of diagnostic messages saying what's wrong.

**Screenshots**
![https://imgur.com/7tFsQdH.png](https://imgur.com/7tFsQdH.png)

**System Information**
 - OS: Linux 5.4.28
 - BOINC Version: 7.16.3

**Additional context**
The additional context is that 

1)I started boinc in the directory that was not defined as BOINC_DATA at compile-time. And zero feedback from boinc about that.
2)Even when I restarted it in the $BOINC_DATA, the directory was not users-writable, and BOINC actually told me ""another instance of boinc running"". 

So boinc diagnostic messages are either unhelpful or misleading. I would humbly ask to (at least) change the ""Disconnected"" message into ""Cannot connect boinc gui to the boing instance, which you are claiming is running on $chosen_hostname_or_ip"". 
",,30260406
564,Dockerfile with all necessary build components preinstalled,open,2020-03-26T19:00:01Z,2020-03-27T06:36:15Z,,NONE,"**Describe the problem**

For Linux users, a Dockerfile with everything preinstalled for building boinc, would be a very fast way indeed to onboard onto the project.

**Describe the solution you'd like**

Add a `Dockerfile`, such as a Debian or CentOS distribution, with all the compilers, dependencies, linters, and so on installed. The Dockerfile generates a Docker image, which is able to build Linux boinc binaries.

**Additional context**

Relates to https://github.com/BOINC/boinc/issues/3529
",I made one 2 years ago. https://github.com/adamradocz/boinc-client-builder-docker,30260406
565,[WIP] track dependencies as git submodules,open,2020-03-26T18:54:07Z,2020-03-27T00:59:58Z,,NONE,"Fixes #3529

**Description of the Change**

Copy dependencies into boinc source tree as git submodules, in order to automate more of the developer environment setup process.

Pros: Dramatically simplifies and organizes dependency management.

Cons: Some CI systems and network environments have difficulty using git submodules.

TODO:

We still need to configure the build system to look in these directories for the dependencies. The current setup expects the dependency directories to appear alongside the top-level boinc directory, whereas the new structure would nest dependencies inside boinc/vendor/<lib>.

We should also update the build documentation, to reflect the fact that the dependencies are managed using the git submodule system, as opposed to manual file and folder munging.

**Alternate Designs**

* We could use a prominent, cross-platform C/C++ package manager, such as vcpkg.

Pros: Free, open source, reasonably mature.

Cons: Some limited support for more obscure platforms such as Illumos, Haiku, DragonflyBSD.

* We could also copy the source trees more directly, using archive files and cryptographic checksums to validate contents.

Pros: More resilient to external outages.

Cons: More cumbersome to manage than git submodules. Binary blobs make the source tree larger. Involves a little shell script glue code to validate and extract dependencies during the build process.

**Release Notes**

Track C/C++ library dependencies as git submodules","A simpler solution would be to create scripts based on 3rdParty/buildMacDependencies.sh. This script automatically downloads and builds all the dependencies listed in mac_build/dependencyNames.sh for the Mac and is used for creating dependency cache used by Travis CI.

It calls downloads all the source files and then calls various scripts to build each dependency. Updating to newer versions of dependencies merely involves updating the dependencyNames.sh file.

A similar set of scripts should be created for development and release builds of the Mac and , by extension, for all platforms.",30260406
566,Copy C/C++ library dependency code into boinc source tree,open,2020-03-26T18:28:32Z,2020-03-27T09:38:36Z,,NONE,"**Describe the problem**

The build system is complex and fragile, expecting contributors to manage dependencies manually. This is especially problematic when any one of the dependent libraries could be offline or accidentally introduce breaking changes.

**Describe the solution you'd like**

Copy dependency library source code into the boinc source tree, such as git submodules or ordinary files. Then configure the build system to reference these local files. So the build behaves more like ""Just Works"" out of the box.

Once this is done, then contributors will be able to onboard onto boinc development much faster!",I already changed that for Windows in VS2019 solution. Once it will be tested and stabilized - it will be the main and only solution to build BOINC for Windows,30260406
567,Format build steps as plain text document,open,2020-03-26T18:25:27Z,2020-03-27T00:42:46Z,,NONE,"Fixes #3527

**Description of the Change**

Move build steps from PDF and RTF stylized documents into more maintainable plain text (e.g. Markdown) document.

**Release Notes**

Format development documentation as plain text (Markdown)","I disagree. I believe you are referring to the Mac build instructions. RTD files are very easy to edit with the TextEdit app on Macs, and trivial to then save a copy as a PDF.

The reason I added the PDF version of mac_build/HowToBuildBOINC_XCode.pdf is so that updates to the build instructions will automatically be available via a link from [here](https://boinc.berkeley.edu/trac/wiki/MacBuild). Changing the format will break this.",30260406
568,Move Mac build instructions to a plain text document,open,2020-03-26T17:57:48Z,2020-03-27T00:42:16Z,,NONE,"**Describe the problem**

PDF's and RTF's are somewhat cumbersome to work with compared to plain text documents, such as Markdown.

**Describe the solution you'd like**

Move the build steps to a plain text document.","I disagree. I believe you are referring to the Mac build instructions. RTD files are very easy to edit with the TextEdit app on Macs, and trivial to then save a copy as a PDF.

The reason I added the PDF version of mac_build/HowToBuildBOINC_XCode.pdf is so that updates to the build instructions will automatically be available via a link from [here](https://boinc.berkeley.edu/trac/wiki/MacBuild). Changing the format will break this.",30260406
569,Add instrumented tests for the Android app.,open,2020-03-25T10:57:38Z,2020-03-27T03:11:21Z,,CONTRIBUTOR,"Fixes #

**Description of the Change**
Add instrumented tests for the classes that implement the Parcelable interface. These tests are run on Android devices or emulators and can thus be used to identify any issues that will negatively impact real-world usage of the app.

**Release Notes**
N/A
","@mcandre, yes, we're aware of it, it's in progress",30260406
570,Create notification channel for use on Android Oreo and later.,open,2020-03-24T06:27:31Z,2020-03-25T15:36:48Z,,CONTRIBUTOR,"Fixes #

**Description of the Change**
Add code to create a notification channel with the ID ""main-channel"", which is specified as the channel ID for BOINC notifications. This is required on Android Oreo and later; otherwise, notifications will not be displayed.

**Release Notes**
Fix display of notifications on Android Oreo and later.
",Thanks. I'll test it today later and merge if everything will be ok,30260406
571,Only GPU tasks,open,2020-03-23T07:39:34Z,2020-03-23T16:48:10Z,,NONE,"**Describe the problem**
I have multiple GPU's in my pc that are idle unfortunately. Therefore I would like that BOINC uses them for computation, however I don't want that BOINC uses the CPU since I will be using that. So I would like to have BOINC only be able to compute GPU tasks.

**Describe the solution you'd like**
A feature that allows the user to be able to enable ""**only** compute GPU"" related tasks. 
","And one asked plenty of times before. If only people did a search of open
issues before asking. See #41


On Mon, 23 Mar 2020, 17:07 Adam Radocz <notifications@github.com> wrote:

> I assumed he wants to suspend CPU tasks temporarily, while he's working.
> So when he goes away from the PC, it can crunch CPU tasks too.
> @jensdenbraber <https://github.com/jensdenbraber> correct me if I'm wrong.
>
> Indeed, BOINC is not allowed to use the CPU for x time between x-y times.
> In the BOINC UI there is the option to suspend the GPU. Able to suspend the
> CPU would be nice to have.
>
> @Ageless93 <https://github.com/Ageless93>
> In this case, setting back and fort the project settings is tedious so
> this is a valid issue.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/3514#issuecomment-602697633>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACS5WU6IJXT7CXH6PJKGTTTRI6CKJANCNFSM4LRVBEGA>
> .
>
",30260406
572,Code improvements results from Cppcheck,open,2020-03-22T16:59:59Z,2020-03-22T16:59:59Z,,NONE,"I ran [cppcheck](https://github.com/danmar/cppcheck) on the Boinc codebase which resulted in the some code improvements. 

Download the results here: [boinc_cppcheck.zip](https://github.com/BOINC/boinc/files/4365631/boinc_cppcheck.zip)


",,30260406
573,[Android] Building on Windows (WSL),open,2020-03-16T12:14:06Z,2020-03-16T12:23:05Z,,CONTRIBUTOR,"**Describe the problem**

With #3503 merged, I can confirm the Android client can be compiled under Windows using WSL. APK can also be built and installed using Windows version of Android Studio. It will be nice to have documentation in using this approach. This should open up more people to try develop for the Android version of BOINC without having to go through fixing VM or Oracle licenses issues.

**Describe the solution you'd like**

I am currently using Ubuntu 18.04 as WSL distro to build using the command:

    sudo apt-get update && \
    sudo apt-get install ubuntu-make git automake libtool pkg-config unzip python -y && \
    git clone https://github.com/boinc/boinc.git ~/boinc && \
    cd ~/boinc/android/ && \
    ./build_all.sh

After building, copy the whole BOINC repo to a path Android Studio can read:

    cp -frv ~/boinc /mnt/c/<AnyReadablePath>/boinc

Then run Android Studio and open project file in `C:\<AnyReadablePath>\boinc\android\BOINC` to proceed development for Android version of BOINC or build the APK.

**Additional context**

1. While Vagrant is nice, I think BOINC should leverage the lighter WSL for building and not just running BOINC apps.
2. Don't bother using msys2 / cygwin + NDK (Windows) as it will fail at OpenSSL compilation due to building for the wrong architecture / format using the same set of scripts.
3. While using this approach, Windows Defender Real Time Protection needs to be temporarily disabled to speed up compilation times and fix some weird issues (fork error). Adding process or path to Exclusion whitelist is good too.
4. WSL acts exactly like a UWP app and can be easily Reset so you don't need to uninstall, download, reinstall again to fix problems.
5. apt-get may pause at the Restart Service stage, selecting yes or no, and it will proceed as usual.",,30260406
574,be explicit how sched_scheduler was meant to be set,open,2020-03-15T20:07:13Z,2020-03-15T20:35:43Z,,CONTRIBUTOR,I repeatedly get an error message that I presume to have this origin - just don't let me guess. ,"# [Codecov](https://codecov.io/gh/BOINC/boinc/pull/3505?src=pr&el=h1) Report
> Merging [#3505](https://codecov.io/gh/BOINC/boinc/pull/3505?src=pr&el=desc) into [master](https://codecov.io/gh/BOINC/boinc/commit/3fe80d6d9550f93b7ede3ba9bf0c315ea3be7a57&el=desc) will **not change** coverage by `%`.
> The diff coverage is `n/a`.

```diff
@@          Coverage Diff           @@
##           master   #3505   +/-   ##
======================================
  Coverage    9.27%   9.27%           
======================================
  Files          36      36           
  Lines        5994    5994           
======================================
  Hits          556     556           
  Misses       5438    5438           
```

",30260406
575,Make blows up with missing curl/curl.h,open,2020-03-13T12:35:46Z,2020-03-13T12:36:26Z,,CONTRIBUTOR,"**Describe the bug**
While compiling server with `./configure --disable-client --disable-manager --enable-fcgi && make` on a system that does not have libcurl installed, configure does not catch the missing dependency and the build blows up.

```
  CXX      remote_submit.o
../lib/remote_submit.cpp:24:10: fatal error: curl/curl.h: No such file or directory
 #include <curl/curl.h>
          ^~~~~~~~~~~~~
compilation terminated.
make[2]: *** [Makefile:820: remote_submit.o] Error 1
```

**Steps To Reproduce**
```
git clone https://github.com/BOINC/boinc.git boinc-git
cd boinc-git
git checkout tags/server_release/1.2/1.2.1 -b server_release/1.2.1
./_autosetup
./configure --disable-client --disable-manager --enable-fcgi
make
```

**Expected behavior**
Configure should notify about the missing lib and abort.",,30260406
576,Android 11: using Scoped Storage,open,2020-03-10T20:33:36Z,2020-03-11T07:37:35Z,,CONTRIBUTOR,"I'm not sure if this has any consequences for BOINC on Android, but Android 11 will use Scoped Storage (see https://www.androidcentral.com/what-scoped-storage) and _Any app that is targeted for Android 10 or later must use the new storage APIs, and that includes Scoped Storage. _
","According to the article, this applies to external (visible to user and other applications) folders and files.
BOINC doesn't use this and stores all files inside it's sandboxed folder.
So currently as far as I see, this change will not affect us.",30260406
577,Update URLs of projects to use HTTPS,open,2020-03-10T19:39:39Z,2020-03-26T16:36:07Z,,NONE,"I've started using Science United in my BOINC client recently and noticed that some projects still use HTTP, although they seem to support HTTPS.
I checked URLs of that projects and updated them to use HTTPS where it is possible.

There are a couple of projects which does not support HTTPS:
* Enigma@Home;
* Gerasim@Home;
* SRBase;
* CAS@home;
* Radioactive@Home.

And there were a couple of URLs that were moved permanently (HTTP 301), I did not update them though.
I remember these two ones:
* https://einstein.phys.uwm.edu/ -> https://einsteinathome.org/uk/home;
* https://denis.usj.es/denisathome/ -> https://denis.usj.es/en_US/.

Fixes #

**Description of the Change**
<!-- We must be able to understand the design of your change from this description. -->

**Alternate Designs**
<!-- Explain what other alternates were considered and why the proposed version was selected -->

**Release Notes**
<!--
Please describe the changes in a single line that explains this improvement in terms that a user can understand.
This text will be used in BOINC's release notes.
If this change is not user-facing or notable enough to be included in release notes you may use the string ""N/A"" here. -->
","@mcandre, take a look at my comment above",30260406
578,[Android] 7.16.5: BOINC instantly crashing on startup,open,2020-03-10T00:43:22Z,2020-03-15T07:57:15Z,,NONE,"**Describe the bug**
Just updated to the new version of BOINC on the Play Store. As soon as I open it, it crashes instantly. If I attempt to ignore the prompt and go to the Event Log, it will be empty. After a few seconds, the app will close itself.

**Steps To Reproduce**
1. Download the new BOINC update from the Play Store.
2. Open it, and wait about 5 seconds.

**Screenshots**
![BOINC](https://user-images.githubusercontent.com/5206120/76268972-109c1d00-62c4-11ea-812c-e18ae7ae0ef1.png)

**System Information**
 - OS: Android 9
 - BOINC Version: 7.16.5

**Additional context**
Restarted my phone, didn't fix the issue. Moto G6 Play running on Android 9 Pie. Didn't have this issue with the previous version of BOINC.",Same here for Android 9 with LG V30. I guess it is because it is because of beta option for BOINC. The 7.16.3 from https://boinc.berkeley.edu/dl/?C=M;O=D runs apparently ok sideloaded,30260406
579,./unzip/unzpriv.h:1009:45: error: format not a string literal and no format arguments [-Werror=format-security],open,2020-03-07T18:42:57Z,2020-03-23T22:16:37Z,,CONTRIBUTOR,"**Describe the bug**
```
OpenWrt-libtool: compile:  ccache_cc -DHAVE_CONFIG_H -I. -I.. -I.. -I../api -I../db -I../lib -I../lib/mac -I../sched -I../tools -I../vda -I/home/moeller/openwrt/sdk/staging_dir/target-arm_cortex-a9+vfpv3_musl_eabi/usr/include -I/home/moeller/openwrt/sdk/staging_dir/toolchain-arm_cortex-a9+vfpv3_gcc-8.3.0_musl_eabi/usr/include -I/home/moeller/openwrt/sdk/staging_dir/toolchain-arm_cortex-a9+vfpv3_gcc-8.3.0_musl_eabi/include/fortify -I/home/moeller/openwrt/sdk/staging_dir/toolchain-arm_cortex-a9+vfpv3_gcc-8.3.0_musl_eabi/include -Wall -Wextra -Wshadow -Wredundant-decls -Wdisabled-optimization -Wpointer-arith -Wstrict-aliasing -Wcast-align -I.. -I../zip/zip -I../zip/unzip -I../lib -DUNIX -DDLL -DUSE_ZIPMAIN -DNO_OFF_T -DNO_CRYPT -DNO_LCHOWN -DNO_LCHMOD -DIZ_PWLEN=80 -Dinflate=inflate_boinc -Ddeflate=deflate_boinc -Dget_crc_table=get_crc_table_boinc -Dlongest_match=longest_match_boinc -Dinflate_codes=inflate_codes_boinc -Dcrc32=crc32_boinc -Os -pipe -fno-caller-saves -fno-plt -fhonour-copts -Wno-error=unused-but-set-variable -Wno-error=unused-result -mfloat-abi=hard -ffile-prefix-map=/home/moeller/openwrt/sdk/build_dir/target-arm_cortex-a9+vfpv3_musl_eabi/boinc-7.16.5=boinc-7.16.5 -Wformat -Werror=format-security -fstack-protector -D_FORTIFY_SOURCE=1 -Wl,-z,now -Wl,-z,relro -Wall -MT extract.lo -MD -MP -MF .deps/extract.Tpo -c ./unzip/extract.c  -fPIC -DPIC -o .libs/extract.o
In file included from ./unzip/unzip.h:718,
                 from ./unzip/extract.c:37:
./unzip/unzpriv.h:228:25: warning: redundant redeclaration of 'UzpMain' [-Wredundant-decls]
 #  define MAIN   UZ_EXP UzpMain   /* was UzpUnzip */
                         ^~~~~~~
./unzip/unzpriv.h:2240:11: note: in expansion of macro 'MAIN'
    int    MAIN                   OF((int argc, char **argv));
           ^~~~
In file included from ./unzip/extract.c:37:
./unzip/unzip.h:678:17: note: previous declaration of 'UzpMain' was here
 int      UZ_EXP UzpMain            OF((int argc, char **argv));
                 ^~~~~~~
In file included from ./unzip/unzip.h:718,
                 from ./unzip/extract.c:37:
./unzip/unzpriv.h:2573:11: warning: redundant redeclaration of 'globalsCtor' [-Wredundant-decls]
 Uz_Globs *globalsCtor    OF((void));                            /* globals.c */
           ^~~~~~~~~~~
In file included from ./unzip/unzpriv.h:2227,
                 from ./unzip/unzip.h:718,
                 from ./unzip/extract.c:37:
./unzip/globals.h:396:11: note: previous declaration of 'globalsCtor' was here
 Uz_Globs *globalsCtor   OF((void));
           ^~~~~~~~~~~
In file included from ./unzip/unzip.h:718,
                 from ./unzip/extract.c:37:
./unzip/extract.c: In function 'extract_or_test_files':
./unzip/unzpriv.h:1009:45: error: format not a string literal and no format arguments [-Werror=format-security]
 #    define LoadFarString(x)        (char *)(x)
                                             ^
./unzip/unzpriv.h:2708:61: note: in definition of macro 'Info'
        (*G.message)((zvoid *)&G, (uch *)(buf), (ulg)sprintf sprf_arg, (flag))
                                                             ^~~~~~~~
./unzip/extract.c:476:23: note: in expansion of macro 'LoadFarString'
                       LoadFarString(ReportMsg)));
                       ^~~~~~~~~~~~~
./unzip/unzpriv.h:1009:45: error: format not a string literal and no format arguments [-Werror=format-security]
 #    define LoadFarString(x)        (char *)(x)
                                             ^
./unzip/unzpriv.h:2708:61: note: in definition of macro 'Info'
        (*G.message)((zvoid *)&G, (uch *)(buf), (ulg)sprintf sprf_arg, (flag))
                                                             ^~~~~~~~
./unzip/extract.c:755:44: note: in expansion of macro 'LoadFarString'
         Info(slide, 0x401, ((char *)slide, LoadFarString(EndSigMsg)));
                                            ^~~~~~~~~~~~~
./unzip/unzpriv.h:1009:45: error: format not a string literal and no format arguments [-Werror=format-security]
 #    define LoadFarString(x)        (char *)(x)
                                             ^
./unzip/unzpriv.h:2708:61: note: in definition of macro 'Info'
        (*G.message)((zvoid *)&G, (uch *)(buf), (ulg)sprintf sprf_arg, (flag))
                                                             ^~~~~~~~
./unzip/extract.c:756:44: note: in expansion of macro 'LoadFarString'
         Info(slide, 0x401, ((char *)slide, LoadFarString(ReportMsg)));
                                            ^~~~~~~~~~~~~
./unzip/extract.c: In function 'extract_or_test_entrylist':
./unzip/extract.c:1026:10: warning: unused parameter 'pnum_bad_pwd' [-Wunused-parameter]
     ulg *pnum_bad_pwd;
          ^~~~~~~~~~~~
./unzip/extract.c:1521:42: warning: this statement may fall through [-Wimplicit-fallthrough=]
                         G.overwrite_mode = OVERWRT_NEVER;
./unzip/extract.c:1523:21: note: here
                     case 'n':
                     ^~~~
cc1: some warnings being treated as errors
make[7]: *** [Makefile:776: extract.lo] Error 1
make[7]: Leaving directory '/home/moeller/openwrt/sdk/build_dir/target-arm_cortex-a9+vfpv3_musl_eabi/boinc-7.16.5/zip'
```

**Steps To Reproduce**

cross-compiling with the OpenWRT SDK.

**System Information**
 - OS: Debian sid, cross-compiling with gcc 8.3 provided by OpenWRT
 - BOINC Version: 7.16.5
","I suggest to ask at one of the upcoming contributors conferences to ask a patch for a respective option in configure. On Linux, the redistribution of unzip is somewhat redundant, but with WIndows or MacOS this may have its merits.",30260406
580,Change to signup.php breaks validation of terms of use,open,2020-03-06T13:38:06Z,2020-03-06T13:38:06Z,,NONE,"The change from 
A clear and concise description of what the bug is.

**Steps To Reproduce**
1. Click Join on a project web page to join a project.
2. This brings you to ""signup.php"" that breaks the terms of use validation introduced earlier that was implemented with create_account_form.php.

Broken since this commit: https://github.com/BOINC/boinc/commit/4a3eb03b2689b52e2b7ba2b7e38d1919b1278ae6

**Expected behavior**
The expected behaviour is for the user to see a form that includes the terms of use for the project (e.g. with create_account_form.php )


**System Information**
 - OS: Server: CentOS 7.7 Client: Any browser
 - BOINC server release 1.2.0

**Additional context**
Affects only GDPR compliant sites with Terms of Use flags enabled.

Can be fixed by reverting to the previous version of bootstrap.inc that points to create_account_form.php
",,30260406
581,Remove remnants to subversion,open,2020-02-28T17:37:13Z,2020-03-16T12:03:03Z,,CONTRIBUTOR,"I had a problem running that code on ""ash"" on OpenWRT. No idea what could possibly be wrong about it, but since there is no need for that piece of code, it should just go IMHO.","# [Codecov](https://codecov.io/gh/BOINC/boinc/pull/3481?src=pr&el=h1) Report
> Merging [#3481](https://codecov.io/gh/BOINC/boinc/pull/3481?src=pr&el=desc) into [master](https://codecov.io/gh/BOINC/boinc/commit/017b90eefa43b41c5065ac0578ed62734245fa60&el=desc) will **not change** coverage by `%`.
> The diff coverage is `n/a`.

```diff
@@          Coverage Diff           @@
##           master   #3481   +/-   ##
======================================
  Coverage    9.27%   9.27%           
======================================
  Files          36      36           
  Lines        5994    5994           
======================================
  Hits          556     556           
  Misses       5438    5438           
```

",30260406
582,Real-time computing,open,2020-02-27T21:55:58Z,2020-02-28T04:49:28Z,,CONTRIBUTOR,"Some proposed apps require real-time capabilities in BOINC:

- distributed rendering of virtual worlds
- processing of streamed radio-telescope data
- distributed machine-learning systems like Ray
- distributed MPI-type parallel programming

This can't be done in the current queued-job model, which has inherent high latency.  We need a new ""real-time"" computing model, which sits alongside the queued-job model.

A real-time job, like a non-CPU-intensive job, is in memory all the time, but not necessarily computing.  The client and the app negotiate when the app computes, using message-passing.  The app initiates this by sending a ""I want to compute now"" or ""I want to compute sometime"" message.  The client tells it when it can compute, and when it has to stop.

The client runs real-time jobs except when a) it's ruled out by preferences, b) the project's scheduling priority becomes too low (i.e. computing is owed to other projects), or c) there are real-time jobs from other projects (in which case scheduling priority determines who runs).

Real-time apps have a specified resource usage (CPUs, GPUs) like other apps.  The set of real-time jobs for a given project must fit within the host's resources.

When a real-time app runs, it inputs and outputs are sent over network connections, either to a central coordinator or in a peer-to-peer scheme.",,30260406
583,AccountManagement: Password restrictions should fit to the account manager,open,2020-02-27T15:32:32Z,2020-02-27T21:05:36Z,,NONE,"The BAM account manager allow very strong passwords with special characters and length 128 characters (maybe more).

- When try to connect via BOINC with such a 128 chars special characters password there is no error message. But BOINC says it is now connected to this accoutn manager. But when opening the ""tools"" menu again BAM does not appear in there.

I set the password to a low one 9 chars with one special and two digits. This works with BOINC and BAM appear in the tool menu so I can sync BOINC with BAM.","This should be fixed in 7.16.4. At least issue with error ignore. I don't know about password length, should be verified separately",30260406
584,[mac] build failure: buildcurl.sh not always using OpenSSL provided by setupForBOINC.sh,open,2020-02-27T09:41:50Z,2020-02-27T11:09:08Z,,CONTRIBUTOR,"**Describe the bug**
`curl` fails to build using `setupForBOINC.sh`:
```
Undefined symbols for architecture x86_64:
  ""_SSL_CTX_set_keylog_callback"", referenced from:
      _ossl_connect_common in libcurl.a(libcurl_la-openssl.o)
```

**Steps To Reproduce**
1. Run `setupForBOINC.sh` on a system with a MacPorts or Homebrew OpenSSL instance

**Expected behavior**
`setupForBOINC.sh` should make sure its own paths take precedence over any system-specific build paths, e.g. set by MacPorts or Homebrew. IOW, ensure your own copy of OpenSSL gets used, not any system-specific version.

**System Information**
 - OS: macOS 10.15 with MacPorts (or Homebrew) and OpenSSL port installed
 - BOINC Version: master

**Additional context**
The MacPorts/Homebrew involvement is just an educated guess, yet pretty much hinted at by this:
```
checking for x86_64-pkg-config... /opt/local/bin/pkg-config
checking for openssl options with pkg-config... found
configure: pkg-config: SSL_LIBS: ""-lssl -lcrypto""
configure: pkg-config: SSL_LDFLAGS: ""-L/opt/local/lib""
configure: pkg-config: SSL_CPPFLAGS: ""-I/opt/local/include""
[...]
configure: Added /opt/local/lib to LD_LIBRARY_PATH
checking for OpenSSL headers version... unknown - 0x1010104fL
checking for OpenSSL library version... 1.1.0
checking for OpenSSL headers and library versions matching... no
configure: WARNING: OpenSSL headers and library versions do not match.
```
I have 1.1.1d installed while `setupForBOINC.sh` uses 1.1.0g. It seems as if `buildcurl.sh` uses the headers installed by MacPorts but its own library, hence the mismatch and probably the subsequent build error.","Ok, so I stripped back my environment and `buildcurl.sh` works as expected. Fun fact, the library/header comparison now fails entirely:
```
checking for OpenSSL headers version... unknown
checking for OpenSSL library version... 1.1.0
checking for OpenSSL headers and library versions matching... fail
configure: WARNING: Can not compare OpenSSL headers and library versions.
```

Anyhow, this proves that my assumption was correct. This is a matter of build environment isolation which needs improving. I'm changing the tag of this issue accordingly.",30260406
585,[mac] buildcurl.sh requires bash without saying so / broken on macOS 10.15,open,2020-02-27T09:11:42Z,2020-03-16T12:11:01Z,,CONTRIBUTOR,"**Describe the bug**
`buildcurl.sh` [requires](https://github.com/BOINC/boinc/blob/master/mac_build/buildcurl.sh#L150) `bash` it seems. That's a requirement which is neither explicitly stated AFAIK and which is also not a given on macOS 10.15 anymore as it (finally) uses `zsh` as default shell.

**Steps To Reproduce**
1. Run `buildcurl.sh` in `zsh`

**Expected behavior**
A working script (e.g. test for `bash` or don't use shell-specific code or run `bash` subshell)

**System Information**
 - OS: macOS 10.15 (or just zsh)
 - BOINC Version: master","> I also encountered a build error for wxWidgets 3.1.0

Just for completeness for others reading this: that's dealt with in #3476",30260406
586,Regression: Makefile broken for libboinc_graphics2,open,2020-02-25T12:55:00Z,2020-02-26T08:38:37Z,,CONTRIBUTOR,"**Describe the bug**
Regression in #2149: Makefile broken for libboinc_graphics2

> If you must build libboinc_graphics2.a using a make file, you must modify the make file to first compile api/MultiGPUMig.defs, which will create the files MultiGPUMig.h, MultiGPUMigServer.c, MultiGPUMigServer.h and MultiGPUMigUser.c.

**Steps To Reproduce**
1. Configure BOINC with `--enable-libraries` (no server, client, manager) and use `make` to build it. 

**Expected behavior**
A successful build

**System Information**
 - OS: macOS
 - BOINC Version: master

I consider this a known regression and I don't understand why the PR got merged in the first place. If the `Makefile` is known to be broken and solution was known as well, then why wasn't the `Makefile` updated as part of the PR?

This issue also shows where integration testing is still lacking coverage.","Sorry Eric, I don't follow. This issue is about building the BOINC libs in order to build science/screensaver apps that depend on them. They simply can't be built right now using the configure/make infrastructure because that (still) lacks the necessary steps to build/produce said derived files. In this context no files would be installed in any ""standard directory"" but in the (temporary) prefix provided. There won't be any collisions (this isn't about the notorious `config.h`). I also don't see why this would need a change to the Xcode project. An extension (as suggested by Charlie in  #2149) to the `Makefile` should be sufficient.

It's a known issue that, according to #2149, was meant to be resolved in a second PR - but that never came.
",30260406
587,"web: if fail to connect to DB, say so rather than ""project stopped"".",open,2020-02-24T06:42:08Z,2020-02-24T06:42:09Z,,MEMBER,,,30260406
588,Use EnvironmentFile /etc/default/boinc-client for systemd startup,open,2020-02-19T06:13:06Z,2020-02-19T07:23:50Z,,NONE,"Fixes #3393 

**Description of the Change**
Allow customizing $BOINC_OPTS in systemd unit files
<!-- We must be able to understand the design of your change from this description. -->

**Alternate Designs**
N/A - Trivial change
<!-- Explain what other alternates were considered and why the proposed version was selected -->

**Release Notes**
Allow systemd startup to be customized via /etc/systemd/system unit file
<!--
Please describe the changes in a single line that explains this improvement in terms that a user can understand.
This text will be used in BOINC's release notes.
If this change is not user-facing or notable enough to be included in release notes you may use the string ""N/A"" here. -->
","This will probably go better to deal with different .conf file paths, without specifying EnvironmentFile.

The usual way is to instruct users to create a file in /etc/systemd/system/boinc-service.d/
```
# cat /etc/systemd/system/boinc-client.service.d/00gentoo.conf 
[Service]
Environment=""BOINC_OPTS=--allow_remote_gui_rpc""
```",30260406
589,boinccmd --get_tasks is missing elapsed time,open,2020-02-13T09:54:41Z,2020-02-20T15:35:37Z,,NONE,"**Describe the bug**
boinccmd --get_tasks and --get_simple_gui_info is missing information about overall elapsed time of task
**Steps To Reproduce**
1. install boinc
2. subscribe to project and wait for tasks to start
3. run either boinccmd --get_tasks or boinccmd --get_simple_gui_info

**Expected behavior**
Something like 
`
1) -----------
   name: 028LDmTjuLwnsSi4apGgGQJmABFKDmABFKDmDyTMDmABFKDm4OMeEm_0
   WU name: 028LDmTjuLwnsSi4apGgGQJmABFKDmABFKDmDyTMDmABFKDm4OMeEm
[... other info ...]
   estimated CPU time remaining: 4353.514025
   CPU time at last checkpoint: 4848.330000
   current CPU time: 5367.610000
   elapsed task time: 1501.687000
   fraction done: 0.190859
   swap size: 9429 MB
   working set size: 10200 MB
`

instead of 

`
1) -----------
   name: 028LDmTjuLwnsSi4apGgGQJmABFKDmABFKDmDyTMDmABFKDm4OMeEm_0
   WU name: 028LDmTjuLwnsSi4apGgGQJmABFKDmABFKDmDyTMDmABFKDm4OMeEm
[... other info ...]
   estimated CPU time remaining: 4353.514025
   CPU time at last checkpoint: 4848.330000
   current CPU time: 5367.610000
   fraction done: 0.190859
   swap size: 9429 MB
   working set size: 10200 MB
`

**System Information**
 - OS: Manjaro or Centos7 
 - BOINC Version: 7.16.1

",,30260406
590,[WIP] add server version to version.h,open,2020-02-04T22:10:28Z,2020-03-26T16:36:59Z,,CONTRIBUTOR,"... and have scheduler report server version, not client version","This PR is not ready to be merged because the whole idea should be revisited, please take a look at the conversation above",30260406
591,Android 7.16.3 crash,open,2020-01-31T23:21:06Z,2020-03-18T20:40:11Z,,CONTRIBUTOR,"A user reports it crashes on startup, always.
Here's the contents of logcat.txt:

```
01-31 18:57:52.863 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=832, size=4059136, usage=0x900
01-31 18:57:52.863 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:57:52.866 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=832, size=4059136, usage=0x900
01-31 18:57:52.866 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:57:52.869 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=832, size=4059136, usage=0x900
01-31 18:57:52.869 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:57:52.871 D/mali_winsys(6095): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:57:52.905 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=832, size=4059136, usage=0xb00
01-31 18:57:52.905 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:57:52.927 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=832, size=4059136, usage=0xb00
01-31 18:57:52.928 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:57:52.953 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=832, size=4059136, usage=0xb00
01-31 18:57:52.954 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:57:53.482 V/ResourceType(6095): For resource 0x7f120173, entry index(371) is beyond type entryCount(157)
01-31 18:57:53.483 V/chatty  (6095): uid=10073(com.pluscubed.matlog) identical 1 line
01-31 18:57:53.483 V/ResourceType(6095): For resource 0x7f120173, entry index(371) is beyond type entryCount(157)
01-31 18:57:53.484 V/ResourceType(6095): For resource 0x7f12012b, entry index(299) is beyond type entryCount(157)
01-31 18:57:53.484 V/chatty  (6095): uid=10073(com.pluscubed.matlog) identical 3 lines
01-31 18:57:53.484 V/ResourceType(6095): For resource 0x7f12012b, entry index(299) is beyond type entryCount(157)
01-31 18:57:53.486 V/ResourceType(6095): For resource 0x7f120111, entry index(273) is beyond type entryCount(157)
01-31 18:57:53.487 V/chatty  (6095): uid=10073(com.pluscubed.matlog) identical 3 lines
01-31 18:57:53.487 V/ResourceType(6095): For resource 0x7f120111, entry index(273) is beyond type entryCount(157)
01-31 18:57:53.488 V/ResourceType(6095): For resource 0x7f120173, entry index(371) is beyond type entryCount(157)
01-31 18:57:53.488 V/chatty  (6095): uid=10073(com.pluscubed.matlog) identical 1 line
01-31 18:57:53.488 V/ResourceType(6095): For resource 0x7f120173, entry index(371) is beyond type entryCount(157)
01-31 18:57:53.490 V/ResourceType(6095): For resource 0x7f110107, entry index(263) is beyond type entryCount(237)
01-31 18:57:53.839 E/BufferQueueProducer(312): [PopupWindow:e58db5a#0] disconnect: not connected (req=1)
01-31 18:57:53.839 W/libEGL  (6095): EGLNativeWindowType 0x7ee3f06010 disconnect failed
01-31 18:57:53.840 D/OpenGLRenderer(6095): endAllActiveAnimators on 0x7ee3ec0000 (MenuPopupWindow$MenuDropDownListView) with handle 0x7ee2396f00
01-31 18:57:55.871 I/ActivityManager(725): START u0 {act=android.intent.action.MAIN cat=[android.intent.category.HOME] flg=0x10200000 cmp=com.android.launcher3/.Launcher (has extras)} from uid 1000
01-31 18:57:55.922 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:57:55.922 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:57:55.926 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:57:55.926 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:57:55.929 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:57:55.929 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:57:55.930 D/mali_winsys(1300): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:57:55.944 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:57:55.944 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:57:55.973 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=1, stride=1088, size=8355840, usage=0x333
01-31 18:57:55.973 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:57:56.077 E/BufferQueueProducer(312): [com.pluscubed.matlog/com.pluscubed.logcat.ui.LogcatActivity#0] disconnect: not connected (req=1)
01-31 18:57:56.077 W/libEGL  (6095): EGLNativeWindowType 0x7ee033d010 disconnect failed
01-31 18:57:56.114 V/ResourceType(839): ResTable_typeSpec entry count inconsistent: given 237, previously 267
01-31 18:57:56.119 V/ResourceType(839): ResTable_typeSpec entry count inconsistent: given 172, previously 176
01-31 18:57:56.119 V/ResourceType(839): ResTable_typeSpec entry count inconsistent: given 157, previously 517
01-31 18:57:56.446 W/OpenGLRenderer(1300): Incorrectly called buildLayer on View: ShortcutAndWidgetContainer, destroying layer...
01-31 18:57:56.446 W/OpenGLRenderer(1300): Incorrectly called buildLayer on View: ShortcutAndWidgetContainer, destroying layer...
01-31 18:57:56.454 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:57:56.454 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:57:56.506 I/zygote64(725): NativeAlloc concurrent copying GC freed 15091(1163KB) AllocSpace objects, 13(452KB) LOS objects, 37% free, 10MB/16MB, paused 129us total 125.847ms
01-31 18:57:56.922 I/ActivityManager(725): START u0 {act=android.intent.action.MAIN cat=[android.intent.category.LAUNCHER] flg=0x10200000 cmp=edu.berkeley.boinc/.SplashActivity bnds=[540,96][789,424]} from uid 10014
01-31 18:57:56.927 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:57:56.927 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:57:56.989 I/ActivityManager(725): Start proc 8716:edu.berkeley.boinc/u0a77 for activity edu.berkeley.boinc/.SplashActivity
01-31 18:57:56.991 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=1, stride=1088, size=8355840, usage=0x933
01-31 18:57:56.991 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:57:57.058 E/BufferQueueProducer(312): [com.android.launcher3/com.android.launcher3.Launcher#0] disconnect: not connected (req=1)
01-31 18:57:57.059 W/libEGL  (1300): EGLNativeWindowType 0x7efc595010 disconnect failed
01-31 18:57:57.118 I/zygote64(725): NativeAlloc concurrent copying GC freed 824(172KB) AllocSpace objects, 0(0B) LOS objects, 36% free, 10MB/16MB, paused 505us total 179.969ms
01-31 18:57:57.341 I/ActivityManager(725): Start proc 8731:edu.berkeley.boinc:remote/u0a77 for service edu.berkeley.boinc/.client.Monitor
01-31 18:57:57.347 D/BOINC_GUI(8716): SplashActivity onResume()
01-31 18:57:57.361 D/OpenGLRenderer(8716): HWUI GL Pipeline
01-31 18:57:57.411 D/BOINC_GUI(8731): Monitor onCreate()
01-31 18:57:57.425 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:57:57.426 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:57:57.429 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:57:57.429 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:57:57.432 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:57:57.432 D/vndksupport(8716): Loading /vendor/lib64/hw/gralloc.hi6250.so from current namespace instead of sphal namespace.
01-31 18:57:57.432 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:57:57.434 D/BOINC_GUI(8731): appPrefs read successful.truetruetrue5truetruetrue
01-31 18:57:57.436 D/BOINC_GUI(8731): Monitor onCreate(): singletons initialized
01-31 18:57:57.443 D/BOINC_GUI(8731): Monitor onBind
01-31 18:57:57.445 D/BOINC_GUI(8731): Monitor onStartCommand()
01-31 18:57:57.446 D/BOINC_GUI(8731): Monitor.onStartCommand() with action code: -1
01-31 18:57:57.447 D/BOINC_GUI(8731): Monitor.clientSetup()
01-31 18:57:57.449 D/BOINC_GUI(8731): BOINC platform: aarch64-android-linux-gnu for os.arch: aarch64
01-31 18:57:57.460 W//system/bin/hwservicemanager(357): getTransport: Cannot find entry android.hardware.configstore@1.0::ISurfaceFlingerConfigs/default in either framework or device manifest.
01-31 18:57:57.462 I/edu.berkeley.boinc(8716): android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0
01-31 18:57:57.463 I/OpenGLRenderer(8716): Initialized EGL, version 1.4
01-31 18:57:57.463 D/OpenGLRenderer(8716): Swap behavior 2
01-31 18:57:57.471 D/mali_winsys(8716): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:57:57.519 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:57:57.520 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:57:57.523 D/vndksupport(8716): Loading /vendor/lib64/hw/android.hardware.graphics.mapper@2.0-impl.so from current namespace instead of sphal namespace.
01-31 18:57:57.524 D/vndksupport(8716): Loading /vendor/lib64/hw/gralloc.hi6250.so from current namespace instead of sphal namespace.
01-31 18:57:57.576 I/ActivityManager(725): Displayed edu.berkeley.boinc/.SplashActivity: +601ms
01-31 18:57:57.655 D/BOINC_GUI(8731): getPidForProcessName(): PID at index: 1 for output: USER           PID  PPID     VSZ    RSS WCHAN            ADDR S NAME                       
01-31 18:57:57.657 D/BOINC_GUI(8731): getPidForProcessName(): /data/data/edu.berkeley.boinc/client/boinc not found in ps output!
01-31 18:57:57.657 D/BOINC_GUI(8731): Starting the BOINC client
01-31 18:57:57.658 W/BOINC_GUI(8731): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
01-31 18:57:57.660 D/BOINC_GUI(8731): Attempting BOINC client connection...
01-31 18:57:57.664 W/BOINC_GUI(8731): connect failure: IO
01-31 18:57:57.664 W/BOINC_GUI(8731): java.io.IOException: Connection refused
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at android.net.LocalSocketImpl.connectLocal(Native Method)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at android.net.LocalSocketImpl.connect(LocalSocketImpl.java:292)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at android.net.LocalSocket.connect(LocalSocket.java:145)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at edu.berkeley.boinc.rpc.RpcClient.open(RpcClient.java:181)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at edu.berkeley.boinc.client.Monitor.connectClient(Monitor.java:665)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:586)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:380)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at edu.berkeley.boinc.client.Monitor.access$200(Monitor.java:77)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:368)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at java.util.TimerThread.mainLoop(Timer.java:555)
01-31 18:57:57.664 W/BOINC_GUI(8731): 	at java.util.TimerThread.run(Timer.java:505)
01-31 18:57:57.664 E/BOINC_GUI(8731): connection failed!
01-31 18:57:57.677 I/BOINC   (8767): 31-Jan-2020 18:57:57 BOINC is initializing...
01-31 18:57:57.706 I/boinc   (8770): type=1400 audit(0.0:3332): avc: denied { search } for name=""base"" dev=""sysfs"" ino=73 scontext=u:r:untrusted_app:s0:c512,c768 tcontext=u:object_r:kernel_devicetree:s0 tclass=dir permissive=1
01-31 18:57:57.751 D/libOpenCLv1(8770): loaded /system/vendor/lib64/egl/libGLES_mali.so
01-31 18:57:58.019 I/BOINC   (8769): 31-Jan-2020 18:57:58 Initialization completed
01-31 18:57:58.666 D/BOINC_GUI(8731): Attempting BOINC client connection...
01-31 18:57:58.667 D/BOINC_GUI(8731): Connected successfully
01-31 18:57:58.668 D/BOINC_GUI(8731): authentication key acquired. length: 32
01-31 18:57:58.696 D/BOINC_GUI(8731): authorize() - Successful
01-31 18:57:59.716 D/BOINC_GUI(8731): reporting hostinfo model name: huawei HUAWEI P9 lite - SDK:27 ABI: arm64-v8a
01-31 18:57:59.716 D/BOINC_GUI(8731): reporting hostinfo os name: Android
01-31 18:57:59.716 D/BOINC_GUI(8731): reporting hostinfo os version: 8.1.0
01-31 18:57:59.719 D/BOINC_GUI(8731): Monitor.clientSetup() - setup completed successfully
01-31 18:57:59.719 D/BOINC_GUI(8731): reportDeviceStatus()
01-31 18:57:59.721 D/BOINC_GUI(8731): Unlmited internet connection - wifi or ethernet - found. type: 1
01-31 18:57:59.724 I/BOINC_GUI(8731): change: true - stationary device: false ; ac: true ; level: 78 ; temperature: 19 ; wifi: true ; user active: false
01-31 18:57:59.736 D/BOINC_GUI(8731): readClientStatus(): screen on, get complete status
01-31 18:57:59.863 D/BOINC_GUI(8731): ClientStatus.appendNewNotices new notice with seq number: 1 is server notice: false
01-31 18:57:59.863 D/BOINC_GUI(8731): ClientStatus.appendNewNotices new notice with seq number: 2 is server notice: false
01-31 18:57:59.864 D/BOINC_GUI(8731): ClientStatus.appendNewNotices new notice with seq number: 3 is server notice: false
01-31 18:57:59.864 V/BOINC_GUI(8731): setClientStatus: #results:0 #projects:3 #transfers:0 // computing: false11 - network: false14097
01-31 18:57:59.867 D/BOINC_GUI(8716): SplashActivity SETUP_STATUS_AVAILABLE
01-31 18:57:59.874 I/ActivityManager(725): START u0 {cmp=edu.berkeley.boinc/.BOINCActivity} from uid 10077
01-31 18:57:59.882 D/BOINC_GUI(8716): SplashActivity onPause()
01-31 18:57:59.899 D/BOINC_GUI(8731): readClientStatus(): computation enabled: false
01-31 18:57:59.902 D/BOINC_GUI(8731): reportDeviceStatus()
01-31 18:57:59.909 D/BOINC_GUI(8716): BOINCActivity onCreate()
01-31 18:57:59.919 D/BOINC_GUI(8731): readClientStatus(): screen on, get complete status
01-31 18:57:59.956 I/zygote64(8716): Rejecting re-init on previously-failed class java.lang.Class<android.support.v4.view.ViewCompat$OnUnhandledKeyEventListenerWrapper>: java.lang.NoClassDefFoundError: Failed resolution of: Landroid/view/View$OnUnhandledKeyEventListener;
01-31 18:57:59.956 I/zygote64(8716):   at void android.support.v4.view.ViewCompat.setBackground(android.view.View, android.graphics.drawable.Drawable) (ViewCompat.java:2341)
01-31 18:57:59.956 I/zygote64(8716):   at void android.support.v7.widget.ActionBarContainer.<init>(android.content.Context, android.util.AttributeSet) (ActionBarContainer.java:62)
01-31 18:57:59.956 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance0(java.lang.Object[]) (Constructor.java:-2)
01-31 18:57:59.956 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance(java.lang.Object[]) (Constructor.java:334)
01-31 18:57:59.956 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createView(java.lang.String, java.lang.String, android.util.AttributeSet) (LayoutInflater.java:647)
01-31 18:57:59.956 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:790)
01-31 18:57:59.956 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet) (LayoutInflater.java:730)
01-31 18:57:59.956 I/zygote64(8716):   at void android.view.LayoutInflater.rInflate(org.xmlpull.v1.XmlPullParser, android.view.View, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:863)
01-31 18:57:59.956 I/zygote64(8716):   at void android.view.LayoutInflater.rInflateChildren(org.xmlpull.v1.XmlPullParser, android.view.View, android.util.AttributeSet, boolean) (LayoutInflater.java:824)
01-31 18:57:59.956 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(org.xmlpull.v1.XmlPullParser, android.view.ViewGroup, boolean) (LayoutInflater.java:515)
01-31 18:57:59.956 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup, boolean) (LayoutInflater.java:423)
01-31 18:57:59.956 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup) (LayoutInflater.java:374)
01-31 18:57:59.956 I/zygote64(8716):   at android.view.ViewGroup android.support.v7.app.AppCompatDelegateImpl.createSubDecor() (AppCompatDelegateImpl.java:607)
01-31 18:57:59.956 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.ensureSubDecor() (AppCompatDelegateImpl.java:518)
01-31 18:57:59.956 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.setContentView(int) (AppCompatDelegateImpl.java:466)
01-31 18:57:59.956 I/zygote64(8716):   at void android.support.v7.app.AppCompatActivity.setContentView(int) (AppCompatActivity.java:140)
01-31 18:57:59.956 I/zygote64(8716):   at void edu.berkeley.boinc.BOINCActivity.onCreate(android.os.Bundle) (BOINCActivity.java:118)
01-31 18:57:59.956 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle, android.os.PersistableBundle) (Activity.java:7009)
01-31 18:57:59.956 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle) (Activity.java:7000)
01-31 18:57:59.956 I/zygote64(8716):   at void android.app.Instrumentation.callActivityOnCreate(android.app.Activity, android.os.Bundle) (Instrumentation.java:1214)
01-31 18:57:59.956 I/zygote64(8716):   at android.app.Activity android.app.ActivityThread.performLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent) (ActivityThread.java:2731)
01-31 18:57:59.956 I/zygote64(8716):   at void android.app.ActivityThread.handleLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:2856)
01-31 18:57:59.956 I/zygote64(8716):   at void android.app.ActivityThread.-wrap11(android.app.ActivityThread, android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:-1)
01-31 18:57:59.956 I/zygote64(8716):   at void android.app.ActivityThread$H.handleMessage(android.os.Message) (ActivityThread.java:1589)
01-31 18:57:59.956 I/zygote64(8716):   at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:106)
01-31 18:57:59.956 I/zygote64(8716):   at void android.os.Looper.loop() (Looper.java:164)
01-31 18:57:59.956 I/zygote64(8716):   at void android.app.ActivityThread.main(java.lang.String[]) (ActivityThread.java:6494)
01-31 18:57:59.956 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Method.invoke(java.lang.Object, java.lang.Object[]) (Method.java:-2)
01-31 18:57:59.956 I/zygote64(8716):   at void com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run() (RuntimeInit.java:438)
01-31 18:57:59.956 I/zygote64(8716):   at void com.android.internal.os.ZygoteInit.main(java.lang.String[]) (ZygoteInit.java:807)
01-31 18:57:59.956 I/zygote64(8716): Caused by: java.lang.ClassNotFoundException: Didn't find class ""android.view.View$OnUnhandledKeyEventListener"" on path: DexPathList[[zip file ""/data/app/edu.berkeley.boinc-DxayF0hBIWD5e28OAGTngw==/base.apk""],nativeLibraryDirectories=[/data/app/edu.berkeley.boinc-DxayF0hBIWD5e28OAGTngw==/lib/arm64, /system/lib64, /system/vendor/lib64]]
01-31 18:57:59.956 I/zygote64(8716):   at java.lang.Class dalvik.system.BaseDexClassLoader.findClass(java.lang.String) (BaseDexClassLoader.java:125)
01-31 18:57:59.956 I/zygote64(8716):   at java.lang.Class java.lang.ClassLoader.loadClass(java.lang.String, boolean) (ClassLoader.java:379)
01-31 18:57:59.956 I/zygote64(8716):   at java.lang.Class java.lang.ClassLoader.loadClass(java.lang.String) (ClassLoader.java:312)
01-31 18:57:59.956 I/zygote64(8716):   at void android.support.v4.view.ViewCompat.setBackground(android.view.View, android.graphics.drawable.Drawable) (ViewCompat.java:2341)
01-31 18:57:59.956 I/zygote64(8716):   at void android.support.v7.widget.ActionBarContainer.<init>(android.content.Context, android.util.AttributeSet) (ActionBarContainer.java:62)
01-31 18:57:59.957 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance0(java.lang.Object[]) (Constructor.java:-2)
01-31 18:57:59.957 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance(java.lang.Object[]) (Constructor.java:334)
01-31 18:57:59.957 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createView(java.lang.String, java.lang.String, android.util.AttributeSet) (LayoutInflater.java:647)
01-31 18:57:59.957 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:790)
01-31 18:57:59.957 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet) (LayoutInflater.java:730)
01-31 18:57:59.957 I/zygote64(8716):   at void android.view.LayoutInflater.rInflate(org.xmlpull.v1.XmlPullParser, android.view.View, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:863)
01-31 18:57:59.957 I/zygote64(8716):   at void android.view.LayoutInflater.rInflateChildren(org.xmlpull.v1.XmlPullParser, android.view.View, android.util.AttributeSet, boolean) (LayoutInflater.java:824)
01-31 18:57:59.957 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(org.xmlpull.v1.XmlPullParser, android.view.ViewGroup, boolean) (LayoutInflater.java:515)
01-31 18:57:59.957 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup, boolean) (LayoutInflater.java:423)
01-31 18:57:59.957 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup) (LayoutInflater.java:374)
01-31 18:57:59.957 I/zygote64(8716):   at android.view.ViewGroup android.support.v7.app.AppCompatDelegateImpl.createSubDecor() (AppCompatDelegateImpl.java:607)
01-31 18:57:59.957 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.ensureSubDecor() (AppCompatDelegateImpl.java:518)
01-31 18:57:59.957 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.setContentView(int) (AppCompatDelegateImpl.java:466)
01-31 18:57:59.957 I/zygote64(8716):   at void android.support.v7.app.AppCompatActivity.setContentView(int) (AppCompatActivity.java:140)
01-31 18:57:59.957 I/zygote64(8716):   at void edu.berkeley.boinc.BOINCActivity.onCreate(android.os.Bundle) (BOINCActivity.java:118)
01-31 18:57:59.957 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle, android.os.PersistableBundle) (Activity.java:7009)
01-31 18:57:59.957 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle) (Activity.java:7000)
01-31 18:57:59.957 I/zygote64(8716):   at void android.app.Instrumentation.callActivityOnCreate(android.app.Activity, android.os.Bundle) (Instrumentation.java:1214)
01-31 18:57:59.957 I/zygote64(8716):   at android.app.Activity android.app.ActivityThread.performLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent) (ActivityThread.java:2731)
01-31 18:57:59.957 I/zygote64(8716):   at void android.app.ActivityThread.handleLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:2856)
01-31 18:57:59.957 I/zygote64(8716):   at void android.app.ActivityThread.-wrap11(android.app.ActivityThread, android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:-1)
01-31 18:57:59.957 I/zygote64(8716):   at void android.app.ActivityThread$H.handleMessage(android.os.Message) (ActivityThread.java:1589)
01-31 18:57:59.957 I/zygote64(8716):   at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:106)
01-31 18:57:59.957 I/zygote64(8716):   at void android.os.Looper.loop() (Looper.java:164)
01-31 18:57:59.957 I/zygote64(8716):   at void android.app.ActivityThread.main(java.lang.String[]) (ActivityThread.java:6494)
01-31 18:57:59.957 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Method.invoke(java.lang.Object, java.lang.Object[]) (Method.java:-2)
01-31 18:57:59.957 I/zygote64(8716):   at void com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run() (RuntimeInit.java:438)
01-31 18:57:59.957 I/zygote64(8716):   at void com.android.internal.os.ZygoteInit.main(java.lang.String[]) (ZygoteInit.java:807)
01-31 18:57:59.957 I/zygote64(8716): 
01-31 18:57:59.960 I/zygote64(8716): Rejecting re-init on previously-failed class java.lang.Class<android.support.v4.view.ViewCompat$OnUnhandledKeyEventListenerWrapper>: java.lang.NoClassDefFoundError: Failed resolution of: Landroid/view/View$OnUnhandledKeyEventListener;
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v4.view.ViewCompat.setBackground(android.view.View, android.graphics.drawable.Drawable) (ViewCompat.java:2341)
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v7.widget.ActionBarContainer.<init>(android.content.Context, android.util.AttributeSet) (ActionBarContainer.java:62)
01-31 18:57:59.960 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance0(java.lang.Object[]) (Constructor.java:-2)
01-31 18:57:59.960 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance(java.lang.Object[]) (Constructor.java:334)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createView(java.lang.String, java.lang.String, android.util.AttributeSet) (LayoutInflater.java:647)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:790)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet) (LayoutInflater.java:730)
01-31 18:57:59.960 I/zygote64(8716):   at void android.view.LayoutInflater.rInflate(org.xmlpull.v1.XmlPullParser, android.view.View, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:863)
01-31 18:57:59.960 I/zygote64(8716):   at void android.view.LayoutInflater.rInflateChildren(org.xmlpull.v1.XmlPullParser, android.view.View, android.util.AttributeSet, boolean) (LayoutInflater.java:824)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(org.xmlpull.v1.XmlPullParser, android.view.ViewGroup, boolean) (LayoutInflater.java:515)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup, boolean) (LayoutInflater.java:423)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup) (LayoutInflater.java:374)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.ViewGroup android.support.v7.app.AppCompatDelegateImpl.createSubDecor() (AppCompatDelegateImpl.java:607)
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.ensureSubDecor() (AppCompatDelegateImpl.java:518)
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.setContentView(int) (AppCompatDelegateImpl.java:466)
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v7.app.AppCompatActivity.setContentView(int) (AppCompatActivity.java:140)
01-31 18:57:59.960 I/zygote64(8716):   at void edu.berkeley.boinc.BOINCActivity.onCreate(android.os.Bundle) (BOINCActivity.java:118)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle, android.os.PersistableBundle) (Activity.java:7009)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle) (Activity.java:7000)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.Instrumentation.callActivityOnCreate(android.app.Activity, android.os.Bundle) (Instrumentation.java:1214)
01-31 18:57:59.960 I/zygote64(8716):   at android.app.Activity android.app.ActivityThread.performLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent) (ActivityThread.java:2731)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.ActivityThread.handleLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:2856)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.ActivityThread.-wrap11(android.app.ActivityThread, android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:-1)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.ActivityThread$H.handleMessage(android.os.Message) (ActivityThread.java:1589)
01-31 18:57:59.960 I/zygote64(8716):   at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:106)
01-31 18:57:59.960 I/zygote64(8716):   at void android.os.Looper.loop() (Looper.java:164)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.ActivityThread.main(java.lang.String[]) (ActivityThread.java:6494)
01-31 18:57:59.960 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Method.invoke(java.lang.Object, java.lang.Object[]) (Method.java:-2)
01-31 18:57:59.960 I/zygote64(8716):   at void com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run() (RuntimeInit.java:438)
01-31 18:57:59.960 I/zygote64(8716):   at void com.android.internal.os.ZygoteInit.main(java.lang.String[]) (ZygoteInit.java:807)
01-31 18:57:59.960 I/zygote64(8716): Caused by: java.lang.ClassNotFoundException: Didn't find class ""android.view.View$OnUnhandledKeyEventListener"" on path: DexPathList[[zip file ""/data/app/edu.berkeley.boinc-DxayF0hBIWD5e28OAGTngw==/base.apk""],nativeLibraryDirectories=[/data/app/edu.berkeley.boinc-DxayF0hBIWD5e28OAGTngw==/lib/arm64, /system/lib64, /system/vendor/lib64]]
01-31 18:57:59.960 I/zygote64(8716):   at java.lang.Class dalvik.system.BaseDexClassLoader.findClass(java.lang.String) (BaseDexClassLoader.java:125)
01-31 18:57:59.960 I/zygote64(8716):   at java.lang.Class java.lang.ClassLoader.loadClass(java.lang.String, boolean) (ClassLoader.java:379)
01-31 18:57:59.960 I/zygote64(8716):   at java.lang.Class java.lang.ClassLoader.loadClass(java.lang.String) (ClassLoader.java:312)
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v4.view.ViewCompat.setBackground(android.view.View, android.graphics.drawable.Drawable) (ViewCompat.java:2341)
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v7.widget.ActionBarContainer.<init>(android.content.Context, android.util.AttributeSet) (ActionBarContainer.java:62)
01-31 18:57:59.960 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance0(java.lang.Object[]) (Constructor.java:-2)
01-31 18:57:59.960 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance(java.lang.Object[]) (Constructor.java:334)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createView(java.lang.String, java.lang.String, android.util.AttributeSet) (LayoutInflater.java:647)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:790)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet) (LayoutInflater.java:730)
01-31 18:57:59.960 I/zygote64(8716):   at void android.view.LayoutInflater.rInflate(org.xmlpull.v1.XmlPullParser, android.view.View, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:863)
01-31 18:57:59.960 I/zygote64(8716):   at void android.view.LayoutInflater.rInflateChildren(org.xmlpull.v1.XmlPullParser, android.view.View, android.util.AttributeSet, boolean) (LayoutInflater.java:824)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(org.xmlpull.v1.XmlPullParser, android.view.ViewGroup, boolean) (LayoutInflater.java:515)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup, boolean) (LayoutInflater.java:423)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup) (LayoutInflater.java:374)
01-31 18:57:59.960 I/zygote64(8716):   at android.view.ViewGroup android.support.v7.app.AppCompatDelegateImpl.createSubDecor() (AppCompatDelegateImpl.java:607)
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.ensureSubDecor() (AppCompatDelegateImpl.java:518)
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.setContentView(int) (AppCompatDelegateImpl.java:466)
01-31 18:57:59.960 I/zygote64(8716):   at void android.support.v7.app.AppCompatActivity.setContentView(int) (AppCompatActivity.java:140)
01-31 18:57:59.960 I/zygote64(8716):   at void edu.berkeley.boinc.BOINCActivity.onCreate(android.os.Bundle) (BOINCActivity.java:118)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle, android.os.PersistableBundle) (Activity.java:7009)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle) (Activity.java:7000)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.Instrumentation.callActivityOnCreate(android.app.Activity, android.os.Bundle) (Instrumentation.java:1214)
01-31 18:57:59.960 I/zygote64(8716):   at android.app.Activity android.app.ActivityThread.performLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent) (ActivityThread.java:2731)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.ActivityThread.handleLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:2856)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.ActivityThread.-wrap11(android.app.ActivityThread, android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:-1)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.ActivityThread$H.handleMessage(android.os.Message) (ActivityThread.java:1589)
01-31 18:57:59.960 I/zygote64(8716):   at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:106)
01-31 18:57:59.960 I/zygote64(8716):   at void android.os.Looper.loop() (Looper.java:164)
01-31 18:57:59.960 I/zygote64(8716):   at void android.app.ActivityThread.main(java.lang.String[]) (ActivityThread.java:6494)
01-31 18:57:59.960 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Method.invoke(java.lang.Object, java.lang.Object[]) (Method.java:-2)
01-31 18:57:59.960 I/zygote64(8716):   at void com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run() (RuntimeInit.java:438)
01-31 18:57:59.960 I/zygote64(8716):   at void com.android.internal.os.ZygoteInit.main(java.lang.String[]) (ZygoteInit.java:807)
01-31 18:57:59.960 I/zygote64(8716): 
01-31 18:57:59.963 I/zygote64(8716): Rejecting re-init on previously-failed class java.lang.Class<android.support.v4.view.ViewCompat$OnUnhandledKeyEventListenerWrapper>: java.lang.NoClassDefFoundError: Failed resolution of: Landroid/view/View$OnUnhandledKeyEventListener;
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v4.view.ViewCompat.setBackground(android.view.View, android.graphics.drawable.Drawable) (ViewCompat.java:2341)
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v7.widget.ActionBarContainer.<init>(android.content.Context, android.util.AttributeSet) (ActionBarContainer.java:62)
01-31 18:57:59.963 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance0(java.lang.Object[]) (Constructor.java:-2)
01-31 18:57:59.963 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance(java.lang.Object[]) (Constructor.java:334)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createView(java.lang.String, java.lang.String, android.util.AttributeSet) (LayoutInflater.java:647)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:790)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet) (LayoutInflater.java:730)
01-31 18:57:59.963 I/zygote64(8716):   at void android.view.LayoutInflater.rInflate(org.xmlpull.v1.XmlPullParser, android.view.View, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:863)
01-31 18:57:59.963 I/zygote64(8716):   at void android.view.LayoutInflater.rInflateChildren(org.xmlpull.v1.XmlPullParser, android.view.View, android.util.AttributeSet, boolean) (LayoutInflater.java:824)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(org.xmlpull.v1.XmlPullParser, android.view.ViewGroup, boolean) (LayoutInflater.java:515)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup, boolean) (LayoutInflater.java:423)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup) (LayoutInflater.java:374)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.ViewGroup android.support.v7.app.AppCompatDelegateImpl.createSubDecor() (AppCompatDelegateImpl.java:607)
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.ensureSubDecor() (AppCompatDelegateImpl.java:518)
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.setContentView(int) (AppCompatDelegateImpl.java:466)
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v7.app.AppCompatActivity.setContentView(int) (AppCompatActivity.java:140)
01-31 18:57:59.963 I/zygote64(8716):   at void edu.berkeley.boinc.BOINCActivity.onCreate(android.os.Bundle) (BOINCActivity.java:118)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle, android.os.PersistableBundle) (Activity.java:7009)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle) (Activity.java:7000)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.Instrumentation.callActivityOnCreate(android.app.Activity, android.os.Bundle) (Instrumentation.java:1214)
01-31 18:57:59.963 I/zygote64(8716):   at android.app.Activity android.app.ActivityThread.performLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent) (ActivityThread.java:2731)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.ActivityThread.handleLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:2856)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.ActivityThread.-wrap11(android.app.ActivityThread, android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:-1)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.ActivityThread$H.handleMessage(android.os.Message) (ActivityThread.java:1589)
01-31 18:57:59.963 I/zygote64(8716):   at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:106)
01-31 18:57:59.963 I/zygote64(8716):   at void android.os.Looper.loop() (Looper.java:164)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.ActivityThread.main(java.lang.String[]) (ActivityThread.java:6494)
01-31 18:57:59.963 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Method.invoke(java.lang.Object, java.lang.Object[]) (Method.java:-2)
01-31 18:57:59.963 I/zygote64(8716):   at void com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run() (RuntimeInit.java:438)
01-31 18:57:59.963 I/zygote64(8716):   at void com.android.internal.os.ZygoteInit.main(java.lang.String[]) (ZygoteInit.java:807)
01-31 18:57:59.963 I/zygote64(8716): Caused by: java.lang.ClassNotFoundException: Didn't find class ""android.view.View$OnUnhandledKeyEventListener"" on path: DexPathList[[zip file ""/data/app/edu.berkeley.boinc-DxayF0hBIWD5e28OAGTngw==/base.apk""],nativeLibraryDirectories=[/data/app/edu.berkeley.boinc-DxayF0hBIWD5e28OAGTngw==/lib/arm64, /system/lib64, /system/vendor/lib64]]
01-31 18:57:59.963 I/zygote64(8716):   at java.lang.Class dalvik.system.BaseDexClassLoader.findClass(java.lang.String) (BaseDexClassLoader.java:125)
01-31 18:57:59.963 I/zygote64(8716):   at java.lang.Class java.lang.ClassLoader.loadClass(java.lang.String, boolean) (ClassLoader.java:379)
01-31 18:57:59.963 I/zygote64(8716):   at java.lang.Class java.lang.ClassLoader.loadClass(java.lang.String) (ClassLoader.java:312)
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v4.view.ViewCompat.setBackground(android.view.View, android.graphics.drawable.Drawable) (ViewCompat.java:2341)
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v7.widget.ActionBarContainer.<init>(android.content.Context, android.util.AttributeSet) (ActionBarContainer.java:62)
01-31 18:57:59.963 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance0(java.lang.Object[]) (Constructor.java:-2)
01-31 18:57:59.963 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Constructor.newInstance(java.lang.Object[]) (Constructor.java:334)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createView(java.lang.String, java.lang.String, android.util.AttributeSet) (LayoutInflater.java:647)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:790)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.createViewFromTag(android.view.View, java.lang.String, android.content.Context, android.util.AttributeSet) (LayoutInflater.java:730)
01-31 18:57:59.963 I/zygote64(8716):   at void android.view.LayoutInflater.rInflate(org.xmlpull.v1.XmlPullParser, android.view.View, android.content.Context, android.util.AttributeSet, boolean) (LayoutInflater.java:863)
01-31 18:57:59.963 I/zygote64(8716):   at void android.view.LayoutInflater.rInflateChildren(org.xmlpull.v1.XmlPullParser, android.view.View, android.util.AttributeSet, boolean) (LayoutInflater.java:824)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(org.xmlpull.v1.XmlPullParser, android.view.ViewGroup, boolean) (LayoutInflater.java:515)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup, boolean) (LayoutInflater.java:423)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.View android.view.LayoutInflater.inflate(int, android.view.ViewGroup) (LayoutInflater.java:374)
01-31 18:57:59.963 I/zygote64(8716):   at android.view.ViewGroup android.support.v7.app.AppCompatDelegateImpl.createSubDecor() (AppCompatDelegateImpl.java:607)
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.ensureSubDecor() (AppCompatDelegateImpl.java:518)
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v7.app.AppCompatDelegateImpl.setContentView(int) (AppCompatDelegateImpl.java:466)
01-31 18:57:59.963 I/zygote64(8716):   at void android.support.v7.app.AppCompatActivity.setContentView(int) (AppCompatActivity.java:140)
01-31 18:57:59.963 I/zygote64(8716):   at void edu.berkeley.boinc.BOINCActivity.onCreate(android.os.Bundle) (BOINCActivity.java:118)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle, android.os.PersistableBundle) (Activity.java:7009)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.Activity.performCreate(android.os.Bundle) (Activity.java:7000)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.Instrumentation.callActivityOnCreate(android.app.Activity, android.os.Bundle) (Instrumentation.java:1214)
01-31 18:57:59.963 I/zygote64(8716):   at android.app.Activity android.app.ActivityThread.performLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent) (ActivityThread.java:2731)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.ActivityThread.handleLaunchActivity(android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:2856)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.ActivityThread.-wrap11(android.app.ActivityThread, android.app.ActivityThread$ActivityClientRecord, android.content.Intent, java.lang.String) (ActivityThread.java:-1)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.ActivityThread$H.handleMessage(android.os.Message) (ActivityThread.java:1589)
01-31 18:57:59.963 I/zygote64(8716):   at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:106)
01-31 18:57:59.963 I/zygote64(8716):   at void android.os.Looper.loop() (Looper.java:164)
01-31 18:57:59.963 I/zygote64(8716):   at void android.app.ActivityThread.main(java.lang.String[]) (ActivityThread.java:6494)
01-31 18:57:59.963 I/zygote64(8716):   at java.lang.Object java.lang.reflect.Method.invoke(java.lang.Object, java.lang.Object[]) (Method.java:-2)
01-31 18:57:59.963 I/zygote64(8716):   at void com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run() (RuntimeInit.java:438)
01-31 18:57:59.963 I/zygote64(8716):   at void com.android.internal.os.ZygoteInit.main(java.lang.String[]) (ZygoteInit.java:807)
01-31 18:57:59.963 I/zygote64(8716): 
01-31 18:57:59.984 I/zygote64(8731): Do partial code cache collection, code=26KB, data=18KB
01-31 18:57:59.985 I/zygote64(8731): After code cache collection, code=26KB, data=18KB
01-31 18:57:59.985 I/zygote64(8731): Increasing code cache capacity to 128KB
01-31 18:57:59.990 V/BOINC_GUI(8731): setClientStatus: #results:0 #projects:3 #transfers:0 // computing: false11 - network: false14097
01-31 18:57:59.993 D/BOINC_GUI(8731): readClientStatus(): computation enabled: false
01-31 18:58:00.005 D/BOINC_GUI(8731): reportDeviceStatus()
01-31 18:58:00.013 I/audio_hw_primary(323): do_out_standby standby: 0, out_device: 2
01-31 18:58:00.023 D/BOINC_GUI(8731): readClientStatus(): screen on, get complete status
01-31 18:58:00.032 I/audio_hw_primary(323): select_devices++ mode[0]
01-31 18:58:00.032 V/audio_hw_primary(323): custom selected input route none
01-31 18:58:00.032 V/audio_hw_primary(323): custom selected output route none
01-31 18:58:00.032 I/audio_hw_primary(323): select_devices() devices 0x2 input src 0 output route none input route none
01-31 18:58:00.032 V/audio_hw_primary(323): forced_out 0 adev_device 0x2 output_device 0 
01-31 18:58:00.033 I/MAX98925_SPK(323): set  MAX98925 SINGLE SPK POWER OFF
01-31 18:58:00.033 I/MAX98925_SPK(323): MAX98925 SPK POWER OFF
01-31 18:58:00.033 I/dsp_common(323): dsp_common_control_algo:set device = 0
01-31 18:58:00.033 V/audio_common(323): get value success, value:hi6402
01-31 18:58:00.033 I/dsp_maxim_ctl(323): maxim_get_prop: get codec_name succ, codec_name = hi6402
01-31 18:58:00.033 V/audio_common(323): get value success, value:false
01-31 18:58:00.033 I/dsp_maxim_ctl(323): maxim_get_prop: get multi mic function enable succ, multi_mic_enable = 0
01-31 18:58:00.036 I/dsp_codec_ctl(323): codec_adsp_send_sync_cmd:ioctl ret=0
01-31 18:58:00.036 I/dsp_maxim_ctl(323): smartpa_control succ with value:0
01-31 18:58:00.048 I/dsp_common(323): dsp_common_control_algo:set device = 0
01-31 18:58:00.049 V/audio_common(323): get value success, value:hi6402
01-31 18:58:00.049 I/dsp_maxim_ctl(323): maxim_get_prop: get codec_name succ, codec_name = hi6402
01-31 18:58:00.049 V/audio_common(323): get value success, value:false
01-31 18:58:00.049 I/dsp_maxim_ctl(323): maxim_get_prop: get multi mic function enable succ, multi_mic_enable = 0
01-31 18:58:00.049 I/audio_hw_primary(323): select_devices--
01-31 18:58:00.065 D/BOINC_GUI(8716): dispatchNavBarOnClick for item with id: 2131558748 title: Attività is project? false
01-31 18:58:00.067 D/BOINC_GUI(8716): displayFragmentForNavDrawer() Attività
01-31 18:58:00.068 D/BOINC_GUI(8731): Monitor onStartCommand()
01-31 18:58:00.068 D/BOINC_GUI(8731): Monitor.onStartCommand() with action code: -1
01-31 18:58:00.073 V/BOINC_GUI(8716): StatusFragment onCreateView
01-31 18:58:00.082 D/BOINC_GUI(8716): TasksFragment onCreateView
01-31 18:58:00.098 V/BOINC_GUI(8716): StatusFragment register receiver
01-31 18:58:00.099 D/BOINC_GUI(8716): TasksFragment register receiver
01-31 18:58:00.100 W/BOINC_GUI(8716): TasksActivity: Could not load data, clientStatus not initialized.
01-31 18:58:00.111 V/BOINC_GUI(8731): getProjectIcon for: https://lhcathome.cern.ch/lhcathome/
01-31 18:58:00.111 V/BOINC_GUI(8731): getProjectIcon could not parse sym link for project: https://lhcathome.cern.ch/lhcathome/
01-31 18:58:00.112 D/BOINC_GUI(8716): NavDrawerItem: created hash code -1112740780 for project LHC@home
01-31 18:58:00.112 V/BOINC_GUI(8731): getProjectIcon for: http://boinc.bakerlab.org/rosetta/
01-31 18:58:00.112 V/BOINC_GUI(8731): getProjectIcon could not parse sym link for project: http://boinc.bakerlab.org/rosetta/
01-31 18:58:00.113 D/BOINC_GUI(8716): NavDrawerItem: created hash code 1979522675 for project Rosetta@home
01-31 18:58:00.113 V/BOINC_GUI(8731): getProjectIcon for: http://www.worldcommunitygrid.org/
01-31 18:58:00.116 V/BOINC_GUI(8731): setClientStatus: #results:0 #projects:3 #transfers:0 // computing: false11 - network: false14097
01-31 18:58:00.116 D/BOINC_GUI(8716): NavDrawerItem: created hash code -1022805997 for project World Community Grid
01-31 18:58:00.116 D/BOINC_GUI(8716): NavDrawerListAdapter.compareAndAddProjects() added: 3
01-31 18:58:00.118 D/BOINC_GUI(8731): readClientStatus(): computation enabled: false
01-31 18:58:00.142 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:58:00.142 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:00.147 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:58:00.147 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:00.151 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:58:00.151 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:58:00.152 D/mali_winsys(8716): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:58:00.153 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() for : Attivitàtruefalsefalse
01-31 18:58:00.163 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() highlighted! ID : 2131558748
01-31 18:58:00.167 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() for : Avvisitruefalsefalse
01-31 18:58:00.173 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() for : Progettifalsefalsefalse
01-31 18:58:00.178 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() for : World Community Gridfalsetruetrue
01-31 18:58:00.182 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() for : Rosetta@homefalsetruetrue
01-31 18:58:00.185 V/BOINC_GUI(8731): getProjectIcon for: http://boinc.bakerlab.org/rosetta/
01-31 18:58:00.185 V/BOINC_GUI(8731): getProjectIcon could not parse sym link for project: http://boinc.bakerlab.org/rosetta/
01-31 18:58:00.187 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() for : LHC@homefalsetruetrue
01-31 18:58:00.189 V/BOINC_GUI(8731): getProjectIcon for: https://lhcathome.cern.ch/lhcathome/
01-31 18:58:00.189 V/BOINC_GUI(8731): getProjectIcon could not parse sym link for project: https://lhcathome.cern.ch/lhcathome/
01-31 18:58:00.191 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() for : Aggiungi progettofalsetruefalse
01-31 18:58:00.197 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() for : Preferenzefalsefalsefalse
01-31 18:58:00.201 D/BOINC_GUI(8716): NavDrawerListAdapter.getView() for : Assistenzafalsefalsefalse
01-31 18:58:00.207 D/BOINC_GUI(8716): BOINCActivity ClientStatusChange - onReceive()
01-31 18:58:00.209 D/BOINC_GUI(8716): StatusFragment ClientStatusChange - onReceive()
01-31 18:58:00.228 D/BOINC_GUI(8716): BOINCActivity onCreateOptionsMenu()
01-31 18:58:00.232 D/BOINC_GUI(8716): BOINCActivity onPrepareOptionsMenu()
01-31 18:58:00.253 D/BOINC_GUI(8716): BOINCActivity onPrepareOptionsMenu()
01-31 18:58:00.254 D/BOINC_GUI(8716): TasksActivity onReceive
01-31 18:58:00.294 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:58:00.294 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:00.348 I/ActivityManager(725): Displayed edu.berkeley.boinc/.BOINCActivity: +453ms
01-31 18:58:00.352 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:58:00.352 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:00.377 E/BufferQueueProducer(312): [edu.berkeley.boinc/edu.berkeley.boinc.SplashActivity#0] disconnect: not connected (req=1)
01-31 18:58:00.377 W/libEGL  (8716): EGLNativeWindowType 0x7ef1361010 disconnect failed
01-31 18:58:00.687 D/BOINC_GUI(8716): SplashActivity onDestroy()
01-31 18:58:00.693 E/SurfaceFlinger(312): Failed to find layer (edu.berkeley.boinc/edu.berkeley.boinc.SplashActivity#0) in layer parent (no-parent).
01-31 18:58:01.014 D/BOINC_GUI(8731): reportDeviceStatus()
01-31 18:58:01.029 D/BOINC_GUI(8731): readClientStatus(): screen on, get complete status
01-31 18:58:01.098 V/BOINC_GUI(8731): setClientStatus: #results:0 #projects:3 #transfers:0 // computing: false20 - network: false20
01-31 18:58:01.100 D/BOINC_GUI(8716): BOINCActivity ClientStatusChange - onReceive()
01-31 18:58:01.105 D/BOINC_GUI(8716): StatusFragment ClientStatusChange - onReceive()
01-31 18:58:01.106 D/BOINC_GUI(8731): readClientStatus(): computation enabled: true
01-31 18:58:01.111 I/WifiService(725): acquireWifiLock uid=10077 lockMode=1
01-31 18:58:01.112 D/BOINC_GUI(8731): wifiLock acquired
01-31 18:58:01.113 D/BOINC_GUI(8731): wakeLock acquired
01-31 18:58:01.113 D/BOINC_GUI(8731): ClientNotification: notification needs update? truefalsetruetruefalse
01-31 18:58:01.118 D/BOINC_GUI(8716): TasksActivity onReceive
01-31 18:58:01.129 D/OpenGLRenderer(8731): HWUI GL Pipeline
01-31 18:58:01.130 D/BOINC_GUI(8716): BOINCActivity onCreateOptionsMenu()
01-31 18:58:01.131 D/BOINC_GUI(8716): BOINCActivity onPrepareOptionsMenu()
01-31 18:58:01.141 E/NotificationService(725): No Channel found for pkg=edu.berkeley.boinc, channelId=main-channel, id=1, tag=null, opPkg=edu.berkeley.boinc, callingUid=10077, userId=0, incomingUserId=0, notificationUid=10077, notification=Notification(channel=main-channel pri=1 contentView=null vibrate=null sound=null defaults=0x0 flags=0x0 color=0x00000000 actions=1 vis=PRIVATE)
01-31 18:58:01.142 D/BOINC_GUI(8731): ClientNotification: update
01-31 18:58:01.151 W/ActivityManager(725): Error showing notification for service
01-31 18:58:01.151 W/ActivityManager(725): java.lang.RuntimeException: invalid channel for service notification: Notification(channel=main-channel pri=1 contentView=null vibrate=null sound=null defaults=0x0 flags=0x40 color=0x00000000 actions=1 vis=PRIVATE)
01-31 18:58:01.151 W/ActivityManager(725): 	at com.android.server.am.ServiceRecord$1.run(ServiceRecord.java:532)
01-31 18:58:01.151 W/ActivityManager(725): 	at android.os.Handler.handleCallback(Handler.java:790)
01-31 18:58:01.151 W/ActivityManager(725): 	at android.os.Handler.dispatchMessage(Handler.java:99)
01-31 18:58:01.151 W/ActivityManager(725): 	at android.os.Looper.loop(Looper.java:164)
01-31 18:58:01.151 W/ActivityManager(725): 	at android.os.HandlerThread.run(HandlerThread.java:65)
01-31 18:58:01.151 W/ActivityManager(725): 	at com.android.server.ServiceThread.run(ServiceThread.java:46)
01-31 18:58:01.154 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:58:01.154 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:58:01.155 D/BOINC_GUI(8731): ClientNotification.setForeground() start service as foreground.
01-31 18:58:01.155 D/AndroidRuntime(8731): Shutting down VM
01-31 18:58:01.155 E/AndroidRuntime(8731): FATAL EXCEPTION: main
01-31 18:58:01.155 E/AndroidRuntime(8731): Process: edu.berkeley.boinc:remote, PID: 8731
01-31 18:58:01.155 E/AndroidRuntime(8731): android.app.RemoteServiceException: Bad notification for startForeground: java.lang.RuntimeException: invalid channel for service notification: Notification(channel=main-channel pri=1 contentView=null vibrate=null sound=null defaults=0x0 flags=0x40 color=0x00000000 actions=1 vis=PRIVATE)
01-31 18:58:01.155 E/AndroidRuntime(8731): 	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1768)
01-31 18:58:01.155 E/AndroidRuntime(8731): 	at android.os.Handler.dispatchMessage(Handler.java:106)
01-31 18:58:01.155 E/AndroidRuntime(8731): 	at android.os.Looper.loop(Looper.java:164)
01-31 18:58:01.155 E/AndroidRuntime(8731): 	at android.app.ActivityThread.main(ActivityThread.java:6494)
01-31 18:58:01.155 E/AndroidRuntime(8731): 	at java.lang.reflect.Method.invoke(Native Method)
01-31 18:58:01.155 E/AndroidRuntime(8731): 	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:438)
01-31 18:58:01.155 E/AndroidRuntime(8731): 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:807)
01-31 18:58:01.162 I/ActivityManager(725): Showing crash dialog for package edu.berkeley.boinc u0
01-31 18:58:01.184 W/BroadcastQueue(725): Background execution not allowed: receiving Intent { act=android.intent.action.DROPBOX_ENTRY_ADDED flg=0x10 (has extras) } to com.google.android.gms/.stats.service.DropBoxEntryAddedReceiver
01-31 18:58:01.184 W/BroadcastQueue(725): Background execution not allowed: receiving Intent { act=android.intent.action.DROPBOX_ENTRY_ADDED flg=0x10 (has extras) } to com.google.android.gms/.chimera.GmsIntentOperationService$PersistentTrustedReceiver
01-31 18:58:01.236 I/OpenGLRenderer(725): Initialized EGL, version 1.4
01-31 18:58:01.236 D/OpenGLRenderer(725): Swap behavior 2
01-31 18:58:01.244 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1280, size=5160960, usage=0x900
01-31 18:58:01.244 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:01.248 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1280, size=5160960, usage=0x900
01-31 18:58:01.248 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:01.252 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1280, size=5160960, usage=0x900
01-31 18:58:01.252 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:58:01.255 D/mali_winsys(725): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:58:01.264 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1280, size=5160960, usage=0xb00
01-31 18:58:01.264 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:01.296 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1280, size=5160960, usage=0xb00
01-31 18:58:01.296 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:02.016 D/BOINC_GUI(8731): reportDeviceStatus()
01-31 18:58:02.032 D/BOINC_GUI(8731): readClientStatus(): screen on, get complete status
01-31 18:58:02.113 V/BOINC_GUI(8731): setClientStatus: #results:0 #projects:3 #transfers:0 // computing: false20 - network: false20
01-31 18:58:02.114 D/BOINC_GUI(8716): BOINCActivity ClientStatusChange - onReceive()
01-31 18:58:02.115 D/BOINC_GUI(8731): readClientStatus(): computation enabled: true
01-31 18:58:02.115 D/BOINC_GUI(8731): ClientNotification: notification needs update? falsefalsefalsefalsefalse
01-31 18:58:02.117 D/BOINC_GUI(8716): StatusFragment ClientStatusChange - onReceive()
01-31 18:58:02.118 D/BOINC_GUI(8716): TasksActivity onReceive
01-31 18:58:03.015 D/BOINC_GUI(8731): reportDeviceStatus()
01-31 18:58:03.031 D/BOINC_GUI(8731): readClientStatus(): screen on, get complete status
01-31 18:58:03.096 V/BOINC_GUI(8731): setClientStatus: #results:0 #projects:3 #transfers:0 // computing: false20 - network: false20
01-31 18:58:03.098 D/BOINC_GUI(8716): BOINCActivity ClientStatusChange - onReceive()
01-31 18:58:03.100 D/BOINC_GUI(8731): readClientStatus(): computation enabled: true
01-31 18:58:03.100 D/BOINC_GUI(8731): ClientNotification: notification needs update? falsefalsefalsefalsefalse
01-31 18:58:03.101 D/BOINC_GUI(8716): StatusFragment ClientStatusChange - onReceive()
01-31 18:58:03.104 D/BOINC_GUI(8716): TasksActivity onReceive
01-31 18:58:03.812 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1280, size=5160960, usage=0xb00
01-31 18:58:03.812 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:58:03.924 W/StreamHAL(323): Error from HAL stream in function get_presentation_position: Operation not permitted
01-31 18:58:03.924 I/audio_hw_primary(323): start_output_stream++: out_type = 0, out_device = 2,out->pcm[PCM_CARD] = 0x0, out->voice_pcm = 0x0
01-31 18:58:03.925 V/audio_hw_primary(323): pcm_open(PCM_CARD) succeed. pcm_device: 0
01-31 18:58:03.925 I/audio_hw_primary(323): select_devices++ mode[0]
01-31 18:58:03.925 V/audio_hw_primary(323): custom selected input route none
01-31 18:58:03.925 V/audio_hw_primary(323): custom selected output route media-speaker
01-31 18:58:03.925 I/audio_hw_primary(323): select_devices() devices 0x2 input src 0 output route media-speaker input route none
01-31 18:58:03.925 V/audio_hw_primary(323): forced_out 0 adev_device 0x2 output_device 0x2 
01-31 18:58:03.926 I/dsp_common(323): dsp_common_control_algo:set device = 2
01-31 18:58:03.926 I/dsp_maxim_ctl(323): get_smartpa_sence:device = 0x2
01-31 18:58:03.926 V/audio_common(323): get value success, value:hi6402
01-31 18:58:03.926 I/dsp_maxim_ctl(323): maxim_get_prop: get codec_name succ, codec_name = hi6402
01-31 18:58:03.926 V/audio_common(323): get value success, value:false
01-31 18:58:03.926 I/dsp_maxim_ctl(323): maxim_get_prop: get multi mic function enable succ, multi_mic_enable = 0
01-31 18:58:03.926 D/audio_route(323): Apply path: media-speaker
01-31 18:58:03.926 D/audio_route(323): Apply path: media-speaker
01-31 18:58:03.936 I/init    (1): type=1400 audit(0.0:3333): avc: denied { sigchld } for scontext=u:object_r:hal_wifi_hwservice:s0 tcontext=u:r:init:s0 tclass=process permissive=1
01-31 18:58:03.941 I/ActivityManager(725): Killing 8731:edu.berkeley.boinc:remote/u0a77 (adj 100): crash
01-31 18:58:03.943 W/system_server(725): kill(-8731, 9) failed: No such process
01-31 18:58:03.943 E/BOINC_GUI(8716): BOINCActivity onServiceDisconnected
01-31 18:58:03.973 E/BufferQueueProducer(312): [Application Error: edu.berkeley.boinc#0] disconnect: not connected (req=1)
01-31 18:58:03.973 W/libEGL  (725): EGLNativeWindowType 0x7edbed4010 disconnect failed
01-31 18:58:03.973 D/OpenGLRenderer(725): endAllActiveAnimators on 0x7edbe65800 (RippleDrawable) with handle 0x7edbf3c7e0
01-31 18:58:03.984 W/system_server(725): kill(-8731, 9) failed: No such process
01-31 18:58:04.014 I/dsp_common(323): dsp_common_control_algo:set device = 2
01-31 18:58:04.014 I/dsp_maxim_ctl(323): get_smartpa_sence:device = 0x2
01-31 18:58:04.014 V/audio_common(323): get value success, value:hi6402
01-31 18:58:04.014 I/dsp_maxim_ctl(323): maxim_get_prop: get codec_name succ, codec_name = hi6402
01-31 18:58:04.014 V/audio_common(323): get value success, value:false
01-31 18:58:04.014 I/dsp_maxim_ctl(323): maxim_get_prop: get multi mic function enable succ, multi_mic_enable = 0
01-31 18:58:04.014 I/dsp_maxim_ctl(323): maxim_control_algo : product_name = VENUS
01-31 18:58:04.020 I/dsp_codec_ctl(323): codec_adsp_send_sync_cmd:ioctl ret=0
01-31 18:58:04.020 I/dsp_maxim_ctl(323): smartpa_control succ with value:1
01-31 18:58:04.020 I/MAX98925_SPK(323): set  MAX98925 SINGLE SPK POWER ON
01-31 18:58:04.021 I/MAX98925_SPK(323): MAX98925 SPK POWER ON
01-31 18:58:04.021 I/audio_hw_primary(323): select_devices--
01-31 18:58:04.021 I/audio_hw_primary(323): start_output_stream--
01-31 18:58:04.028 W/system_server(725): kill(-8731, 9) failed: No such process
01-31 18:58:04.028 I/system_server(725): Successfully killed process cgroup uid 10077 pid 8731 in 85ms
01-31 18:58:04.043 D/AudioFlinger(323): mixer(0xec40cbc0) throttle end: throttle time(10)
01-31 18:58:04.143 W/AppOps  (725): Finishing op nesting under-run: uid 1000 pkg android code 24 time=0 duration=0 nesting=0
01-31 18:58:05.896 V/android.os.Debug(725): failed to create memtrack_proc
01-31 18:58:06.805 I/ActivityManager(725): START u0 {flg=0x10804000 cmp=com.android.systemui/.recents.RecentsActivity} from uid 10025
01-31 18:58:06.809 V/BOINC_GUI(8716): BOINCActivity onPause()
01-31 18:58:06.810 V/BOINC_GUI(8716): StatusFragment remove receiver
01-31 18:58:06.816 D/mali_winsys(839): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:58:06.820 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1024, size=733184, usage=0x300
01-31 18:58:06.820 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:06.827 D/BOINC_GUI(8716): TasksFragment remove receiver
01-31 18:58:06.835 E/BufferQueueProducer(839): [unnamed-839-10] disconnect: not connected (req=1)
01-31 18:58:06.835 W/libEGL  (839): EGLNativeWindowType 0x7efc5cf010 disconnect failed
01-31 18:58:06.837 V/BufferItemConsumer(839): [unnamed-839-10] Failed to release buffer: Unknown error -1 (1)
01-31 18:58:06.876 E/BufferQueueProducer(312): [edu.berkeley.boinc/edu.berkeley.boinc.BOINCActivity#0] disconnect: not connected (req=1)
01-31 18:58:06.876 W/libEGL  (8716): EGLNativeWindowType 0x7ee2285010 disconnect failed
01-31 18:58:06.916 D/mali_winsys(839): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:58:06.920 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1024, size=733184, usage=0x300
01-31 18:58:06.920 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:06.932 E/BufferQueueProducer(839): [unnamed-839-11] disconnect: not connected (req=1)
01-31 18:58:06.932 W/libEGL  (839): EGLNativeWindowType 0x7efc5cf010 disconnect failed
01-31 18:58:06.935 V/BufferItemConsumer(839): [unnamed-839-11] Failed to release buffer: Unknown error -1 (1)
01-31 18:58:06.971 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:58:06.971 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:58:06.978 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:58:06.979 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:06.995 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:58:06.996 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:06.997 D/mali_winsys(839): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:58:07.023 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:58:07.023 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:58:07.081 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=1, stride=1088, size=8355840, usage=0x333
01-31 18:58:07.081 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:07.139 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:58:07.139 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:07.158 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:58:07.158 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:58:07.460 I/OpenGLRenderer(725): Initialized EGL, version 1.4
01-31 18:58:07.460 D/OpenGLRenderer(725): Swap behavior 2
01-31 18:58:07.573 I/zygote64(725): NativeAlloc concurrent copying GC freed 14989(1007KB) AllocSpace objects, 7(332KB) LOS objects, 37% free, 10MB/16MB, paused 268us total 127.279ms
01-31 18:58:08.256 D/BOINC_GUI(8716): BOINCActivity onDestroy()
01-31 18:58:08.258 I/ActivityManager(725): Killing 8716:edu.berkeley.boinc/u0a77 (adj 900): remove task
01-31 18:58:08.289 W/system_server(725): kill(-8716, 9) failed: No such process
01-31 18:58:08.371 I/WindowManager(725): WIN DEATH: Window{715cf7d u0 edu.berkeley.boinc/edu.berkeley.boinc.BOINCActivity}
01-31 18:58:08.390 W/ActivityManager(725): setHasOverlayUi called on unknown pid: 8716
01-31 18:58:08.334 V/chatty  (725): uid=1000(system) ActivityManager identical 1 line
01-31 18:58:08.380 W/system_server(725): kill(-8716, 9) failed: No such process
01-31 18:58:08.391 I/zygote64(725): NativeAlloc concurrent copying GC freed 610(226KB) AllocSpace objects, 1(20KB) LOS objects, 36% free, 10MB/16MB, paused 209us total 100.708ms
01-31 18:58:08.428 W/system_server(725): kill(-8716, 9) failed: No such process
01-31 18:58:08.429 I/system_server(725): Successfully killed process cgroup uid 10077 pid 8716 in 140ms
01-31 18:58:08.768 D/mali_winsys(839): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:58:08.771 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1024, size=733184, usage=0x300
01-31 18:58:08.771 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:08.781 E/BufferQueueProducer(839): [unnamed-839-12] disconnect: not connected (req=1)
01-31 18:58:08.781 W/libEGL  (839): EGLNativeWindowType 0x7ef1340010 disconnect failed
01-31 18:58:08.782 V/BufferItemConsumer(839): [unnamed-839-12] Failed to release buffer: Unknown error -1 (1)
01-31 18:58:08.817 W/ActivityManager(725): cancelTaskWindowTransition: taskId=25 not found
01-31 18:58:08.836 D/mali_winsys(839): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:58:08.838 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1024, size=733184, usage=0x300
01-31 18:58:08.838 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:08.849 E/BufferQueueProducer(839): [unnamed-839-13] disconnect: not connected (req=1)
01-31 18:58:08.849 W/libEGL  (839): EGLNativeWindowType 0x7ef1340010 disconnect failed
01-31 18:58:08.851 V/BufferItemConsumer(839): [unnamed-839-13] Failed to release buffer: Unknown error -1 (1)
01-31 18:58:08.856 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:58:08.856 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:58:08.859 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:58:08.859 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:08.866 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1088, size=8486912, usage=0x900
01-31 18:58:08.866 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:08.867 D/mali_winsys(6095): EGLint new_window_surface(egl_winsys_display*, void*, EGLSurface, EGLConfig, egl_winsys_surface**, egl_color_buffer_format*, EGLBoolean) returns 0x3000
01-31 18:58:09.061 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e200): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:58:09.061 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e200)
01-31 18:58:09.204 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755b64e600): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:58:09.204 I/gralloc (607): alloc_device_free:586: Free handle(0x755b64e600)
01-31 18:58:09.236 I/gralloc (607): alloc_device_alloc:564: Alloc handle(0x755ae14200): interfmt=200000001, stride=1088, size=8486912, usage=0xb00
01-31 18:58:09.236 I/gralloc (607): alloc_device_free:586: Free handle(0x755ae14200)
01-31 18:58:09.288 V/android.os.Debug(725): failed to create memtrack_proc
01-31 18:58:09.330 E/BufferQueueProducer(312): [com.android.systemui/com.android.systemui.recents.RecentsActivity#0] disconnect: not connected (req=1)
01-31 18:58:09.331 W/libEGL  (839): EGLNativeWindowType 0x7efc5cf010 disconnect failed
01-31 18:58:09.363 D/asd     (323): [asd_proc_hal_data],hal data soundness: L[0],R[0]
01-31 18:58:09.413 I/zygote64(839): Explicit concurrent copying GC freed 3696(252KB) AllocSpace objects, 0(0B) LOS objects, 49% free, 5MB/11MB, paused 79us total 105.381ms

```",It seems a fix was merged a while ago but was never pushed to the Google Play store.  ,30260406
592,client 7.16.4 does not give a correct <n_usable_coprocs> in client_state.xml,open,2020-01-28T10:05:29Z,2020-02-20T15:31:39Z,,NONE,"Since the update to 7.16.4, from 7.16.3, I don't have anymore a correct n_usable_coprocs information in client_state.xml, that I use in a script.
The n_usable_coprocs is always 0, whereas my boinc client uses 2 GPUs.
Below is the client log file.
The version I compiled is git 12b95c3, to have the correct version number, as 7.16.4 tarballs on git were released with the old 7.16.3 number.

---------------
28-Jan-2020 09:25:35 [---] Starting BOINC client version 7.16.4 for x86_64-pc-linux-gnu
28-Jan-2020 09:25:35 [---] Libraries: libcurl/7.68.0 OpenSSL/1.1.1d zlib/1.2.11 libidn2/2.3.0 libpsl/0.21.0 (+libidn2/2.1.1)
28-Jan-2020 09:25:35 [---] Running as a daemon
28-Jan-2020 09:25:35 [---] Data directory: /opt/boinc
28-Jan-2020 09:25:35 [---] OpenCL: AMD/ATI GPU 0: AMD CAICOS (DRM 2.50.0 / 5.4.15-jpmr, LLVM 9.0.1) (driver version 19.3.2, device version OpenCL 1.1 Mesa 19.3.2, 2048MB, 2048MB available, 100 GFLOPS peak)
28-Jan-2020 09:25:35 [---] OpenCL: AMD/ATI GPU 1: AMD VERDE (DRM 2.50.0, 5.4.15-jpmr, LLVM 9.0.1) (driver version 19.3.2, device version OpenCL 1.1 Mesa 19.3.2, 4096MB, 4096MB available, 592 GFLOPS peak)
28-Jan-2020 09:25:35 [---] [libc detection] gathered: 2.30, GNU libc
28-Jan-2020 09:25:35 [---] Host name: Altair
28-Jan-2020 09:25:35 [---] Processor: 16 AuthenticAMD AMD Ryzen 7 2700 Eight-Core Processor [Family 23 Model 8 Stepping 2]
28-Jan-2020 09:25:35 [---] Processor features: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate sme ssbd sev ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca
28-Jan-2020 09:25:35 [---] OS: Linux LFS-Phil: Linux From Scratch by Phil [5.4.15-jpmr|libc 2.30 (GNU libc)]
28-Jan-2020 09:25:35 [---] Memory: 15.66 GB physical, 16.00 GB virtual
28-Jan-2020 09:25:35 [---] Disk: 62.50 GB total, 36.12 GB free
28-Jan-2020 09:25:35 [---] Local time is UTC +1 hours
28-Jan-2020 09:25:35 [---] Config: GUI RPCs allowed from:
28-Jan-2020 09:25:35 [---]     127.0.0.1
28-Jan-2020 09:25:35 [---] Config: don't use VirtualBox
28-Jan-2020 09:25:35 [---] Config: use all coprocessors

",,30260406
593,Simple View: Suspend button must do what? Suspend or Snooze? ,open,2020-01-27T22:20:36Z,2020-01-28T15:47:09Z,,CONTRIBUTOR,"In Advanced view when Activity on any of the items is set to Suspend, it will suspend that item for eternity until 'based on preferences' or 'always' is chosen. 

In Simple view there is a Suspend button at the bottom, which suspends calculations for just one hour. 
It does the same thing as the Snooze option in the (Windows) right-click BOINC Manager icon in the system tray. 

For consistency, either the Suspend button in Simple view should suspend until the Resume button is pressed, or this button should be renamed to Snooze and have the mouse-over text changed to show it only suspends for an hour. 

Writing this up as a bug, because I feel as if we didn't want the behaviour to be that of a Snooze option. ",You mean like the suggestions in #1432,30260406
594,CTRL+SHIFT+E (Open Event Log) will make screen shot when Radeon Software is installed,open,2020-01-27T20:49:28Z,2020-01-28T18:15:19Z,,CONTRIBUTOR,"I found tonight when trying to open the Event Log from BOINC Manager that each time I pressed CTRL+SHIFT+E simultaneously my AMD Radeon Software made a screen shot. At least CTRL+SHIFT+S is also used by this software and it takes precedence over BOINC. 

I'm not sure how this should be fixed, but I think we should at least put a warning out for AMD users who update to the new Radeon Software 20.1.1 or above, as they'll be affected in this way. 

Screen shot: https://photos.app.goo.gl/63xbTrTPvNdW2j5q6","Well, it's perhaps not something we should try to fix, but a warning in the User Manual documentation ought to be possible. I'm just wondering if it only does this under Windows? ",30260406
595,Client GUI: misleading display of pseudo-progress,open,2020-01-25T16:40:21Z,2020-02-20T15:29:17Z,,CONTRIBUTOR,"**Describe the bug**
Progress pseudo-%age not reset on restart from beginning

**Steps To Reproduce**
I was running an unusual Einstein@Home task, which only checkpoints six times in 24 hours. It was paused by BOINC for lack of memory, and - not unreasonably - removed from memory too, despite LAIM active. Screenshot below shows just after the restart - elapsed time 1 minute 21 seconds, progress 4.898%. The progress is the 'invented' reassurance created by BOINC when the task has neither checkpointed nor written a boinc_task_state.xml file.

**Expected behavior**
If elapsed time is reset to zero, so should progress %age - at least, after the 1 minute point where pseudo-progress starts to be reported.

**Screenshots**
https://i.imgur.com/Be9S2yh.png (live display not working)

**System Information**
 - OS: Windows 10
 - BOINC Version: 7.16.4 (but probably not new in this version)
",,30260406
596,[client] Add support of UPower to detect battery status on linux,open,2020-01-20T11:05:52Z,2020-01-29T07:32:13Z,,MEMBER,Signed-off-by: Vitalii Koshura <lestat.de.lionkur@gmail.com>,"Don't know.  I don't understand programming syntax.  I can read it is about all I can do.  Comprehension is beyond my skills presently.

I have your branch.  Now just have to compile it into a new client for testing.  Will get to it tomorrow.  Then I can pull the plug on my UPS while watching upower state and  see if the Manager drops the compute load.",30260406
597,Wrapper strips quotes from command line,open,2020-01-17T07:51:25Z,2020-01-20T08:30:28Z,,CONTRIBUTOR,"**Describe the bug**
If a job command line contains doublequote characters, they get stripped and thus a single parameter with spaces becomes multiple params.

For example,

`<command_line>-query query -outfmt ""6 qacc""</command_line>`

becomes this when executed, breaking the outfmt param:

`wrapper: running (APP) ( -query query -outfmt 6 qacc)`

**Expected behavior**
Quotes should be retained:

`wrapper: running (APP) ( -query query -outfmt ""6 qacc"")`

**System Information**
 - OS: Tested on Windows only.

**Additional context**
Single quotes get retained, but Windows does not handle single quotes as a single param.
","Manually escaping with `\""` works, but I do not see why there should be a need for any escaping of the command line there. The wrapper should do it by itself.",30260406
598,Client should reject file downloads with empty download URLs,open,2020-01-16T16:27:19Z,2020-01-17T08:57:11Z,,CONTRIBUTOR,"**Describe the bug**
If a file contains an empty download_url field, the download never gets completed. Instead, the file gets stuck in the download queue with a transient download error on each attempt.

**Expected behavior**
Such downloads should get rejected immediately with a permanent download error.

**Additional context**
I don't think it is possible for a standard work generator to generate such a file record. Nevertheless, custom generators like mine can.",I also sent it to you in Slack,30260406
599,Server Stable release 1.2: internal server errors ...,open,2020-01-08T11:56:31Z,2020-01-08T13:09:11Z,,CONTRIBUTOR,"**Describe the bug**
... when clients request work using the 'Anonymous Platform' mechanism.

**Steps To Reproduce**
1. Create and activate an app_info.xml file
2. Request work from a project reporting 'Server version 715'.
Internal server errors observed at SETI@Home, LHC, LHC-dev

**System Information**
 - OS: Windows 7, Linux Mint
 - BOINC Version: 7.16.3, 7.16.1

**Additional context**
This problem came to light following a server upgrade at SETI@Home on 20 December 2019: existing users with existing, working, app_info.xml files were unable to fetch new work. I reproduced the problem on 22 December at LHC@Home, and reported it by email to @lfield (server release manager).

Further testing at LHC-dev with Laurence and Nils Høimyr has generated the attached debug log. Nils' interpretation is that the scheduler is entering an endless loop and crashes when out of memory.

Nils reports that he has tried to reproduce a stack trace as suggested in https://github.com/BOINC/boinc/issues/1994#issuecomment-318261851, but that the gdb 'trick' ""does not work in our environment"".

[LHC-dev debug log.zip](https://github.com/BOINC/boinc/files/4035061/LHC-dev.debug.log.zip)

---
I would like to suggest that anonymous platform testing is made a formal part of the Server Release process. This is not the first time that an error of this type has slipped through the net: see #194, #690. In the latter case, I remember both @SETIguy and @bema-aei wasting several days each trying to solve their respective problems at the project level, before @JuhaSointusalo tied the threads together and provided the (common) solution.

In the current case, too, the problem was reported in May 2019 as an LHC project-level problem, but not followed through:
https://lhcathome.cern.ch/lhcathome/forum_thread.php?id=5034
https://lhcathomedev.cern.ch/lhcathome-dev/forum_thread.php?id=473

I understand that it has also been reported at SETI Beta, but with that server currently being down because of hardware issues, I can't provide a reference.",,30260406
600,BOINC Roadmap 2020,open,2020-01-07T10:20:14Z,2020-03-24T21:38:43Z,,MEMBER,"Reference: [BOINC Roadmap 2019](https://docs.google.com/document/d/1vy36Bvyj4MIqNpfqXHcIQ8xjpxuPpECgqul9BvStvis/edit)

#### Android 10 problem
The current BOINC client doesn’t work on Android 10 because of changes to background process mechanisms.

Related issues:
- [ ] #3334: [Android] 7.16.3, Another volunteer app is already running on this device
- [ ] #3333: [Android] 7.16.3, doesn't run, always suspended - on batteries
- [ ] #2467: BOINC on OnePlus 5 Android 8 stops running by itself after 10 minutes
- [ ] #2211: user-reported Android issue

#### Lower VM process priority
Make VMs run at low process priority.  VBox used to have this feature, but now it doesn’t.  Either get them to restore it, or make our own fork of VBox.
This is critical.  Without VM apps, setting up projects is too hard.  Without low priority, VMs degrade computer performance unacceptably.

Related issues:
- [ ] #2785: Run VM apps at low process priority
- [ ] #2273: Windows VirtualBox Process Priority support

#### Eliminate GPU streaming glitches
Goal: eliminate glitches in streaming video playback because of GPU usage

Related issues:
- [ ] #2301: Don't use GPU while watching movie

#### Fan noise
Goal: avoid excessive fan noise

Related issues:
- [ ] #2215: Throttle computing based on fan speeds
- [ ] #1378: CPU temperatures monitoring
- [ ] #1379: GPU Temperature Monitoring
- [ ] #1474: BOINC Monitoring daemon
",,30260406
601,Build: libnotify required even though --disable-manager is set,open,2020-01-01T23:28:55Z,2020-01-23T18:06:11Z,,NONE,"**Describe the bug**

Building the BOINC client without the manager fails because of missing libnotify.

**Steps To Reproduce**

Without libnotify available, run:

```
./_autosetup
./configure --disable-server --disable-manager
```
It fails with:

```
./configure: line 36044: syntax error near unexpected token `LIBNOTIFY,'
./configure: line 36044: ` PKG_CHECK_MODULES(LIBNOTIFY, libnotify)'
```

at `./configure` line 36044 there is:

```
if test ""${enable_manager}"" = yes ; then
    PKG_CHECK_MODULES(LIBNOTIFY, libnotify)
fi
```
It seems that the `--disable-manager` is not properly applied.

**Expected behavior**
Building the client without the manager shouldn't require libnotify since there's no desktop integration.

**Screenshots**
N/A

**System Information**
 - OS: linux x64
 - BOINC Version: 7.16.1

**Additional context**
I'd like to build the client only to run it on a server. To that end, I'm creating a [habitat](https://www.habitat.sh/) package of the boinc client.
","The package build is scripted, and here's the relevant part:

```
  ./_autosetup
  ./configure --disable-server --disable-manager --prefix=""${pkg_prefix}""
  make
```
It looks good to me :)",30260406
602,[Simple View] Ubuntu 19.10 Empty Computing Preferences screen,open,2020-01-01T02:42:56Z,2020-01-03T10:28:38Z,,NONE,"I've installed BOINC from the Software Package Manager and it runs / computes fine. The GUI theme however is pretty broken. The preferences page is 100% blank.

![Screenshot from 2020-01-01 12-39-14](https://user-images.githubusercontent.com/4373888/71637648-e9a5e700-2c93-11ea-82f4-c31191357e5f.png)
","Thanks @AenBleidd, that fixed it for me.",30260406
603,Transfer my BOINC iOS app to this repository?,open,2019-12-29T14:51:25Z,2020-01-02T10:39:32Z,,NONE,"What would it take to have my [BOINC iOS app repo](https://github.com/AustinConlon/BOINC-iOS-watchOS) transferred to this official repo? The thought came to mind after seeing the Android app here. This iOS app I started is a convenience for checking credits rather than running jobs on device, but I figure if the project gets more visibility from being in this repo instead then maybe more ambitious features might be collaboratively implemented.",@adamradocz my experience is in native Swift development so I'd rather stick to that given finite time. Thanks for mentioning it though.,30260406
604,Add: Daily Schedules from sunrise to sunset x/y hours delay for solar computing,open,2019-12-26T05:43:49Z,2020-02-19T02:05:22Z,,NONE,"**Describe the problem**
Add the option to use as much energy during the day. I am thinking of setting up a small solar array and use the extra energy for boinc. During the night, boinc should consume minimum energy similar to the sleep mode.

**Describe the solution you'd like**
The sunrise and sunset hours should be retrieved based on timezone, and the x hours should be the delay after sunrise while y hours should be the delay before sunset. The actual hours should be displayed in the window for the current day for reference.

**Additional context**
Option should be disabled by default. Android client should not be a priority.
","You can grab the sunrise/sunset data from` https://openweathermap.org/` for any city in the world for free.  Even has an API if needed.
`https://openweathermap.org/api`",30260406
605,[WIP] User option to exclude a task from being scheduled to run if it is simulated to likely miss its deadline,open,2019-12-24T18:17:21Z,2019-12-24T23:41:54Z,,NONE,"Fixes #2957 

**Description of the Change**
Whenever BOINC main() polls to generate the list of tasks to run - and the order in which to run them - any tasks that are at risk of slipping past its deadline (as simulated by BOINC) is excluded.

This is a WIP and simply adds the logic to exclude a task from being scheduled if the result is simulated to likely slip past the deadline. I will work on the user configuration option next, e.g. ""Exclude jobs if it is at risk of missing its deadline (Y/N)""?

**Alternate Designs**
The original Feature Request was to add an option to explicitly **abort** a task. In my view, this is a cleaner solution to the problem the Feature Request is looking to solve. It relies on the existing, underlying logic for scheduling, prioritizing, and clean-up of tasks, i.e. nothing is explicitly aborted, only excluded from the run list. Thus, the only change would be in the logic applied in cpu_sched->can_schedule().

**Release Notes**
Add user-configurable option to exclude tasks that are likely to miss their deadline from being scheduled to run.
","I don't know if this still true, but I seem to remember that the estimates for how long a task will run are often very wrong, especially when a project application is first deployed. I believe the server uses actual run times to refine its predictions for subsequent tasks it sends later. 

I believe there are also some projects where it is impossible to accurately predict run time because of variations in the input data. So I wonder if this would prevent tasks from running that would actually finish on time, thus defeating BOINC's method for refining its estimates.

I also have a concern that advanced users may be a tiny minority of BOINC users; most just want to install BOINC and let it run by itself. I worry that as we add more options for advanced users, the complexity will scare away more users. I have long felt that the path to getting more volunteer users is to simplify BOINC by eliminating options, rather than adding more. )I feel the Simple View is not enough to take care of this, as there are too many things you can do only with the Advanced View.)
",30260406
606,Paste in BOINC does not work in Mac OS,open,2019-12-24T17:24:17Z,2020-01-02T11:47:36Z,,NONE,"I try to add a account manager in BOINC running on Mac OS and the paste (Command-V) does not work for pasting password.

Please make the paste function for account manager password to work.","Yes, that's possible because the command - c is absorbed by Cancel button",30260406
607,[CRITICAL] boinc daemon hangs if port TCP/6006 is not an X11 server.,open,2019-12-17T18:53:26Z,2020-01-02T11:46:56Z,,NONE,"**Describe the bug**
A clear and concise description of what the bug is.
Tilps says:
I got a complete strace.  ""Boinc"" happily doing things for a while - then it tries to look for a domain 
socket, doesn't find it - tries to connect to port 6000, no answer, tries to look for a different domain
 socket, doesn't find it, then port 6001 - it repeats this sequence until it gets to port 6006 - 
finds it can connect, and then hangs.
6006 is the port tensorboard uses
It appears to be searching for an x windows session
but since there isn't an x windows session it keeps searching until it hits the tensorboard port
so either we disable its need to try and find an x windows session for whatever reason - 
or have an x windows session for it to find ... or we reconfigure tensorboard to use non-default port number

BOINC is searching for an x windows session, and when it finds port 6006 is open, but not responding in the way it would expect from an x windows session, it hangs.


**Steps To Reproduce**
1. Run boinc daemon -- on a CLI-only server (no GUI, no X11)
2. Run TensorFlow (it uses port TCP/6006 by default) -- or any other software 
that uses port TCP/6006.
steps to reproduce (BOINC side):
```
# apt-get install boinc-client
cd /var/lib/boinc-client/
boinccmd --project_attach http://www.worldcommunitygrid.org/  $KEY
boinccmd --set_network_mode always
boinccmd --set_run_mode always
boinccmd --set_gpu_mode never
# service boinc-client restart
What actually happens ?
root@rampage-107:~# boinccmd --read_global_prefs_override
Operation failed: read() failed
root@rampage-107:~#
```
at this stage ""boinc"" daemon gets stuck, and no work units get processed anymore.

====================================


**Expected behavior**
A clear and concise description of what you expected to happen.
```
root@rampage-107:~# boinccmd --read_global_prefs_override
root@rampage-107:~#
boinccmd must run without errors.
```

**Screenshots**
If applicable, add screenshots to help explain your problem.

**System Information**
 - OS: Linux - Ubuntu 18.04 LTS
 - BOINC Version: root@rampage-107:~# boinc --version (boinc as supplied by Ubuntu)
7.9.3 x86_64-pc-linux-gnu


**Additional context**
Add any other context about the problem here.

In practice any Linux server (CLI only) running Deep Learning (TensorFlow) and BOINC -- 
boinc will get stuck after about 30 minutes or so...

this server has enough RAM memory and disk space, so those issues can be ruled out:
```
root@rampage-107:~# uptime
 00:31:18 up 44 days,  2:37, 16 users,  load average: 57.85, 59.09, 57.01

root@rampage-107:~# free -h
              total        used        free      shared  buff/cache   available
Mem:           125G         36G        882M        1.0G         88G         87G
Swap:          8.0G        100M        7.9G

root@rampage-107:~# df -h
Filesystem      Size  Used Avail Use% Mounted on
udev             63G     0   63G   0% /dev
tmpfs            13G  2.8M   13G   1% /run
/dev/sda2       916G  358G  512G  42% /
tmpfs            63G  100K   63G   1% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs            63G     0   63G   0% /sys/fs/cgroup
/dev/loop2       90M   90M     0 100% /snap/core/8039
tmpfs            13G     0   13G   0% /run/user/1000
/dev/loop0       90M   90M     0 100% /snap/core/8213
tmpfs            13G     0   13G   0% /run/user/0

```
-Technologov, 17.12.2019.","I would presume that there is a standard way to open an X port and, upon
success, check whether an X server is running there.

If there is a service that uses an X TCP port (6000:6063) that causes such
a check to hang, that service is broken.

So the question is, are we not detecting X properly, or is TensorFlow
broken?

On Tue, Dec 17, 2019 at 2:02 PM David Anderson <notifications@github.com>
wrote:

> Currently the client tries ports 6000..6006.
> In theory X is allocated 6000..6063:
>
> https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=x11
> Since there's already a reported non-X11 use of 6003, I propose changing
> the client
> to check only 6000..6002.
>
> BTW, does anyone know if X11-based idle detection even works?
> If not we may as well remove it.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/3405?email_source=notifications&email_token=ACS5ZMQEZCDIRKLANN3WJIDQZFEAXA5CNFSM4J3763KKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHEDLSI#issuecomment-566769097>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACS5ZMQO2YZR2EVEABV27S3QZFEAXANCNFSM4J3763KA>
> .
>


-- 
Eric Korpela
korpela@ssl.berkeley.edu
AST:7731^29u18e3
",30260406
608,Added Estimated Completion column to Advanced view > Tasks,open,2019-12-13T21:15:35Z,2020-01-13T08:53:18Z,,NONE,"Fixes #2115

**Description of the Change**
Added a new column -- ""Estimated Completion"" -- to the Advanced > Tasks screen to show the calculated date and time by which a task should complete. It is positioned immediately next to the existing ""Deadline"" column to make it easier for users to visually assess which task(s) may not complete by the deadline.

**Alternate Designs**
I opted to design ""estimated completion"" as a derived field. It is calculated as ""current date and time"" + ""remaining time"". Therefore, I opted to *not* add a new saved field to the data model for it.

Also, the original issue (#2115) had specified another new field to be added, to explicitly calculate and display the delta between the estimated completion date/time and the deadline date/time. However, it is my opinion that this additional field is not necessary and that having the estimated completion date/time displayed right next to the deadline date/time is enough information for a user to assess whether a task is likely to complete by the deadline, or not. Therefore, I opted to not implement this part of the issue for now.

**Release Notes**
Added ""Estimated Completion"" as a new column in Advanced view > Tasks to show the date and time by which a task should complete.
","# [Codecov](https://codecov.io/gh/BOINC/boinc/pull/3403?src=pr&el=h1) Report
> Merging [#3403](https://codecov.io/gh/BOINC/boinc/pull/3403?src=pr&el=desc) into [master](https://codecov.io/gh/BOINC/boinc/commit/f2334800d42c6de73f6f101eedc06a56e1f4c243?src=pr&el=desc) will **not change** coverage.
> The diff coverage is `n/a`.

```diff
@@          Coverage Diff           @@
##           master   #3403   +/-   ##
======================================
  Coverage    9.27%   9.27%           
======================================
  Files          36      36           
  Lines        5993    5993           
======================================
  Hits          556     556           
  Misses       5437    5437
```

| [Impacted Files](https://codecov.io/gh/BOINC/boinc/pull/3403?src=pr&el=tree) | Coverage Δ | |
|---|---|---|
| [lib/md5\_file.cpp](https://codecov.io/gh/BOINC/boinc/pull/3403/diff?src=pr&el=tree#diff-bGliL21kNV9maWxlLmNwcA==) | `0% <0%> (ø)` | :arrow_up: |
",30260406
609,Android: BOINC v7.16.3 testing,open,2019-12-08T17:53:02Z,2020-03-12T12:12:16Z,,CONTRIBUTOR,"**Describe the bug**
Fails to fetch new work

Forked from #3357 

From attached Event log:

8 Dec 2019 17:08:24|Einstein@Home|
[work_fetch] set_request() for Mali-G71: ninst 1 nused_total 0.00 nidle_now 1.00 fetch share 1.00 req_inst 1.00 req_secs 47520.00

8 Dec 2019 17:08:24|Einstein@Home|
[work_fetch] request: CPU (0.00 sec, 0.00 inst) Mali-G71 (47520.00 sec, 1.00 inst)

Einstein project expects CPU request...

**System Information**
 - OS: Android v9
 - BOINC Version: 7.16.3

[Event log.txt](https://github.com/BOINC/boinc/files/3936644/Event.log.txt)
","> 
> 
> Tasks tab is already fixed in master

Please release new BETA version in Google Play! It's very important, I want to help with testing on Android 10, but I can't :(",30260406
610,Linux client: the OOBE for volunteers,open,2019-12-06T15:12:05Z,2020-01-02T11:44:55Z,,CONTRIBUTOR,"This is going to touch on two previous issues, and I hope provoke some thinking:

#3105: On Linux, the manager (user?) cannot start the core client as a service
#2993: Computing prefs 2.0

It arises from https://boinc.berkeley.edu/forum_thread.php?id=13284, which at first glance seems simple: how to free one (only) from two GPUs for daytime CAD use. Easy - we have a cc_config.xml option for this. Further issues arise, however.

1) This is Ubuntu 19.04, BOINC v7.14.2. cc_config.xml isn't in 'the data directory'. Find out where to put it (I told him to use the GUI).

2) cc_config.xml itself is in a read-only folder (/etc/boinc-client)  for ordinary users. However, it can be edited after creation through the sym-link in /var/lib/boinc-client

3) This particular option requires a client restart to activate.

We got through all that, but at the end of the day - how do you re-activate the reserved GPU? https://boinc.berkeley.edu/forum_thread.php?id=13285 - edit the read-only file again, restart the client again. It didn't take the first time - we suspect a finger-fumble stopping the client - but it did work eventually.

This particular use-case doesn't seem unreasonable: use one GPU while the machine is 'in use', use both GPUs when the machine is idle. It would also cover 'watch movie on one screen, check social media on the other'. Can we include it when we work up the cited issues, please?",,30260406
611,[Explained]systemd service not passing arguments to client,open,2019-12-05T19:05:23Z,2020-02-19T07:18:45Z,,NONE,"Arguments to a service, such as the boinc client, need to be put into an environmental file.  That file exists, but is not identified in the service.

in /lib/systemd/system/boinc-client.service make the following change:

from
```
WorkingDirectory=/var/lib/boinc
ExecStart=/usr/bin/boinc
ExecStop=/usr/bin/boinccmd --quit
```

to 
```
WorkingDirectory=/var/lib/boinc
ExecStart=/usr/bin/boinc $BOINC_OPTS
EnvironmentFile=/etc/default/boinc-client
ExecStop=/usr/bin/boinccmd --quit
```

the script /etc/init.d/boinc-client attempts to pass $BOINC_OPTS but fails.  The spec
https://wiki.archlinux.org/index.php/Systemd
calls for arguments to be in an environmental file.
If that init.d file is depreciated (so I have heard) then the /etc/default/boinc-client should not be depreciated as it is useful for passing arguments and should be modified to only have something like the following
```
# Here you can specify additional options to pass to the BOINC core client.
# Type 'boinc --help' or 'man boinc' for a full summary of allowed options.
#BOINC_OPTS=""--allow_remote_gui_rpc""
BOINC_OPTS=""""
```

Right now it has a lot of stuff used by the init.d script but it worked fine when I ran
```
systemctl start boinc-client
```
and the client accepted the arguments I had put it.","I setup a PR to avoid some issues with different .conf file paths for this.

We can get away with just specifying $BOINC_OPTS during ExecStart, in boinc-client.service.in

Then users can customize their own unit files like so:

```
# cat /etc/systemd/system/boinc-client.service.d/00gentoo.conf 
[Service]
Environment=""BOINC_OPTS=--allow_remote_gui_rpc""
```

Yields:
```
● boinc-client.service - Berkeley Open Infrastructure Network Computing Client
     Loaded: loaded (/usr/lib/systemd/system/boinc-client.service; enabled; vendor preset: disabled)
    Drop-In: /etc/systemd/system/boinc-client.service.d
             └─00gentoo.conf
     Active: active (running) since Wed 2020-02-19 02:15:07 EST; 3s ago
       Docs: man:boinc(1)
   Main PID: 195328 (boinc)
      Tasks: 8 (limit: 38427)
     Memory: 144.1M
        CPU: 6.254s
     CGroup: /system.slice/boinc-client.service
             ├─195328 /usr/bin/boinc --allow_remote_gui_rpc
...
```

Seems to work  :+1: ",30260406
612,Forum Moderation Fuction,open,2019-12-05T13:40:54Z,2020-01-02T10:54:06Z,,CONTRIBUTOR,"The forum moderation function is not working. It reports the following message. 
```
Your report could not be recorded. Please wait a while and try again.
If this is not a temporary error, please report it to the project developers.
```",Note that this code was touched in PR #3058 ,30260406
613,Stripcharts and Show/Grep Ops Functions,open,2019-12-05T13:26:36Z,2020-01-02T10:52:19Z,,CONTRIBUTOR,"On the BOINC admin web interface ops pages, there are a number of functions to debug applications. The main functions that query the database work ok, but there are some options like _Stripcharts_ and _Show/Grep logs_ that do not work. 

The reason why these functions are disabled is that the scripts do not run in a safe environment, and no longer are available from the Web since BOINC security updates a few years back. 

Is there is a safe way to offer this functionality or should they be removed from the admin web interface?","Well, you should not be able to execute system scripts from PHP.   The show_log.php page refers to a file `../bin/grep_logs` that is not available. This file is located under bin/ in the server directory, but is not (and should not be) exposed to the web server.  

There may be alternatives, such as a cgi to search in log files, or something else.  But given that this functionality has not worked for many years, maybe it is simpler to simply remove it.

Stripcharts refers to a non-existent cgi script:  `/stripchart.cgi`  One could add this to a protected cgi location, but currently it is not part of the server code.  Hence both these functions from the OPS pages are broken.",30260406
614,Feature crowdfunding,open,2019-12-02T10:43:58Z,2020-01-02T10:38:12Z,,MEMBER,"I think a program for crowdfunding features would be nice. There's a website [BountySource](https://www.bountysource.com/), where people can donate money for a feature, and anybody who develops that feature gets the bounty.

Thought?",See that over on the Gridcoin developer network.  Get rewarded for tackling a programming wanted feature or bugfix.,30260406
615,Moved max_cpus check,open,2019-12-02T10:10:47Z,2020-01-02T10:49:48Z,,CONTRIBUTOR,"Moving the max_cpus check to address #3386.

The max_cpus preference is used to set the maximum number of CPUs (threads) to be used by a VM application.","@davidpanderson, could you please review this? Thanks in advance",30260406
616,MAX_JOBS_PREF and MAX_CPUS_PREF not behaving as expect. ,open,2019-11-27T13:59:08Z,2020-01-02T10:50:19Z,,CONTRIBUTOR,"This is related to the MAX_JOBS_PREF and MAX_CPUS_PREF project preferences as added in this [commit](https://github.com/BOINC/boinc/commit/fb5b9d).

The current the behavior for a multi-threaded application is as follows (threads are cpus for the VM apps):
```
Max 1 CPU, Max 1 Job  = >  1 single threaded job
Max 2 CPU, Max 1 Job => 2 threaded job 
Max 1 CPU, Max 2 Job  => 1 single threaded job
Max 2 CPU, Max 2 Job => 2 x 2 threaded jobs
```
In practice Max CPUs is used to set the number of threads and hence CPUs to be used by a VM, hence Max 1 CPU, Max 2 Job  => 1 single threaded job does not function as wished. In this case it should run two single CPU jobs.","The number of CPUs used by a VM can be controlled if the if statement from sched_send is added to plan_class.cpp after the [max_threads](https://github.com/BOINC/boinc/blob/master/sched/plan_class_spec.cpp#L923) check. The remaining issue is that in one request if 3 ncpus are available and max_cpu = 2, two 2 CPU jobs are returned where as it should be one 2 CPU and one 1 CPU. If the requests are made separately the correct response is given. ",30260406
617,python scripts don't work with python2,open,2019-11-27T03:09:30Z,2020-01-02T10:45:40Z,,CONTRIBUTOR,The python3 changes should have been compatible with python2.,,30260406
618,BOINC Manager for macOS should be optimized for Retina displays,open,2019-11-26T07:58:39Z,2020-03-03T11:45:45Z,,NONE,"It's very blurry on modern Mac displays.

https://support.apple.com/en-us/HT202471","Reopening, because #3473 only fixes the rendering of text. Fixing graphic elements such as icons requires providing high-resolution artwork and additional changes.",30260406
619,"Unexpected side effect  when using ""--detach"" ",open,2019-11-14T21:51:40Z,2020-01-02T10:41:47Z,,NONE,"Conceivably, (yes, it is a stretch of imagination) one might use this as an exploit.

The program main.cpp has the following code to handle detachments.
```
argv[index] = ""-detach_phase_two"";
snprintf(commandLine, sizeof(commandLine), ""\""%s\"""", execpath);
  for (i = 1; i < argc; i++) {
        strlcat(commandLine, "" "", len);
        strlcat(commandLine, argv[i], len);
  }
```
However, argv[I] might be null as in (for example) 
```
--gui_rpc_port """" --detach
```
If it is null then the program executes with the new command line
```
--gui_rpc_port -detach_phase_two
```
No telling what happens then!
This effect occurs on any command line that has an operand.  If the operand is """" then detach_phase_two is used as the operand if ""--detach"" follows that operand

the following should work
```
for (i = 1; i < argc; i++) {
      strlcat(commandLine, "" "", len);
      if (strlen(argv[i]) == 0)
                strcat(commandLine, ""\""\"""");
      else strlcat(commandLine, argv[i], len);
 }
            
```
Originally I tried --set_password """" and got ""-detach_phase_two"" for the password in gui_rpc_authorize.cfg.   However, that affected only my ""special mod"" of the client to get a password added in.

Rather than use my suggested ""fix"" perhaps someone with better inside knowledge can look at the detach function especially in light of the ""FIXME FIXME"" in the comments just above the code. 
",,30260406
620,Slight Changes to HowToBuild rtf and pdf,open,2019-11-14T15:28:49Z,2019-11-14T15:31:06Z,,CONTRIBUTOR,"Fixes #

**Description of the Change**
Slight changes to the heading of the docs to initiate Pull

**Alternate Designs**
Couldn't see any alternatives as original commits don't seem to have amdeit

**Release Notes**
N/A
","@ShanghaiTimes, if you require assistance with git/github usage, please write me on Slack or Discord",30260406
621,Unable to connect to the core client (macOS Catalina 10.15.1),open,2019-11-06T21:24:58Z,2019-11-13T19:00:32Z,,NONE,"<img width=""547"" alt=""Screen Shot 2019-11-06 at 1 24 48 PM"" src=""https://user-images.githubusercontent.com/1305957/68339120-cfd39380-0098-11ea-8cb1-2ed636dd7baf.png"">
",We will be issuing a new version of BOINC for OS 10.15 Catalina soon.,30260406
622,runtime stats by application version,open,2019-10-30T21:51:06Z,2019-10-31T00:12:00Z,,NONE,"Fixes #3356

**Description of the Change**
Added these 4 fields to both the app_version table and the host_app_version table:
`runtime_n`
`runtime_avg`
`cputime_n`
`cputime_avg`
Modified validator to track the values.  Added the necessary function to db_update to make the database change.  Added average run time and cpu time columns to the apps.php page.

**Release Notes**
Adds fields to database to track average run times by application version.  Modifies apps.php to display the average run times and cpu times.
","We've got two projects that want this change.  I want it because the
scheduler is broken for GPU and multithreaded apps and the validator credit
is broken for multithreaded apps.  It's always been broken, and this is a
small step toward fixing that. Having suffered through hundreds of database
changes no project was calling for, I understand that inconvenience.


On Wed, Oct 30, 2019 at 3:50 PM David Anderson <notifications@github.com>
wrote:

> The whole thing. Additions to the DB affect all projects, and there needs
> to be a compelling case for them.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/pull/3362?email_source=notifications&email_token=ACS5ZMS4Q6LHY4N7EFZXPDTQRIFRRA5CNFSM4JHA2DGKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECWASQA#issuecomment-548145472>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACS5ZMVHJOOCIYDV76QNHETQRIFRRANCNFSM4JHA2DGA>
> .
>


-- 
Eric Korpela
korpela@ssl.berkeley.edu
AST:7731^29u18e3
",30260406
623,Manager: missing/inconsistent keyboard handlers,open,2019-10-30T11:21:50Z,2019-10-31T11:16:28Z,,CONTRIBUTOR,"**Describe the bug**
Keyboard shortcuts for moving between controls on the Manager's 'Advanced View' display are broken. User report from user with broken pointing device, in https://boinc.berkeley.edu/forum_thread.php?id=13193

**Steps To Reproduce**
Under Windows -
The TAB key moves focus forward through all active controls on a page (command buttons, list controls). It does **not** move focus to the tab label.
Shift-TAB moves focus back through all controls, **including** the tab label - but doesn't continue to re-start the loop.
Ctrl-TAB and Ctrl-Shift-TAB cycle though the notebook pages. If the movement was started with the focus on a tab label, it moves through all pages. If the movement was started with the focus on a different control, it reaches the Notices page and stops there. There's no way off the Notices page (unless it has focus) from the keyboard, except by going via the View menu.

Under Linux -
TAB and Shift-TAB cycle through all controls, including the tab label, and continue on to the next control in the loop.
Ctrl-TAB does nothing.

**System Information**
 - OS: Windows 7 and 10: Linux Mint, Cinnamon desktop: I don't have a Mac
 - BOINC Version: up to and including v7.16.3

**Additional context**
This is poor design for users without the manual dexterity to use a precision pointing device. It also hinders users with a broken or missing pointing device.",,30260406
624,runtime stats for application versions,open,2019-10-29T04:51:24Z,2019-10-31T11:10:56Z,,NONE,"It would be nice to keep track of average run times and cpu times for each application version in the database.

This information could then be displayed directly in apps.php (and possibly other places).  This would give desirable information for both project developers and users.  For example, you could compare average run times between 32 bit app versions and 64 bit app versions; or between a CPU version and a GPU version.

To implement this, 4 new fields would need to be added to both the app_version table and the host_app_version table:
     `runtime_n`
     `runtime_avg`
     `cputime_n`
     `cputime_avg`
These would then be tracked in the validator.  Impact to server performance would be negligible.

SETIguy created a branch called runtime_stats, where the above solution was implemented as a proof of concept.  I have been using it for several months in a private test server and it works very well.  Here is a screenshot showing the runtimes in apps.php:
![appVersionsScreenshot](https://user-images.githubusercontent.com/46617583/67738682-da24cc00-f9cc-11e9-943f-73e9fe42eec2.png)
",@SETIguy - try #2949,30260406
625,[Explained] 7.16.3 systemd startup file prevents LHC VirtualBox jobs running,open,2019-10-28T21:18:17Z,2019-11-07T14:34:48Z,,NONE,"**Describe the bug**
I tried an update to 7.16.3, but it produced an oddity, so I switched back to 7.14.2
But I left the 7.16.3 systemd start-up file in place.

The result was that an LHC CMS task started, but then stalled.
It ran for ~6mins (during which the VirtualBox process wasn't consuming CPU) then this showed up in the log:
```
Oct 28 20:03:53 benuc boinc[16837]: 28-Oct-2019 20:03:53 [LHC@home] Task CMS_1316885_1572237828.669283_0 postponed for 86400 seconds: Communication with VM Hypervisor failed.
```

Re-installing the 7.14.2 start-up file and re-starting BOINC (no other change) has allowed the job to run.

**Steps To Reproduce**
1. Try running an LHC CMS job using teh 7.16.3 start-up file.

**Expected behavior**
I'd expect jobs to be able to run successfully.

**System Information**
 - OS: Kubuntu 19.10
 - BOINC Version: 7.16.3 (start-up file only)

**Additional context**
I'll post the working and non-working systemd start-up files in the follow-ups.
","> FYI: I've suspended a CMS job on the system until I have time to start BOINC up with various options from the 3 additions missing, to discover which is/are causing the problem.

I switched BOINC to 7.16.3 (although the version doesn't actually matter) and started BOINC with various of the 3 options enabled.

The `stderr.txt` for the job (which is accumulative) is pasted at the end.

18:43:04 start is with all 3 additions commented out. Starts OK.
18:47:42 start is with PrivateTemp enabled. Starts OK.
18:49:35 start is with ProtectSystem also enabled. This fails. vboxwrapper starts, but after 3 mins it fails with ""Error in guest additions for VM"".
18:55:08 is with ProtectSystem disabled, but PrivateTemp and ProtectControlGroups anabled. Starts OK.

So the problem is with `ProtectSystem=strict` being set.

Here's the `stderr.txt` file covering these starts.
```
2019-10-30 18:43:04 (19070): vboxwrapper (7.7.26196): starting
2019-10-30 18:43:04 (19070): Feature: Checkpoint interval offset (251 seconds)
2019-10-30 18:43:04 (19070): Detected: VirtualBox VboxManage Interface (Version: 6.0.14)
2019-10-30 18:43:04 (19070): Detected: Minimum checkpoint interval (600.000000 seconds)
2019-10-30 18:43:04 (19070): Detected: Heartbeat check (file: 'heartbeat' every 1200.000000 seconds)
2019-10-30 18:43:04 (19070): Successfully copied 'init_data.xml' to the shared directory.
2019-10-30 18:43:04 (19070): Create VM. (boinc_deaaf2f10e33db57, slot#5)
2019-10-30 18:43:05 (19070): Setting Memory Size for VM. (2048MB)
2019-10-30 18:43:05 (19070): Setting CPU Count for VM. (1)
2019-10-30 18:43:05 (19070): Setting Chipset Options for VM.
2019-10-30 18:43:05 (19070): Setting Boot Options for VM.
2019-10-30 18:43:05 (19070): Setting Network Configuration for NAT.
2019-10-30 18:43:05 (19070): Enabling VM Network Access.
2019-10-30 18:43:05 (19070): Disabling USB Support for VM.
2019-10-30 18:43:05 (19070): Disabling COM Port Support for VM.
2019-10-30 18:43:05 (19070): Disabling LPT Port Support for VM.
2019-10-30 18:43:05 (19070): Disabling Audio Support for VM.
2019-10-30 18:43:05 (19070): Disabling Clipboard Support for VM.
2019-10-30 18:43:06 (19070): Disabling Drag and Drop Support for VM.
2019-10-30 18:43:06 (19070): Adding storage controller(s) to VM.
2019-10-30 18:43:06 (19070): Adding virtual disk drive to VM. (vm_image.vdi)
2019-10-30 18:43:06 (19070): Adding VirtualBox Guest Additions to VM.
2019-10-30 18:43:06 (19070): Adding network bandwidth throttle group to VM. (Defaulting to 1024GB)
2019-10-30 18:43:06 (19070): forwarding host port 45653 to guest port 80
2019-10-30 18:43:06 (19070): Enabling remote desktop for VM.
2019-10-30 18:43:06 (19070): Enabling shared directory for VM.
2019-10-30 18:43:07 (19070): Starting VM. (boinc_deaaf2f10e33db57, slot#5)
2019-10-30 18:43:08 (19070): Successfully started VM. (PID = '19555')
2019-10-30 18:43:08 (19070): Reporting VM Process ID to BOINC.
2019-10-30 18:43:08 (19070): Guest Log: BIOS: VirtualBox 6.0.14
2019-10-30 18:43:08 (19070): Guest Log: CPUID EDX: 0x178bfbff
2019-10-30 18:43:08 (19070): Guest Log: BIOS: ata0-0: PCHS=16383/16/63 LCHS=1024/255/63
2019-10-30 18:43:08 (19070): VM state change detected. (old = 'poweroff', new = 'running')
2019-10-30 18:43:08 (19070): Detected: Web Application Enabled (http://localhost:45653)
2019-10-30 18:43:08 (19070): Detected: Remote Desktop Enabled (localhost:59707)
2019-10-30 18:43:08 (19070): Preference change detected
2019-10-30 18:43:08 (19070): Setting CPU throttle for VM. (70%)
2019-10-30 18:43:08 (19070): Setting checkpoint interval to 600 seconds. (Higher value of (Preference: 600 seconds) or (Vbox_job.xml: 600 seconds))
2019-10-30 18:43:10 (19070): Guest Log: BIOS: Boot : bseqnr=1, bootseq=0032
2019-10-30 18:43:10 (19070): Guest Log: BIOS: Booting from Hard Disk...
2019-10-30 18:43:13 (19070): Guest Log: BIOS: KBD: unsupported int 16h function 03
2019-10-30 18:43:13 (19070): Guest Log: BIOS: AX=0305 BX=0000 CX=0000 DX=0000 
2019-10-30 18:43:31 (19070): Guest Log: vgdrvHeartbeatInit: Setting up heartbeat to trigger every 2000 milliseconds
2019-10-30 18:43:31 (19070): Guest Log: vboxguest: misc device minor 56, IRQ 20, I/O port d020, MMIO at 00000000f0400000 (size 0x400000)
2019-10-30 18:43:56 (19070): Guest Log: VBoxService 5.2.6 r120293 (verbosity: 0) linux.amd64 (Jan 15 2018 14:51:00) release log
2019-10-30 18:43:56 (19070): Guest Log: 00:00:00.000123 main     Log opened 2019-10-30T18:43:55.132615000Z
2019-10-30 18:43:56 (19070): Guest Log: 00:00:00.000345 main     OS Product: Linux
2019-10-30 18:43:56 (19070): Guest Log: 00:00:00.000376 main     OS Release: 4.14.44-11.cernvm.x86_64
2019-10-30 18:43:56 (19070): Guest Log: 00:00:00.000391 main     OS Version: #1 SMP Mon May 28 16:49:00 CEST 2018
2019-10-30 18:43:56 (19070): Guest Log: 00:00:00.000404 main     Executable: /usr/share/vboxguest52/usr/sbin/VBoxService
2019-10-30 18:43:56 (19070): Guest Log: 00:00:00.000405 main     Process ID: 2948
2019-10-30 18:43:56 (19070): Guest Log: 00:00:00.000406 main     Package type: LINUX_64BITS_GENERIC
2019-10-30 18:43:56 (19070): Guest Log: 00:00:00.000921 main     5.2.6 r120293 started. Verbose level = 0
2019-10-30 18:44:15 (19070): Guest Log: [INFO] Mounting the shared directory
2019-10-30 18:44:15 (19070): Guest Log: [INFO] Shared directory mounted, enabling vboxmonitor
2019-10-30 18:44:15 (19070): Guest Log: [DEBUG] Testing network connection to cern.ch on port 80
2019-10-30 18:44:15 (19070): Guest Log: [DEBUG] Connection to cern.ch 80 port [tcp/http] succeeded!
2019-10-30 18:44:15 (19070): Guest Log: [DEBUG] 0
2019-10-30 18:44:15 (19070): Guest Log: [DEBUG] Testing VCCS connection to vccs.cern.ch on port 443
2019-10-30 18:44:15 (19070): Guest Log: [DEBUG] Connection to vccs.cern.ch 443 port [tcp/https] succeeded!
2019-10-30 18:44:15 (19070): Guest Log: [DEBUG] 0
2019-10-30 18:44:15 (19070): Guest Log: [DEBUG] Testing connection to Condor server on port 9618
2019-10-30 18:44:15 (19070): Guest Log: [DEBUG] Connection to vccondor01.cern.ch 9618 port [tcp/condor] succeeded!
2019-10-30 18:44:15 (19070): Guest Log: [DEBUG] 0
2019-10-30 18:45:19 (19070): Guest Log: [DEBUG] Probing CVMFS ...
2019-10-30 18:45:20 (19070): Guest Log: Probing /cvmfs/grid.cern.ch... OK
2019-10-30 18:45:20 (19070): Guest Log: VERSION PID UPTIME(M) MEM(K) REVISION EXPIRES(M) NOCATALOGS CACHEUSE(K) CACHEMAX(K) NOFDUSE NOFDMAX NOIOERR NOOPEN HITRATE(%) RX(K) SPEED(K/S) HOST PROXY ONLINE
2019-10-30 18:45:20 (19070): Guest Log: 2.4.4.0 3691 1 25840 10619 3 1 1459451 10240001 2 65024 0 3 100 0 0 http://s1cern-cvmfs.openhtc.io/cvmfs/grid.cern.ch DIRECT 1
2019-10-30 18:45:27 (19070): Guest Log: [INFO] Reading volunteer information
2019-10-30 18:45:27 (19070): Guest Log: [INFO] Volunteer: Gordon Lack (578185)
2019-10-30 18:45:28 (19070): Guest Log: [INFO] VMID: c5ab4400-4ef6-44a9-aaed-bee78432355d
2019-10-30 18:45:28 (19070): Guest Log: [INFO] Requesting an X509 credential from LHC@home
2019-10-30 18:45:28 (19070): Guest Log: [INFO] Running the fast benchmark.
2019-10-30 18:46:03 (19070): Guest Log: [INFO] Machine performance 14.04 HEPSPEC06
2019-10-30 18:46:03 (19070): Guest Log: [INFO] CMS application starting. Check log files.
2019-10-30 18:46:04 (19070): Guest Log: [DEBUG] HTCondor ping
2019-10-30 18:46:05 (19070): Guest Log: [DEBUG] 0
2019-10-30 18:47:42 (22831): vboxwrapper (7.7.26196): starting
2019-10-30 18:47:42 (22831): Feature: Checkpoint interval offset (2 seconds)
2019-10-30 18:47:42 (22831): Detected: VirtualBox VboxManage Interface (Version: 6.0.14)
2019-10-30 18:47:43 (22831): Detected: Minimum checkpoint interval (600.000000 seconds)
2019-10-30 18:47:43 (22831): Detected: Heartbeat check (file: 'heartbeat' every 1200.000000 seconds)
2019-10-30 18:47:43 (22831): Guest Log: BIOS: VirtualBox 6.0.14
2019-10-30 18:47:43 (22831): Guest Log: CPUID EDX: 0x178bfbff
2019-10-30 18:47:43 (22831): Guest Log: BIOS: ata0-0: PCHS=16383/16/63 LCHS=1024/255/63
2019-10-30 18:47:43 (22831): Guest Log: BIOS: Boot : bseqnr=1, bootseq=0032
2019-10-30 18:47:43 (22831): Guest Log: BIOS: Booting from Hard Disk...
2019-10-30 18:47:43 (22831): Guest Log: BIOS: KBD: unsupported int 16h function 03
2019-10-30 18:47:43 (22831): Guest Log: BIOS: AX=0305 BX=0000 CX=0000 DX=0000 
2019-10-30 18:47:43 (22831): Guest Log: vgdrvHeartbeatInit: Setting up heartbeat to trigger every 2000 milliseconds
2019-10-30 18:47:43 (22831): Guest Log: vboxguest: misc device minor 56, IRQ 20, I/O port d020, MMIO at 00000000f0400000 (size 0x400000)
2019-10-30 18:47:43 (22831): Guest Log: VBoxService 5.2.6 r120293 (verbosity: 0) linux.amd64 (Jan 15 2018 14:51:00) release log
2019-10-30 18:47:43 (22831): Guest Log: 00:00:00.000123 main     Log opened 2019-10-30T18:43:55.132615000Z
2019-10-30 18:47:43 (22831): Guest Log: 00:00:00.000345 main     OS Product: Linux
2019-10-30 18:47:43 (22831): Guest Log: 00:00:00.000376 main     OS Release: 4.14.44-11.cernvm.x86_64
2019-10-30 18:47:43 (22831): Guest Log: 00:00:00.000391 main     OS Version: #1 SMP Mon May 28 16:49:00 CEST 2018
2019-10-30 18:47:43 (22831): Guest Log: 00:00:00.000404 main     Executable: /usr/share/vboxguest52/usr/sbin/VBoxService
2019-10-30 18:47:43 (22831): Guest Log: 00:00:00.000405 main     Process ID: 2948
2019-10-30 18:47:43 (22831): Guest Log: 00:00:00.000406 main     Package type: LINUX_64BITS_GENERIC
2019-10-30 18:47:43 (22831): Guest Log: 00:00:00.000921 main     5.2.6 r120293 started. Verbose level = 0
2019-10-30 18:47:43 (22831): Guest Log: [INFO] Mounting the shared directory
2019-10-30 18:47:43 (22831): Guest Log: [INFO] Shared directory mounted, enabling vboxmonitor
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] Testing network connection to cern.ch on port 80
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] Connection to cern.ch 80 port [tcp/http] succeeded!
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] 0
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] Testing VCCS connection to vccs.cern.ch on port 443
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] Connection to vccs.cern.ch 443 port [tcp/https] succeeded!
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] 0
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] Testing connection to Condor server on port 9618
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] Connection to vccondor01.cern.ch 9618 port [tcp/condor] succeeded!
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] 0
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] Probing CVMFS ...
2019-10-30 18:47:43 (22831): Guest Log: Probing /cvmfs/grid.cern.ch... OK
2019-10-30 18:47:43 (22831): Guest Log: VERSION PID UPTIME(M) MEM(K) REVISION EXPIRES(M) NOCATALOGS CACHEUSE(K) CACHEMAX(K) NOFDUSE NOFDMAX NOIOERR NOOPEN HITRATE(%) RX(K) SPEED(K/S) HOST PROXY ONLINE
2019-10-30 18:47:43 (22831): Guest Log: 2.4.4.0 3691 1 25840 10619 3 1 1459451 10240001 2 65024 0 3 100 0 0 http://s1cern-cvmfs.openhtc.io/cvmfs/grid.cern.ch DIRECT 1
2019-10-30 18:47:43 (22831): Guest Log: [INFO] Reading volunteer information
2019-10-30 18:47:43 (22831): Guest Log: [INFO] Volunteer: Gordon Lack (578185)
2019-10-30 18:47:43 (22831): Guest Log: [INFO] VMID: c5ab4400-4ef6-44a9-aaed-bee78432355d
2019-10-30 18:47:43 (22831): Guest Log: [INFO] Requesting an X509 credential from LHC@home
2019-10-30 18:47:43 (22831): Guest Log: [INFO] Running the fast benchmark.
2019-10-30 18:47:43 (22831): Guest Log: [INFO] Machine performance 14.04 HEPSPEC06
2019-10-30 18:47:43 (22831): Guest Log: [INFO] CMS application starting. Check log files.
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] HTCondor ping
2019-10-30 18:47:43 (22831): Guest Log: [DEBUG] 0
2019-10-30 18:47:43 (22831): Starting VM. (boinc_deaaf2f10e33db57, slot#5)
2019-10-30 18:47:44 (22831): Successfully started VM. (PID = '22958')
2019-10-30 18:47:44 (22831): Reporting VM Process ID to BOINC.
2019-10-30 18:47:44 (22831): VM state change detected. (old = 'poweroff', new = 'running')
2019-10-30 18:47:44 (22831): Detected: Web Application Enabled (http://localhost:45653)
2019-10-30 18:47:44 (22831): Detected: Remote Desktop Enabled (localhost:59707)
2019-10-30 18:47:44 (22831): Preference change detected
2019-10-30 18:47:44 (22831): Setting CPU throttle for VM. (70%)
2019-10-30 18:47:44 (22831): Setting checkpoint interval to 600 seconds. (Higher value of (Preference: 600 seconds) or (Vbox_job.xml: 600 seconds))
2019-10-30 18:49:35 (24352): vboxwrapper (7.7.26196): starting
2019-10-30 18:52:41 (24352): Error in guest additions for VM: -2142830585
Command:
VBoxManage -q list systemproperties
Output:
VBoxManage: error: Failed to create the VirtualBox object!
VBoxManage: error: Code NS_BASE_STREAM_WOULD_BLOCK (0x80470007) - Stream operation would block (extended info not available)
VBoxManage: error: Most likely, the VirtualBox COM server is not running or failed to start.

2019-10-30 18:52:41 (24352): Feature: Checkpoint interval offset (114 seconds)
2019-10-30 18:52:41 (24352): Detected: VirtualBox VboxManage Interface (Version: 6.0.14)
2019-10-30 18:55:08 (24736): vboxwrapper (7.7.26196): starting
2019-10-30 18:55:08 (24736): Feature: Checkpoint interval offset (355 seconds)
2019-10-30 18:55:08 (24736): Detected: VirtualBox VboxManage Interface (Version: 6.0.14)
2019-10-30 18:55:08 (24736): Detected: Minimum checkpoint interval (600.000000 seconds)
2019-10-30 18:55:08 (24736): Detected: Heartbeat check (file: 'heartbeat' every 1200.000000 seconds)
2019-10-30 18:55:08 (24736): Guest Log: BIOS: VirtualBox 6.0.14
2019-10-30 18:55:08 (24736): Guest Log: CPUID EDX: 0x178bfbff
2019-10-30 18:55:08 (24736): Guest Log: BIOS: ata0-0: PCHS=16383/16/63 LCHS=1024/255/63
2019-10-30 18:55:08 (24736): Guest Log: BIOS: Boot : bseqnr=1, bootseq=0032
2019-10-30 18:55:08 (24736): Guest Log: BIOS: Booting from Hard Disk...
2019-10-30 18:55:08 (24736): Guest Log: BIOS: KBD: unsupported int 16h function 03
2019-10-30 18:55:08 (24736): Guest Log: BIOS: AX=0305 BX=0000 CX=0000 DX=0000 
2019-10-30 18:55:08 (24736): Guest Log: vgdrvHeartbeatInit: Setting up heartbeat to trigger every 2000 milliseconds
2019-10-30 18:55:08 (24736): Guest Log: vboxguest: misc device minor 56, IRQ 20, I/O port d020, MMIO at 00000000f0400000 (size 0x400000)
2019-10-30 18:55:08 (24736): Guest Log: VBoxService 5.2.6 r120293 (verbosity: 0) linux.amd64 (Jan 15 2018 14:51:00) release log
2019-10-30 18:55:08 (24736): Guest Log: 00:00:00.000125 main     Log opened 2019-10-30T18:48:26.132457000Z
2019-10-30 18:55:08 (24736): Guest Log: 00:00:00.000258 main     OS Product: Linux
2019-10-30 18:55:08 (24736): Guest Log: 00:00:00.000286 main     OS Release: 4.14.44-11.cernvm.x86_64
2019-10-30 18:55:08 (24736): Guest Log: 00:00:00.000301 main     OS Version: #1 SMP Mon May 28 16:49:00 CEST 2018
2019-10-30 18:55:08 (24736): Guest Log: 00:00:00.000337 main     Executable: /usr/share/vboxguest52/usr/sbin/VBoxService
2019-10-30 18:55:08 (24736): Guest Log: 00:00:00.000338 main     Process ID: 2902
2019-10-30 18:55:08 (24736): Guest Log: 00:00:00.000338 main     Package type: LINUX_64BITS_GENERIC
2019-10-30 18:55:08 (24736): Guest Log: 00:00:00.001296 main     5.2.6 r120293 started. Verbose level = 0
2019-10-30 18:55:08 (24736): Guest Log: [INFO] Mounting the shared directory
2019-10-30 18:55:08 (24736): Guest Log: [INFO] Shared directory mounted, enabling vboxmonitor
2019-10-30 18:55:08 (24736): Guest Log: [DEBUG] Testing network connection to cern.ch on port 80
2019-10-30 18:55:08 (24736): Guest Log: [DEBUG] Connection to cern.ch 80 port [tcp/http] succeeded!
2019-10-30 18:55:08 (24736): Guest Log: [DEBUG] 0
2019-10-30 18:55:08 (24736): Guest Log: [DEBUG] Testing VCCS connection to vccs.cern.ch on port 443
2019-10-30 18:55:08 (24736): Guest Log: [DEBUG] Connection to vccs.cern.ch 443 port [tcp/https] succeeded!
2019-10-30 18:55:08 (24736): Guest Log: [DEBUG] 0
2019-10-30 18:55:08 (24736): Guest Log: [DEBUG] Testing connection to Condor server on port 9618
2019-10-30 18:55:08 (24736): Guest Log: [DEBUG] Connection to vccondor01.cern.ch 9618 port [tcp/condor] succeeded!
2019-10-30 18:55:08 (24736): Guest Log: [DEBUG] 0
2019-10-30 18:55:08 (24736): Starting VM. (boinc_deaaf2f10e33db57, slot#5)
2019-10-30 18:55:09 (24736): Successfully started VM. (PID = '24863')
2019-10-30 18:55:09 (24736): Reporting VM Process ID to BOINC.
2019-10-30 18:55:10 (24736): VM state change detected. (old = 'poweroff', new = 'running')
2019-10-30 18:55:10 (24736): Detected: Web Application Enabled (http://localhost:45653)
2019-10-30 18:55:10 (24736): Detected: Remote Desktop Enabled (localhost:59707)
2019-10-30 18:55:10 (24736): Preference change detected
2019-10-30 18:55:10 (24736): Setting CPU throttle for VM. (70%)
2019-10-30 18:55:10 (24736): Setting checkpoint interval to 600 seconds. (Higher value of (Preference: 600 seconds) or (Vbox_job.xml: 600 seconds))
```",30260406
626,[Explained] 7.16.3 CPU benchmarks are incorrect,open,2019-10-28T21:05:48Z,2019-11-01T14:10:35Z,,NONE,"**Describe the bug**
The CPU benchmark reported by 7.16.3 are significantly lower figures than 7.14.2

**Steps To Reproduce**
1. Start 7.16.3
2. Run CPU benchmarks (look in log)
3. Start 7.14.2
4. Run CPU benchmarks (look in log)

**Expected behavior**
I'd expect the CPU benchmarks to be essentially the same between releases.

**Screenshots**

```
Start-up with 7.16.3:

Oct 26 12:01:49 benuc boinc[16216]: 26-Oct-2019 12:01:49 [---] Starting BOINC client version 7.16.3 for x86_64-pc-linux-gnu
   ...
Oct 26 12:01:49 benuc boinc[16216]: 26-Oct-2019 12:01:49 [---] Processor: 4 GenuineIntel Intel(R) Core(TM) i3-8109U CPU @ 3.00GHz [Family 6 Model 142 Stepping 10]
   ...
Oct 26 12:01:49 benuc boinc[16216]: 26-Oct-2019 12:01:49 [---] OS: Linux Ubuntu: Ubuntu 19.10 [5.3.0-19-generic|libc 2.30 (Ubuntu GLIBC 2.30-0ubuntu2)]
   ...
Oct 26 12:01:49 benuc boinc[16216]: 26-Oct-2019 12:01:49 [---] Running CPU benchmarks
Oct 26 12:01:49 benuc boinc[16216]: 26-Oct-2019 12:01:49 [---] Suspending computation - CPU benchmarks in progress
Oct 26 12:02:20 benuc boinc[16216]: 26-Oct-2019 12:02:20 [---] Benchmark results:
Oct 26 12:02:20 benuc boinc[16216]: 26-Oct-2019 12:02:20 [---]    Number of CPUs: 2
Oct 26 12:02:20 benuc boinc[16216]: 26-Oct-2019 12:02:20 [---]    3307 floating point MIPS (Whetstone) per CPU
Oct 26 12:02:20 benuc boinc[16216]: 26-Oct-2019 12:02:20 [---]    9960 integer MIPS (Dhrystone) per CPU
==========

Start-up with 7.14.2:

Oct 26 12:18:35 benuc boinc[17200]: 26-Oct-2019 12:18:35 [---] Starting BOINC client version 7.14.2 for x86_64-pc-linux-gnu
   ...
Oct 26 12:18:35 benuc boinc[17200]: 26-Oct-2019 12:18:35 [---] Processor: 4 GenuineIntel Intel(R) Core(TM) i3-8109U CPU @ 3.00GHz [Family 6 Model 142 Stepping 10]
   ...
Oct 26 12:18:35 benuc boinc[17200]: 26-Oct-2019 12:18:35 [---] OS: Linux Ubuntu: Ubuntu 19.10 [5.3.0-19-generic|libc 2.30 (Ubuntu GLIBC 2.30-0ubuntu2)]
   ...
Oct 26 12:18:35 benuc boinc[17200]: 26-Oct-2019 12:18:35 [---] Running CPU benchmarks
Oct 26 12:18:35 benuc boinc[17200]: 26-Oct-2019 12:18:35 [---] Suspending computation - CPU benchmarks in progress
Oct 26 12:19:06 benuc boinc[17200]: 26-Oct-2019 12:19:06 [---] Benchmark results:
Oct 26 12:19:06 benuc boinc[17200]: 26-Oct-2019 12:19:06 [---]    Number of CPUs: 2
Oct 26 12:19:06 benuc boinc[17200]: 26-Oct-2019 12:19:06 [---]    5659 floating point MIPS (Whetstone) per CPU
Oct 26 12:19:06 benuc boinc[17200]: 26-Oct-2019 12:19:06 [---]    26590 integer MIPS (Dhrystone) per CPU
==========
```


**System Information**
 - OS: Kubuntuu Linux 19.10
 - BOINC Version: 7.16.3 (and 7.14.2)

**Additional context**
In the past updates to BOINC have resulted in similar CPU benchmarks on a system.
","OK. I now know what is happening and hence have ""fixed"" it.
Although I'm not sure that the result, or rather the reason, is a good one.

I build my own BOINC code (so that I can have the same version across OS distributions, and also because several years ago I needed(wanted) a feature that was in the latest code, but not the standard OS version).

My 7.14.2 configure script included `CXXFLAGS=""-O3""`.

My 7.16.3 configure script included various options I found on the web site (--disable-silent-rules --enable-dynamic-client-linkage ...) and I omitted the -O3 option.

I've just rebuilt with the -O3 compilation option in place and:

- The executables and libraries are significantly smaller.
- The benchmarks are now back to what I normally see.

```
Thu 31 Oct 2019 11:34:12 GMT |  | Version change (7.14.2 -> 7.16.3)
...
Thu 31 Oct 2019 11:34:12 GMT |  | Running CPU benchmarks
Thu 31 Oct 2019 11:34:12 GMT |  | Suspending computation - CPU benchmarks in progress
Thu 31 Oct 2019 11:34:43 GMT |  | Benchmark results:
Thu 31 Oct 2019 11:34:43 GMT |  | Number of CPUs: 2
Thu 31 Oct 2019 11:34:43 GMT |  | 5710 floating point MIPS (Whetstone) per CPU
Thu 31 Oct 2019 11:34:43 GMT |  | 26614 integer MIPS (Dhrystone) per CPU
```

Whether CPU benchmarks should be **so** dependent on the compilation options of the Manager  code is something to wonder about...",30260406
627,BoincMGR cannot connect to Raspberry pi when boinc is running as a service.,open,2019-10-27T17:34:52Z,2019-10-31T11:08:39Z,,NONE,"**Describe the bug**

Remote computer cannot connect to raspberry pi if  boinc is started as a service. If I manually run the app from command line.. IE sudo boinc the remote computer can connect. I have enabled gui_rpc_dbg and when the started as a service the following message is in the event log.

 [gui_rpc] Local control only allowed

When started manually.

10/27/2019 9:48:55 AM |  | [gui_rpc] Remote control allowed
10/27/2019 9:48:55 AM |  | [gui_rpc] Listening on port 31416

10/27/2019 9:48:55 AM |  | Config: GUI RPCs allowed from:
10/27/2019 9:48:55 AM |  | 172.16.0.184



**Steps To Reproduce**
1. On Raspberry pi start boinc as a service. Ensure you have gui_rpc_auth.cfg and remote_hosts.cfg set up in /var/lib/boinc and is owned by boinc 
2. Run BoincManager on another computer and connect to Raspberry pi. It will not connect.
3. If you stop the service and run the boinc with sudo boinc. It will work.

**Expected behavior**
The Remote Computer should be able to connect to the Raspberry Pi Boinc when started as a service.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**System Information**
 - OS: Linux Raspbian Raspbian GNU/Linux 10 (buster) [4.19.75-v7l+|libc 2.28 (Debian GLIBC 2.28-10+rpi1)]
 - BOINC Version: 7.14.2

**Additional context**

I have another installation of boinc on fedora 30 release and has no issues connecting when a remote computer connect.

I compared  the boinc-client.service files from the pi and fedora 30 release and they are the same.
","My `gui_rpc_auth.cfg` setting is:
```
-rw-r----- 1 boinc boinc  1 Aug  6  2012 gui_rpc_auth.cfg
```
(I don't have a password as this is only accessible from my home network).

So why you just adding group-read access to an account that can _already_ read it (since world-read is clearly not required) may remain a mystery,
",30260406
628,Short filename for win_build is not always win_bu~1 causing vs2013 build failure,open,2019-10-22T23:46:13Z,2020-01-02T13:33:58Z,,NONE,"When building boinc the build tool ""buildenv.cmd"" fails when the source code resides on network attached storage (for example: Synology).  That build tool contains the line
`SET BUILDROOT=%BUILDROOT:\win_bu~1\=%`

That works only if win_build is shortened to ```win_bu~1```
which is usually, but not always, the case.  In addition, an error in the script contributes to the problem.

The following test was run at ""z:\s9000\boinc\win_build"" on a win 10 system

8dot3name shows that 8.3 naming is enabled (the ""2"").
the first ""find"" shows that ```win_bu~1``` is in the build tool.
Note that the build tool fails.
The directory listing shows the name was actually shorted to WOH7UM~G
the second ""find"" shows that the tool was edited and corrected
Note that the build succeeds and the vs2013 eventual builds correctly.

```
Z:\s9000\boinc\win_build>fsutil 8dot3name query
The registry state is: 2 (Per volume setting - the default).

Z:\s9000\boinc\win_build>find ""SET BUILDROOT=%BUILDROOT"" *.cmd

---------- BUILDENV.CMD
SET BUILDROOT=%BUILDROOT:\win_bu~1\=%

Z:\s9000\boinc\win_build>

Z:\s9000\boinc\win_build>buildenv type release platform x64
Initializing BOINC Build Environment for Windows
Software Platform Detected: Visual Studio 2013
Software NOT Detected: Build Tools...

Z:\s9000\boinc\win_build>dir /x  ..\win*
 Directory of Z:\s9000\boinc
10/22/2019  06:03 PM    <DIR>          WOH7UM~G     win_build

Z:\s9000\boinc\win_build>

Z:\s9000\boinc\win_build>notepad buildenv.cmd

Z:\s9000\boinc\win_build>find ""SET BUILDROOT=%BUILDROOT"" *.cmd

---------- BUILDENV.CMD
SET BUILDROOT=%BUILDROOT:\WOH7UM~G\=%

Z:\s9000\boinc\win_build>buildenv type release platform x64
Initializing BOINC Build Environment for Windows
Software Platform Detected: Visual Studio 2013

Z:\s9000\boinc\win_build>dir *.sln
 Volume in drive Z is Archives
 Volume Serial Number is 2A24-E5D4

 Directory of Z:\s9000\boinc\win_build

10/22/2019  05:04 PM            48,191 boinc_vs2010.sln
10/22/2019  05:04 PM            46,880 boinc_vs2013.sln
10/22/2019  05:04 PM            23,270 boinc_vs2013_ibm.sln
10/22/2019  05:04 PM            56,731 wcg_vs2010.sln
               4 File(s)        175,072 bytes
               0 Dir(s)  4,782,282,739,712 bytes free

Z:\s9000\boinc\win_build>boinc_vs2013.sln

1>  libboinc_vs2013.vcxproj -> Z:\s9000\boinc\win_build\.\Build\x64\Release\libboinc.lib
2>------ Build started: Project: boinc, Configuration: Release x64 ------
2>  Generating Code...
2>  boinc_cli_vs2013.vcxproj -> Z:\s9000\boinc\win_build\.\Build\x64\Release\boinc.exe
```
[BIG EDIT}
Had more time to look a this and there are two problems both with fixes.

Easy fix:
somewhere down near line 145 is the following
```
rem file magic to parse the branch name from the name of the directory
rem hosting the build tree. This can be different than just the version
rem number, we have had to rev the branch name for certain releases due 
rem to late changing features. 6.6 vs 6.6a
FOR /F ""usebackq tokens=3,* delims=_"" %%I IN ('%BUILDBRANCHNAME%') DO (
    SET BUILDTOOLSNAME=boinc_depends_win_%BUILDCOMPILERDETECTED%_%%J
```
The iterator %%J is incorrect, it should be %%I
There is no J defined anywhere.

Harder:
```
SET BUILDROOT=%~dps0
SET BUILDROOT=%BUILDROOT:\win_bu~1\=%
```
The above script (IMHO) is a BATCH TRICK to get the folder name that ""win_build"" is in and the folder will NOT have a backslash.  The script expects a path with no backslash.

Unfortunately, that does not work if the shortened name is not the standard Microsoft 8dot3name..  On some NFS systems or where windows has not enabled 8dot3name the name is not correct.

Solution:
```
SET BUILDROOT=%~dp0 
SET BUILDROOT=%BUILDROOT:~0,-12%
```
This gets rid of the folder named ""win_build"" and strips the last backslash from the root folder
However, the name of the folder has to be ""win_build"" or removing the last 12 chars causes problems

The following was suggested from stackoverflow and does not have the -12 problem
```
for %%a in (""%~dp0.."") do SET BUILDROOT=%%~fa
```

",,30260406
629,"[Android] Use Android-own global_prefs_override.xml file, not that of main BOINC",open,2019-10-22T12:57:34Z,2019-10-23T08:48:23Z,,CONTRIBUTOR,"At the moment the BOINC client under Android comes with a global_prefs_override.xml file that's the same as that of a desktop BOINC. However, the desktop BOINC version of the file is easily editable in an ASCII editor because the data directory under Windows, Linux or Mac is easily reached. Not so however under Android, where one needs to root the device to be able to normally read from and write to the data directory. 

Most of the things in the override file are also useless for Android-BOINC because these cannot be set through the GUI, but must be set through web preferences - thus global_prefs.xml
This means that some of the default options in global_prefs_override.xml immediately override those set via the website, mostly those about running while in use. 

Instead of a normal global_prefs_override.xml file, Android-BOINC should get its own and only have those options that can be set through the GUI. All the rest can be done away with:

So from
`<global_preferences>
  <run_on_batteries>0</run_on_batteries>
  <battery_charge_min_pct>90.0</battery_charge_min_pct>
  <battery_max_temperature>40.0</battery_max_temperature>
  <run_gpu_if_user_active>0</run_gpu_if_user_active>
  <run_if_user_active>0</run_if_user_active>
  <idle_time_to_run>3.0</idle_time_to_run>
  <suspend_cpu_usage>0.0</suspend_cpu_usage>
  <start_hour>0.0</start_hour>
  <end_hour>0.0</end_hour>
  <net_start_hour>0.0</net_start_hour>
  <net_end_hour>0.0</net_end_hour>
  <max_ncpus_pct>50.0</max_ncpus_pct>
  <leave_apps_in_memory>0</leave_apps_in_memory>
  <dont_verify_images>0</dont_verify_images>
  <work_buf_min_days>0.1</work_buf_min_days>
  <work_buf_additional_days>0.5</work_buf_additional_days>
  <disk_interval>60.0</disk_interval>
  <cpu_scheduling_period_minutes>60.0</cpu_scheduling_period_minutes>
  <disk_max_used_gb>0.0</disk_max_used_gb>
  <disk_max_used_pct>90.0</disk_max_used_pct>
  <disk_min_free_gb>0.1</disk_min_free_gb>
  <ram_max_used_busy_pct>50.0</ram_max_used_busy_pct>
  <ram_max_used_idle_pct>50.0</ram_max_used_idle_pct>
  <max_bytes_sec_up>0.0</max_bytes_sec_up>
  <max_bytes_sec_down>0.0</max_bytes_sec_down>
  <cpu_usage_limit>100.0</cpu_usage_limit>
  <daily_xfer_limit_mb>0.0</daily_xfer_limit_mb>
  <daily_xfer_period_days>0</daily_xfer_period_days>
  <network_wifi_only>1</network_wifi_only>
</global_preferences>`

Go to 
`<global_preferences>
  <run_on_batteries>0</run_on_batteries>
  <battery_charge_min_pct>90.0</battery_charge_min_pct>
  <battery_max_temperature>40.0</battery_max_temperature>
  <suspend_cpu_usage>50.0</suspend_cpu_usage>
  <max_ncpus_pct>50.0</max_ncpus_pct>
  <disk_interval>60.0</disk_interval>
  <disk_max_used_pct>90.0</disk_max_used_pct>
  <disk_min_free_gb>0.1</disk_min_free_gb>
  <ram_max_used_busy_pct>50.0</ram_max_used_busy_pct>
  <cpu_usage_limit>100.0</cpu_usage_limit>
  <daily_xfer_limit_mb>0.0</daily_xfer_limit_mb>
  <network_wifi_only>1</network_wifi_only>
</global_preferences>`",It's part of the scheduler reply.,30260406
630,Incorrect Total Disk Usage graph,open,2019-10-21T14:27:36Z,2020-03-22T03:11:51Z,,MEMBER,"From boinc_alpha mailing list:


I have noticed this problem in 7.14.2, but now that I'm looking at version 7.16.3, I thought I'd mention this.  The pie chart for Total disk usage is not summing up to 100 for some reason.  Per the attached screen shot, the total disk usage is:
Used by BOINC, 355.12 MB
Free, available to BOINC:  373.97 GB
Free, not available to BOINC:  1.00 GB (which matches my settings, that is good)
Used by other programs, 89.90 GB

I haven't looked at the numbers too closely, but it is the graph I see a problem.  We are charting four different data points, but on the graph there are five slices to the pie.  It looks like the chart starts at 0 radians, rotates counter-clockwise filling the chart with the data points sequentially as listed above.  Somehow, it gets to the end and there is an empty spot of the pie not accounted for.  I assume this is a rounding error?

![Missing Pie](https://user-images.githubusercontent.com/3234209/67214110-783df280-f41f-11e9-9327-133abb2d1814.jpg)
![Screen Shot 2019-10-09 at 08 44 30](https://user-images.githubusercontent.com/3234209/67214129-7ecc6a00-f41f-11e9-8bcd-2116a791a31f.jpeg)
![Screen Shot 2019-10-09 at 08 48 33](https://user-images.githubusercontent.com/3234209/67214121-7c6a1000-f41f-11e9-97ea-d0416beae34d.jpeg)
","I know this is a trivial issue, but I wanted to report that I have had this occur on a second computer.  Considering it is happening on another version of boinc and on another OS, I don't think my original bug was just a fluke.


PC:  Dell Inspiron 5720
OS:  Ubuntu 18.04
Boinc:  7.17.04, costamagnagianfranco

I'm not sure what else you may want to know.  I just did a clean install of Ubuntu, then Boinc, and this is the first thing I see after I attached Einstein:

![Screenshot from 2020-03-18 21-33-11](https://user-images.githubusercontent.com/58096400/77241473-97080700-6bc0-11ea-8de9-0bfed600726f.png)

Now that I think about it, Einstein is one common denominator here.  I don't see how a project would affect Boinc Manager, but who knows?  I should attach it to another computer I have and see what happens.

I have zero experience with Linux, but I would be happy to get more information for anyone that wants to look at this.",30260406
631,client:  ip address for non-allowed rpc access was invalid,open,2019-10-20T13:25:13Z,2019-12-10T20:53:45Z,,NONE,"fixes #3340 

show_connect_error improperly obtained the ip address of the non-allowed system.  This was fixed by using source code from hostinfo_network.cpp as suggested in issue #3340 ",I don't see you either. Do we use the same BOINC slack server?,30260406
632,BOINC Manager freezes when trying to connect to remote computer,open,2019-10-20T11:46:54Z,2020-01-15T10:41:38Z,,NONE,"(I couldn't find a duplicate issue.  Is this fixed in 7.16.x?)

**Describe the bug**
When using a local BOINC Manager GUI to connect to a remote host, if the remote host is not available (for whatever reason), the local BOINC Manager freezes and must be force closed.

**Steps To Reproduce**
1.  File --> Select computer... --> Enter a remote computer
2.  Manager becomes 99% unresponsive to input.

**Expected behavior**
An error message e.g. ""Cannot connect to remote computer"" would be preferable to just freezing.

**System Information**
 - OS:  Win10 Pro x64, all versions (e.g. 1803, 1809, etc.)
 - BOINC Version:  Tested with 7.14.2 x64","Similar issue from user:

> My main rig is Windows 10 x64 running BOINC 7.14.2. The server I connect to is a Linux Ubuntu 18.04 x64 server running BOINC 7.9.3 (most recent repo version, haven't been bothered to update manually)
> 
> I use both the machine name and it's local IP (192.168 range). For some reason mDNS isn't reliable on my network so i typically use the IP when I can. I usually encounter the bug by accident when I try to use the hostname but mDNS decided it wanted to be broken today, so it doesn't route. But if I fat-finger the IP or the host is turned off the same thing happens.
> 
> If the host goes offline while I'm still connected to it, the lag does NOT happen. I can click through the GUI just fine and tell it to connect to a different client. It only happens when I'm currently connected to a client, and then try to connect to an offline one.",30260406
633,Wrong cast when reporting non-allowed rpc access,open,2019-10-18T22:29:32Z,2019-10-31T11:03:26Z,,NONE,"For some time, reports from new installations of Linux show non-allowed access from ip addresses starting with 2.  2.x.x.x is typically a French address.
reference issue #3246.  I could not find an issue like I put together here but I might have missed it.

In actuality, the 2 is the identifier AF_INET due to wrong cast.
In program gui_rpc_server.cpp function ""show_connect_error"" the following lines of code:
```
#ifdef _WIN32
    sockaddr_in* sin = (sockaddr_in*)&s;
    safe_strcpy(buf, inet_ntoa(sin->sin_addr));
#else
    inet_ntop(s.ss_family, &s, buf, 256);
#endif
```
The reference s.ss_family causes the ""2"" to be picked up.  The actual ip address starts at s.ss_data which is not defined AFAICT

I ran a test from 192.168.1.241 and attempted to access the client.  The remote_host.cfg file has to be empty as that causes the problem somehow.
=====================bug================
18-Oct-2019 14:03:49 [---] Setting up GUI RPC socket
18-Oct-2019 14:03:49 [---] Checking presence of 0 project files
18-Oct-2019 14:03:49 [---] This computer is not attached to any projects
18-Oct-2019 14:03:49 Initialization completed
18-Oct-2019 14:04:14 [---] GUI RPC request from non-allowed address 2.0.233.131
(note that the above can change but the 2.0 will not as that is the value AF_INET)
If you dump out the hex values of the buffer ""buf"" one gets the following

18-Oct-2019 15:36:37 [---] 
debug2 02 00 e9 83 c0 a8 01 f1 00 00 00 00 00 00 00 00
----------------^^ this is 233
------------------^^  this is 131
the 2.0.233.131 came from the buffer ""buf"" 

however, the computer that triggered the access was 192.168.1.241 and that value is also in that buffer but not at the location the coder expected 
debug2 02 00 e9 83 c0 a8 01 f1 00 00 00 00 00 00 00 00
----------------------^^ this is 192
--------------------------^^ this is 168
----------------------------^^ this is the 1
-------------------------------^^ this is the 241

A ""fix"" is to use the same code as the win32.  I took a guess and changed
```
#ifdef _WIN32
    sockaddr_in* sin = (sockaddr_in*)&s;
    safe_strcpy(buf, inet_ntoa(sin->sin_addr));
#else
    inet_ntop(s.ss_family, &s, buf, 256);
#endif
```
to 

```
#ifdef _WIN32
    sockaddr_in* sin = (sockaddr_in*)&s;
    safe_strcpy(buf, inet_ntoa(sin->sin_addr));
#else
    sockaddr_in* sin = (sockaddr_in*)&s;
    safe_strcpy(buf, inet_ntoa(sin->sin_addr));
#endif
```

the above ran correctly
File tempfix.png shows the correct ip address and the hex dump of the buffer
file tempfix1.png shows the code that dumped out the buffer

![temp_fix](https://user-images.githubusercontent.com/37131659/67131873-09b52680-f1cc-11e9-8039-6fbb0a56e172.png)

![temp_fix1](https://user-images.githubusercontent.com/37131659/67131878-0cb01700-f1cc-11e9-8445-0783096967a8.png)

[EDIT]  Not necessary for remote_access.cfg to be actually empty or null for the bug to show up.  ","Thanks Vitalii  I read that article.  Had to set up git on my ubuntu system.  Going to make a note of what I did for reference here
--got an ssh key on ubuntu and put it on GitHub...
git clone git@github.com:JStateson/boinc.git
git remote add upstream git@github.com:BOINC/boinc.git
--made that code change---
git commit -m ""fix-non-allowed""
git checkout -b fix-non-allowed
git push -u origin fix-non-allowed
=====question======
Why does the pull request state that I want to merge 2 commits into the master? THere was only one!",30260406
634,"[Android] 7.16.3, empty task list",open,2019-10-16T20:35:50Z,2020-02-26T16:11:35Z,,CONTRIBUTOR,"From a user email:

Essentially I've found that it works, although one major bug is that my task list is empty yet I'm certain that work has been downloaded. Judging by a post in the forum I think last year it seems to occur when too much work has been downloaded and the task list is greater then a given number (the forum poster suggested 163, but I cant confirm).

The other small bug is that the tray icon doesn't show at any point (I've tried all setting permutations I can think of) - Its quite minor but along with the above bug its hard to tell whether any work is being done at all.
",The limit to trigger this bug can now be easily hit on moderately high chipsets (like Snapdragon 855) and a project that likes to give lots of smaller tasks all at once (such as Einstein@Home),30260406
635,Option to force a specific hostname,open,2019-10-16T17:39:05Z,2019-11-17T16:30:42Z,,NONE,"Problem:  When creating multiple clients, some users (the SUG comes to mind) edit the client_state.xml file changing a few characters in the <host_cpid>  field.  A better solution would be to assign a unique hostname and let the client calculate the correct cpid.  I code this feature into the 7.16.3 source (not sure how old my source is now)
The changes were made to
client_state.cpp & h
cs_cmdline.cpp
hostinfo_network.cpp
hostinfo.cpp & h

Example of how to use it follows:

```
jstateson@jyslinux1:~/Downloads/boinc-master/client$ ./boinc --help
The command-line options for ./boinc are intended for debugging.
The recommended command-line interface is a separate program,'boinccmd'.
Run boinccmd in the same directory as ./boinc.


Usage: ./boinc [options]
    --abort_jobs_on_exit           when client exits, abort and report jobs
    --allow_remote_gui_rpc         allow remote GUI RPC connections
    --allow_multiple_clients       allow >1 instances per host
    --attach_project <URL> <key>   attach to a project
    --force_hostname <name>        use this as hostname-jys
    --check_all_logins             for idle detection, check remote logins too
    --daemon                       run as daemon (Unix)

```

It would be used as follows (for example):


`sudo ./boinc --force_hostname test0 --gui_rpc_port 31420 --dir /home/WOWevent/test0
`

the resulting client_state.xml file looks like this on my ""jyslinux1"" box (note that jyslinux1 is not there)

```
<client_state>
<host_info>
    <timezone>-18000</timezone>
    <domain_name>test0</domain_name>
    <ip_addr>192.168.1.169</ip_addr>
    <host_cpid>cbf8fadc4b0de158ac46a0450e7f42ec</host_cpid>
    <p_ncpus>12</p_ncpus>

```

Useful, but not required, would be to edit /etc/hosts to include ""test0"" such as on the following line

`127.0.0.1 test0 localhost`

",The default (windows) installation password is a bunch of random characters buried down somewhere in ProgramData and one must find it and put it into the manager (BM or BT) or edit the file and delete all the characters.  That seems to be a PITA when working with a lot of remote boinc systems.  Possibly that is why it is left blank on Linux installs.,30260406
636,"[Android] 7.16.3, Another volunteer app is already running on this device",open,2019-10-16T00:05:27Z,2020-03-15T08:00:06Z,,CONTRIBUTOR,"I can only start BOINC once on my Huawei P20 lite (Android 9) and look at it, all other times thereafter, whether I try to recall it from memory or start by its icon, I immediately get a popup that another volunteer app is already running on this device and that only one at a time can run. I have to force close BOINC and restart it for it to work as normal for a bit, before the pop up returns. ",Same here with Android 9 LG V30,30260406
637,"[Android] 7.16.3, doesn't run, always suspended - on batteries",open,2019-10-16T00:02:11Z,2020-02-26T16:12:39Z,,CONTRIBUTOR,"Putting my Huawei P20 lite (Android 9) phone on the charger, when BOINC 7.16.3 starts it stops at the end of the loading with Suspending computation - on batteries. It hasn't run any of the work it had in cache since I upgraded to 7.16.3, and the phone's been on the charger every night, and in between in the car (doubling as navigation). 

Returning to 7.4.53 immediately started work (although the previous tasks were lost of course). ","Okay, I'll have to reinstall it on the phone to test something there. 
The problem with any new install is that it uses the default preferences from the website for things like communications. I have a separate Home location set up for my Android devices, but forgot to add this device to that location. So will have to test it again. Will wait for present work in cache to be done though. So that'll be tomorrow at the soonest. 

Edit: thinking about that, I installed 7.16.3 on top of 7.4.53 so it should've continued with all the preferences in place. That host shows to be on the Home location for me, so that answers my test already. ",30260406
638,[Android][AM] When adding AM it adds project that are not supported by Android,open,2019-10-14T07:58:53Z,2019-10-29T13:17:53Z,,MEMBER,"**Describe the bug**
When adding AM on Android device, all projects are added even those that are not supported on this particular device (or on Android at all)",Please check Science United if you get a chance.,30260406
639,"[Android] stderrdae.txt, stdoutdae.old, stdoutdae.txt should be available for users",open,2019-10-11T09:34:10Z,2020-03-17T14:44:27Z,,MEMBER,Either move this file to public folder that could be easily readable by user or add some mechanism to copy these files (or theirs content) from the UI,"This will require a lot of effort to provide Android-specific file locations.
And correct me if I wrong, but manual file edit is not an Android way.
I'd rather provide some UI to manipulate values in config file",30260406
640,[Android] Log output has extra EOLs before and after every message,open,2019-10-11T09:30:56Z,2019-10-29T13:17:11Z,,MEMBER,"**System Information (please complete the following information):**
 - OS:  Android
 - BOINC Version: 7.16.3

",,30260406
641,rpc query job status,open,2019-10-08T22:16:38Z,2019-10-09T14:30:58Z,,NONE,"Fixes #

**Description of the Change**

Combine the rpc:query_batches and rpc:query_batch procedures into a single rpc:query_batches_status procedure.  The combination eliminates redundant updating of workunit status in the database.  When querying on the order of 100K workunits the time to complete the query was reduced by  factor of 10.

**Alternate Designs**

**Release Notes**

Added streamlined procedure for querying job status through the submit_rpc.


","Where I used the term workunits  I should have used results.   show_job_status() will scan fewer results.
show_job_status() in large part is show_job_details() with get_cpu_time = NO.  I think that there are efficiency gains in show_job_status() that could be applied to show_job_details().",30260406
642,Linux journal logs: apply limit to growth,open,2019-10-08T11:57:42Z,2019-10-31T10:58:35Z,,CONTRIBUTOR,"**Describe the problem**
On my still-young Linux test machine, the BOINC client journal (equivalent to stdoutdae.txt) has grown to 270MB in just over three months:

-- Logs begin at Tue 2019-07-02 19:42:56 BST, end at Tue 2019-10-08 12:50:44 BST. --

**Describe the solution you'd like**
A space control mechanism similar to <max_stdout_file_size> in cc_config.xml","One thing that would reduce the boinc log size would be to find some way to prevent this entry:
```
Oct 16 10:21:55 benuc boinc[21636]: No protocol specified
```
which occurs every minute.",30260406
643,[Android] When screen is rotated boinc logo takes most of the space on screen,open,2019-10-05T17:31:04Z,2019-10-14T08:11:02Z,,NONE," As the title says, when you rotate screen on projects list, you see projects through a really small space which is even hard to click and choose because boinc logo takes the whole space on [screen.](https://ibb.co/CthK2LY)   

if it is ok, I will just make boinc logo smaller when screen is rotated so we can see projects.","Yes, please, make the logo smaller on Projects screen when phone is in landscape mode.",30260406
644,Value Converson warnings,open,2019-10-03T08:38:13Z,2019-10-31T10:56:31Z,,CONTRIBUTOR,"In the Mac build, there is a file particularly, named parse.h that is throwing a lot of Value Conversion issues. Like this.

eg
/Users/robert/Documents/GitHub/ShanghaiTimes/boinc/lib/parse.h:322:13: Implicit conversion loses integer precision: 'long' to 'int'

As this is referring to 32 bit code - int - should it be chased down and converted to 64 bit code, ie int_fast64_t instead of int.  

Not that easy of course because in one case where x = y it's referring an int to a double.

It's not stopping the build. Is it worth chasing down. ","My two cents on this comes up with two answers.

On one hand, this was probably coded in older, more parsimonious, times - when memory was rare and expensive. It was probably a valuable optimisation at the time, and provided the values in question never outgrow either limit, the warnings are harmless and can be ignored.

But on the other hand, we have already seen cases where poor variable type choices caused significant problems. Some years ago, the SETI@Home project had to be shut down for rebuilding when first Task, and then Workunit, ID numbers outgrew the chosen database storage. And just a couple of months ago, we had a case where a misplaced comma allowed a 10-digit number to be written to a variable which should only have accepted the two boolean values - true or false.

I cringe whenever I see absurd choices for value storage and expression - like speeds calculated to a millionth of a flop/sec (or one floating point operation per 11.5 days). The advice on these boards has usually been to disregard - even to disable - compiler warnings, but I think that's shortsighted. It would be better to use programming standards (or even a programming language) which prevented us from putting our collective feet in our mouths.

But that's for another day.",30260406
645,Exiting - BOINC tries to kill too many processes when 1 process doesn't exit timely,open,2019-10-02T17:33:08Z,2019-10-31T10:54:43Z,,NONE,"**Describe the bug**

When exiting BOINC while tasks are running, some projects' processes do not always exit timely. One example is GPUGrid's acemd-923-80.exe process. BOINC waits a minute for all the processes to exit, then proceeds to attempt to kill any remaining ones. However, there are some bugs, especially with CPU throttling (say, ""Use 5% CPU Time"").

- BOINC correctly attempts to kill the process that did not respond timely, but incorrectly also attempts to kill OTHER processes that have already exited timely, per Task Manager and Process Explorer. See logs below. It looks like it finds the first PID in the list that didn't exit gracefully yet, then erroneously tries to kill PIDs further down the list even if they had exited gracefully per Task Manager and Process Explorer.
- BOINC logs that it fails to kill them, even though killing them should have succeeded (and did?). Perhaps it is trying to kill PIDs that have already exited gracefully?

**Steps To Reproduce**

1. Attach to a couple projects, including a project that has a buggy app, such as GPUGrid acemd-923-80.exe using an NVIDIA GPU.
2. Make sure BOINC is running several tasks, preferably with CPU throttling at ""Use 5% CPU Time"".
3. Try to exit BOINC normally
4. Sometimes, the acemd-923-80.exe app will not respond within a minute of being told to exit, and BOINC logs will show that it tries to kill processes.
5. BUG: BOINC tries to kill too many processes.
6. BUG: BOINC says kill_program failed, probably from trying to kill PIDs that have already exited gracefully.

**Expected behavior**

- Make sure processes are given requests to shut down, when user requests shutdown, even if CPU throttling is being used.
- Only attempt to kill processes that are still running at the 1 minute timeout.

**System Information (please complete the following information):**

 - OS: Windows 10 x64
 - BOINC Version: 7.14.2

**Additional context**

7.14.2, Exiting with Use 5% of CPU Time

General behavior when exiting, Process Explorer seems to show:
- Non-VM CPU task processes disappear right away
- VM CPU task processes disappear a little while later, probably because VM CPU Throttling is handled differently, and BOINC powers off VMs
- Buggy GPUGrid NVIDIA acemd-923-80.exe processes take a while longer to disappear, but usually disappear before 60 second timeout, but not always.

BUG: BOINC SOMETIMES thinks it needs to kill them all after a minute?
- Seems like it may be pretty rare. Takes me several attempts to make it happen.
- Seems HIGHLY related/dependent on a buggy GPUGrid NVIDIA acemd-923-80.exe process running despite being told to exit.
- These acemd-923-80.exe tasks are notorious for having problems responding timely to suspend requests, and problems responding timely to exit requests.
- Below are log files for the 2 times it happened out of several attempts.
- I can probably repro on demand with effort.
- Not sure why BOINC thinks it needs to kill them all these processes, when Process Explorer showed that only a single GPUGrid NVIDIA acemd-923-80.exe process was still running erroneously at the 60 second timeout.
- Process explorer shows that the rogue process does indeed get killed.
- I never saw any problems with any of the other non-GPUGrid jobs - they all exited gracefully well within timeout.


01-Oct-2019 18:44:31 [---] Exiting
01-Oct-2019 18:44:31 [---] [task_debug] requesting tasks to exit
01-Oct-2019 18:44:31 [RNA World] [task] task_state=QUIT_PENDING for cmsvm_GA-p[e30-50MB_Lin64f]_1_Drosophila-pseudoobscura-pseudoobscura_CM000070.lin.EMBL_RF00028_Intron_gpI_1330438623_38444_19 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 13232 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 19124
01-Oct-2019 18:44:31 [RNA World] [task] task_state=QUIT_PENDING for cmsvm2_GA-p[e20-30MB_Lin64f]_1_Drosophila-yakuba_CM000162.lin.EMBL_RF00028_Intron_gpI_1349111823_31584_100 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 21420 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 9932
01-Oct-2019 18:44:31 [RNA World] [task] task_state=QUIT_PENDING for cmsvm2_GA-p[e30-50MB_Lin64f]_1_Canis-lupus-familiaris-(dog)_CM000028.lin.EMBL_RF00028_Intron_gpI_1330438623_9632_64 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 13160 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 17124
01-Oct-2019 18:44:31 [GPUGRID] [task] task_state=QUIT_PENDING for e68s169_e66s78p0f60-PABLO_V4_UCB_p27_sj403_short_005_salt_IDP-0-2-RND0106_0 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 8732 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 24476
01-Oct-2019 18:44:31 [GPUGRID] [task] task_state=QUIT_PENDING for e68s167_e57s145p1f90-PABLO_V4_UCB_p27_sj403_short_005_salt_IDP-0-2-RND8731_0 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 24488 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 20892
01-Oct-2019 18:44:31 [Einstein@Home] [task] task_state=QUIT_PENDING for h1_0580.85_O2C02Cl1In0__O2AS20-500_581.00Hz_326_0 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 19284 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 19732
01-Oct-2019 18:44:31 [Rosetta@home] [task] task_state=QUIT_PENDING for foldit_2008115_1065_fold_and_dock_SAVE_ALL_OUT_868488_71_0 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 18852 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 3708
01-Oct-2019 18:44:31 [WUProp@Home] [task] task_state=QUIT_PENDING for data_collect_v4_1567899602_400197_0 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 22636 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 20520
01-Oct-2019 18:44:31 [TN-Grid Platform] [task] task_state=QUIT_PENDING for 158605_Hs_T083809-SH2D2A_wu-140_1569933924328_1 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 22832 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 4916
01-Oct-2019 18:44:31 [SETI@home] [task] task_state=QUIT_PENDING for blc34_2bit_guppi_58643_48014_HIP5944_0015.17552.818.24.47.94.vlar_1 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 21188 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 11964
01-Oct-2019 18:44:31 [Milkyway@Home] [task] task_state=QUIT_PENDING for de_modfit_82_bundle4_4s_south4s_bgset_2_1564052102_20898451_1 from request_exit()
01-Oct-2019 18:44:31 [---] request_exit(): PID 6392 has 1 descendants
01-Oct-2019 18:44:31 [---]    PID 10080
01-Oct-2019 18:44:43 [RNA World] [task] task_state=EXITED for cmsvm_GA-p[e30-50MB_Lin64f]_1_Drosophila-pseudoobscura-pseudoobscura_CM000070.lin.EMBL_RF00028_Intron_gpI_1330438623_38444_19 from has_task_exited
01-Oct-2019 18:44:43 [RNA World] [task] task_state=EXITED for cmsvm2_GA-p[e20-30MB_Lin64f]_1_Drosophila-yakuba_CM000162.lin.EMBL_RF00028_Intron_gpI_1349111823_31584_100 from has_task_exited
01-Oct-2019 18:44:43 [RNA World] [task] task_state=EXITED for cmsvm2_GA-p[e30-50MB_Lin64f]_1_Canis-lupus-familiaris-(dog)_CM000028.lin.EMBL_RF00028_Intron_gpI_1330438623_9632_64 from has_task_exited
01-Oct-2019 18:45:31 [---] [task_debug] all tasks haven't exited after 60 sec; killing them
01-Oct-2019 18:45:31 [---] [task] kill_program(19284) failed: kill() or TerminateProcess() failed
01-Oct-2019 18:45:31 [---] [task] kill_program(18852) failed: kill() or TerminateProcess() failed
01-Oct-2019 18:45:31 [---] [task] kill_program(22636) failed: kill() or TerminateProcess() failed
01-Oct-2019 18:45:31 [---] [task] kill_program(22832) failed: kill() or TerminateProcess() failed
01-Oct-2019 18:45:31 [---] [task] kill_program(21188) failed: kill() or TerminateProcess() failed
01-Oct-2019 18:45:31 [---] [task] kill_program(6392) failed: kill() or TerminateProcess() failed
01-Oct-2019 18:45:31 [GPUGRID] [task] task_state=EXITED for e68s169_e66s78p0f60-PABLO_V4_UCB_p27_sj403_short_005_salt_IDP-0-2-RND0106_0 from has_task_exited
01-Oct-2019 18:45:31 [GPUGRID] [task] task_state=EXITED for e68s167_e57s145p1f90-PABLO_V4_UCB_p27_sj403_short_005_salt_IDP-0-2-RND8731_0 from has_task_exited
01-Oct-2019 18:45:31 [Einstein@Home] [task] task_state=EXITED for h1_0580.85_O2C02Cl1In0__O2AS20-500_581.00Hz_326_0 from has_task_exited
01-Oct-2019 18:45:31 [Rosetta@home] [task] task_state=EXITED for foldit_2008115_1065_fold_and_dock_SAVE_ALL_OUT_868488_71_0 from has_task_exited
01-Oct-2019 18:45:31 [WUProp@Home] [task] task_state=EXITED for data_collect_v4_1567899602_400197_0 from has_task_exited
01-Oct-2019 18:45:31 [TN-Grid Platform] [task] task_state=EXITED for 158605_Hs_T083809-SH2D2A_wu-140_1569933924328_1 from has_task_exited
01-Oct-2019 18:45:31 [SETI@home] [task] task_state=EXITED for blc34_2bit_guppi_58643_48014_HIP5944_0015.17552.818.24.47.94.vlar_1 from has_task_exited
01-Oct-2019 18:45:31 [Milkyway@Home] [task] task_state=EXITED for de_modfit_82_bundle4_4s_south4s_bgset_2_1564052102_20898451_1 from has_task_exited
01-Oct-2019 18:45:31 [---] [task_debug] all tasks exited


01-Oct-2019 19:07:45 [---] Exiting
01-Oct-2019 19:07:45 [---] [task_debug] requesting tasks to exit
01-Oct-2019 19:07:45 [RNA World] [task] task_state=QUIT_PENDING for cmsvm_GA-p[e30-50MB_Lin64f]_1_Drosophila-pseudoobscura-pseudoobscura_CM000070.lin.EMBL_RF00028_Intron_gpI_1330438623_38444_19 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 10040 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 2804
01-Oct-2019 19:07:45 [RNA World] [task] task_state=QUIT_PENDING for cmsvm2_GA-p[e20-30MB_Lin64f]_1_Drosophila-yakuba_CM000162.lin.EMBL_RF00028_Intron_gpI_1349111823_31584_100 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 9488 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 14372
01-Oct-2019 19:07:45 [RNA World] [task] task_state=QUIT_PENDING for cmsvm2_GA-p[e30-50MB_Lin64f]_1_Canis-lupus-familiaris-(dog)_CM000028.lin.EMBL_RF00028_Intron_gpI_1330438623_9632_64 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 8612 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 22840
01-Oct-2019 19:07:45 [GPUGRID] [task] task_state=QUIT_PENDING for e68s169_e66s78p0f60-PABLO_V4_UCB_p27_sj403_short_005_salt_IDP-0-2-RND0106_0 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 1240 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 22296
01-Oct-2019 19:07:45 [GPUGRID] [task] task_state=QUIT_PENDING for e68s167_e57s145p1f90-PABLO_V4_UCB_p27_sj403_short_005_salt_IDP-0-2-RND8731_0 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 8732 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 2644
01-Oct-2019 19:07:45 [Einstein@Home] [task] task_state=QUIT_PENDING for h1_0580.85_O2C02Cl1In0__O2AS20-500_581.00Hz_326_0 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 22148 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 17332
01-Oct-2019 19:07:45 [Rosetta@home] [task] task_state=QUIT_PENDING for pass_build_agb.bp_20190515161530_0001_fragments_fold_SAVE_ALL_OUT_835689_847_0 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 24164 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 20324
01-Oct-2019 19:07:45 [WUProp@Home] [task] task_state=QUIT_PENDING for data_collect_v4_1567899602_400197_0 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 15252 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 10728
01-Oct-2019 19:07:45 [TN-Grid Platform] [task] task_state=QUIT_PENDING for 158605_Hs_T083809-SH2D2A_wu-140_1569933924328_1 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 20132 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 12244
01-Oct-2019 19:07:45 [SETI@home] [task] task_state=QUIT_PENDING for blc34_2bit_guppi_58643_48014_HIP5944_0015.17552.818.24.47.94.vlar_1 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 21472 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 2596
01-Oct-2019 19:07:45 [Milkyway@Home] [task] task_state=QUIT_PENDING for de_modfit_80_bundle4_4s_south4s_bgset_2_1564052102_20971803_0 from request_exit()
01-Oct-2019 19:07:45 [---] request_exit(): PID 17844 has 1 descendants
01-Oct-2019 19:07:45 [---]    PID 17956
01-Oct-2019 19:07:57 [RNA World] [task] task_state=EXITED for cmsvm_GA-p[e30-50MB_Lin64f]_1_Drosophila-pseudoobscura-pseudoobscura_CM000070.lin.EMBL_RF00028_Intron_gpI_1330438623_38444_19 from has_task_exited
01-Oct-2019 19:07:57 [RNA World] [task] task_state=EXITED for cmsvm2_GA-p[e20-30MB_Lin64f]_1_Drosophila-yakuba_CM000162.lin.EMBL_RF00028_Intron_gpI_1349111823_31584_100 from has_task_exited
01-Oct-2019 19:07:57 [RNA World] [task] task_state=EXITED for cmsvm2_GA-p[e30-50MB_Lin64f]_1_Canis-lupus-familiaris-(dog)_CM000028.lin.EMBL_RF00028_Intron_gpI_1330438623_9632_64 from has_task_exited
01-Oct-2019 19:08:39 [GPUGRID] [task] task_state=EXITED for e68s169_e66s78p0f60-PABLO_V4_UCB_p27_sj403_short_005_salt_IDP-0-2-RND0106_0 from has_task_exited
01-Oct-2019 19:08:45 [---] [task_debug] all tasks haven't exited after 60 sec; killing them
01-Oct-2019 19:08:45 [---] [task] kill_program(8732) failed: kill() or TerminateProcess() failed
01-Oct-2019 19:08:45 [---] [task] kill_program(22148) failed: kill() or TerminateProcess() failed
01-Oct-2019 19:08:45 [---] [task] kill_program(24164) failed: kill() or TerminateProcess() failed
01-Oct-2019 19:08:45 [---] [task] kill_program(15252) failed: kill() or TerminateProcess() failed
01-Oct-2019 19:08:45 [---] [task] kill_program(20132) failed: kill() or TerminateProcess() failed
01-Oct-2019 19:08:45 [---] [task] kill_program(21472) failed: kill() or TerminateProcess() failed
01-Oct-2019 19:08:45 [---] [task] kill_program(17844) failed: kill() or TerminateProcess() failed
01-Oct-2019 19:08:45 [GPUGRID] [task] task_state=EXITED for e68s167_e57s145p1f90-PABLO_V4_UCB_p27_sj403_short_005_salt_IDP-0-2-RND8731_0 from has_task_exited
01-Oct-2019 19:08:45 [Einstein@Home] [task] task_state=EXITED for h1_0580.85_O2C02Cl1In0__O2AS20-500_581.00Hz_326_0 from has_task_exited
01-Oct-2019 19:08:45 [Rosetta@home] [task] task_state=EXITED for pass_build_agb.bp_20190515161530_0001_fragments_fold_SAVE_ALL_OUT_835689_847_0 from has_task_exited
01-Oct-2019 19:08:45 [WUProp@Home] [task] task_state=EXITED for data_collect_v4_1567899602_400197_0 from has_task_exited
01-Oct-2019 19:08:45 [TN-Grid Platform] [task] task_state=EXITED for 158605_Hs_T083809-SH2D2A_wu-140_1569933924328_1 from has_task_exited
01-Oct-2019 19:08:45 [SETI@home] [task] task_state=EXITED for blc34_2bit_guppi_58643_48014_HIP5944_0015.17552.818.24.47.94.vlar_1 from has_task_exited
01-Oct-2019 19:08:45 [Milkyway@Home] [task] task_state=EXITED for de_modfit_80_bundle4_4s_south4s_bgset_2_1564052102_20971803_0 from has_task_exited
01-Oct-2019 19:08:45 [---] [task_debug] all tasks exited

",,30260406
646,Wrapper: Monitor status of child task process even if parent exits,open,2019-09-25T18:59:19Z,2019-10-31T17:33:46Z,,CONTRIBUTOR,"**Description of the Change**
On Windows, CreateProcess() is used to launch tasks, but this on its own does not handle child processes; if the parent task process exits, the workunit will be terminated. If <wait_for_children> is set in the job file, attach the task process to a job object instead, which can then be monitored to determine when all child processes are finished.

**Alternate Designs**
<!-- Explain what other alternates were considered and why the proposed version was selected -->

**Release Notes**
<!--
Please describe the changes in a single line that explains this improvement in terms that a user can understand.
This text will be used in BOINC's release notes.
If this change is not user-facing or notable enough to be included in release notes you may use the string ""N/A"" here. -->
Add <wait_for_children> option for tasks in job.xml","Normally they do, but we are trying to wrap an application that does not follow the normal pattern. It spawns child processed and exits, which in turn makes the wrapper and thus BOINC think the app has finished computations.",30260406
647,[Web] ATI brand is used in System requirements page,open,2019-09-24T19:41:53Z,2019-10-31T10:54:02Z,,MEMBER,The ATI brand should be renamed to AMD in the https://boinc.berkeley.edu/wiki/System_requirements page.,"Essentially everything on that page and the related page
https://boinc.berkeley.edu/wiki/List_of_projects_by_system_requirements
is outdated by a decade or two.
Maybe we should not link to them.",30260406
648,Improve logic behind WF_MAX_RUNNABLE_JOBS = 1000,open,2019-09-20T04:04:06Z,2019-10-31T10:53:15Z,,NONE,"I'm running into limitations with WF_MAX_RUNNABLE_JOBS hard-coded to 1000.  Even on a 16-threaded CPU, 1000 short-running work units can be processed in 6-8 hours, leaving the machine idle if it loses Internet connectivity and nobody is home.  I prefer to buffer 1-2 days of work on all my devices, but if the device can process 3000 work units in 1 day, one quickly hits a wall.

I *do* see the reasoning behind the failsafe mechanism to prevent unlimited fetching, but as more and more multi-core behomoths come to market e.g. AMD EPYC Rome 64 core/128 thread, it makes it much more difficult for them to participate in certain volunteer distributed computing on the BOINC platform, especially very short work units.

**Ideas:**
1.  Simply increase the constant; e.g. 5000.  I think this doesn't solve the underlying problem and merely kicks the can down the road.  It is also too small for the largest systems that have 128 or 256 threads.

2.  Make WF_MAX_RUNNABLE_JOBS scalable and dependent on the number of CPUs.  Example numbers:
     - If number of CPUs is <8, then WF_MAX_RUNNABLE_JOBS = 2000.
     - 8-15:  3000
     - 16-23:  5000
     - 24-63:  7500
     - 64+:  10000

3.  Expose this number in cc_config.xml.  This lets an advanced user change the limit, especially if they have a beefy newer multicore system.

4.  Other

5.  Some combination of the above","@smoe - please review the client Event Log carefully. Although you are not the first to complain that ""Einstein sent..."", that can't be literally true: work is only ever allocated as the result of a **request** by the client. The times it's happened to me, I have investigated and it has turned out that my client had repeated the **same request** again and again and again, requesting and receiving the same amount of work every 60 seconds (even though the amount of work cached locally was increasing each time).

We need to isolate and identify the cause of this problem, before rushing to treat the symptoms.

The abortive report I started at #3309 over the weekend seemed to be another case. That one turned out to have a different and irrelevant cause, but my own Einstein cases have been harder to dismiss.",30260406
649,No computing on Android 10,open,2019-09-12T21:25:26Z,2020-03-13T06:28:18Z,,CONTRIBUTOR,"From a user email.  Not sure if this is a dup.

Hi,

Are you aware that since the update to Android 10 there's nothing beeing processed any more?
","Yeap, but SETI work normally on Android 4.3.1 and 6.0.1. I understand that a SETI error is more likely, but still wrote about it.",30260406
650,Closing files after opening them.,open,2019-09-10T13:09:38Z,2020-03-20T18:12:13Z,,CONTRIBUTOR,"This patch is with Debian for a really long time, likely to have its origins also in the cppcheck and/or clang runs. Just like PR https://github.com/BOINC/boinc/pull/3284, this also addresses the reported leaking of file descriptors (https://github.com/BOINC/boinc/issues/1175).

Anchor: https://github.com/BOINC/boinc/issues/3260","There is more to the PR than the closing of files. @davidpanderson is however right that the file-closing concerns are indeed driven by short-lived applications. Feel free to cut that. However, from my POV I do not care if it is a long-running process or a short-running one. Did not even look - otherwise I may not even have submitted it to avoid discussions. I always close what is no longer needed. And free what is no longer needed. ASAP, ""P"" as in ""there is no semantics associated with 'having a lock on a file'"". That way the code is also allowed to move into other parts of BOINC. There is no unnecessary context required to understand that part of the code. And I admittely sense some satisfaction over that elegance.",30260406
651,Complete list of files to be installed for tools,open,2019-09-10T12:33:21Z,2019-09-10T12:33:21Z,,CONTRIBUTOR,Anchor: https://github.com/BOINC/boinc/issues/3260,,30260406
652,adds error message in case of dir open failure,open,2019-09-10T12:21:53Z,2019-09-11T10:59:45Z,,CONTRIBUTOR,"Trivial thing. Should also help to get better reports by users when they experience something exceptional.

Anchor: https://github.com/BOINC/boinc/issues/3260","I am not sure that I want the message to appear on the event log. But maybe that would be a good thing, indeed. ",30260406
653,Allow compilation of BOINC when there is no MAX_PATH(length),open,2019-09-10T12:14:26Z,2020-03-26T18:56:57Z,,CONTRIBUTOR,"Linux has a maximal length of paths. Other platforms, notably HURD,
are unlimited in the length of their paths and filenames.
This patch (and there have been others before) allow to compile
and use BOINC on such technically advanced(?) OSs by simply imposing
the same maximal path length as for Linux.

Anchor: https://github.com/BOINC/boinc/issues/3260","Good changes. I would also recommend generating MAX_PATH like this, to account for more different kinds of OS's and environments:

https://github.com/mcandre/tonixxx/blob/master/examples/fewer/lib/fewer.h#L20-L35",30260406
654,cppcheck: React to failing memory reallocation,open,2019-09-10T12:06:36Z,2019-10-15T08:47:17Z,,CONTRIBUTOR,"This is another one of the concerns that cppcheck raised. If the the memory that ""we"" own is not extendable, then this is likely an out of memory situation to which BOINC should possibly react nicely as nicely as it can. That patch meant to prevent a memory leak in low-memory situations. It does not look nice, admittedly. Maybe someone else has a constructive idea about it.

Anchor: https://github.com/BOINC/boinc/issues/3260",,30260406
655,cppcheck: independent iterators declared independently,open,2019-09-10T11:47:00Z,2019-12-19T21:31:50Z,,CONTRIBUTOR,"Another very old patch with Debian, again one that originated from invoking cppcheck on the source tree.

The exact cppcheck warning I don't recall. I would have expected the iterator to know about how to properly terminate itself before going through the next iteration. The indentiation of the code is disturbed by the extra blocks to minimize the size of the patch. Alternatively, ""it"" could be renamed or the indentiation be updated.

Personally, I do not like this patch too much since the exact need to have to declarations of the iterator is not immediately apparent. A comment would be nice at least. But I very much like not to declare an iterator at the beginning of a function that is only needed in a branch of the code. So, I expect a bit of a discussion about this patch and confidently expect this code to improve over it.

Anchor: https://github.com/BOINC/boinc/issues/3260
","Running cppcheck git updated today with enable=all gives:
Checking client/rrsim_test.cpp ...
client/rrsim_test.cpp:42:5: warning: Member variable 'PROJECT::cpu_shortfall' is not initialized in the constructor. [uninitMemberVar]
    PROJECT(char* n, double rs) {
    ^
client/rrsim_test.cpp:42:5: warning: Member variable 'PROJECT::rrsim_proc_rate' is not initialized in the constructor. [uninitMemberVar]
    PROJECT(char* n, double rs) {
    ^
client/rrsim_test.cpp:42:5: warning: Member variable 'PROJECT::rr_sim_deadlines_missed' is not initialized in the constructor. [uninitMemberVar]
    PROJECT(char* n, double rs) {
    ^
client/rrsim_test.cpp:65:5: warning: Member variable 'RESULT::rrsim_finish_delay' is not initialized in the constructor. [uninitMemberVar]
    RESULT(PROJECT* p, char* n, double e, double rd) {
    ^
client/rrsim_test.cpp:65:5: warning: Member variable 'RESULT::rrsim_cpu_left' is not initialized in the constructor. [uninitMemberVar]
    RESULT(PROJECT* p, char* n, double e, double rd) {
    ^
client/rrsim_test.cpp:65:5: warning: Member variable 'RESULT::rr_sim_misses_deadline' is not initialized in the constructor. [uninitMemberVar]
    RESULT(PROJECT* p, char* n, double e, double rd) {
    ^
client/rrsim_test.cpp:65:5: warning: Member variable 'RESULT::last_rr_sim_missed_deadline' is not initialized in the constructor. [uninitMemberVar]
    RESULT(PROJECT* p, char* n, double e, double rd) {
    ^
nofile:0:0: information: Cppcheck cannot find all the include files (use --check-config for details) [missingIncludeSystem]
So nothing about iterator pb.

I don't like too the fact that vars are at beginning of the function whereas it's a c++ file.
About the fact to declare this var only when using, I agree but since there's the big ""while (active.size())"" (line 249), I wonder if it would have some runtime cost.",30260406
656,cppcheck: avoid chance to read from uninitialised data,open,2019-09-10T10:44:09Z,2019-11-20T12:47:15Z,,CONTRIBUTOR,"This was once found by cppcheck and lives for a long time in
the Debian distribution. The decrypt_public function should set the data
attribute, so this is of no functional danger. But as much
as this irritated cppcheck, a human reader is confused at first
sight. Also, at some point the function called may be happy to
know that it is not overwriting a string or the well-defined
string helps some debug output.

Most notably, with this patch, the final copy will always copy
reasonable data, even if the decrypt_public function changes its return
codes.

Anchor: https://github.com/BOINC/boinc/pull/3276",,30260406
657,Suggestions for Android GUI changes,open,2019-09-04T21:54:01Z,2019-10-31T10:49:09Z,,CONTRIBUTOR,"(a user email)

Hi there,
I'm a daily user of BOINC and I love the idea of a volunteer run computing network. The BOINC app for Android is very smooth and bug-free, however it looks quite dated. A redesign (and perhaps a dark mode to save battery for computing) would give a better impression to potential users of the Android client. For inspiration, you could either use Google's Material Design, or you could draw on HTC's version of BOINC, Power to Give, or a similar app not related to BOINC called Dreamlab, by Australian telecommunications company Vodafone. The main things I would love to see would be a graphical representation of statistics such as computing hours, as well as direct embedding of project websites and messages.",,30260406
658,Not all pages in BOINC server code are up for translation,open,2019-08-31T19:50:30Z,2019-10-31T10:46:41Z,,CONTRIBUTOR,"Since about a year I've taken over as translator for Dutch translations of all things BOINC. In this time I haven't really checked if all available translations always make it all right. But a couple of weeks ago while CPDN was testing the new server code, I found that not all pages switch to Dutch, when Dutch is the selected language. 

For example, the server status page isn't fully translated, leaving full sentences like ""Tasks ready to send"" untranslated right next to ""Taken in uitvoering"" which did make it into Dutch. So my question is, shouldn't everything in the /html/user/ directory be available for translation? Because apparently not everything from https://github.com/BOINC/boinc/blob/master/html/user/server_status.php is. 

It's pretty much not the only thing that isn't available for translation. 
Page names, such as user_search.php aren't translated either. In Dutch it shows as ""User search"", instead of a more correct ""Zoek gebruiker"". 
The same for stats.php, ""Credit leader boards and statistics "".

Or footers such as in apps.php, ""Total average computing: 0 GigaFLOPS""
cpu_list.php, ""This table shows peak CPU speed (based on Whetstone benchmarks) of computers participating in this project.""
host_stats.php,""Computer breakdown by operating system"" and ""View breakdown by BOINC version"", ""View breakdown by operating system"".  

Can someone please check that we're sending everything needed for translation to Transifex? ",Have you tried to check if the untranslated strings are also untranslated on Transifex?,30260406
659,Revisited Debian patch 'more_maxpathlen.patch',open,2019-08-29T13:29:03Z,2019-10-10T12:26:35Z,,CONTRIBUTOR,"This patch is about 7 years old in Debian. I don't recally the exact original. Typically the compiler warns about something or say, the (admittedly, mostly irrelelvant) Hurd platform does not have a limit in the maxpathlen and so one has a look into that.

The patch is about using MAXPATHLEN both to limit the number of bytes copied into a char[] and to prepare the size of the char[]s for it. When there are MAXPATHLEN characters allowed, a buffer needs MAXPATHLEN+1 characters max because of the terminating 0. snprintf adds the terminating 0 itself and the ""n"" in snprintf helps to avoid buffer overruns.

When looking at this code, I wish the boinc_getcwd would be allowed to fail. It happens all the time to me that I have removed a directory that I was still in with another shell. And the same could well happen with some magic performed on the /var partition or whatever magic all these clouds and containers may now be preparing to come up with. This has not been addressed in this patch but should possibly be addressed as a separate issue.

Anchor: https://github.com/BOINC/boinc/issues/3260","thanks for this, it might solve even security issues or possible buffer overflows!",30260406
660,The Fedora + Debian/Ubuntu packages better lose their patches,open,2019-08-27T09:09:21Z,2019-10-31T10:45:31Z,,CONTRIBUTOR,"So far so obvious, there should not be any need for a distro's packages to have any patches since all changes are either:
 * not necessary -> don't patch
 * of general interest -> have it upstream
 * an indication of a deficiency to configure/install BOINC -> improve upstream

I would like to leave this issue here as an anchor and over time revisit all patches that have accumulated downstream over the past decade for [Debian](https://salsa.debian.org/pkg-boinc-team/boinc/tree/master/debian/patches) and [Fedora](https://src.fedoraproject.org/rpms/boinc-client/tree/master). ","@Germano0 - I don't know what to tell you.  It's running fine for me now on my laptop on Fedora 30 without that patch.  It's running docked with three displays, usb keyboard and mouse, and two chargers plugged in, and the idle detection still works.  After removing the patch, I no longer have to manually toggle GPU processing on and off, which was highly tedious.",30260406
661,Python2 compatible changes on print() and Exception(),open,2019-08-26T17:21:27Z,2019-09-25T14:33:06Z,,CONTRIBUTOR,"Hello,

Please grant this patch some scrutiny that is meant to address (bits of) https://github.com/BOINC/boinc/issues/3241 and may continue work in https://github.com/BOINC/boinc/pull/2477 .

The 'from __future__ import print_function' likely preserves compatibility with Python2. I suggest to add this until the transition to Python3 is completed.

Best,
Steffen",I am very confident wrt the print instructions.  The exceptions may need a bit of extra love.,30260406
662,Allow the validator to flag compared results as `runtime_outlier`,open,2019-08-23T15:22:19Z,2019-10-31T10:44:46Z,,NONE,"`runtime_outlier` is a flag that allows to disregard the use of a specific result to update the FPOPs estimate of a specific host. This mechanism can be important when the result is generated with a number of FPOPS way smaller than expected; otherwise, in presence of many of these results, the FPOPs of a host can be corrupted.

The aim of SixTrack simulations is to find the boundary between stable and chaotic motion in accelerator beams. Chaotic motion leads to beam losses and hence an early end of computation, in presence of a set of input files fully correct and a totally correct job processing. Since a single job covers a tiny bit of phase space to be studied, it can easily happen that a job ends way earlier than expected simply because the initial conditions falls in a region of chaotic motion.

Since SixTrack validates results, it is essential to spot validated results obtained with a number of FPOPS way smaller than expected. The validation file reports if losses took place or not; hence, the best place where to set `runtime_outlier=true` is in `files_match` in `compare_results`. At present, we can flag only the first result as runtime outlier, since the second one is passed via the interfaces as `const`.",,30260406
663,Allow the validator to flag compared results as `runtime_outlier`,open,2019-08-23T14:58:48Z,2019-09-06T07:40:41Z,,NONE,"Fixes # https://github.com/BOINC/boinc/issues/3256

**Description of the Change**
<!-- We must be able to understand the design of your change from this description. -->
Give the possibility to the validator to set `result.runtime_outlier=true` all the compared results based on the content of the results after validation. This can be achieved via removing tagging the second result in `compare_results` as `const`.

**Alternate Designs**
<!-- Explain what other alternates were considered and why the proposed version was selected -->
Setting `result.runtime_outlier=true` could be performed outside of `files_match`, but this would have implied either to store the value of the flag in a temporary variable and use it somewhere else, or parse again the validation file.

**Release Notes**
<!--
Please describe the changes in a single line that explains this improvement in terms that a user can understand.
This text will be used in BOINC's release notes.
If this change is not user-facing or notable enough to be included in release notes you may use the string ""N/A"" here. -->
Give the possibility to the validator to set `result.runtime_outlier=true` all the compared results based on the content of the results after validation.","So, @davidpanderson , what you suggest is to read the validation file in init_result() to see that the simulation finished early, and we flag it as runtime outlier no matter if the result is valid or not.
Parsing twice the file seems to me a bit redundant - we do it already in compare_results() and get the appropriate info there",30260406
664,physical_cpus plan class feature documented but not implemented?,open,2019-08-21T10:56:50Z,2019-10-31T10:44:00Z,,CONTRIBUTOR,"https://boinc.berkeley.edu/trac/wiki/AppPlanSpec#Hyperthreading states that 7.14+ distinguishes physical CPUs from hyperthreads. I tried to use it and found that it does not work; I tried to find code that handles it, and there actually is none (use github code search to look for `physical_cpus`).

Usually BOINC has undocumented features. It is uncommon to find a feature that is documented but does not exist!

Before I remove that section, any idea why it is there? I see it was added by @davidpanderson back in 2018.","It's #2624.  It's implemented for Win and Linux, not sure about Mac.",30260406
665,Preferences are reset when connect to project,open,2019-08-18T01:18:33Z,2019-08-18T01:18:33Z,,MEMBER,"**Describe the bug**
When project is connected, local preferences are reseted with web preferences from the project

**Steps To Reproduce**
1. Install BOINC with no projects added (clean install)
2. Make some local changes (e.g. computation settings)
3. Add project (e.g. Einstein@home)

**Expected behavior**
Local changes to be saved

**System Information (please complete the following information):**
 - OS: Windows 10 (may be applicable to any other OS but not tested)
 - BOINC Version: 7.14.2",,30260406
666,Define XML code style,open,2019-08-02T21:46:11Z,2019-10-31T10:40:00Z,,MEMBER,"Based on pull request #3044, there is a need to define a XML document format style for the project that future pull requests will expect to be coded against.","Don’t forget that XML also has to be parsed in php, python etc. ",30260406
667,Finish migration to python3,open,2019-08-02T21:36:21Z,2019-10-31T10:37:13Z,,MEMBER,"Pull Request #2477 modified the scripts necessary so that creating and updating a project worked with python3 instead of python2.  I believe the following additional python scripts need to be updated to finish the migration to python3:

- ./stripchart/samples/parse_config
- ./tools/parse_config
- ./tools/dbcheck_files_exist
- ./tools/check_project
- ./tools/appmgr
- ./sched/start","If possible, all these scripts should work with either Python 2 or Python 3.",30260406
668,[Web] Allowing custom venue names,open,2019-07-29T11:08:19Z,2019-10-31T10:36:39Z,,MEMBER,"**Describe the problem**
The issue was described and discussed in #3223.",,30260406
669,Error produced when using remote_file feature of create_work,open,2019-07-26T16:53:35Z,2019-10-31T10:35:47Z,,NONE,"I am using the create_work feature to submit new workunits from the command line. This is described in:

https://boinc.berkeley.edu/trac/wiki/JobSubmission

I am trying to use the remote_file feature. For some unknown reason the create_work changes the name of the file to be downloaded, it changes the name to 'jf_...'. This is done in:

boinc/tools/process_input_template.cpp  (line 298)

It is not clear why this is done. What it means is that when the BOINC client tries to download the file on the client it cannot download it.

I spoke to Bernd about this in the BOINC workshop. He came up with a fix:

https://github.com/bema-aei/boinc/commit/99b7dcb0211fd4128e220a81ee736fb9cb2ecb9e

However this produces the following error messages when trying to submit new work using create_work:

missing MD5 info file for <REMOTE_FILENAME>
process_input_template(): file missing
create_work: file missing

I'd be grateful for this to be looked at, as this is acting a bit of a blocker at the moment. Many thanks.
","According to your message, this item can be closed after documentation update",30260406
670,Add potentially missing mac header.,open,2019-07-11T14:34:58Z,2019-09-20T07:14:47Z,,CONTRIBUTOR,From PR #3179,"On the first appearance, I agree with Charlie.  Mac does not install gcc and in fact, is moving even further away from having third party programs and programming options on board. So others wishing to simplify the process on Mac by using just XCode have enough to do even now making sure the steps as outlined in the HOW-TO files are followed. 
I've just opened XCode and tested the build again of BOINC_Client and it's built and run cleanly. I haven't done this for months and months now, so a little surprised that it still built ok.
This is XCode Version 10.3 on Mohave, and I have everything installed according to the howto document that I recently updated.
I'd prefer to await XCode 11's release before proceeding too much further because that's sure to change a lot of things. Even more things will be removed, and probably added.
In short - I agree with Charlie. I reject this change on the same grounds.
Building headers and adding things to XCode is a pathway to ruin. Caveat Utilitor

This is the output of the build and run just now.
`20-Sep-2019 08:06:46 [---]    (to change preferences, visit a project web site or select Preferences in the Manager)
20-Sep-2019 08:06:46 [---] Setting up project and slot directories
20-Sep-2019 08:06:46 [---] Checking active tasks
20-Sep-2019 08:06:46 [---] Setting up GUI RPC socket
20-Sep-2019 08:06:46 [---] Checking presence of 0 project files
20-Sep-2019 08:06:46 [---] This computer is not attached to any projects
20-Sep-2019 08:06:46 Initialization completed
20-Sep-2019 08:06:46 [---] Suspending GPU computation - computer is in use`
",30260406
671,"Integrate BOINC with ""Extreme-scale scientific software stack""",open,2019-07-09T18:46:09Z,2019-10-31T10:35:30Z,,CONTRIBUTOR,"Automate the process of running existing science-app containers on BOINC.
https://e4s-project.github.io/",,30260406
672,[Website] Update GuiRpcProtocol's wiki,open,2019-07-06T18:36:34Z,2019-10-31T10:34:14Z,,MEMBER,[https://boinc.berkeley.edu/trac/wiki/GuiRpcProtocol#get_file_transfers](https://boinc.berkeley.edu/trac/wiki/GuiRpcProtocol#get_file_transfers) doesn't contain the `hostname` tag.,"Can you be more specific, maybe post how the entire section should look like, and I'll put it into the documentation?",30260406
673,[Manager] Add popup menu,open,2019-07-04T04:43:58Z,2019-09-25T11:28:51Z,,MEMBER,"Added popup menu for Projects, Work and Transfers tabs.

This fixes #602

Signed-off-by: Vitalii Koshura <lestat.de.lionkur@gmail.com>
","
An update if I may. After very careful investigation.
These are the modified files that should implement the Context menu.
These files live in the clientgui folder under boinc. ie; boinc/clientgui

As near as I can make out, these are the only files that have changed






The code builds under Build All, and under Build for Profiling where the install binaries are made.
The binaries install as teh BOINC app and run.

I renamed the version 7.15.1 so that I know it’s the one I’m working with.




It all works as normal - but unless I”m doing something that I can’t see happening - nothing makes that context menu pop up.
Not Ctrl-Click, not Right-Click . nada.




What can I assume? I know that XCode (11.0) moves in mysterious ways. So perhaps it needs something else.

cheers
Robert



> On 24 Sep 2019, at 16:18, Robert “FOZABOG The Elder” Chalmers <racuk12@gmail.com> wrote:
> 
> @ VitaliiKoshura   It builds and runs from the XCode GUI.
> 
> Do you have a list there of the modified/added files? I want to check what’s been added. Make sure it’s up to date, without doing it through Git, which I always stuff up and end up with unusable code.
> 
> Next job is to Install it and see if it runs. probably tomorrow. I still can’t do that from within XCode, because it needs the BuildInstall folder stuff. Outside the XCode build. Which is a pain.
> 
> <Screenshot 2019-09-24 16.10.01.png>
> 
> 
>> On 24 Sep 2019, at 09:22, Vitalii Koshura <notifications@github.com <mailto:notifications@github.com>> wrote:
>> 
>> I have no Mac, but I believe building with XCode should be enough. If you want - we can continue private conversation on Slack or Discord to build it properly
>> 
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub <https://github.com/BOINC/boinc/pull/3208?email_source=notifications&email_token=AA6KCQWAGBH64YGIXI3G55LQLHE3LA5CNFSM4H5SJUV2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7NQZUA#issuecomment-534449360>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AA6KCQXPGZDRLONPZGJ3T2DQLHE3LANCNFSM4H5SJUVQ>.
>> 
> 
> Robert Chalmers
> https://robert-chalmers.uk <https://robert-chalmers.uk/>
> author@robert-chalmers.uk
> @R_A_Chalmers
> 



",30260406
674,TODO_OLD and TODO - Safe to delete?,open,2019-07-01T21:11:11Z,2019-10-31T10:33:56Z,,NONE,"Starting with the file TODO_OLD, it looks like it is really outdated, being last updated in 2002. The TODO file looks like it hasn't been updated since 2007. I assume that since those files are that old, they aren't really needed to be referenced from. Even if for some reason we might need it later, we'll still have the files in the git history. ",Sure,30260406
675,several -Wformat-truncation warnings,open,2019-07-01T14:08:04Z,2019-10-31T10:33:32Z,,CONTRIBUTOR,"I just compiled master with default g++ of conda which brings
```
file_xfer.cpp:88:5: warning: '%s' directive output may be truncated writing up to 4095 bytes into a region of size between 3591 and 3846 [-Wformat-truncation=]
 int FILE_XFER::init_upload(FILE_INFO& file_info) {
     ^~~~~~~~~
file_xfer.cpp:131:17: note: 'snprintf' output between 377 and 5717 bytes into a destination of size 4096
         snprintf(header, sizeof(header),
         ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~
             ""<data_server_request>\n""
             ~~~~~~~~~~~~~~~~~~~~~~~~~
             ""    <core_client_major_version>%d</core_client_major_version>\n""
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             ""    <core_client_minor_version>%d</core_client_minor_version>\n""
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             ""    <core_client_release>%d</core_client_release>\n""
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             ""<file_upload>\n""
             ~~~~~~~~~~~~~~~~~
             ""<file_info>\n""
             ~~~~~~~~~~~~~~~
             ""<name>%s</name>\n""
             ~~~~~~~~~~~~~~~~~~~
             ""<xml_signature>\n""
             ~~~~~~~~~~~~~~~~~~~
             ""%s""
             ~~~~
             ""</xml_signature>\n""
             ~~~~~~~~~~~~~~~~~~~~
             ""<max_nbytes>%.0f</max_nbytes>\n""
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             ""</file_info>\n""
             ~~~~~~~~~~~~~~~~
             ""<nbytes>%.0f</nbytes>\n""
             ~~~~~~~~~~~~~~~~~~~~~~~~~
             ""<md5_cksum>%s</md5_cksum>\n""
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             ""<offset>%.0f</offset>\n""
             ~~~~~~~~~~~~~~~~~~~~~~~~~
             ""<data>\n"",
             ~~~~~~~~~~~
             BOINC_MAJOR_VERSION, BOINC_MINOR_VERSION, BOINC_RELEASE,
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             file_info.name,
             ~~~~~~~~~~~~~~~
             file_info.xml_signature,
             ~~~~~~~~~~~~~~~~~~~~~~~~
             file_info.max_nbytes,
             ~~~~~~~~~~~~~~~~~~~~~
             file_info.nbytes,
             ~~~~~~~~~~~~~~~~~
             file_info.md5_cksum,
             ~~~~~~~~~~~~~~~~~~~~
             file_info.upload_offset
             ~~~~~~~~~~~~~~~~~~~~~~~
         );
         ~

http_curl.cpp: In member function 'void HTTP_OP::setup_proxy_session(bool)':
http_curl.cpp:813:6: warning: '%s' directive output may be truncated writing up to 255 bytes into a region of size 128 [-Wformat-truncation=]
 void HTTP_OP::setup_proxy_session(bool no_proxy) {
      ^~~~~~~
http_curl.cpp:852:21: note: 'snprintf' output between 2 and 512 bytes into a destination of size 128
             snprintf(m_curl_user_credentials, sizeof(m_curl_user_credentials),
             ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                 ""%s:%s"",
                 ~~~~~~~~
                 pi.http_user_name, pi.http_user_passwd
                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             );
             ~
http_curl.cpp:813:6: warning: '%s' directive output may be truncated writing up to 255 bytes into a region of size 128 [-Wformat-truncation=]
 void HTTP_OP::setup_proxy_session(bool no_proxy) {
      ^~~~~~~
http_curl.cpp:873:21: note: 'snprintf' output between 2 and 512 bytes into a destination of size 128
             snprintf(m_curl_user_credentials, sizeof(m_curl_user_credentials),
             ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                 ""%s:%s"",
                 ~~~~~~~~
                 pi.socks5_user_name, pi.socks5_user_passwd
                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             );
             ~
http_curl.cpp: In member function 'int HTTP_OP::libcurl_exec(const char*, const char*, const char*, double, double, bool)':
http_curl.cpp:417:5: warning: '%s' directive output may be truncated writing up to 255 bytes into a region of size 254 [-Wformat-truncation=]
 int HTTP_OP::libcurl_exec(
     ^~~~~~~
http_curl.cpp:80:17: note: 'snprintf' output between 4 and 259 bytes into a destination of size 256
         snprintf(buf, sizeof(buf), "" (%s)"", gstate.client_brand);
         ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
result.cpp:343:5: warning: '%s' directive output may be truncated writing up to 255 bytes into a region of size between 238 and 248 [-Wformat-truncation=]
 int RESULT::write_gui(MIOFILE& out) {
     ^~~~~~
result.cpp:416:21: note: 'snprintf' output between 23 and 288 bytes into a destination of size 256
             snprintf(resources, sizeof(resources),
             ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                 ""%.3g %s + %s GPU (missing)"",
                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                 avp->avg_ncpus,
                 ~~~~~~~~~~~~~~~
                 cpu_string(avp->avg_ncpus),
                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
                 avp->missing_coproc_name
                 ~~~~~~~~~~~~~~~~~~~~~~~~
             );
             ~
```",,30260406
676,Fix duplicate GPU problem,open,2019-06-28T04:00:21Z,2019-10-31T10:31:49Z,,CONTRIBUTOR,"The GPU detection logic sometimes decides that 1 GPU is actually 2.  Apparently this can happen because of buggy drivers, or because there are multiple OpenCL ""platforms"" (e.g. POCL is present).

Note: gpu_opencl.cpp contains the comment
//TODO: Must we check if multiple platforms found the same GPU and merge the records?
","@RichardHaselgrove Actually I didn't mean my analysis to be complete.

To narrow down this problem: I would *guess* that all the ""duplicate device"" problems arise from the OpenCL device handling. Is there any case of ""double device"" which involves *only* CUDA or ATI (CAL, Stream?) Apps?",30260406
677,BOINC.exe quits quietly with no error code,open,2019-06-28T03:20:09Z,2019-10-31T10:31:04Z,,NONE,"BOINC does not run on a fresh new install on Win 10 1809. Attempting to run BOINC.exe with cmd to get error message, I've got none. 
```
C:\Users\user>""C:\Program Files (x86)\BOINC\boinc.exe""
27-Jun-2019 17:17:16 [---] Starting BOINC client version 7.14.2 for windows_x86_64
27-Jun-2019 17:17:16 [---] log flags: file_xfer, sched_ops, task
27-Jun-2019 17:17:16 [---] Libraries: libcurl/7.47.1 OpenSSL/1.0.2g zlib/1.2.8
27-Jun-2019 17:17:16 [---] Data directory: C:\ProgramData\BOINC
27-Jun-2019 17:17:16 [---] Running under account sheng
27-Jun-2019 17:17:16 [---] OpenCL: Intel GPU 0: Intel(R) HD Graphics 500 (driver version 26.20.100.6912, device version OpenCL 1.2 NEO, 3215MB, 3215MB available, 72 GFLOPS peak)
27-Jun-2019 17:17:16 [---] OpenCL CPU: Intel(R) Celeron(R) CPU J3455 @ 1.50GHz (OpenCL driver vendor: Intel(R) Corporation, driver version 7.6.0.0228, device version OpenCL 1.2 (Build 0))
27-Jun-2019 17:17:16 [---] Host name: DESKTOP-V5QBTJB
27-Jun-2019 17:17:16 [---] Processor: 4 GenuineIntel Intel(R) Celeron(R) CPU J3455 @ 1.50GHz [Family 6 Model 92 Stepping 9]
27-Jun-2019 17:17:16 [---] Processor features: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss htt tm pni ssse3 cx16 sse4_1 sse4_2 movebe popcnt aes rdrandsyscall nx lm vmx tm2 pbe fsgsbase smep
27-Jun-2019 17:17:16 [---] OS: Microsoft Windows 10: Professional x64 Edition, (10.00.17763.00)
27-Jun-2019 17:17:16 [---] Memory: 7.85 GB physical, 9.10 GB virtual
27-Jun-2019 17:17:16 [---] Disk: 465.16 GB total, 168.07 GB free
27-Jun-2019 17:17:16 [---] Local time is UTC -10 hours
27-Jun-2019 17:17:16 [---] No WSL found.
27-Jun-2019 17:17:16 [---] Last benchmark was 18075 days 03:17:16 ago
27-Jun-2019 17:17:16 [---] General prefs: from http://www.worldcommunitygrid.org/ (last modified 25-May-2019 18:59:18)
27-Jun-2019 17:17:16 [---] Host location: none
27-Jun-2019 17:17:16 [---] General prefs: using your defaults
27-Jun-2019 17:17:16 [---] Reading preferences override file
27-Jun-2019 17:17:16 [---] Preferences:
27-Jun-2019 17:17:16 [---]    max memory usage when active: 4018.65 MB
27-Jun-2019 17:17:16 [---]    max memory usage when idle: 7233.56 MB
27-Jun-2019 17:17:16 [---]    max disk usage: 167.97 GB
27-Jun-2019 17:17:16 [---]    max CPUs used: 3
27-Jun-2019 17:17:16 [---]    don't use GPU while active
27-Jun-2019 17:17:16 [---]    (to change preferences, visit a project web site or select Preferences in the Manager)
27-Jun-2019 17:17:16 [---] Setting up project and slot directories
27-Jun-2019 17:17:16 [---] Checking active tasks
27-Jun-2019 17:17:16 [---] Setting up GUI RPC socket
27-Jun-2019 17:17:16 [---] Checking presence of 0 project files
27-Jun-2019 17:17:16 [---] This computer is not attached to any projects
27-Jun-2019 17:17:16 Initialization completed
27-Jun-2019 17:17:16 [---] Suspending GPU computation - computer is in use

C:\Users\user>","I did find some errors  in stderrdae.txt.

13-Jun-2019 08:27:59 getaddrinfo(wks-mac-mini-005.local): nodename nor servname provided, or not known.

Not sure if this would cause the app to exit and why would it fail to lookup the localhost.

------------------------------------------------------------------------------------------------------

I looked  in /Library/Log/DiagnosticReports. The OpenCL clients are causing wakeup events.

Date/Time:       2019-06-04 01:30:49 -0700
End time:        2019-06-04 01:33:30 -0700
OS Version:      Mac OS X 10.14.5 (Build 18F132)
Architecture:    x86_64h
Report Version:  28
Incident Identifier: 81E7BCDF-F5B9-4A1D-9C5D-7020BD9E09EF

Data Source:     Microstackshots
Shared Cache:    0x1a53d000 E235CEA5-B214-3953-8F7E-9046347475B5

Command:         setiathome_8.00_x86_64-apple-darwin__opencl_intel_gpu_sah
Path:            /Library/Application Support/BOINC Data/projects/setiathome.berkeley.edu/setiathome_8.00_x86_64-apple-darwin__opencl_intel_gpu_sah
Version:         ??? (???)
PID:             91593

Event:           wakeups
Action taken:    none
Wakeups:         45005 wakeups over the last 299 seconds (151 wakeups per second average), exceeding limit of 150 wakeups per second over 300 seconds
Wakeups limit:   45000
Limit duration:  300s
Wakeups caused:  45005
Duration:        160.74s
Steps:           6

Hardware model:  Macmini8,1
Active cpus:     12


Heaviest stack for the target process:
  5  start_wqthread + 13 (libsystem_pthread.dylib + 9213) [0x7fff71f5a3fd]

",30260406
678,Move installer from InstallShield to NSIS,open,2019-06-25T12:13:03Z,2019-10-07T02:45:50Z,,MEMBER,"Currently BOINC uses InstallShield to create a installer. It is not free and rather expensive.
I suggest to move away from it and use NSIS instead ( https://nsis.sourceforge.io/Main_Page ).
It is free and easy to use.
I played a little bit with it and found it very easy. I can create test installer next week or so. 
If we consider to use it we should also rewrite boinccas dll that is built with VS2010 now and I found no way to build it with newer versions of VS because of OLE dll usage that is used to communicate with InstallShield installer. So we will need either to rewrite this library on NSIS script language or just clean up existing library from all InstalShield references.
Also moving to NSIS will give us an opportunity to extend existing functionality of installer that is requested by users.
Any thoughts?
I'd like to discuss this on the upcoming Contributions call 6/27/19.",,30260406
679,Client never asks for work when there are stalled downloads,open,2019-06-24T13:10:14Z,2019-10-31T10:03:14Z,,CONTRIBUTOR,"**Describe the bug**
I created some work using a temporary download server. The download server was then terminated, but some hosts were still downloading. They are now unable to download, and they no longer ask for work: ""Not requesting tasks: some download is stalled"".

The downloads never terminate as they point to a non-existent DNS name (""can't resolve hostname"") - download termination is done when a 404 error exists, but not when DNS name does not exist.

I cannot obtain the DNS name again as the provider does not allow one to choose names and instead generates random ones.

**Steps To Reproduce**
1. Create work with data files stored on a temporary domain name (e.g. AWS S3 bucket)
2. Download work, but terminate client while the downloads are still in progress.
3. Terminate the temporary download server
4. Restart the client. Downloads will now fail, and client won't ever ask for more work.

**Expected behavior**
Terminate downloads after some time of continuing failures (e.g. 72 hours).","The project is also in control for the problem you describe. If they know they will have resources for uploads, they can continue issuing work; otherwise they can stop and wait for enough uploads to happen. There's already intervention needed within the project in case of outages, e.g. stop resending work, extend deadlines on the server side, etc.",30260406
680,Alternative CPU settings,open,2019-06-19T08:40:12Z,2020-01-12T01:05:57Z,,NONE,"During the day, I use my machine and need to spare some CPU speed for my usage.
But at night, I can fully load the CPU.

**I would like to be able to switch between two CPU setting profiles automatically**, based on the time of day : few CPU for BOINC during office hours and full access at night.

***

Why would it be needed while BOINC is running with a massive nice setting under GNU+Linux ? It's **because my CPU works at variable speeds** :
- If only one core is running alone it can reach 3GHz (for a short period of time)
- but if my two cores are running at full speed they are stuck at ~1GHz each, and I feel the difference when my tasks are running ~3 time slower.

But I noticed that running both cores + HyperThreading produces more computation for BOINC than one core alone at full speed (mainly because the 3GHz spikes don't last long).

***
Such a feature is available in Transmission BitTorrent client, and I miss it in BOINC :-)
In Transmission, we have, moreover, a turtle icon shown in the status bar (at the bottom of the window) which allows to easily switch between the two setting profiles, overriding the schedule when needed.",There are already tickets for this: #1378 and #1379,30260406
681,develop new GUI using Electron,open,2019-06-18T01:38:36Z,2019-10-31T10:01:35Z,,CONTRIBUTOR,"Electron (https://electronjs.org/) is a possible framework for developing a new GUI.
It's cross-platform, uses HTML for layout and CSS for styling.
It seems to be widely used and well-supported.

If anyone is interested in working on this, please let me know.

BTW, the BOINC client exposes GUI RPCs via HTTP;
I've verified that these work with the HTTP and XML features built into Javascript.","Electron creates a stand alone app. The chromium inside electron is a browser just for rendering html/css and running javascript. It doesn't has the ""same origin policy"" and other security feature which a normal browser has. I wrote already electron applications which communicate via http to different server or which run soap operations and also parses xml.",30260406
682,VirtualBox Jobs crashing if Hyper-V was installed,open,2019-06-16T21:50:56Z,2019-10-31T10:01:10Z,,MEMBER,"**Describe the bug**
I use VirtualBox to run VM jobs and everything was working fine. I had to install Hyper-V to do some job, then I uninstalled it. After the uninstall the VM jobs are crashing.

**Steps To Reproduce**
1. Install VirtualBox and run a Job.
2. Install Hyper-V.
3. Remove Hyper-V.
4. Run VirtualBox jobs again.

**System Information (please complete the following information):**
 - OS: Win 10
 - BOINC Version: 7.14.2

**Additional context**
The log shows: `Vbox app stderr indicates CPU VM extensions disabled`, but it's enabled.
",Nothing.,30260406
683,Client: generalize proxy management in cc_config.xml,open,2019-06-12T10:34:44Z,2019-10-31T10:00:21Z,,CONTRIBUTOR,"**Describe the problem**
Forwarding a wish list request from https://setiathome.berkeley.edu/forum_thread.php?id=84300

**Describe the solution you'd like**
> I wish the HTTP_CURL.CPP module compared only the -last (length of extracted argument from no_host) characters- of the URL. Then a generic
""don't use a proxy for *.name.HLD"" would work for multiple servers, and automatically exclude any additional servers as well.

**Additional context**
See source thread.
",One way to do this is to allow regular expressions in the <no_proxy> list.,30260406
684,Add AVX-512 detection,open,2019-06-06T15:05:27Z,2019-06-06T21:01:33Z,,MEMBER,"**Describe the problem**
The Boinc Client currently detects AVX and AVX2 instruction capabilities, but not the [AVX-512](https://www.intel.com/content/www/us/en/architecture-and-technology/avx-512-overview.html). 
Intel says about this new instruction set:

> Intel® AVX-512 is a set of new instructions that can accelerate performance for workloads and usages such as scientific simulations, financial analytics, artificial intelligence (AI)/deep learning, 3D modeling and analysis, image and audio/video processing, cryptography and data compression.2

The first CPU with AVX-512 instruction was released in [2015](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions).",,30260406
685,Add option to not stop non-CPU intensive (NCI) tasks.,open,2019-06-04T13:40:11Z,2019-10-31T09:59:25Z,,MEMBER,"**Describe the problem**
I have non-CPU intensive tasks alongside CPU intensive tasks. I use the snooze option quite regularly and it would be nice if the NCI tasks are won't stop.

**Describe the solution you'd like**
I'd like an option inside the ""Computing"" preferences to not stop NCI tasks, even if BOINC is suspended.","If anybody feels minded to work this issue up into a full PR, please also add the final missing option ""snooze/suspend CPUs, but leave GPUs running"".

Yes, I'm aware of the formal statement that every GPU task requires CPU support, but let's be real-world about this. Anybody who's ever run a GPU app under BOINC knows that the actual CPU demand varies massively from project to project, from application to application, from programming language to programming language, from manufacturer to manufacturer. The person who knows most about the performance characteristics of the particular computer is the person sitting in front of the keyboard and screen - the user. Give them the choice.",30260406
686,mac build + unix makefiles,open,2019-05-28T11:40:43Z,2019-10-31T09:46:51Z,,CONTRIBUTOR,"Before @CharlieFenton et al say, I know this is not the supported method.

Some notes on the suggested xcode method, it does not build boinc_api_fortran.o and I would prefer to use gcc since our work needs a fortran compiler, and also we can use the same compiler across all platforms. Note - we only build the core libraries and not the client, etc.

Problem 1:
```
  OBJC     libboinc_graphics2_la-macglutfix.lo
macglutfix.m:34:9: fatal error: 'MultiGPUMig.h' file not found
#import ""MultiGPUMig.h""
        ^~~~~~~~~~~~~~~
1 error generated. 
```

Fixed by running the api folder:
```
mig  -header MultiGPUMig.h -sheader MultiGPUMigServer.h MultiGPUMig.defs
```

Problem 2:
```
  CXX      libboinc_la-procinfo_mac.lo
procinfo_mac.cpp: In function 'int procinfo_setup(PROC_MAP&)':
procinfo_mac.cpp:125:14: error: 'strchr' was not declared in this scope
  125 |         lf = strchr(p.command, '\n');
      |              ^~~~~~
procinfo_mac.cpp:37:1: note: 'strchr' is defined in header '<cstring>'; did you forget to '#include <cstring>'?
   36 | #include ""mac_branding.h""
  +++ |+#include <cstring>
   37 | 
procinfo_mac.cpp:130:42: error: 'strcasestr' was not declared in this scope
  130 |         p.is_boinc_app = (p.id == pid || strcasestr(p.command, ""boinc""));
      |                                          ^~~~~~~~~~
make[2]: *** [libboinc_la-procinfo_mac.lo] Error 1
```

Fixed by including the above mentioned header.

Problem 3:

```
  CXX      libboinc_la-mac_spawn.lo
In file included from /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks/CoreGraphics.framework/Headers/CGContext.h:21,
                 from /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks/CoreGraphics.framework/Headers/CGBitmapContext.h:9,
                 from /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks/CoreGraphics.framework/Headers/CoreGraphics.h:11,
                 from /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks/ApplicationServices.framework/Headers/ApplicationServices.h:35,
                 from /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks/Carbon.framework/Headers/Carbon.h:24,
                 from mac/mac_spawn.cpp:23:
/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks/CoreGraphics.framework/Headers/CGPath.h:391:15: error: expected unqualified-id before '^' token
  391 | typedef void (^CGPathApplyBlock)(const CGPathElement * element);
      |               ^
/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks/CoreGraphics.framework/Headers/CGPath.h:391:15: error: expected ')' before '^' token
  391 | typedef void (^CGPathApplyBlock)(const CGPathElement * element);
      |              ~^
      |               )
```
and so on.

The problem here is that a .cpp file is including Carbon.h which is an objective C header.
This is the case in a few of the mac files - they look like c++ but are actually objective c++, and the simple fix is to just rename them from .cpp to .mm, then everything is happy. A small libtool fix is also required.

Problem 4:
The makefile does not build mac/mac_branding.cpp
Adding this to the Makefile.am fixes the linking issues one will obtain.



In general this is just a series of trivial fixes for which I can make a pull request, with the only question here being if I can rename a few .cpp files to .mm?","You may want to give a look to [this code](https://github.com/BOINC/boinc/files/3262003/gnome_idle.txt) I have written, and how I let GCC compile the C code I included in C++ BOINC client code",30260406
687,VBoxWrapper: Allow more retries when registration fails,open,2019-05-27T13:51:14Z,2019-11-04T21:06:37Z,,NONE,"When VBoxWrapper fails to register a VM, allow more retries so the user may be able to fix the problem. Currently, 26200 will error the task with ""Computation error"", for a scenario I could have corrected if giving more chances. I deem that a bug.

In my case, I lost 108 days of work because a power outage happened during taking a snapshot. It appeared that, after 2 restarts of the VM, VBoxWrapper tried to re-register it, got an error, and immediately error'd the task. Very frustrating. Logs below.

I'm requesting to give the user more chances. I think in other places in code, we allow something like ""up to 10 or 20 or 30"" retries. We should use that logic here. Maybe look for similar places where the user may be able to fix it, and allow more retries.

Thanks.

Task error link:
https://www.rnaworld.de/rnaworld/result.php?resultid=14954358

Relevant portion of Log:
> 2019-05-23 11:03:46 (9572): Creating new snapshot for VM.
2019-05-23 11:04:02 (9572): Deleting stale snapshot.
2019-05-23 11:04:02 (9572): Checkpoint completed.
2019-05-23 11:33:50 (9572): Creating new snapshot for VM.
2019-05-23 11:34:06 (9572): Deleting stale snapshot.
2019-05-23 11:34:06 (9572): Checkpoint completed.
2019-05-23 12:03:55 (9572): Creating new snapshot for VM.
2019-05-23 17:59:46 (7384): vboxwrapper (7.9.26200): starting
2019-05-23 17:59:46 (7384): Feature: Checkpoint interval offset (442 seconds)
2019-05-23 17:59:46 (7384): Feature: Enabling trickle-ups (Interval: 14400.000000)
2019-05-23 17:59:46 (7384): Detected: VirtualBox COM Interface (Version: 5.2.31)
2019-05-23 17:59:46 (7384): Detected: Minimum checkpoint interval (1800.000000 seconds)
2019-05-23 17:59:46 (7384): Restore from previously saved snapshot.
2019-05-23 17:59:46 (7384): Restore completed.
2019-05-23 17:59:46 (7384): Starting VM. (boinc_d3bace1e9e4bf26c, slot#1)
2019-05-24 00:03:24 (6596): vboxwrapper (7.9.26200): starting
2019-05-24 00:03:24 (6596): Feature: Checkpoint interval offset (395 seconds)
2019-05-24 00:03:24 (6596): Feature: Enabling trickle-ups (Interval: 14400.000000)
2019-05-24 00:03:24 (6596): Detected: VirtualBox COM Interface (Version: 5.2.31)
2019-05-24 00:03:24 (6596): Detected: Minimum checkpoint interval (1800.000000 seconds)
2019-05-24 00:03:24 (6596): Register VM. (boinc_d3bace1e9e4bf26c, slot#1)
2019-05-24 00:03:24 (6596): Error 0x80004005 in vbox52::VBOX_VM::register_vm (c:\users\david\documents\boinc_git\boinc\samples\vboxwrapper\vbox_mscom_impl.cpp:885)
2019-05-24 00:03:24 (6596): Error Source     : MachineWrap
2019-05-24 00:03:24 (6596): Error Description: Trying to open a VM config 'D:\BOINC Data\slots\1\boinc_d3bace1e9e4bf26c\boinc_d3bace1e9e4bf26c.vbox' which has the same UUID as an existing virtual machine
2019-05-24 00:03:24 (6596): Could not register
2019-05-24 00:03:24 (6596): Powering off VM.
2019-05-24 00:03:24 (6596): Deregistering VM. (boinc_d3bace1e9e4bf26c, slot#1)",,30260406
688,Docker web-based manager,open,2019-05-24T00:30:03Z,2019-10-31T09:46:09Z,,MEMBER,"I created a [Boinc Web Manager](https://github.com/adamradocz/boinc-manager) in asp.net core. It uses Docker. It's very rudimentary, currently, it can only show information about the client, but it has multiclient support and very useful if you want to monitor multiple clients on your network.

The [Boinc Client Docker](https://github.com/BOINC/boinc-client-docker) is very popular, it's over 1 million downloads, so I think this one could be too.
I'd like to create an official Boinc Web Manager in Docker Hub.

@davidpanderson, @AenBleidd @marius311 @TheAspens What do you guys think about it?
",,30260406
689,Linux systemd: delay BOINC startup until after video drivers have loaded,open,2019-05-16T12:59:50Z,2019-10-31T09:44:57Z,,CONTRIBUTOR,"**Describe the problem**
If the BOINC client starts before drivers are ready, GPU detection fails and any GPU is unusable throughout the session.

See, for example, https://boinc.berkeley.edu/forum_thread.php?id=12946

**Describe the solution you'd like**
Re-position BOINC client startup in repository scripts
",@SETIguy https://en.wikipedia.org/wiki/Udev,30260406
690,"If watching a project's graphics, and the task reaches 100%, subsequent tasks will error out until graphics closed",open,2019-05-16T06:45:51Z,2019-10-31T09:44:20Z,,NONE,"**Describe the bug**
When watching the graphics of a task via the ""Show Graphics"" button, when the task reaches 100% completion, a domino effect of errors of subsequent tasks will occur until the graphics window is closed.  I am unsure if this is a BOINC issue or a project specific issue.  I am able to re-create this in World Community Grid's Microbiome Immunity Project application.

**Steps To Reproduce**
1.  Click ""Show Graphics"" on a work unit that is nearing 100% completion (i.e., in the 90's with a few minutes left).
2.  Wait for the work unit to reach 100% successful completion.
3.  Observe a domino effect of errors as subsequent work units in the queue error out one by one.
4.  Close the graphics window for the work unit that just completed.
5.  Observe the errors stop.

**Expected behavior**
I probably expect that when a work unit reaches 100% completion for the graphics window to automatically close, requiring the user to select a new work unit to watch if so desired.

**Screenshots**
![BOINC MIP Computation Error](https://user-images.githubusercontent.com/25557897/57831947-768f4500-7784-11e9-8b8b-75ee99575953.PNG)

![BOINC MIP Computation Error 2](https://user-images.githubusercontent.com/25557897/57831977-83ac3400-7784-11e9-945c-ff2c5f9c3489.PNG)

**System Information (please complete the following information):**
 - OS:  Windows 10 Pro 64-bit.  Version 1809 (Build 17763.45)
 - BOINC Version:  BOINC 7.14.2 for Windows 64-bit

**Additional context**
BOINC is configured to run as a service.  I don't know if this is relevant to this bug.
","(Not sure why line breaks did not carry through when copying and pasting.)

**Result Log**

```
Result Name: MIP1_ 00189279_ 0839_ 0--
<core_client_version>7.14.2</core_client_version>
<![CDATA[
<message>
(unknown error) - exit code -1073741819 (0xc0000005)</message>
<stderr_txt>
[2019- 5-16 2:21: 4:] :: BOINC:: Initializing ... ok.
[2019- 5-16 2:21: 4:] :: BOINC :: boinc_init()
INFO: result number = 0
BOINC:: Setting up shared resources ... ok.
failed to create shared mem segment: minirosetta Size: 25057688


Unhandled Exception Detected...

- Unhandled Exception Record -
Reason: Access Violation (0xc0000005) at address 0x012E8F10 write attempt to address 0x017D7EC9

Engaging BOINC Windows Runtime Debugger...



********************


BOINC Windows Runtime Debugger Version 7.7.0


Dump Timestamp : 05/16/19 02:21:05
Install Directory :
Data Directory : C:\ProgramData\BOINC
Project Symstore :
LoadLibraryA( C:\ProgramData\BOINC\dbghelp.dll ): GetLastError = 126
Loaded Library : dbghelp.dll
LoadLibraryA( C:\ProgramData\BOINC\symsrv.dll ): GetLastError = 126
LoadLibraryA( symsrv.dll ): GetLastError = 126
LoadLibraryA( C:\ProgramData\BOINC\srcsrv.dll ): GetLastError = 126
LoadLibraryA( srcsrv.dll ): GetLastError = 126
LoadLibraryA( C:\ProgramData\BOINC\version.dll ): GetLastError = 126
Loaded Library : version.dll
Debugger Engine : 4.0.5.0
Symbol Search Path: C:\ProgramData\BOINC\slots\1;C:\ProgramData\BOINC\projects\www.worldcommunitygrid.org


ModLoad: 0000000000860000 000000000342a000 C:\ProgramData\BOINC\projects\www.worldcommunitygrid.org\wcgrid_mip1_rosetta_7.16_windows_intelx86 (-exported- Symbols Loaded)
Linked PDB Filename :

ModLoad: 0000000076f20000 000000000019c000 C:\Windows\SYSTEM32\ntdll.dll (6.2.17763.475) (-exported- Symbols Loaded)
Linked PDB Filename : wntdll.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000074b90000 00000000000e0000 C:\Windows\System32\KERNEL32.DLL (6.2.17763.475) (-exported- Symbols Loaded)
Linked PDB Filename : wkernel32.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000075a80000 00000000001fa000 C:\Windows\System32\KERNELBASE.dll (6.2.17763.475) (-exported- Symbols Loaded)
Linked PDB Filename : wkernelbase.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000075e20000 000000000005f000 C:\Windows\System32\WS2_32.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : ws2_32.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 00000000745b0000 00000000000bf000 C:\Windows\System32\RPCRT4.dll (6.2.17763.379) (-exported- Symbols Loaded)
Linked PDB Filename : wrpcrt4.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000074590000 0000000000020000 C:\Windows\System32\SspiCli.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : wsspicli.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000074580000 000000000000a000 C:\Windows\System32\CRYPTBASE.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : cryptbase.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000076ba0000 0000000000062000 C:\Windows\System32\bcryptPrimitives.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : bcryptprimitives.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 00000000747f0000 0000000000079000 C:\Windows\System32\sechost.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : sechost.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000076d70000 0000000000199000 C:\Windows\System32\USER32.dll (6.2.17763.168) (-exported- Symbols Loaded)
Linked PDB Filename : wuser32.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000075e80000 0000000000017000 C:\Windows\System32\win32u.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : wwin32u.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000075ea0000 0000000000023000 C:\Windows\System32\GDI32.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : wgdi32.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000074d90000 0000000000167000 C:\Windows\System32\gdi32full.dll (6.2.17763.475) (-exported- Symbols Loaded)
Linked PDB Filename : wgdi32full.pdb
File Version : 10.0.17763.475 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.475

ModLoad: 00000000761e0000 0000000000080000 C:\Windows\System32\msvcp_win.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : msvcp_win.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000075950000 0000000000122000 C:\Windows\System32\ucrtbase.dll (6.2.17763.404) (-exported- Symbols Loaded)
Linked PDB Filename : ucrtbase.pdb
File Version : 10.0.17763.404 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.404

ModLoad: 0000000074670000 000000000007e000 C:\Windows\System32\ADVAPI32.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : advapi32.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000076cb0000 00000000000c0000 C:\Windows\System32\msvcrt.dll (7.0.17763.475) (-exported- Symbols Loaded)
Linked PDB Filename : msvcrt.pdb
File Version : 7.0.17763.475 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 7.0.17763.475

ModLoad: 00000000702e0000 0000000000029000 C:\Windows\SYSTEM32\ntmarta.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : ntmarta.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000073960000 000000000018f000 C:\Windows\SYSTEM32\dbghelp.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : dbghelp.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1

ModLoad: 0000000073f70000 0000000000008000 C:\Windows\SYSTEM32\version.dll (6.2.17763.1) (-exported- Symbols Loaded)
Linked PDB Filename : version.pdb
File Version : 10.0.17763.1 (WinBuild.160101.0800)
Company Name : Microsoft Corporation
Product Name : Microsoft&#174; Windows&#174; Operating System
Product Version : 10.0.17763.1



*** Dump of the Process Statistics: ***

- I/O Operations Counters -
Read: 5, Write: 0, Other 442

- I/O Transfers Counters -
Read: 0, Write: 113, Other 0

- Paged Pool Usage -
QuotaPagedPoolUsage: 159000, QuotaPeakPagedPoolUsage: 159000
QuotaNonPagedPoolUsage: 8624, QuotaPeakNonPagedPoolUsage: 8624

- Virtual Memory Usage -
VirtualSize: 153366528, PeakVirtualSize: 153366528

- Pagefile Usage -
PagefileUsage: 45871104, PeakPagefileUsage: 45871104

- Working Set Size -
WorkingSetSize: 27680768, PeakWorkingSetSize: 27684864, PageFaultCount: 6895

*** Dump of thread ID 5252 (state: Waiting): ***

- Information -
Status: Wait Reason: UserRequest, , Kernel Time: 156250.000000, User Time: 468750.000000, Wait Time: 5805242.000000

- Unhandled Exception Record -
Reason: Access Violation (0xc0000005) at address 0x012E8F10 write attempt to address 0x017D7EC9

- Registers -
eax=00000000 ebx=04f4f960 ecx=810676a6 edx=00000000 esi=00000000 edi=00d5219e
eip=012e8f10 esp=049ed0e4 ebp=049ffb6c
cs=0023 ss=002b ds=002b es=002b fs=0053 gs=002b efl=00010206

- Callstack -
ChildEBP RetAddr Args to Child
049ffb6c 00d4ddfb 81075872 00d5219e 00d5219e 00000000 wcgrid_mip1_rosetta_7!cppdb::backend::statements_cache::active+0x0
049ffe6c 00d52121 0000000e 04f4f960 04f4b440 810758aa wcgrid_mip1_rosetta_7!cppdb::atomic_counter::get+0x0
049ffeb4 74bb0419 03ff4000 74bb0400 049fff20 76f8662d wcgrid_mip1_rosetta_7!cppdb::atomic_counter::get+0x0
049ffec4 76f8662d 03ff4000 610958d6 00000000 00000000 KERNEL32!BaseThreadInitThunk+0x0
049fff20 76f865fd ffffffff 76fa51cd 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0
049fff30 00000000 00d5219e 03ff4000 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0

*** Dump of thread ID 5104 (state: Waiting): ***

- Information -
Status: Wait Reason: EventPairLow, , Kernel Time: 0.000000, User Time: 0.000000, Wait Time: 5805233.000000

- Registers -
eax=00000000 ebx=04f4af20 ecx=00000000 edx=00000000 esi=04f4ad60 edi=04f49328
eip=76f9216c esp=05f1f708 ebp=05f1f8c4
cs=0023 ss=002b ds=002b es=002b fs=0053 gs=002b efl=00000202

- Callstack -
ChildEBP RetAddr Args to Child
05f1f8c4 74bb0419 04f49328 74bb0400 05f1f930 76f8662d ntdll!NtWaitForWorkViaWorkerFactory+0x0
05f1f8d4 76f8662d 04f49328 60675ec6 00000000 00000000 KERNEL32!BaseThreadInitThunk+0x0
05f1f930 76f865fd ffffffff 76fa51cd 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0
05f1f940 00000000 76f6e230 04f49328 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0

*** Dump of thread ID 6792 (state: Waiting): ***

- Information -
Status: Wait Reason: EventPairLow, , Kernel Time: 0.000000, User Time: 0.000000, Wait Time: 5805233.000000

- Registers -
eax=00000000 ebx=04f4b820 ecx=00000000 edx=00000000 esi=04f4b660 edi=04f49328
eip=76f9216c esp=0691fc0c ebp=0691fdc8
cs=0023 ss=002b ds=002b es=002b fs=0053 gs=002b efl=00000206

- Callstack -
ChildEBP RetAddr Args to Child
0691fdc8 74bb0419 04f49328 74bb0400 0691fe34 76f8662d ntdll!NtWaitForWorkViaWorkerFactory+0x0
0691fdd8 76f8662d 04f49328 630759c2 00000000 00000000 KERNEL32!BaseThreadInitThunk+0x0
0691fe34 76f865fd ffffffff 76fa51cd 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0
0691fe44 00000000 76f6e230 04f49328 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0

*** Dump of thread ID 10876 (state: Waiting): ***

- Information -
Status: Wait Reason: EventPairLow, , Kernel Time: 0.000000, User Time: 0.000000, Wait Time: 5805240.000000

- Registers -
eax=00000000 ebx=04f4fd40 ecx=00000000 edx=00000000 esi=04f4fb80 edi=04f49328
eip=76f9216c esp=0731f880 ebp=0731fa3c
cs=0023 ss=002b ds=002b es=002b fs=0053 gs=002b efl=00000212

- Callstack -
ChildEBP RetAddr Args to Child
0731fa3c 74bb0419 04f49328 74bb0400 0731faa8 76f8662d ntdll!NtWaitForWorkViaWorkerFactory+0x0
0731fa4c 76f8662d 04f49328 62a75d5e 00000000 00000000 KERNEL32!BaseThreadInitThunk+0x0
0731faa8 76f865fd ffffffff 76fa51cd 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0
0731fab8 00000000 76f6e230 04f49328 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0

*** Dump of thread ID 2976 (state: Waiting): ***

- Information -
Status: Wait Reason: ExecutionDelay, , Kernel Time: 0.000000, User Time: 0.000000, Wait Time: 5805238.000000

- Registers -
eax=01928ca0 ebx=08bffce8 ecx=00000000 edx=00000000 esi=00000000 edi=08bffce8
eip=76f907ec esp=08bffca8 ebp=08bffd0c
cs=0023 ss=002b ds=002b es=002b fs=0053 gs=002b efl=00000202

- Callstack -
ChildEBP RetAddr Args to Child
08bffd0c 75b8dbdf 00000064 00000000 08bfff48 01928ccb ntdll!ZwDelayExecution+0x0
08bffd1c 01928ccb 00000064 01928ca0 01928ca0 00000000 KERNELBASE!Sleep+0x0
08bfff48 74bb0419 00000000 74bb0400 08bfffb4 76f8662d wcgrid_mip1_rosetta_7!cppdb::backend::statements_cache::active+0x0
08bfff58 76f8662d 00000000 6d295842 00000000 00000000 KERNEL32!BaseThreadInitThunk+0x0
08bfffb4 76f865fd ffffffff 76fa51cd 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0
08bfffc4 00000000 01928ca0 00000000 00000000 00000000 ntdll!RtlGetAppContainerNamedObjectPath+0x0


*** Debug Message Dump ****


*** Foreground Window Data ***
Window Name :
Window Class :
Window Process ID: 0
Window Thread ID : 0

Exiting...

</stderr_txt>
]]>
```",30260406
691,Error compiling BOINC server from source on Fedora RedHat 8.2,open,2019-05-09T20:11:16Z,2020-03-24T13:50:06Z,,NONE,"I am getting an error when trying to compile a BOINC server from source.
automake and configure work fine, but when I try make I get this error
/usr/bin/ld: db_dump.o undefined reference to symbol gzwrite
//usr/lib64/libz.so.1: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status


","Not sure how this issue was opened for RHEL 8.2 in May 2019 when RHEL 8 came out in May 2019. I'm able to compile the client on RHEL 8.2 beta but so far not the server. I get a different error:

CXXLD    libboinc.la
libtool:   error: 'libboinc_la-keyword.lo' is not a valid libtool object
make[2]: *** [Makefile:1164: libboinc.la] Error 1
make[2]: *** Waiting for unfinished jobs....
  CXXLD    libboinc.la

But that appears to be a multi-threaded make error. If I just run make a few times or with a single thread it builds the client and server just fine.

",30260406
692,[Manager] Support dark theme mode on Linux,open,2019-05-02T11:39:19Z,2020-01-02T10:24:25Z,,MEMBER,"Currently BOINC Manager is almost unusable when using dark theme on Linux.
It is a consolidated ticket that includes next issues:

- #812
- #980
- #1072
- #1126
- #2919
- #3112 
- #3415",,30260406
693,"On Linux, the manager cannot start the core client as a service",open,2019-04-18T15:04:03Z,2019-09-27T17:44:20Z,,MEMBER,"This issue comes from discussions during the contributor call that was hosted on April 18, 2019 when discussing pull request #3094 

The installation on Linux sets up the core client to run as a service.  If the user sets their permissions correctly and then runs the BOINC Manager with the directory to /var/lib/boinc then they are able to connect to the core client.

The user is then able to cause the core client to exit.  If they then open the manager again then the manager will start up a instance of the core-client with the directory set to /var/lib/boinc the manager will start up the core-client but run it under the user's id instead of the boinc id.  This can cause a lot of issues.

The BOINC manager should instead detect that the /var/lib/boinc was run as a service and if the client is not running, it should then attempt to start the service or provide the users to instructions on how to do so.","Actually rather than configuring /etc/sudoers.d, another way is with a polkit configuration:

/etc/polkit-1/rules.d/57-manage-boinc-client-service.rules (centos/fedora)

```
polkit.addRule(function(action, subject) {
    if (action.id == ""org.freedesktop.systemd1.manage-units"" &&
        action.lookup(""unit"") == ""boinc-client.service"" &&
        subject.group == ""boinc"") {
        return polkit.Result.YES;
    }
});
```

Unfortunately, this is not available until polkit v 226 and RedHat/CentOS aren't on a high enough level. (ref: https://superuser.com/questions/1064616/polkit-systemd-interaction)

[edit:  as a FYI - I was testing and writing this and didn't see @Ferrion 's response until after I posted]",30260406
694, client crash,open,2019-04-15T17:58:45Z,2019-10-31T09:41:25Z,,CONTRIBUTOR,"I've seen this crash several times, over the last several years.

>	boinc.exe!MESSAGE_DESCS::insert(PROJ_AM * p, int priority, int now, char * message)  Line 188 + 0xd bytes	C++
 	boinc.exe!show_message(PROJ_AM * p, char * msg, int priority, bool is_html, const char * link)  Line 92	C++
 	boinc.exe!msg_printf(PROJ_AM * p, int priority, const char * fmt, ...)  Line 153	C++
 	boinc.exe!CLIENT_STATE::handle_scheduler_reply(PROJECT * project, char * scheduler_url)  Line 656	C++
 	boinc.exe!SCHEDULER_OP::poll()  Line 507 + 0x2c bytes	C++
 	boinc.exe!CLIENT_STATE::scheduler_rpc_poll()  Line 442	C++
 	boinc.exe!CLIENT_STATE::poll_slow_events()  Line 1153 + 0xd bytes	C++
 	boinc.exe!boinc_main_loop()  Line 384 + 0xc bytes	C++
 	boinc.exe!main(int argc, char * * argv)  Line 507 + 0x5 bytes	C++
 	boinc.exe!__tmainCRTStartup()  Line 555 + 0x19 bytes	C
 	boinc.exe!mainCRTStartup()  Line 371	C
 	kernel32.dll!00000000778759cd() 	
 	[Frames below may be incorrect and/or missing, no symbols loaded for kernel32.dll]	
 	ntdll.dll!0000000077aaa561() 	
","No crash dump?
",30260406
695,merge generic Unix and Solaris man pages,open,2019-04-08T18:23:26Z,2019-10-31T09:35:59Z,,CONTRIBUTOR,"We have generic Unix man pages in [doc/manpages](https://github.com/BOINC/boinc/tree/master/doc/manpages) and we have Solaris man pages in [packages/solaris/CSW/boincclient](https://github.com/BOINC/boinc/tree/master/packages/solaris/CSW/boincclient) and [packages/solaris/CSW/boincmanager](https://github.com/BOINC/boinc/tree/master/packages/solaris/CSW/boincmanager).

There is no point in duplicating the man pages. Instead the man pages should be merged so that in the end we'll have them only in DocBook source format. The Solaris man pages have some bits of Solaris specific information. The generic version could have some place for such OS / Linux distro specific information.","The packaging responsibility have been taken over by the people involved in the various distributions. Their packing configuration files and scripts are in their own distribution specific repositories for Fedora, Debian and OpenSUSE. ",30260406
696,Enable browser-based computing in BOINC,open,2019-04-03T23:26:55Z,2019-10-31T09:35:42Z,,CONTRIBUTOR,"It's possible to do scientific computing with GPUs using WebGL, on a variety of devices including phones.  The easiest way to do this is from Javascript running in a browser.

Here's an idea: write a lightweight ""BOINC client"" in JS, which gets jobs from BOINC servers via the scheduler RPC protocol (or maybe a JSON-ized version of it).  The jobs would be JS programs, possibly using WebGL.

Note: modern browsers have a provision for ""background jobs"" that run independently of browser windows.","It looks like [WebAssemly](https://webassembly.org/) is the future. Maybe worth considering converting the client to Wasm, rather than JavaScript. It will be much faster, especially in the future, when the WebAssemly gets more optimization.
[A Real-World WebAssembly Benchmark](https://news.ycombinator.com/item?id=17463898)",30260406
697,Investigate data privacy,open,2019-03-26T22:44:42Z,2019-10-31T09:34:56Z,,CONTRIBUTOR,"Lack of data privacy is a barrier to the use of BOINC
in biomedical and commercial applications.

Investigate ways to provide data privacy, perhaps using ideas from here:
https://en.wikipedia.org/wiki/Secure_multi-party_computation
or hardware features such as Intel SGX:
https://en.wikipedia.org/wiki/Software_Guard_Extensions",,30260406
698,Client: review work request issues following #2918 and #3001,open,2019-03-21T16:03:14Z,2019-10-31T09:34:15Z,,CONTRIBUTOR,"#2918 (fix job scheduling bug) has been accepted and merged. But in the process, adjustments were made to work fetch which were less than ideal. Quoting from the PR,

https://github.com/BOINC/boinc/pull/2918#issuecomment-450718595: @davidpanderson wrote

> I'm going to stop here with this PR. Maintaining a full buffer in the presence of max_concurrent would require nontrivial changes to the RR simulation which I might do later.

https://github.com/BOINC/boinc/pull/2918#issuecomment-460804035: @JuhaSointusalo wrote

> Even though I merged this I don't consider the job done yet. As previously discussed in my opinion the work fetch code now needs improving.

- If work fetch is blocked as long as there are max_concurrent or more tasks in buffer then before work fetch resumes there has to be an idle device.
- Those who want to use max_concurrent will now have to choose between using max_concurrent and having work buffered. My gut feeling is that people will choose work buffer and that means they won't be able to upgrade BOINC beyond 7.14 as long as the work fetch works the way it does now.

My view remains that the work fetch ceiling should, as now, reflect the sum of the base and additional work buffers, expressed in wall-clock time but proportional to the number of devices in use (max_concurrent rather than nCPUs).

#3001 (fix possible overflow in peak FLOPS) has also been merged, and also fixed in server release 1.0.4. But in [Message Board conversation](https://boinc.berkeley.edu/forum_thread.php?id=12842&postid=90266#90266) I noticed an anomaly, not previously reported on GitHub.

Work fetch is currently determined by [Client Scheduling October 2010 - Estimated credit](https://boinc.berkeley.edu/trac/wiki/ClientSchedOctTen#Estimatedcredit). I compared two separate work fetch logs, the first with the Peak FLOPS overflow bug, and the second without.

```
				RAC		    REC
Einstein@Home		  18,906.01	142,297,886.589
MilkyWay		   4,143.10	    216,419.883
SETI@home		   6,462.70	605,565,081.292

OpenCL: AMD/ATI GPU 0: AMD Radeon(TM) Vega 8 Graphics (43,980,464 GFLOPS peak)
```
The SETI GPU REC has been blown out of the water by the GFlops error. I don't think we'd thought of that before. If that's the only GPU in the system, it shouldn't make much difference - but it would unbalance the work fetch calculations between CPU, other GPUs, and the Ryzen GPU component.
```
				RAC		    REC
Einstein@Home		   8,119.18	      8,322.576
GPUGRID			 391,764.22	    303,164.317
NumberFields@home	   3,222.80	      1,834.406
SETI@home		  10,627.91	    367,015.648

OpenCL: NVIDIA GPU 0: GeForce GTX 970 (4,087 GFLOPS peak)
OpenCL: NVIDIA GPU 1: GeForce GTX 750 Ti (1,639 GFLOPS peak)
OpenCL: Intel GPU 0: Intel(R) HD Graphics 4600 (192 GFLOPS peak)
```
This is one of my machines, and I run:

- Einstein on the Intel GPU
- GPUGrid on the GTX 970
- SETI on the GTX 750 Ti and (rarely) on the GTX 970
- NumberFields on the CPU

Given the relative speeds of the two NVidia cards, the REC for SETI should be not much over half the REC for GPUGrid: instead, it's higher. That suggests that REC is being calculated from 'best card in system' (as reported to the server), rather than 'card actually used' (which should be knowable by the client). Again, future work requests could be unbalanced.","Hmmm, I attempted to create a scenario.  Uploaded all the files but no scenario has appeared.  Didn't see any warning messages or errors.",30260406
699,Server/scheduler: Current default code doesn't inhibit work supply to faulty hosts,open,2019-03-14T19:16:39Z,2019-10-31T09:26:51Z,,CONTRIBUTOR,"**Describe the bug**
SETI@Home users have drawn attention to [SETI host 8625200](http://setiathome.berkeley.edu/show_host_detail.php?hostid=8625200).

The host was created on 28 Nov 2018, by a new user who also joined the project on the same day.

At the time of writing, the results.php display for the host is showing 1663 tasks in progress and 313 error results. Those figures include 48 apparently successful downloads so far today (14 March 2019), and 16 'Error while downloading'. I can find no evidence that the host has ever completed even a single task from any of the 10 [application versions](http://setiathome.berkeley.edu/host_app_versions.php?hostid=8625200) it has attempted.

**Expected behavior**
Hosts which consistently fail to return valid work should have their maximum task quota gradually reduced to no more than one task per application per day.

Additionally, for projects which enforce a 'tasks in progress' limit, that limit should apply to all hosts - it would be 200 in this case.

**System Information (please complete the following information):**
 - OS: Windows 7
 - BOINC Version: 7.14.2
 - Hardware: Intel i7 CPU, AMD Radeon GPU 

**Additional context**
SETI is a project with multiple application versions and generous deadlines. Tasks require 2nd. instance validation. Some of the 'in progress' tasks date back to 23 Jan 2019, and won't time out until 17 Mar 2019. The extended retention of so many task and workunit records is detrimental both to the project database and to fellow users.

I don't know whether this is related to the ""punitive validation"" mechanism described in #3024, and I can't know because the only context given in opening that PR by @davidpanderson was ""This is for @lfield"". Without an issue number, or the text of the email, we're none the wiser.

What I'm describing here isn't a need for 'punishment': all I'm suggesting is that normal operation of the regular restrictive rules appears to be broken. Fixing it might be all that @lfield requires.

","@davidpanderson - can you review the following?

The limits on tasks per day is governed by the checks in the method '[daily_quota_exceeded](https://github.com/BOINC/boinc/blob/64b78dfbc03885cd43b210ffe827c65f4295ea51/sched/sched_version.cpp#L126)' 

That method is called in get_app_version_anonymous [here](https://github.com/BOINC/boinc/blob/64b78dfbc03885cd43b210ffe827c65f4295ea51/sched/sched_version.cpp#L178) and in get_app_version [here](https://github.com/BOINC/boinc/blob/64b78dfbc03885cd43b210ffe827c65f4295ea51/sched/sched_version.cpp#L797).

In get_app_version, the code path for homogeneous app version and where an app version id has already been set for the task results in the app version being returned without regards to the user having reached their daily limit for the job.  See [here](https://github.com/BOINC/boinc/blob/64b78dfbc03885cd43b210ffe827c65f4295ea51/sched/sched_version.cpp#L602).

For projects that use homogeneous app version, this could result in a client being able to obtain considerably more work than it should be able to.

I think a fix would be something like putting the following at [line 522](https://github.com/BOINC/boinc/blob/64b78dfbc03885cd43b210ffe827c65f4295ea51/sched/sched_version.cpp#L522).

```
    //check to see if we exceeded the quota for this app version
    gavid = get_app_version_id(&bav);
    if (daily_quota_exceeded(gavid, bav.host_usage)) {
        if (config.debug_version_select) {
            log_messages.printf(MSG_NORMAL,
                ""[version] [AV#%d] daily quota exceeded\n"", gavid
            );
        }
        return NULL;
    }
```
I don't know if this would match what is being seen but it looks suspicious.",30260406
700,BSOD crash report (BOINC on Windows 7) ,open,2019-03-14T12:17:03Z,2019-10-31T09:22:42Z,,NONE,"When the BSoD happened, I was doing some GPU based (CUDA) processing and BOINC is running in the background (also using GPU through Einstein@Home) .

**System Information (please complete the following information):**
 - OS: Window Embeded System 7
 - BOINC Version: 7.14.2
 - GPU: NVidia GTX 1070 (driver version: 397.64)

**Additional context**
from the windbg output with the [dump file](https://github.com/BOINC/boinc/files/2969232/031419-25396-01.zip), it looks like similar with the problem mentioned in #2056.


>BugCheck F4, {3, fffffa80828dcb10, fffffa80828dcdf0, fffff80001fda8c0}
Probably caused by : wininit.exe
Followup: MachineOwner

>CRITICAL_OBJECT_TERMINATION (f4)
A process or thread crucial to system operation has unexpectedly exited or been
terminated.
Several processes and threads are necessary for the operation of the
system; when they are terminated (for any reason), the system can no
longer function.
Arguments:
Arg1: 0000000000000003, Process
Arg2: fffffa80828dcb10, Terminating object
Arg3: fffffa80828dcdf0, Process image file name
Arg4: fffff80001fda8c0, Explanatory message (ascii)

>Debugging Details:
PROCESS_OBJECT: fffffa80828dcb10
DEBUG_FLR_IMAGE_TIMESTAMP:  0
MODULE_NAME: wininit
FAULTING_MODULE: 0000000000000000 
PROCESS_NAME:  boinc.exe
BUGCHECK_STR:  0xF4_boinc.exe
CUSTOMER_CRASH_COUNT:  1
DEFAULT_BUCKET_ID:  VISTA_DRIVER_FAULT
CURRENT_IRQL:  0
LAST_CONTROL_TRANSFER:  from fffff80002064892 to fffff80001ccf440
STACK_TEXT:  
fffff880`378469d8 fffff800`02064892 : 00000000`000000f4 00000000`00000003 fffffa80`828dcb10 fffffa80`828dcdf0 : nt!KeBugCheckEx
fffff880`378469e0 fffff800`020220db : 00000000`00000001 fffffa81`148ae060 fffffa80`828dcb10 00000000`00000001 : nt!PspCatchCriticalBreak+0x92
fffff880`37846a20 fffff800`01f8b144 : 00000000`00000001 00000000`00000370 fffffa80`828dcb10 fffff800`00000008 : nt! ?? ::NNGAKEGL::`string'+0x27256
fffff880`37846a70 fffff800`01cce6d3 : 00000000`00000370 fffffa81`148ae060 fffffa80`828dcb10 00000000`000003f8 : nt!NtTerminateProcess+0x284
fffff880`37846ae0 00000000`7735bffa : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSystemServiceCopyEnd+0x13
00000000`0030c568 00000000`00000000 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : 0x7735bffa
STACK_COMMAND:  kb
FOLLOWUP_NAME:  MachineOwner
IMAGE_NAME:  wininit.exe
FAILURE_BUCKET_ID:  X64_0xF4_boinc.exe_IMAGE_wininit.exe
BUCKET_ID:  X64_0xF4_boinc.exe_IMAGE_wininit.exe
Followup: MachineOwner
",,30260406
701,mgr: Failure to add project error message needs wrapping,open,2019-03-12T19:23:30Z,2019-10-31T09:21:08Z,,CONTRIBUTOR,"**Describe the bug**
If project server returns an error message when trying to add the project with Add project wizard the error message doesn't necessarily fit in the window but the text doesn't wrap.

**Expected behavior**
The text wraps.

**Screenshots**
![image](https://user-images.githubusercontent.com/15806192/54228216-57690280-450a-11e9-8e96-219b5c760e4d.png)

**System Information (please complete the following information):**
 - OS: Windows 10
 - BOINC Version: 7.15

**Additional context**
The window is also unresizable and unscrollable making it impossible to see the rest of the text.",,30260406
702,The wrapper prints the version twice,open,2019-03-12T15:25:54Z,2019-10-30T16:48:25Z,,CONTRIBUTOR,"The wrapper prints the version twice. The message is printed [here](https://github.com/BOINC/boinc/blob/master/samples/wrapper/wrapper.cpp#L1146) and [here](https://github.com/BOINC/boinc/blob/master/samples/wrapper/wrapper.cpp#L1201).

Let me know which one to keep and I will make a PR to remove the other.","I would still like to do something so it isn't duplicated, i.e. make the strings different. Addressing this is on my todo list. ",30260406
703,Clarification needed: TOU opt-in / user consent workflow in client/manager and AMs,open,2019-03-12T12:55:17Z,2019-10-30T16:47:43Z,,CONTRIBUTOR,"In #2413 we established ways to let users express their consent to the terms of use (TOU) of a given project. The current implementation...

* [publishes](https://github.com/BOINC/boinc/blob/master/html/user/get_project_config.php#L120) a project's TOU as soon as they are defined and the `ENROLL` consent type is enabled.
* has the **web interface** (optionally) require existing users to consent to the TOU via the extra `config.xml` setting `enable_login_mustagree_termsofuse` (see https://github.com/BOINC/boinc/commit/f172822a949a2bdccf2b95b97e495c7f0ab8109a).

I like to know (or confirm) the following:
* what should make the user's consent to the TOU a requirement during the **sign-up** process in the **client/manager** or **account manager**? Is the simple existence of the TOU in the project's config enough, or should this requirement be explicitly enabled by a not yet existing `get_project_config` RPC variable, based on `enable_login_mustagree_termsofuse`? Said setting implies to cover the login process only, though.
* Likewise: if `enable_login_mustagree_termsofuse` is solely meant to control the opt-in requirement during the **login** procedure (for existing accounts, without prior consent), then this should be implemented in the client/manager as well (and made available via the `get_project_config` RPC), right?

My assumption would be the following:
* Yes, the sole existence of the terms of use should make their acceptance a requirement. This is, to my knowledge, how the BOINC manager and BAM! work today. Not sure about `boinccmd`, though.
* `enable_login_mustagree_termsofuse` should be communicated to client/manager via a new config variable, say, `terms_of_use_consent_required` (actual value, not simple existence) such that they can enforce the acceptance. If the existing user doesn't accept the TOU the client/manager should not start working on the project. Question: should the project even be detached in that case (maybe after ensuring upload of finished tasks)?

This issue should should arrive at the ""official"" answer to those question which should subsequently written down in the [BOINC wiki](https://boinc.berkeley.edu/trac/wiki/UserOptInConsent) as well.

Thanks",,30260406
704,IE11 in BOINC Forums Does Not Limit Size of Large Images,open,2019-03-07T21:46:08Z,2019-10-30T16:47:23Z,,MEMBER,"If you view this post https://setiathome.berkeley.edu/forum_thread.php?id=83445&postid=1974366#1974366 in IE 11, you will see that the image expands beyond the width of browser.

The image should respect the max-width setting that is set for the class img-responsive from bootstrap.

The image should not cause the width of the forum post to cause the browser to do horizontal scrolling in the forums.
","It's already present there but broken on IE11

пт, 8 марта 2019 г. в 09:06, numbermaniac <notifications@github.com>:

> What about CSS' max-width?
> https://www.w3schools.com/cssref/pr_dim_max-width.asp
>
> —
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/3047#issuecomment-470841611>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ADFZoeImAqTUAqhYlQ5bnkPFnvZJS3DPks5vUhnngaJpZM4bkLQ->
> .
>
-- 
Best regards,
Vitalii Koshura

Sent via iPhone
",30260406
705,Too many notifications in Android?,open,2019-02-28T09:22:46Z,2019-10-30T16:47:15Z,,CONTRIBUTOR,"from an Android user:

I think there is a problem with notifications/messages. I am getting notifications for the same messages multiple times per day, every day. Like, every 2 hours, and I'm still getting ""season's greetings"" type Xmas messages. It's super annoying, but I haven't turned off my notifications yet because Boinc needs to be able to run in the notification tray, right? I have been using 2 Motorola devices, a Z Play, and a G6
","We should have two levels of notification....  1) Appears in the BOINC GUI
 2) Appears in the GUI generates a desktop or android notification.

On Thu, Feb 28, 2019 at 1:22 AM David Anderson <notifications@github.com>
wrote:

> from an Android user:
>
> I think there is a problem with notifications/messages. I am getting
> notifications for the same messages multiple times per day, every day.
> Like, every 2 hours, and I'm still getting ""season's greetings"" type Xmas
> messages. It's super annoying, but I haven't turned off my notifications
> yet because Boinc needs to be able to run in the notification tray, right?
> I have been using 2 Motorola devices, a Z Play, and a G6
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/3038>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AKXcshKSXa4DWZJ7jTu0NqPu_NfjymHwks5vR5_pgaJpZM4bWW3N>
> .
>


-- 
Eric Korpela
korpela@ssl.berkeley.edu
AST:7731^29u18e3
",30260406
706,Evaluate competing mobile apps,open,2019-02-28T01:11:52Z,2019-10-30T16:47:01Z,,CONTRIBUTOR,"Email from a user.
To me, this suggests that we survey other mobile volunteer computing apps
(this one, Power Sleep, Folding@home),
compare them with ours,
maybe do a focus group study,
and decide what we should to do,
with the goal of increasing mobile adoption and retention.

---------------
Hello Professor Anderson,


I just wanted to drop you a little note that you should check out Vodafone's (cell phone service provider) collaboration with the Garvan Institute of Medical Research, both in Australia. I also posted a bit ago at the SU forum. Vodafone customers do not loose any allotted data while transferring data related to the app/research (roaming excluded).

https://www.vodafone.com.au/foundation/dreamlab

Garvan Medical Research:
https://www.garvan.org.au/Diseases/Research


They partnered together to create an Android cancer research app called DreamLab. The app though was actually created by another Aussie company that specializes in this type of development called Transpire.

https://www.transpire.com/our-work/vodafone-dreamlab/


So, I suggested a few possibilities for Science United:

* Partner with a US carrier (or other international carriers) to do something similar in promotion.

* Consult with Transpire on building a SU app(s) since they have the experience already.


I wanted to share my impressions with you so you would understand why I am suggesting these ideas. I tested it out on my Samsung tablet because it was a little unclear from their website how the cell data usage works. Although there is a cautionary warning from the Play Store the app may not be optimized, it displays and runs just fine on my S2. Android 4.4 or higher is required.

The interface is clean and fairly intuitive. There's actually 2 projects you can select from (not noted on the main website info), but the app automatically picks one (the newer one) and notes you can go change your selection at any time under the projects tab (which are placed along the bottom). One nice aesthetic item is the app starts off in dark mode (a recent addition I see from the news), fitting in with the idea you run it while you are dreaming (sleeping).

Similar to BOINC, it doesn't run until the battery charge hits 80%, although you can tap a simple button to 'power' it on anytime.

It seems to run nicely in the background without noticable impact to the device use. (Notes say that on iOS, you can't run in background, and app just remain in found to run.)

It does track your contribution per session and per single device. There is no goal tracking though, but there are noted plans for leaderboards in the future. So if you reset your device, uninstall, or switch to another device, the counter starts over. There's no point system, it simply tracks 'calculations' which is a little vague on definition.

I did see after installing you can set BOTH the WiFi and cell data usage limits. WiFi defaults to unlimited. If you reach your limit (period is per each CALENDAR month), DreamLab will prompt you to increase that limit to continue doing more calculations else it will start up again with the new calendar month. The volunteer is responsible for ensuring they check this against their data plan period.

For Power Users, DreamLab leaves a number of things to be desired obviously. For your target audience of every day folks, I'd say it's 95+% of what your are aiming for. The loss of record of work contributed might be disappointing to some even if currently the contribution is oriented device-wise. There's no registration required although you can sign up for the mailing list for future news. There's also a News tab in the app.


I'm thinking I'll suggest this to the Folding@home folks too. Their partnership on the Android app with Sony ended early last year. I never got to trying it out before they closed shop on it, but the app stats were apparently independent of the main stats system. At this time they still haven't released a new app. Also, their latest desktop clients broke screensaver connectivity (it won't display volunteer info or the model of the current protein). Supposedly they are focusing to clean up their website and transition everything to the new scheme, but it appears that is still very much in progress. So don't feel too bad you are behind on things. ;)
",HTC's Power to Give is also noteworthy and that was my first exposure to BOINC when I bought an HTC device a few years ago (and I am on a US carrier): https://www.htc.com/us/go/power-to-give/,30260406
707,Client: make 'pseudo-progress' report configurable,open,2019-02-11T16:21:59Z,2019-10-30T16:46:19Z,,CONTRIBUTOR,"**Describe the problem**
The sequence of commits 506c3b6e419e5b524ad72fcd16e363cd0e296814, 4f0cd6b29caf78c65a5622a4821570e3c94f26b1, and 61d6d9a20ab70389b50416eebed8cb04deecc43f reassures nervous users that something is happening, even when the application isn't reporting it. But it's hard to distinguish that from the debug case where the app would normally report progress, but is failing to do so on new, untested, hardware.

**Describe the solution you'd like**
A configuration option to turn this off when an exact progress report is needed: either a new config flag, or it could be piggy-backed on `<fraction_done_exact/>` in app_config.xml

**Additional context**
https://einsteinathome.org/content/another-intel-gpu-opencl-thread
",,30260406
708,Android beta version 7.14.1 “Aquire lock”issues,open,2019-02-06T20:43:05Z,2019-10-30T16:45:33Z,,NONE,"I’m running the beta version which has been running fine until the last day or too. I often find tasks sitting there with a “ready” status, but not crunching (battery charge is good). I have just looked at the event log and I see:

Client Messages

```
29/01/2019 8:05:09 PM|World Community Grid|<![CDATA[
Task ZIKA_000386641_x1r6r_DENV2_capsid_NMR_21_s2_0555_0 postponed for 600 seconds: Waiting to acquire lock
]]>
29/01/2019 8:04:47 PM|World Community Grid|<![CDATA[
Tasks are committed to other platforms
]]>
29/01/2019 8:04:47 PM|World Community Grid|<![CDATA[
No tasks are available for Mapping Cancer Markers
]]>
29/01/2019 8:04:47 PM|World Community Grid|<![CDATA[
No tasks are available for Microbiome Immunity Project
]]>
29/01/2019 8:04:47 PM|World Community Grid|<![CDATA[
No tasks sent
]]>
29/01/2019 8:04:47 PM|World Community Grid|<![CDATA[
Scheduler request completed: got 0 new tasks
]]>
<SNIP>

```
This is now occurring all the time. Within a few minutes of starting boinc this behaviour occurs. ",,30260406
709,"Scheduler URL prepended by two ""http://"" strings",open,2019-02-05T18:05:03Z,2019-10-30T16:45:20Z,,NONE,"Greetings,

Not sure if this is a bug or our set-up error, but the scheduler.txt file was showing two ""http://"" in the URL - a friendly user pointed this out:

""
```
<!-- <scheduler>http://http://boinc.tacc.utexas.edu/boincserver_cgi/cgi</scheduler> -->
<link rel=""boinc_scheduler"" href=""http://boinc.tacc.utexas.edu/boincserver_cgi/cgi"">
```
""

->

""
```
<!-- <scheduler>http://http://boinc.tacc.utexas.edu/boincserver_cgi/cgi</scheduler> -->
<link rel=""boinc_scheduler"" href=""http://boinc.tacc.utexas.edu/boincserver_cgi/cgi"">
```
""

Since we did not create this file, it must be getting created automatically during the set-up process. Is it possible that we provided a wrong input during the set-up process? Perhaps we are including ""http"" in the URL when it may not be required? 

Thanks,
Ritu","@ritua2

I changed to issue title to something that describes the problem you are seeing. I also had to edit the issue text so that the important part would actually show. GitHub uses Markdown which renders HTML in not always entirely obvious ways. Preview before posting :)",30260406
710,Computing prefs 2.0,open,2019-01-29T22:16:18Z,2020-02-09T17:17:27Z,,CONTRIBUTOR,"The current prefs system can't handle some plausible requirements,
e.g. disabling a particular GPU when the computer is in use.
This is a proposal for a more general prefs system, which I'll call P2.

P2 is intended to be as general as possible,
i.e. it allows any conceivable preferences to be expressed.
That way we only have to do this once.

Like the current system, P2 uses XML to represent preference sets.
It would be possible to use another data representation language such as JSON,
or to use an embedded programming language like Tcl,
but I think XML is the best choice.

A P2 spec consists of a sequence of ""clauses"".
Each clause consists of a ""condition"" specifying when it's in effect,
and one or more preferences.
For example, the preference mentioned above would be represented as:

```
<clause>
    <condition>
        <not><idle_time_more_than>120</idle_time_more_than></not>
    </condition>
    <prefs>
        <gpu>
            <type>nvidia</type>
            <number>1</number>
            <enable>0</enable>
        <gpu>
    </prefs>
</clause>
```

If no condition is given, the clause is always in effect.
Typically a spec will begin with such a clause.

The clauses are evaluated in order.
Later clauses (if their conditions are met) override earlier ones.
For example, if we want to enable GPU use from 5PM to 7AM,
regardless of idleness, we'd add another clause

```
<clause>
    <condition>
        <daytime>17-7:<daytime>
    </condition>
    <prefs>
        <gpu>
            <enable>1</enable>
        </gpu>
    </prefs>
</clause>
```

--------------------
Conditions

P2 conditions are boolean expressions.
The primitives are

`<idle_time_more_than>`
    true if mouse/keyboard input in last N seconds
`<on_batteries>`
    true if running on batteries
`<program_running>name</program_running>`
    true if ""name"" is executing
`<daytime>X:Y</daytime>`
    true if day is in range X and time is in range Y
`<cpu_usage_more_than>X</cpu_usage_more_than>`
    non-BOINC CPU usage exceeds X
`<external>X</external>`
    the GUI has told us that condition ""X"" holds.
`<battery_charge_more_than>`
`<battery_temperature_more_than>`
`<on_wifi>`
    connected via WiFi (mobile)

Boolean expression can be formed using
`<not>X</not>`
    negation of X
`<or>...</or>`
    or of 2 or more expressions
`<and>...</and>`
    and of 2 or more expressions

Expressions can be nested.

---------------------
Prefs

```
<max_ncpus_pct>
<cpu_usage_limit>
<gpu>
    enable or disable one or more GPUs
<ram_max_used_pct>
<vm_max_used_pct>
<network>
    enable or disable all network communication
<file_xfer>
    enable or disable file transfers
```

The following are typically static but it doesn't hurt to make them dynamic.

```
<leave_apps_in_memory>
<work_buf_min_days>
<work_buf_additional_days>
<cpu_scheduling_period>
<disk_interval>
<disk_max_used_gb>
<disk_min_free_gb>
<disk_max_used_pct>
<daily_xfer_limit_mb>
<daily_xfer_period_days>
<dont_verify_images>
<max_bytes_sec_down>
<max_bytes_sec_up>
```

---------------------
Implementation, Phase 1

The goal of this phase is to make the features of P2
available to users who are willing to write XML specs.
Steps:

a) define a C++ data structure to represent a P2 preference set.

b) write code that parses an old-style specs (global_prefs.xml, cc_config.xml)
into this data structure.

c) enforce prefs based on the new data structure, rather than GLOBAL_PREFS.
Need to evaluate prefs when
- time of day (if any conditions use it)
- app starts/stops running
- idle time exceeds a limit or returns to zero
- etc.

This is entirely within the client; I should do this part.

---------------------
Implementation, Phase 2

The goal of this phase is to provide GUIs for editing P2 prefs.

In both web and desktop, the primary GUI should be choosing a preset
(standard, max computing, green, etc.).
An ""Advanced"" button leads to a full-featured prefs editor, which lets you

- create/delete/reorder clauses
- edit conditions (including boolean expressions)
- edit prefs

This editor should be implemented using HTML and Javascript.
There are various JS frameworks that could be used.

In the desktop case (i.e. BOINC Manager or Android GUI)
the Advanced button should take you to a prefs-editor web page,
served from the BOINC web site.
The ""Save"" button writes the resulting XML to a local file.
In other words, we should NOT implement P2 prefs editors
in the WxWidgets and Android GUIs.

Developing the editor is a self-contained task and anyone
with HTML/JS experience could do it.
Integrating it with the BOINC web code and with the GUIs
would be a small amount of additional work.
","BOINC grids such as WCG & LHC are mixing very different kinds of projects under the same Preferences. E.g., WCG now has two projects that require a fast turn around; the serial FAH2 and long ARF with sparse checkpoints. Clients need more effective ways to control their work.
Please add BOINC commands <maintain> & <priority> that tell the server what to do.
```
<app_config>
<app>
    <name>ATLAS</name>
    <!-- Xeon E5-2699 v4  22c44t  32 GB RAM L3 Cache = 55 MB  -->
    <maintain>18</maintain>
    <max_concurrent>16</max_concurrent>
</app>
<app_version>
    <app_name>ATLAS</app_name>
    <plan_class>native_mt</plan_class>
    <avg_ncpus>1</avg_ncpus>
    <cmdline>--nthreads 1</cmdline>
</app_version>
<app>
    <name>sixtrack</name>
    <maintain>9</maintain>
    <max_concurrent>6</max_concurrent>
</app>
<app>
    <name>Theory</name>
    <maintain>44</maintain>
</app>
<app>
    <name>CMS</name>
    <maintain>0</maintain>
</app>
</app_config>
```

**And even better would be.**
```
<app_config>
<app>
    <name>ATLAS</name>
    <!-- Xeon E5-2699 v4  22c44t  32 GB RAM L3 Cache = 55 MB  -->
    <priority>1</priority>
    <max_concurrent>16</max_concurrent>
</app>
<app_version>
    <app_name>ATLAS</app_name>
    <plan_class>native_mt</plan_class>
    <avg_ncpus>1</avg_ncpus>
    <cmdline>--nthreads 1</cmdline>
</app_version>
<app>
    <name>sixtrack</name>
    <priority>3</priority>
</app>
<app>
    <name>Theory</name>
    <priority>2</priority>
</app>
<app>
    <name>CMS</name>
    <priority>0</priority>
</app>
</app_config>
```
`<maintain>` is useful for very large WUs like LHC's ATLAS that are ~250 MB. Since internet providers choke the upload speed it takes a long time for an ATLAS completed WU queue to clear before server knows to send more work. By maintaining a couple extras in the wings work continues uninterrupted. Zero means don't ever send me this class of WU even if ""If no work for selected applications is available, accept work from other applications? yes"" is selected. E.g., LHC CMS has no native Linux ap and I do not run Oracle's VirtualCatBox.
`<priority>` allows one to keep serial projects like FAH2 always running and to avoid time-slicing.",30260406
711,Client configuration: fill gaps in provision,open,2019-01-29T13:36:50Z,2019-10-30T16:44:45Z,,CONTRIBUTOR,"[Project-level configuration](https://boinc.berkeley.edu/wiki/Client_configuration#Project-level_configuration)

**Describe the problem**
As the above link makes clear, <max_concurrent> is not defined at the <app_version> level.

**Describe the solution you'd like**
Add missing option so that control is available at all levels.

**Additional context**
The current configuration options are implicitly based on the assumptions that all applications and versions operate the same on all physical devices of the same class. SETI@Home illustrates many ways in which this is not true:

GPUs - applications written in the OpenCL programming language for NVidia GPUs require different levels of CPU support from NVidia CUDA language applications, and from AMD/ATI OpenCL applications.

CPUs - For a project which requires intensive floating-point mathematics, hyper-threaded CPUs or physical cores which share a single FPU require different consideration from full-featured physical cores. Applications using only integer mathematics are affected differently by CPU geometry.",,30260406
712,Fine Grained Control Of Multi-GPU Configurations,open,2019-01-29T00:02:32Z,2019-10-30T16:44:03Z,,NONE,"My BOINC use case:
- Ubuntu 18.04.1 Desktop running BOINC 7.14.2
- SETI@home
- GPU 0: NVIDIA GeForce GTX 1060
- GPU 1: NVIDIA GeForce GT 1030

I have two monitors connected to the GTX 1060
No monitors are connected to the GT 1030

I would like the ability to:
- Tell BOINC to Suspend GPU 0 when computer is in use
- Tell BOINC to Always use GPU 1 regardless of whether computer is in use or not

Right now the GT 1030 sits idle when I am on my Ubuntu Desktop machine and I would like for it to be crunching away all the time",This issue is related to #2993,30260406
713,Fine tune task search at Seti@Home,open,2019-01-21T09:55:59Z,2019-10-30T16:42:54Z,,CONTRIBUTOR,"The current task search option on Seti apparently only allows for the complete correct name of the task to be searched, it does not allow booleans asterisk (*) and question mark (?) anywhere in the text. Or any booleans for that matter. 

Is it possible that these are added? ",,30260406
714,config.h is not installed,open,2019-01-12T22:01:16Z,2020-03-04T12:35:33Z,,MEMBER,"In #1624 @tomasbrod mentioned:

> I am affected by this issue. config.h is not installed. Almost all header files installed in /usr/include include that ""config.h"" file making them useless without the missing header.","I just re-read that reply and I agree. (I think I missed it before.) If you build using the oldest OS you are supporting, then `configure/make` is fine. As long as others who use it also understand that restriction.

> Another idea: could config.h be installed conditionally, e.g. when prefix doesn't point to /usr or /usr/local?

Sure, that sounds like it could work.",30260406
715,Option to automatically suspend tasks if estimated time > deadline + X hours,open,2019-01-11T21:17:23Z,2019-12-24T23:45:04Z,,NONE,"**Describe the problem**
As per title

**Describe the solution you'd like**
I would like to have an option to check every e.g. 15 minutes, if estimated time > deadline + X hours, abort the task automatically.
So that I do not do this manually, nor do I complete the task but not receive any credit.

**Additional context**
Option should be disabled by default.
I have a low Nvidia card running GPUGRID tasks, and sometimes estimated time > deadline.
Similar to https://github.com/BOINC/boinc/issues/2115
","Thank you for wanting to contribute to BOINC, but I wonder if there are other tasks that might be a better choice for your first effort, especially since the scheduler and the way the client and server interact for scheduling are among the most complex parts of BOINC. I believe there are long wish lists of changes that would be very helpful.",30260406
716,Start manager minimized,open,2019-01-08T22:16:25Z,2019-10-30T16:41:11Z,,CONTRIBUTOR,"A user asked me if there's an option to start the Manager minimized on Win, on system startup.  As far as I know there's not (other than to create a shortcut with the /s flag).

There should at least be an option for this; maybe it should be the default.  BOINC should be as unobtrusive as possible.

One way to do this would be to have the manager remember its minimized state; if it was minimized at shutdown it should be minimized on startup.
","@davidpanderson, could you please add necessary info to the online documentation and close this issue?",30260406
717,Use Default character set when creating server database,open,2019-01-05T10:45:41Z,2019-10-30T16:40:42Z,,MEMBER,"**Describe the bug**
A default mariadb installation on Debian 9 uses the `utf8mb4` character set as default for new databases and tables. This has implications on the maximum key size of an index of `VARCHAR` fields in an innodb table. Using `utf8mb4` the maximum field size is 191 when using `utf8` the maximum is 255. BOINC is using a lot of `VARCHAR(254)` fields.

The error message is:
```
Setting up database
Traceback (most recent call last):
  File ""./make_project"", line 234, in <module>
    project.install_project()
  File ""/home/boincadm/boinc-src/py/Boinc/setup_project.py"", line 600, in install_project
    drop_first = options.drop_db_first
  File ""/home/boincadm/boinc-src/py/Boinc/database.py"", line 291, in create_database
    _execute_sql_script(cursor, os.path.join(srcdir, 'db', file))
  File ""/home/boincadm/boinc-src/py/Boinc/database.py"", line 277, in _execute_sql_script
    cursor.execute(query)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 226, in execute
    self.errorhandler(self, exc, value)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
    raise errorvalue
_mysql_exceptions.OperationalError: (1071, 'Specified key was too long; max key length is 767 bytes')
```

**Steps To Reproduce**
1. Install a fresh Debian 9 system
2. run `tools/make_project --delete_prev_inst --drop_db_first --test_app test4vm 'Testproject for VirtualMachine'`

**Expected behavior**
Changing the default character set in `/etc/mysql/mariadb.conf.d/50-*.cnf` from `utf8mb4`to `utf8`, restarting mysql and running the command again creates the project just fine.

**System Information (please complete the following information):**
 - OS: Debian 9","I just hit this issue while creating a project on Ubuntu 18.4 LTS. At the minimum, create_project should detect this and not fail, but in reality, I think we should start supporting this charset.",30260406
718,[Server] Scheduler: estimate GPU app needs better,open,2019-01-02T16:08:50Z,2019-10-30T16:40:21Z,,CONTRIBUTOR,"**Describe the problem**
It's now 2019, and that marks 10 years since the introduction of GPU computing in BOINC. (Technically, the launch was at the end of 2008, but the first fully-working GPU application - v6.08 for SETI@Home - wasn't available until 15 January 2009). Since then, we've grown to accept different makes of GPUs and different programming languages, and the highest speed GPUs are orders of magnitude faster than the early devices. But I think we're still using some of the early assumptions about their capabilities.

In particular, different GPU types, and different programming languages, place different demands on the host computer. A visual example:

![cpu efficiency](https://user-images.githubusercontent.com/14886436/50600671-bd516200-0ea9-11e9-8ae8-5a3db4e2f06e.PNG)

NumberFields is a CPU-only application, running at typically 97% CPU usage
Einstein is an OpenCL application for Intel_gpu, requiring less than 3% CPU support
GPUGrid is a CUDA application for NVidia GPU, requiring over 30% CPU
SETI is an OpenCL application for NVidia, requiring over 97% CPU - as much as the pure CPU apps

But we don't even attempt to track and measure those CPU support requirements. Code like https://github.com/BOINC/boinc/blob/0cc45a13950aad6181320b870d4e257f96fb4aec/sched/sched_customize.cpp#L505 still relies on assumptions about the relative speeds of CPU and GPU devices, and the proportion of the work to be done on each device. As speeds have diverged, these assumptions have become less and less realistic. The medium-use GPUGrid case above emerges from the server as

```
    <plan_class>cuda80</plan_class>
    <avg_ncpus>0.975476</avg_ncpus>
    <max_ncpus>0.975476</max_ncpus>
    <flops>131171048789.023773</flops>
    <coproc>
        <type>CUDA</type>
        <count>1.000000</count>
    </coproc>
```
demanding as much CPU reservation as the worst-case OpenCL application.

**Describe the solution you'd like**
Track and use actual measurements of application performance on individual computers.

We already track equivalent metrics in the database tables app_version and host_app_version. And we receive reports of elapsed time and CPU time with every completed task. All we need to do is to store, update, and use the running average ratio of those values when issuing new work. The existing system needs to be kept as a fallback for edge cases like newly attached hosts, new app versions, and non-standard clients which don't report all current fields.

**Additional context**
This would be particularly useful at SETI@Home, where separate CUDA and OpenCL app_versions are available, each capable of processing the same tasks on the same hardware. At the moment, all app_versions are allocated the same <avg_ncpus> value, although their needs are very different.

In particular, the OpenCL app for NVidia performs poorly unless a full CPU core is kept clear of CPU apps while it is running: the client doesn't do this even for fractional <avg_ncpus> values as high as 97% - and nor should it. Instead, if the actual measured average recorded on the server is over, say, 95% (precise value tbc), it should be rounded up to 1.00 to give the appropriate direction to current clients not to schedule that last CPU core.
","This issue is intended to benefit the majority of users who simply accept BOINC work as it comes, and don't get involved in manual tuning. If a user chooses to set an environment variable and succeeds (many don't, or need help understanding the exact format required), then they can also set up an app_config to match - and any volunteer who suggests one should suggest both.

Mind you, if setting SWAN_SYNC results in 95%+ CPU recorded time, then this proposal would automatically result in a full CPU reservation for that host only, provided the server update was backported to GPUGrid: they lack experienced BOINC support in their server ops area, and are reluctant to upgrade beyond their current server v6.13",30260406
719,"[Manager] ""Select computer"" window is disappearing on wrong password",open,2019-01-02T12:15:30Z,2019-10-30T16:38:52Z,,MEMBER,"**Describe the problem**
When you want to connect to another machine and mistype the password, you get an error message, then the ""Select computer"" window is disappearing.

**Describe the solution you'd like**
When you mistype the password after the error message the ""Select computer"" window has to be shown again. The user clearly wants to connect to another computer, then why the window has to be opened manually again?
",It's not a regression but feature request.,30260406
720,EU FOSSA project,open,2019-01-01T18:36:22Z,2019-10-30T16:38:35Z,,MEMBER,The [EU is sponsoring security bug fixes for open source projects](https://juliareda.eu/2018/12/eu-fossa-bug-bounties/). I think it would worth to contact with them.,,30260406
721,Configuring/Moving the Data Directory from UI or Config file,open,2018-12-24T10:01:12Z,2019-05-03T16:06:48Z,,NONE,"**Describe the problem**
Many people including myself are now using smaller SSDs to boot the OS & larger Hard Drives for home partitions. Usually SSDs are just enough for the OS. That is so in my case with about 15GB left over, which is absolutely required for SSD to function at optimum speeds. I moved my data directory manually under Ubuntu Bionic, redirected the symlink /var/lib/boinc to the new data directory manually. But due to some reason the boinc install does not get the data & Boinc-Manager crashes without showing an error. Boinc however, works perfectly if launched from the new home directory in terminal. Spent over an hour to find the error then gave up. 

**Describe the solution you'd like**
1. Data Directory should be GUI changeable or at least be changed in config file. I know at least one person who stopped running boinc due to same issue as mine....no space in OS SSD.
2. Boinc-Manager should not crash without showing errors.

**Additional context**

","@SanjayArora, 
please try to use next solutions:
- http://alpheratz.net/how-to-move-boinc-data-directory-linux/
- https://boinc.berkeley.edu/dev/forum_thread.php?id=8037",30260406
722,test upcoming android releases,open,2018-12-24T07:27:43Z,2020-03-15T08:19:53Z,,NONE,"Is there any way to test upcoming android releases?

I want to download/test Client Release 7.16, since in Google Play only 7.14.1 is available ","I'm ok with @AenBleidd being the Android Build Master.
I have one suggestion though. I think every PR merge should automatically create a new Android apk, which can be used for signing. So the latest apk is potentially ready for release without contacting the Android Build Master.",30260406
723,manager: support Mojave Dark Mode,open,2018-12-19T20:19:35Z,2020-03-27T09:02:54Z,,CONTRIBUTOR,"From https://boinc.berkeley.edu/forum_thread.php?id=12755:

Hi, would be wonderful if you add a dark theme to reflect Mojave color. Moreover doing a dark theme avoid drastic color changes while switching between apps.

From https://boinc.berkeley.edu/forum_thread.php?id=12684:

The BOINC menu bar icon for macOS is really awful in the Dark Mode of Mojave…

![image](https://user-images.githubusercontent.com/15806192/50245558-7eb4a580-03db-11e9-983d-5eeb41c46ae7.png)

Because the current one was drawn for a white background.

![image](https://user-images.githubusercontent.com/15806192/50245577-8a07d100-03db-11e9-9657-d25e0b06d363.png)

Is it possible to have another icon displayed in dark mode, to add support for Mojave?
Or at least please a simple gray color of the icon? It will be more convenient.",Same issue on macOS 10.15.4 but only with BOINC Manager 7.14.3. Version 7.14.2 works fine.,30260406
724,support VBox 6.x,open,2018-12-19T00:39:27Z,2019-10-30T16:36:06Z,,CONTRIBUTOR,"Check whether vboxwrapper works with VBox 6.X
(with both COM and vbox_manage interfaces).
If not, fix it.","Crystal Pellet reported success on Windows 7 here:
https://lhcathome.cern.ch/lhcathome/forum_thread.php?id=5034&postid=38894

I plan on redoing my testing using a release OS version instead of Insider version, and using a release version of VirtualBox instead of a Testbuild. Perhaps I'm hitting on a beta bug. It would explain why 26202 seemed to work fine for me earlier, and now doesn't..",30260406
725,restart deadlocked apps?,open,2018-12-17T04:36:08Z,2019-05-03T16:05:29Z,,CONTRIBUTOR,"I've noticed several times recently that the MW@h multi-thread app
(MilkyWay@Home N-Body Simulation 1.72 (mt))
is running and accumulating CPU time very slowly (like 1% of real time) but not making progress.
It seems to be deadlocked in some way.
If I restart things, it goes back to normal (for a while).

We (or MW@h) needs to figure out why this is happening;
I suspect CPU throttling is involved; I haven't seen it when not throttling.

But it also raises the question of whether the client should do something in situations like this - maybe restart the app if it's not accumulating CPU time fast enough.",,30260406
726,"Built with -ffast-math option in gcc/g++, boinc client doesn't recognize beignet (intel gpu) library on Linux",open,2018-12-14T04:42:50Z,2019-05-03T16:04:10Z,,NONE,"**Describe the bug**
A clear and concise description of what the bug is.
I am building boinc client from git master tree on Fedora 29 linux.
Building boinc client by configuring 'CFLAGS=""-O4 -mavx2 -funroll-loops -fforce-addr -ffast-math"" CXXFLAGS=$CFLAGS ./configure --disable-server --disable-manager', boinc client doesn't recognize intel gpu, though it detects nvidia gpu (my machine has both).  If -ffast-math is removed, boinc client works for both intel and nvidia.   Here I use ""-O4 -mavx2"", but they don't matter.  With only ""-O3"", it happens.  -O4 directs more aggressive optimization.   -mavx2 is just for my cpu (Haswell).
Funnily, usage of -ffast-math is introduced in boinc wiki https://boinc.berkeley.edu/wiki/Compiling_the_core_client.

**Steps To Reproduce**
1. Just configure with 'CFLAGS=""-O4 -mavx2 -funroll-loops -fforce-addr -ffast-math"" CXXFLAGS=$CFLAGS ./configure --disable-server --disable-manager' under boinc directory.
2. make and install boinc and launch.

**Expected behavior**
A clear and concise description of what you expected to happen.
Intel integrated gpu must be detected and used by boinc client.

**Screenshots**
If applicable, add screenshots to help explain your problem.
![boinc-intel-missing](https://user-images.githubusercontent.com/28835810/49983410-f0d55680-ffa5-11e8-8476-40fb30c04388.png)

**System Information (please complete the following information):**
 - OS:  Linux Fedora 29 x86_64
 - BOINC Version: master branch (7.15.0)

**Additional context**
Add any other context about the problem here.
This happens with boinc 7.14.2 also.","@maverick6664 
Looks like it is internal driver issue:
https://unix.stackexchange.com/questions/414656/darktable-doesnt-recognize-intel-graphics-620s-opencl-beignet-in-fedora-27/419590
Also described here:
https://www.freedesktop.org/wiki/Software/Beignet/
Could you please try this:

> ""Beignet: self-test failed"" and 15-30 unit tests fail on 4th Generation (Haswell) hardware. On Haswell, shared local memory (__local) does not work at all on Linux <= 4.0, and requires the i915.enable_ppgtt=2 [boot parameter](https://wiki.ubuntu.com/Kernel/KernelBootParameters) on Linux 4.1.
> 
> This is fixed in Linux 4.2; older versions can be fixed with [this patch](https://01.org/zh/beignet/downloads/linux-kernel-patch-hsw-support).
> 
> If you do not need __local, you can override the self-test with
> 
> export OCL_IGNORE_SELF_TEST=1
> 
> but using __local after this may silently give wrong results.",30260406
727,Proposed work on boinccmd output,open,2018-12-10T13:26:36Z,2019-05-03T16:03:03Z,,CONTRIBUTOR,"**Describe the problem**
We've had a cogent and well-argued request to add additional data - already present in BOINC Manager project properties dialog - to the --get_project_status output of boinccmd
[Detecting situations where, ""Scheduler RPC deferred for xx:yy:zz"" has been issued.](https://boinc.berkeley.edu/forum_thread.php?id=12742&postid=89147#89147)

**Describe the solution you'd like**
I've provisionally implemented min_rpc_time.
But looking at the existing output, there are multiple other bugs - additional blank lines (ctime supplies its own /n), zero reports for 'disk usage' and 'project files downloaded', and probably more as I look further. I'd also propose to move the stats block from 'jobs succeeded' to 'cross-project ID' above the project GUI_URL block.

**Additional context**
Especially in the context of #1457, are there any format constraints or design restrictions on the current layout that I should take into account? I doubt I'm ready to covert the whole of lib/gui_rpc_client_print.cpp to json (although I've generated rtf programatically in the past), but I think I could take on a first-stage cleanup as an isolated project - similar to #2327. I'll ask for comments at the next contributor call.
",,30260406
728,[Android] AndroidBuild.sh obsolete?,open,2018-12-07T19:55:23Z,2019-05-03T16:01:18Z,,MEMBER,I think the `client/android/AndroidBuild.sh` never used. Am I right that it is obsolete and can be deleted or what its purpose?,Travis and the `build_all.sh` do the same but they are working. What part of this could be useful?,30260406
729,QEMU support for VM tasks.,open,2018-12-04T15:05:40Z,2019-05-03T16:00:28Z,,NONE,"**Describe the problem**
Support for only VirtualBox for VM tasks makes it extremely difficult for people who need to use different virtualization tools to support projects which require use of a VM to run tasks.

**Describe the solution you'd like**
QEMU is one of the most widely deployed type-2 hypervisors around.  Support for it would go a long way to enabling users who can't (or don't want to) have VirtualBox on their systems to still contribute to projects which require VM usage to run tasks.

**Additional context**
QEMU itself is easy to use in an automated/scripted manner, and provides (limited) native VDI support.  It can also run without need for hardware-assisted virtualization (albeit slowly), and doesn't need the host CPU to match the emulated CPU (also slow, but better than nothing).

Documentation is, unfortunately, somewhat lacking, but I've got some experience dealing with QEMU and am more than happy to help by answering any questions that arise.  I've also got a couple of systems I can use to help with testing of any implementation.","Virtualbox isn't in Debian testing main repos and considering https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=794466, it seems it won't be in following Debian versions too (derivatives like Ubuntu too?).
Of course, it seems possible to download Virtualbox from Oracle but wouldn't like to play with dependencies.",30260406
730,signup.php doesn't show terms of use,open,2018-12-03T21:53:16Z,2019-10-30T16:34:52Z,,CONTRIBUTOR,"**Describe the bug**
I have set up test project with terms of use and enabled `<enable_login_mustagree_termsofuse>`. If I click the green _Join $PROJECT_ button, fill in account details and click the _Join_ button I am told I need to go back to the previous page and agree to the terms of use. But the page doesn't show TOU.

**Steps To Reproduce**
1. `run make_project --test_app blaah blaah`
2. enable GDPR compliance setting per the [wiki page](https://boinc.berkeley.edu/trac/wiki/GdprCompliance)
3. open project web page in browser
4. click  the green _Join $PROJECT_ button
5. fill in the form, notice that there are no terms shown and click green _Join_ button

**Expected behavior**
The page shows terms of use so that I can accept them.

**Additional context**
The big green button on the front page takes to `signup.php`. The _Sign Up_ link in upper right corner takes to `create_account_form.php`. Having two pages for signing up is probably a bug in itself. And looking around there is also `join.php`.
","Thanks. We should also show the terms of use in ""prefs.php"" when users want to edit their project preferences.  Right now, it mentions ""General project terms-of-use"", but there is no link from the text.",30260406
731," ""Store up to an additional N days of work"" is legit?",open,2018-12-01T14:21:52Z,2020-02-09T20:20:34Z,,MEMBER,"What's the point of the ""Store up to an additional N days of work"" option?
It seems the difference between the ""Store at least N days of work"" and the ""Store up to an additional N days of work"" is minimal, and it could confuse people.","I find it confusing. I hope it will be condensed into a single parameter. A bigger issue that this does not address are servers that send too much work, more than the sum of those two values.",30260406
732,GDPR preference page improvements,open,2018-11-30T09:25:10Z,2019-05-03T15:56:52Z,,CONTRIBUTOR,"**Describe the bug**
User feedback from testing GDPR

**Steps To Reproduce**
Visit the project preferences page. 

**Expected behavior**

Agreeing to the terms and conditions should be formulated as a question.
Links to the terms-of-use/data protection should be placed on the prefs page.
Force a logoff (+ show a warning) when the box is unchecked.

**Additional context**
Add any other context about the problem here.
","Fine, but as the concept of user consent is new to BOINC, it would make sense to ask for consent both for STATSEXPORT and ENROLL and the same time, with 2 tickboxes on the same screen.  This would be easier for existing volunteers, so that they are made aware of the changes as soon as they login.


",30260406
733,Change the domain name creation process on Android,open,2018-11-24T20:42:45Z,2020-02-26T16:13:55Z,,MEMBER,"**Describe the problem**
The Boinc Client generates a domain name like `android_xxxxxxxx` on Android devices. It is really hard to track on the project's website  which device how much point generated when you have multiple devices.

**Describe the solution you'd like**
Change the domain name generation procedure to `Manufacturer - Model`, which is more human readable and recognizable then a random string.

**Additional context**
The `boinc/client/hostinfo_network.cpp` file have to be modified. ","Yes, there is some information but if you have several same devices they will show equal or almost equal information because they have same hardware and probably the same version of software. So device name is the only way to identify definite device",30260406
734,Don't store IP addresses or system names,open,2018-11-21T08:51:04Z,2019-05-03T15:54:47Z,,CONTRIBUTOR,"Currently the server stores external/internal IP addresses and host name.
These are used for 2 purposes:
- to show to the user in their host list
- as a factor in deciding if two hosts might be the same
  (this is probably not useful).

Since these can be viewed as personal info, it would be good to not store them unless there's a reason to.

So I propose:
- add a project pref for whether to store these items
  (for users who want to see them on their computer list)
- don't store these items unless a config.xml flag says to or the user pref is set.

The default will then be that these items are not stored for new accounts.
",Hashing name/IP address would be good. However would that affect export of statistics to 3rd party websites managing overall stats? ,30260406
735,OpenCL sometimes doesn't detect Intel GPU,open,2018-11-20T22:35:20Z,2019-10-30T16:34:31Z,,CONTRIBUTOR,"On my laptop (Lenovo W530) the client sometimes detects the built-in Intel GPU (via OpenCL) and sometimes it doesn't. This leads to ""stranded"" jobs.  Happens with both 32 and 64 bit clients.","Having said that driver problems had gone away: driver problems haven't (necessarily) gone away.

https://www.intel.com/content/www/us/en/support/articles/000031275/graphics-drivers.html

> Microsoft is moving to a new Universal Drivers architecture starting with Windows® 10 October 2018 Update. This represents a shift in our Intel® Graphics Drivers.

> Intel is targeting November, 28th 2018 for the release of our first graphics Modern Windows Drivers via DownloadCenter and Intel® Driver and Support Assistant (IDSA).

I'll run some tests.",30260406
736,[WIP] Boinc security and user separation improvement,open,2018-11-18T14:49:03Z,2020-03-27T09:36:16Z,,NONE,"Fixes #2779 - DON'T MERGE, REFERENCE ONLY

**Description of the Change**
Remove excessive privileges and unnecessary user accounts created by boinc on windows.

**Implementation state**

- [x] Use virtual account instead of boinc_master
- [x] Remove boinc_master on update
- [ ] Make boinc.exe drop privileges from it's access token on startup. Currently it doesn't remove ""SeImpersonatePrivilege"", ""SeAssignPrimaryTokenPrivilege"", ""SeShutdownPrivilege"", and some other non required privileges.
![image](https://user-images.githubusercontent.com/2544867/48673814-afd45880-eb45-11e8-8df2-efb31841cac2.png)
- [ ] Make boinc.exe lower it's integrity level, currently boinc and its sub processes run with highest available integrity ""high"", the process should automatically lower it to the minimum required integrity level, currently this seems to be ""medium"". https://msdn.microsoft.com/en-us/library/bb625963.aspx
- [x] Make boinc projects run with low integrity https://msdn.microsoft.com/en-us/library/bb625960.aspx
- [ ] Make boinc projects run as another windows service (one per project) and use virtual accounts there too.
- [ ] Evaluate compatibility to operating systems < Win7. They are out of support anyway, so can thay be eventually be dropped entirely?

**Release Notes**
Remove excessive privileges and unnecessary user accounts created by boinc on windows.",This whole PR should be revisited as soon as we decided to drop Windows XP and Windows Vista support for newest versions of BOINC,30260406
737,Move proposed developments from trac/wiki to GitHub as Issues,open,2018-11-14T17:42:56Z,2019-05-03T15:36:21Z,,MEMBER,"For better project management the still valid [proposed developments on track/wiki](https://boinc.berkeley.edu/trac/wiki/DevProjects) should be moved to GitHub as issues.

Any thoughts on this?
","I think similar but slightly different. The #2820 only a Wiki -> Wiki migration, but this issue propose to create issues all the proposed development projects and simply delete them from the wiki.",30260406
738,Move the Trac/Wiki to Github,open,2018-11-14T16:33:23Z,2019-11-04T21:47:20Z,,MEMBER,"I think moving the development related [trac/wiki](https://boinc.berkeley.edu/trac/wiki) from the website to the GitHub could have a few benefits.

1. More people could edit the documents if necessary.
2. The development related stuff like source and its documentation would be in one place.

Any thoughts on this?","There are lots of tools for converting trac wiki to markdown.
Below is one I found.  It gets the trac pages from the sqlite DB;
I can send this DB to anyone if they want.

Anyway, this doesn't do everything correctly.
I fixed a couple of things but there are more.
Someone who understands regular expressions can do this
in a straightforward way; I don't have time right now unfortunately.

```
#!/usr/bin/python
# vim:set fileencoding=utf-8 sw=2 ai:

# downloaded from https://gist.github.com/tcchau/4628317
# ... which is a fork from https://gist.github.com/sgk/1286682

import sqlite3
import datetime
import re
# additional imports --ah
import os
import sys

SQL = '''
  select
      name, version, time, author, text
    from
      wiki w
    where
      version = (select max(version) from wiki where name = w.name)
'''

# Modified expression to generate a file for every version (overwriting previous version) --ah
SQLgit = '''
  select
      name, version, time, author, text, comment
    from
      wiki w
    order by time
'''

# exclude wiki pages starting with.... --ah
excludepages = [ ""Trac"" ]

# Add function to try to preserve author information in git --ah
def gitauthor(author):
  if author == ""foo@bar"":
    return "" --author \""Foo Bar <"" + author + "">\"" ""
  # add more here if you want to preserve author information for edits...
  elif author == ""trac"":
    return "" --author \""trac <"" + author + ""@example.com>\"" ""
  else:
    print ""WARNING: unknown author: "" + author
    return """"

# helper functions from https://gist.github.com/gazpachoking/9540849

def convert_wiki_link(link):
  if link.startswith('wiki:'):
    link = link[5:]
  return link.strip(""'"").replace('/', '-')


def sub_full_wiki_link(m):
  return '[%s](%s)' % (m.group(2), convert_wiki_link(m.group(1)))


def sub_simple_wiki_link(m):
  return '[[%s]]' % convert_wiki_link(m.group(1))


def sub_fenced_block(m):
  if m.group(1) == 'html':
    return '\n%s\n' % m.group(2)
  elif m.group(1):
    return '```%s\n%s\n```' % (m.group(1), m.group(2))
  return '```\n%s\n```' % m.group(2)


def sub_table(m):
  lines = []
  for group in m.group(0).strip().split('\n'):
    lines.append(' | '.join(group.strip().split('||')).strip())
  width = len(m.group(1).strip().split('||')) - 2
  lines.insert(1, '| %s |' % ' | '.join('---' for x in range(width)))
  return '\n%s\n' % '\n'.join(lines)

# use alternative SQL if using git to preserve history of changes --ah
if ""--git"" in sys.argv:
  SQL = SQLgit

conn = sqlite3.connect('../trac.db')
result = conn.execute(SQL)
for row in result:
  #print(row)
  name = row[0]
  version = row[1]
  time = row[2]
  author = row[3]
  text = row[4]
  #comment = row[5]
  comment=''

  # add possibility to exclude certain pages. --ah

  if name.startswith(tuple(excludepages)):
    print ""Skipping "" + name + "" (version "" + str(version) + "")""
    continue

  text = re.sub('\r\n', '\n', text)
  text = re.sub(r'{{{(.*?)}}}', r'`\1`', text)
  # below 2 line from https://gist.github.com/gazpachoking/9540849
  text = re.sub(r'(?sm){{{\n(?:#!([a-z]+)\n)?(.*?)\n}}}', sub_fenced_block, text)
  text = re.sub(r'(?m)^(\|\|[^\n]+\|\| *\n?)+$', sub_table, text)

  def indent4(m):
    return '\n    ' + m.group(1).replace('\n', '\n    ')
  text = re.sub(r'(?sm){{{\n(.*?)\n}}}', indent4, text)
  # Modified lines below to allow sloppy trailing spaces --ah
  text = re.sub(r'(?m)^====\s+(.*?)\s+====$', r'#### \1', text)
  text = re.sub(r'(?m)^====\s+(.*?)\s+====\s\#(.*?)$', r'### \1 <a name=""\2""></a>', text)
  text = re.sub(r'(?m)^===\s+(.*?)\s+===$', r'### \1', text)
  text = re.sub(r'(?m)^===\s+(.*?)\s+===\s\#(.*?)$', r'### \1 <a name=""\2""></a>', text)
  text = re.sub(r'(?m)^==\s+(.*?)\s+==$', r'## \1', text)
  text = re.sub(r'(?m)^==\s+(.*?)\s+==\s\#(.*?)$', r'### \1 <a name=""\2""></a>', text)
  text = re.sub(r'(?m)^=\s+(.*?)\s+=$', r'# \1', text)
  text = re.sub(r'(?m)^=\s+(.*?)\s+=\s\#(.*?)$', r'### \1 <a name=""\2""></a>', text)
  text = re.sub(r'^       * ', r'****', text)
  text = re.sub(r'^     * ', r'***', text)
  text = re.sub(r'^   * ', r'**', text)
  text = re.sub(r'^ * ', r'*', text)
  text = re.sub(r'^ \d+. ', r'1.', text)

  a = []
  for line in text.split('\n'):
    if not line.startswith('    '):
      #line = re.sub(r'\[(https?://[^\s\[\]]+)\s([^\[\]]+)\]', r'[\2](\1)', line)
      #line = re.sub(r'\[(wiki:[^\s\[\]]+)\s([^\[\]]+)\]', r'[\2](/\1/)', line)
      #line = re.sub(r'\!(([A-Z][a-z0-9]+){2,})', r'\1', line)
      #line = re.sub(r'\'\'\'(.*?)\'\'\'', r'*\1*', line)
      #line = re.sub(r'\'\'(.*?)\'\'', r'_\1_', line)
      # --- above original replaced with parts from https://gist.github.com/gazpachoking/9540849
      line = re.sub(r'(?<!\[)\[([^\s\[\]]+?)\]', sub_simple_wiki_link, line)
      line = re.sub(r'\[([a-z]+?://[^\s\[\]]+)\s([^\[\]]+)\]', r'[\2](\1)', line)
      line = re.sub(r'\[(wiki:?[^\s\[\]]+)\s([^\[\]]+)\]', sub_full_wiki_link, line)
      line = re.sub(r'\!(([A-Z][a-z0-9]+){2,})', r'\1', line)
      line = re.sub(r'\'\'\'(.*?)\'\'\'', r'*\1*', line)
      line = re.sub(r'\'\'(.*?)\'\'', r'_\1_', line)
    a.append(line)
  text = '\n'.join(a)

  # Modification to create directory path when needed. --ah
  if ""/"" in name:
    try:
      os.makedirs(os.path.dirname(name))
    except:
      pass


  fp = file('%s.md' % name, 'w')
  fp.write(text.encode('utf-8'))
  fp.close()

  # commit newly created/overwritten file to preserve history. --ah
  if comment != """":
   comment = ""\n\n%s"" %comment
  if ""--git"" in sys.argv:
    os.system(""git add .; git commit --quiet "" + gitauthor(author) + "" --date=\"""" + datetime.datetime.fromtimestamp(time/1000000).strftime('%Y/%m/%d %H:%M:%S') + ""\"" -avm \""Import "" + name + "", version "" + str(version) + str(comment) + ""\"""")
```",30260406
739,Apps don't exit when client exits,open,2018-11-13T00:08:55Z,2019-10-30T16:34:02Z,,CONTRIBUTOR,"On Win 7, I sporadically notice that when I exit the client, apps keep running.  Happens on multiple projects - I'm currently seeing it with SETI@home and Amicable Numbers, but have seen it with others.

This is doubly bad; when BOINC is resumed, apps can't acquire the lock in the slot directory.  Eventually the original process exits but BOINC isn't aware of it.  It may rerun the job from the beginning, depending on how the app is written.

The BOINC runtime system is supposed to prevent this.  It used to do so by checking for heartbeat messages from the client.  More recently it gets passed the PID of the client, and periodically checks whether this process still exists.

AFAIK, the apps in question were built recently and are ""native mode"", i.e. they use the standard runtime system.  I don't have any theories.  Either the PID check isn't getting done, or it's not working.","Possible workaround: use [Job Objects](https://docs.microsoft.com/en-us/windows/desktop/ProcThread/job-objects). They allow to terminate group of processes together. When process is assigned to a Job Object, its child processes also are automatically added to it. This should allow to properly clean them up.

**Edit:** On Linux you can use [Process Groups](https://en.wikipedia.org/wiki/Process_group) for this.",30260406
740,Update the Windows build dependencies,open,2018-11-11T00:27:28Z,2019-05-03T15:33:17Z,,MEMBER,All the dependencies have a newer version containing bug fixes and improvements. They should be updated.,"I'll take care about this but this will require significant test efforts to
find possible regression

Best regards,
Vitalii

Sent via Android

сб, 10 нояб. 2018 г., 19:27 Adam Radocz notifications@github.com:

> All the dependencies have a newer version containing bug fixes and
> improvements. They should be updated.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/2813>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADFZoTVnkN3BYMPYq-iGjP1qypM7jflKks5ut27xgaJpZM4YYQMs>
> .
>
",30260406
741,Building on AlpineLinux outputs many more notes,open,2018-11-09T14:48:11Z,2019-11-04T21:48:10Z,,NONE,"The amount of warnings and notes output during compilation has increased significantly.

Log for 7.12.1 (4643 lines):
https://api.travis-ci.org/v3/job/415043084/log.txt

Log for 7.14.2 (52838 lines):
https://api.travis-ci.org/v3/job/452886252/log.txt

I haven't gone through all of the output yet, but it would be great if anyone could provide pointers for removing as many of these notes and warnings as possible. Thanks!","The gcc version between the two logs you posted jumped from 6.4.0-r8 to 8.2.0-r0. The later version of course creates more warnings because it does more checks. Thus it is not unusal and you can't strictly compare the number of warnings between the two.

This does not mean we should not address the warnings generated by newer gcc versions. But the reason for the increase in warnings is not AlpineLinux or the BOINC version build.

A lot seem to come from an outdated use of `memset`on structs. A lot are generated for code that is a dependency (zip, wxWidgets). One should focus on the `memset` warnings and the incompatible pointer type warnings (look for `CAccountManagerProcessingPageEvent` in the second log for an example).",30260406
742,BOINC crashes when gdm is up or shutdown on fedora 29 (or maybe whatever) with nvidia driver,open,2018-11-05T13:44:08Z,2019-05-03T15:31:25Z,,NONE,"This is a double post of Google groups, but I guess this is the right place.  So I post the same thing here.

This may be a rare case, but I need help.

I am using BOINC on Fedora 29, with two nvidia cards (1070/750ti) and Intel integrated graphics using OpenCL of nvida cards on my core i7 4790k rig.  For BOINC, I use both nvidia cards' OpenCL for Collatz Conjecture and cpu for Citizen Science Grid/Amicable number.  My monitor is connected to cpu's integrated graphics, and nvidia cards are used only for BOINC so that x window manipulation doesn't slow down boinc calculation.

To build this configuration, I installed like this:
1. Install Fedora with gnome as usual.
2. Install boinc and nvidia graphic drivers which includes kernel modules and libraries for cuda/OpenCL and glx driver(and more?).
3. Only with above configuration, when loggin on/off, boinc crashes, so I removed /lib/modules/(kernel version)/kernel/drivers/video/nvidia-drm.ko which is one of four modules installed by nvidia graphics driver.  Then reinstall x server again, because nvidia graphics driver replaces libglx.so with one for nvidia cards (symlink to nvidia's glx driver).  This libglx.so replacement was found in /var/log/Xorg.0.log.  libglx.so is in /usr/lib64/xorg/modules/extensions/libglx.so.

Then I got my system up and running as long as gdm is on, but if gdm is shut down with ""systemctl isolate multi-user"" to get into console mode, boinc crashes.  And before that, I have to start boinc after logging into x, because when gdm starts, boinc crashes.

I suspect x server is doing something wrong to boinc client, but looking at x server source I can't solve this.   What does boinc client see when x server is up or shut down?

Does anyone help?

Thanks in advance!!

-Tetsuji","I confirmed my problem after reinstalling boinc-client package.

But this should be a problem of systemd, since a similar problem happens with wpa_supplicant on another machine.   Transition from multi-user to graphical or vice versa seems to kill a service.  On one of my machine, it is boinc-client which is killed, and on another machine, it's wpa_supplicant.

Anyway, here I put the command series what happens with boinc-client service. (I have disabled boinc-client.service now)
In graphical.target mode:
------------------------------------------
$ systemctl status boinc-client

● boinc-client.service - Berkeley Open Infrastructure Network Computing Client
   Loaded: loaded (/usr/lib/systemd/system/boinc-client.service; disabled; vend>
   Active: active (running) since Thu 2018-12-06 06:36:34 JST; 32min ago
     Docs: man:boinc(1)
 Main PID: 12309 (boinc)
    Tasks: 50 (limit: 4915)
   Memory: 3.9G
   CGroup: /system.slice/boinc-client.service
           ├─12309 /usr/bin/boinc
           ├─12376 ../../projects/csgrid.org_csg/exact_client_0.33 --training_f>
           ├─12377 ../../projects/csgrid.org_csg/exact_client_0.33 --training_f>
           ├─12378 ../../projects/csgrid.org_csg/exact_client_0.33 --training_f>
           ├─12379 ../../projects/csgrid.org_csg/exact_client_0.33 --training_f>
           ├─12380 ../../projects/csgrid.org_csg/exact_client_0.33 --training_f>
           ├─12382 ../../projects/csgrid.org_csg/exact_client_0.33 --training_f>
           ├─12385 ../../projects/csgrid.org_csg/exact_client_0.33 --training_f>
           ├─14535 ../../projects/boinc.thesonntags.com_collatz/collatz_sieve_1>
           └─15523 ../../projects/boinc.thesonntags.com_collatz/collatz_sieve_1>

Dec 06 07:07:29 maverick boinc[12309]: No protocol specified
Dec 06 07:07:29 maverick boinc[12309]: No protocol specified
Dec 06 07:07:30 maverick boinc[12309]: No protocol specified
Dec 06 07:07:30 maverick boinc[12309]: No protocol specified
Dec 06 07:07:31 maverick boinc[12309]: No protocol specified
Dec 06 07:07:31 maverick boinc[12309]: No protocol specified
Dec 06 07:07:42 maverick boinc[12309]: 06-Dec-2018 07:07:42 [Citizen Science Gr>
Dec 06 07:07:42 maverick boinc[12309]: 06-Dec-2018 07:07:42 [Citizen Science Gr>
Dec 06 07:07:45 maverick boinc[12309]: 06-Dec-2018 07:07:45 [Citizen Science Gr>
Dec 06 07:07:45 maverick boinc[12309]: 06-Dec-2018 07:07:45 [Citizen Science Gr>
----------------------------------------------
after ""systemctl isolate multi-user"" as root
-----------------------------------------------
# systemctl status boinc-client
● boinc-client.service - Berkeley Open Infrastructure Network Computing Client
   Loaded: loaded (/usr/lib/systemd/system/boinc-client.service; disabled; vendor preset: disabled)
      Active: inactive (dead)
           Docs: man:boinc(1)

Dec 06 07:07:31 maverick boinc[12309]: No protocol specified
Dec 06 07:07:31 maverick boinc[12309]: No protocol specified
Dec 06 07:07:42 maverick boinc[12309]: 06-Dec-2018 07:07:42 [Citizen Science Grid] Sending scheduler request: To fetch work.
Dec 06 07:07:42 maverick boinc[12309]: 06-Dec-2018 07:07:42 [Citizen Science Grid] Requesting new tasks for CPU
Dec 06 07:07:45 maverick boinc[12309]: 06-Dec-2018 07:07:45 [Citizen Science Grid] Scheduler request completed: got 0 new tasks
Dec 06 07:07:45 maverick boinc[12309]: 06-Dec-2018 07:07:45 [Citizen Science Grid] Project has no tasks available
Dec 06 07:10:08 maverick systemd[1]: Stopping Berkeley Open Infrastructure Network Computing Client...
Dec 06 07:10:08 maverick boinc[12309]: 06-Dec-2018 07:10:08 [---] Received signal 15
Dec 06 07:10:08 maverick boinc[12309]: 06-Dec-2018 07:10:08 [---] Exiting
Dec 06 07:10:15 maverick systemd[1]: Stopped Berkeley Open Infrastructure Network Computing Client.
------------------------------------------------------------------
then, start boinc-client by ""systemctl start boinc-client""
-------------------------------------------------------------------
● boinc-client.service - Berkeley Open Infrastructure Network Computing Client
   Loaded: loaded (/usr/lib/systemd/system/boinc-client.service; disabled; vendor preset: disabled)
      Active: active (running) since Thu 2018-12-06 07:11:51 JST; 3s ago
           Docs: man:boinc(1)
	    Main PID: 16861 (boinc)
	        Tasks: 22 (limit: 4915)
		   Memory: 1.4G
		      CGroup: /system.slice/boinc-client.service
		                 ├—16861 /usr/bin/boinc
				            ├—16929 ../../projects/csgrid.org_csg/exact_client_0.33 --training_file training_samples.bin --validation_file validation_samples.bin --testing_file testing_samples.bin>           ├—16930 ../../projects/csgrid.org_csg/exact_client_0.33 --training_file training_samples.bin --validation_file validation_samples.bin --testing_file testing_samples.bin>           ├—16931 ../../projects/csgrid.org_csg/exact_client_0.33 --training_file training_samples.bin --validation_file validation_samples.bin --testing_file testing_samples.bin>           ├—16932 ../../projects/csgrid.org_csg/exact_client_0.33 --training_file training_samples.bin --validation_file validation_samples.bin --testing_file testing_samples.bin>           ├—16934 ../../projects/csgrid.org_csg/exact_client_0.33 --training_file training_samples.bin --validation_file validation_samples.bin --testing_file testing_samples.bin>           ├—16935 ../../projects/csgrid.org_csg/exact_client_0.33 --training_file training_samples.bin --validation_file validation_samples.bin --testing_file testing_samples.bin>           ├—16938 ../../projects/csgrid.org_csg/exact_client_0.33 --training_file training_samples.bin --validation_file validation_samples.bin --testing_file testing_samples.bin>           ├—16940 ../../projects/boinc.thesonntags.com_collatz/collatz_sieve_1.40_x86_64-pc-linux-gnu__opencl_nvidia --device 0
					               └—16942 ../../projects/boinc.thesonntags.com_collatz/collatz_sieve_1.40_x86_64-pc-linux-gnu__opencl_nvidia --device 1

Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [---]    max disk usage: 85.84 GB
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [---]    (to change preferences, visit a project web site or select Preferences in the Manager)
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [---] Setting up project and slot directories
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [---] Checking active tasks
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [---] Setting up GUI RPC socket
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [---] gui_rpc_auth.cfg is empty - no GUI RPC password protection
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [---] Checking presence of 2069 project files
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 Initialization completed
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [Citizen Science Grid] Sending scheduler request: To fetch work.
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [Citizen Science Grid] Requesting new tasks for CPU
------------------------------------------------------------------------
again, get into graphical mode
--------------------------------------------------------------------------
[root@maverick ~]# systemctl isolate graphical
[root@maverick ~]# systemctl status boinc-client
● boinc-client.service - Berkeley Open Infrastructure Network Computing Client
   Loaded: loaded (/usr/lib/systemd/system/boinc-client.service; disabled; vendor preset: disabled)
      Active: inactive (dead)
           Docs: man:boinc(1)

Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [---] Checking presence of 2069 project files
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 Initialization completed
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [Citizen Science Grid] Sending scheduler request: To fetch work.
Dec 06 07:11:53 maverick boinc[16861]: 06-Dec-2018 07:11:53 [Citizen Science Grid] Requesting new tasks for CPU
Dec 06 07:11:56 maverick boinc[16861]: 06-Dec-2018 07:11:56 [Citizen Science Grid] Scheduler request completed: got 0 new tasks
Dec 06 07:11:56 maverick boinc[16861]: 06-Dec-2018 07:11:56 [Citizen Science Grid] Project has no tasks available
Dec 06 07:13:07 maverick systemd[1]: Stopping Berkeley Open Infrastructure Network Computing Client...
Dec 06 07:13:07 maverick boinc[16861]: 06-Dec-2018 07:13:07 [---] Received signal 15
Dec 06 07:13:08 maverick boinc[16861]: 06-Dec-2018 07:13:08 [---] Exiting
Dec 06 07:13:14 maverick systemd[1]: Stopped Berkeley Open Infrastructure Network Computing Client.
------------------------------------------------------------------

As seen above, boinc-client.service is killed after ""systemctl isolate graphical"" or ""systemctl isolate multi-user""
",30260406
743,Improve the efficiency/scalabilty of submit_rpc_handler.php,open,2018-11-01T09:56:55Z,2019-05-03T15:25:23Z,,CONTRIBUTOR,"When submitting 30K jobs, the submit_rpc_handler.php exceeded the default memory limit of 128M on line [230](https://github.com/BOINC/boinc/blob/master/html/user/submit_rpc_handler.php#L230). The XML file is only 8.7MB in size.

","Per-job parameters are not passed to create_work on the command line;
they're passed via stdin.",30260406
744,The python API should gracefully handle fatal errors from Apache,open,2018-11-01T09:47:44Z,2019-05-03T15:24:00Z,,CONTRIBUTOR,"When trying to submit 30K jobs using the python API, the following response was returned:

```
REPLY: <?xml version=""1.0"" encoding=""ISO-8859-1"" ?>
<submit_batch>
<br />
<b>Fatal error</b>:  Allowed memory size of 134217728 bytes exhausted (tried to allocate 2883692 bytes) in <b>/boincdata/boinc/project/lhcathome-dev/html/user/submit_rpc_handler.php</b> on line <b>230</b><br />
```

This led to the following exception:

```
Traceback (most recent call last):
  File ""submit.py"", line 38, in <module>
    response = submit_batch(b)
  File ""/home/lfield/cernbox/vc/boinc-rpc/submit_api.py"", line 203, in submit_batch
    return do_http_post(req.to_xml('submit_batch'), req.project)
  File ""/home/lfield/cernbox/vc/boinc-rpc/submit_api.py"", line 121, in do_http_post
    return ET.fromstring(reply)
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 1312, in XML
    return parser.close()
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 1671, in close
    self._raiseerror(v)
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 1523, in _raiseerror
    raise err
xml.etree.ElementTree.ParseError: no element found: line 5, column 0
```

",,30260406
745,Run VM apps at low process priority,open,2018-10-29T18:58:12Z,2018-11-16T12:28:54Z,,CONTRIBUTOR,"Currently VM apps (i.e. vbox_headless.exe) run at normal process priority.
This slows down system response; they should run at low priority.

I recall that VBox has no provision for running VMs at low priority.
If this is still the case, perhaps the client can identify VM processes
and lower the priority itself.",It could be a security issue in some setups because it lets you impact the QoS of the services running in the VM.,30260406
746,Windows Setup failing,open,2018-10-23T15:11:24Z,2019-07-02T08:00:26Z,,NONE,"The windows Setup is failing if the user accounts created by boinc are deleted.
It is than not possible to uninstall and/or reinstall boinc.
The setup is throwing the error message ""Failed to be able to obtain the SID for the selected user on the localhost"".

Maybe Virtual Accounts and/or Group Managed Service Accounts should be used here instead?
 - [Virtual Accounts](https://docs.microsoft.com/en-us/windows/security/identity-protection/access-control/service-accounts#a-href-idbkmk-virtualserviceaccountsavirtual-accounts) for standalone computers.
 - [Group Managed Service Accounts](https://docs.microsoft.com/en-us/windows/security/identity-protection/access-control/service-accounts#a-href-idbkmk-virtualserviceaccountsavirtual-accounts) for domain joined computers/servers.","There was an attempt to use virtual accounts in PR #2831 but unfortunately, it has been devoted because Windows XP doesn't support it. I'd have dropped XP support when Microsoft did that, but it has been devoted too in #2348.",30260406
747,[C++17] Update the code to use std::string_view,open,2018-10-21T20:36:03Z,2018-10-22T23:27:59Z,,MEMBER,"In C++17 there is a new data type called `std::string_view`. It's much faster then the `string const&`, according to [this forum](https://stackoverflow.com/questions/40127965/how-exactly-stdstring-view-is-faster-than-const-stdstring) and [this post](https://www.bfilipek.com/2018/07/string-view-perf.html).

Maybe it's worth to consider updating the code to use the new data.","Amdahl's law (serial version) applies here.

On Sun, Oct 21, 2018 at 11:04 PM Adam Radocz <notifications@github.com>
wrote:

> Boinc uses the string const& data type 420 times. The std::string_view
> type could be 10X faster in certain scenarios, and I think any performance
> related update is welcoming.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/2774#issuecomment-431747004>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AA8KgeZvK4WwQ_cQnM77o_5qbGus_MkSks5unV_agaJpZM4Xyp6S>
> .
>
",30260406
748,Client stops,open,2018-10-17T21:15:21Z,2019-02-08T22:54:27Z,,NONE,It seems like when the Boinc application is Quit it doesn't start again correctly. I'm getting a permissions error as an alert. I'm running OS X 10.4 with Client 7.12 The only solution I've been able to determine is to simply reinstall the client? But over time the issue re-emerges again? I'm getting no mention as to what file is in error.,@DeepSeaOrca42  - Did 7.14.2 resolve your problems?,30260406
749,Support for S3/Ceph direct uploads. ,open,2018-10-15T12:12:43Z,2018-11-01T18:48:38Z,,CONTRIBUTOR,"For some projects data management could be improved by uploading files directly into an S3/Ceph storage system close to the volunteer. This can be done with a [presigned URL](https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html) after authentication. The [data server protocol](https://boinc.berkeley.edu/trac/wiki/FileUpload) could be updated to send back to the client a direct URL where to put the file.  More details can be found in the following [paper](http://iopscience.iop.org/article/10.1088/1742-6596/898/6/062041).


","Oh - I like this presigned URL mechanism - I like this idea since I think that shifting to using object storage will be an important thing in the future.  In fact it would be great if everything from create_work, downloads, uploads, validators and assimilators could use the object storage.  This would greatly simplify project storage.",30260406
750,Simple View Task Status icon changes when hovering task with differing status,open,2018-10-11T22:17:17Z,2018-10-12T08:13:03Z,,NONE,"Simple View Task Status icon changes when hovering task with differing status

Simple View Task Status icon erroneously changes when combobox is expanded and mouse hovers a task with a different status.

See screenshot, where the 2 cmsearch tasks are in a running state, but the combobox shows ""suspended"" because I'm hovering a ""Long runs"" task that is suspended.

Expected behavior: The combobox task at the very top of the list, should not change status nor text, when hovering other tasks in the list.

Screenshot:
![20181011 boinc simple view status icon wrong](https://user-images.githubusercontent.com/13267799/46836990-a5be7d80-cd81-11e8-89a2-d329545dfa75.png)
",I think it's a bug (or inherent in) WxWidgets.  I looked at the code and it's changing the icon of only the selected item.,30260406
751,BOINC Service Uninstall - Leaves boinc_master user profile remnants,open,2018-10-11T11:20:24Z,2018-10-11T11:42:12Z,,NONE,"BOINC Service Uninstall - Leaves boinc_master user profile remnants

Note sure if BOINC bug, or Windows bug. But I presume BOINC bug. See screenshot at bottom of post.

Steps to reproduce:
- Install BOINC, choosing Service Install
- Restart the PC when requested
- Uninstall BOINC using Settings > Apps > BOINC > Uninstall
- Restart the PC
- **BUG PART 1: User Profile ""Account unknown"" is erroneously not deleted, in Control Panel > System > Advanced system settings > User Profiles > Settings**
- **BUG PART 2: Folder ""boinc_master"" is erroneously not deleted, in File Explorer > C:\Users**
- Install BOINC, choosing Service Install
- Restart the PC when requested
- Uninstall BOINC using Settings > Apps > BOINC > Uninstall
- Restart the PC
- **BUG PART 3: These account remnants ""stack up"", creating a new folder each time.**

Note: The reason my screenshot doesn't show the stack in the Control Panel, is because apparently installing an updated build of Windows 10 does not ""carry over"" the ""Account unknown"" stack. But they do stack, when uninstalled/reinstalled more than once on a single Windows 10 build.

Reproduced on:
BOINC 7.14.2 x64 on Windows 10 x64 Insider Build 18252 (latest Insider Build).

See screenshot.
![20181011 boinc service uninstall - leaves boinc_master user profile remnants](https://user-images.githubusercontent.com/13267799/46799965-36ff0700-cd24-11e8-805c-09fe535e2d76.png)
",,30260406
752,Win screensaver problem,open,2018-10-10T19:26:25Z,2019-10-30T13:56:57Z,,CONTRIBUTOR,"from a user email:

On a win7 box with BOINC (7.14.0) installed but two projects suspended and no tasks in queue, and Malwarebytes scan running, BOINC screen server displays the  BOINC screensaver loading message briefly, then screen goes completely blank. The scan uses well over the 25% default ""other activity"" threshold so maybe that makes sense,and the screen stays blank until keyboard or mouse activity happens. This contrasts with the ""BOINC screensaver loading"" message that moves around the screen on the same box with task slideshow running.",,30260406
753,Integration with slurm / GridEngine / Torque / you-name-it queueing system,open,2018-10-02T12:55:54Z,2020-03-26T14:00:38Z,,CONTRIBUTOR,"Hello,
Is there a humane way to submit BOINC tasks as jobs in slurm/... ? Nodes have Internet access and a shared home directory. I would not mind if the boinccmd would submit those jobs on my behalf, but what I had in mind was to submit the boinccmd itself within a bash script for a certain number of tasks (likely 1) and it would in my responsibility to reserve sufficient time for it.
Many thanks
Steffen",Version 7.17.0 doesn't seem to have the option ```-dir``` or ```--dir```.  How do I change the directory used by the job?,30260406
754,BOINC on Guix,open,2018-09-27T03:45:12Z,2019-10-30T13:56:10Z,,NONE,It would be nice if BOINC were available as a Guix package (https://www.gnu.org/software/guix/).,This would require someone familiar with Guix to provide the package. ,30260406
755,"Consolidate all user preferences/configuration values into one file, or at least remove them from client_state.xml",open,2018-09-24T08:43:02Z,2020-02-09T20:37:59Z,,CONTRIBUTOR,"While helping someone figure out why something doesn't work as it should on his BOINC, I again found that I have to steer this person to various different files in the data directory, to check on values. At this moment BOINC and BOINC Manager store the user preferences/configuration files in four different places:
- client_state.xml
- global_prefs.xml
- global_prefs_override.xml
- cc_config.xml

I'm wondering if it isn't better to have all of the configuration/preferences values in one file, or in the least remove them from client_state.xml and into cc_config.xml or another (new?) configuration file. 

Because now when the user has to manually make a change in client_state.xml and he makes a mistake, all work he's got stored in it will be removed and although we have a backup file (client_state_prev.xml), this isn't used immediately as a backup. Instead, as I found out, it's just overwritten with the new client_state.xml contents without all the lost work. This happens whether you're quick to exit & restart the client or not. 

I also understand that this requires a substantial rewrite of the client and Manager, and may require that the PMC agree on it. And that it's therefore not for a next version.  

But just asking how much it would take?","1. I've never found a reason to edit my client_state.xml file but plenty of reasons to look up values and names in it. Having said that,
2. LHC ATLAS has a problem with how they use LHC_Preferences/Max # WUs and Max # CPUs. computezrmle came up with this solution to give the client proper control:
```
<app_version>
    <app_name>ATLAS</app_name>
    <plan_class>native_mt</plan_class>
    <avg_ncpus>1</avg_ncpus>
    <cmdline>--nthreads 1</cmdline>
</app_version>
```
Followed by a BOINC restart when it overwrites these values into ones client_state.xml file. 
If it's decided to alter these files please assure clients can emulate that level of control.",30260406
756,Devise mechanism for updating all_projects_list.xml in github,open,2018-09-19T11:16:55Z,2019-10-30T13:55:22Z,,CONTRIBUTOR,"all_projects_list.xml is dynamic data which is distributed to new users as a part of the installation process. Historically, it's held in this repository at master/win_build/installerv2/redist (and hence automatically incorporated in new release branches): although the location flags it as a Windows build file, the same resource is also used in Mac installer builds.

The file is also fetched periodically (every 14 days) by the BOINC client, unless disabled by corporate policies like <dont_contact_ref_site>.

At the moment, the file in master is 5 months old, and thus breaks the policy that master should be held in a state of readiness for the Release Manager when he or she decides to make a new client release branch.

This new issue is intended to act as a reminder that we have 'unfinished business' from the last few client releases, so that previous references like #2247 and #2579 can be closed or remain closed. This issue should be read in conjunction with @ChristianBeer comments in #2642, where the confusion between 'project config' and 'master' urls is discussed: this affects whether http or https is used in the primary url field in all_projects_list - see note on Einstein@Home in attached spreadsheet showing the current differences between versions of the file.

[Comparison of All_Projects_List versions 2018-09-19.zip](https://github.com/BOINC/boinc/files/2396794/Comparison.of.All_Projects_List.versions.2018-09-19.zip)",,30260406
757,"""Use at most N% of the CPUs"" vs ""Use at most N CPUs""",open,2018-09-19T07:55:35Z,2020-02-09T20:25:23Z,,CONTRIBUTOR,"Why did we ever change from ""Use at most N CPUs"" to ""Use at most N% of the CPUs""? Isn't it way easier to just set the amount of CPUs or cores someone wants to use, instead of having them do algebra to get to an answer? 

I mean, what if I have 128 CPU cores and I want to use just 33 of them.. What is easier to set?
1. Set the value to 33?
2. Set the value to the outcome of 128 / 100 * 33? 

The latter requires a pad of paper and a sharp mind, or a calculator. 
If it was done to make it easier for BOINC to set the amount of cores, that can be done in code with just a set value, can't it? And since the value is an integer anyway, we can't set to use 33.5 CPUs. So why need a percentage of CPUs or cores? ","I keep a spreadsheet for my fleet using this formula to calculate what to enter in this ambiguous BOINC box:
`=ROUNDUP((threads-Not4boinc)/threads,2)`
Future-proof is no reason for using such an annoying and error-prone approach. Please switch to N_CPUs.",30260406
758,Improve server documentation,open,2018-09-17T20:06:44Z,2019-10-30T13:54:49Z,,CONTRIBUTOR,"The server documentation has been fair-to-poor since the dawn of time.
I started improving it, but there's work to be done.

Structure: there are now two top-level pages:

""Computing with BOINC"" (https://boinc.berkeley.edu/trac/wiki/BoincOverview)
The target audience is someone (not necessarily technical) who needs cheap HTC.
The goal is to explain to them what BOINC does and what its advantages are;
i.e. to get them interested in using BOINC.

""Technical documentation"" (https://boinc.berkeley.edu/trac/wiki/ProjectMain)
The target audience is a technical person charged with evaluating BOINC or creating a project.

All other pages are linked to from one of these top-level pages (no buried pages).

I've shuffled some pages to fit into this structure.
The BOINC web site has links to each of the top-level pages.

-------------

The technical documentation starts with a ""cookbook"" for creating a project
that does actual distributed computing, quickly and with minimal user prerequisites.
This used to be https://boinc.berkeley.edu/trac/wiki/QuickStart, but that uses a kludgy approach.
I changed the link to point to Marius' instructions for a Docker-based server.
We need to flesh these out a bit, and test them.

The rest of the technical documentation suffers from some general problems:
- Some of it is too low level.  Unnecessary implementation details are mixed in.
  It should be more ""cookbook"" in nature.  More examples.
- Some of it is outdated or wrong.
- Redundancy.
- Inconsistent terminology
- Missing info (e.g. how to use the admin interface to track down app problems)

Please send me suggestions or edits.
For now I'd like to do the final editing myself, to keep it consistent.
-----------------------------

~90% of the documentation is for features the average project won't use,
which makes it all seem hopelessly complex.
It's tempting to separate this out, i.e. divide the page into
1) quick start
2) stuff the average project needs to know
3) esoterica
However, it's not clear to me how to divide 2) and 3), unless we make drastic assumptions
about what the ""average project"" is.",,30260406
759,cross-project certificate broken by bad external link,open,2018-09-12T02:18:56Z,2019-10-30T13:54:22Z,,MEMBER,"A user pointed out that the cert_all page is broken, which I've tracked down to that:

https://github.com/BOINC/boinc/blob/5a42f595cbb3e337863bee9395143b90e2690f37/html/inc/user.inc#L36-L37

is a broken link. E.g. for my user on Cosmology@Home the link is http://boinc.netsoft-online.com/get_user.php?cpid=da600bb3c0e54c1327d76a64b04fec36 which gives a forbidden error.

Does anyone know the correct URL? ",Oh! This related only to https://boinc.thesonntags.com website. For more see comments here: https://github.com/BOINC/boinc/issues/2975,30260406
760,BOINC Manager Crashes,open,2018-09-05T14:24:18Z,2019-10-30T13:54:04Z,,MEMBER,"The BOINC Manager will crash under the following scenario:

1) Attach to a project using a username with a space
2) Remove the project
3) Open the Add Project Wizard

This is reliably reproducable from a build from master on Linux.  I don't know if this happens on Windows or Mac.

Here is the backtrace:
```
*** Error in `./boincmgr': corrupted size vs. prev_size: 0x0000000000e7fa90 ***
======= Backtrace: =========
/usr/lib64/libc.so.6(+0x7f574)[0x7fae32e04574]
/usr/lib64/libc.so.6(+0x82d64)[0x7fae32e07d64]
/usr/lib64/libc.so.6(+0x8466c)[0x7fae32e0966c]
/usr/lib64/libc.so.6(realloc+0x1b2)[0x7fae32e0ae42]
/usr/lib64/libglib-2.0.so.0(g_realloc+0x16)[0x7fae339c0fa6]
/usr/lib64/libgobject-2.0.so.0(+0x21406)[0x7fae33ca6406]
/usr/lib64/libgobject-2.0.so.0(g_signal_connect_data+0x49e)[0x7fae33cace7e]
/usr/lib64/libgtk-3.so.0(+0x32339b)[0x7fae3565739b]
/usr/lib64/libgobject-2.0.so.0(g_type_create_instance+0x1fb)[0x7fae33cb661b]
/usr/lib64/libgobject-2.0.so.0(+0x1525d)[0x7fae33c9a25d]
/usr/lib64/libgobject-2.0.so.0(g_object_new_with_properties+0x27d)[0x7fae33c9bb0d]
/usr/lib64/libgobject-2.0.so.0(g_object_new+0xc1)[0x7fae33c9c4f1]
/usr/lib64/libgtk-3.so.0(gtk_text_view_new_with_buffer+0xe)[0x7fae3565d89e]
/usr/lib64/libwx_gtk3u_core-3.0.so.0(_ZN10wxTextCtrl6CreateEP8wxWindowiRK8wxStringRK7wxPointRK6wxSizelRK11wxValidatorS4_+0x15b)[0x7fae3847ea1b]
/usr/lib64/libwx_gtk3u_core-3.0.so.0(_ZN10wxTextCtrlC2EP8wxWindowiRK8wxStringRK7wxPointRK6wxSizelRK11wxValidatorS4_+0x234)[0x7fae3847f0f4]
./boincmgr[0x438c4a]
./boincmgr[0x4396d6]
./boincmgr[0x558e1f]
./boincmgr[0x559987]
./boincmgr[0x55b1e8]
./boincmgr[0x451085]
/usr/lib64/libwx_baseu-3.0.so.0(_ZNK16wxAppConsoleBase16CallEventHandlerEP12wxEvtHandlerR14wxEventFunctorR7wxEvent+0x3e)[0x7fae378e162e]
/usr/lib64/libwx_baseu-3.0.so.0(_ZN12wxEvtHandler23ProcessEventIfMatchesIdERK21wxEventTableEntryBasePS_R7wxEvent+0x52)[0x7fae37a5fd42]
/usr/lib64/libwx_baseu-3.0.so.0(_ZN16wxEventHashTable11HandleEventER7wxEventP12wxEvtHandler+0x83)[0x7fae37a5fdf3]
/usr/lib64/libwx_baseu-3.0.so.0(_ZN12wxEvtHandler11TryHereOnlyER7wxEvent+0x3d)[0x7fae37a6014d]
/usr/lib64/libwx_baseu-3.0.so.0(_ZN12wxEvtHandler19ProcessEventLocallyER7wxEvent+0x33)[0x7fae37a601c3]
/usr/lib64/libwx_baseu-3.0.so.0(_ZN12wxEvtHandler12ProcessEventER7wxEvent+0x45)[0x7fae37a60225]
/usr/lib64/libwx_gtk3u_core-3.0.so.0(_ZN12wxWindowBase8TryAfterER7wxEvent+0x68)[0x7fae385718d8]
/usr/lib64/libwx_baseu-3.0.so.0(_ZN12wxEvtHandler18SafelyProcessEventER7wxEvent+0x7)[0x7fae37a5ffb7]
/usr/lib64/libwx_gtk3u_core-3.0.so.0(_ZN10wxMenuBase9SendEventEii+0x9a)[0x7fae385339ca]
/usr/lib64/libwx_gtk3u_core-3.0.so.0(+0x2edcdb)[0x7fae3845ccdb]
/usr/lib64/libgobject-2.0.so.0(g_closure_invoke+0x138)[0x7fae33c949d8]
/usr/lib64/libgobject-2.0.so.0(+0x2207d)[0x7fae33ca707d]
/usr/lib64/libgobject-2.0.so.0(g_signal_emit_valist+0xe11)[0x7fae33caf0f1]
/usr/lib64/libgobject-2.0.so.0(g_signal_emit+0x8f)[0x7fae33caf3df]
/usr/lib64/libgtk-3.so.0(gtk_widget_activate+0x7c)[0x7fae356bd96c]
/usr/lib64/libgtk-3.so.0(gtk_menu_shell_activate_item+0x106)[0x7fae35588a06]
/usr/lib64/libgtk-3.so.0(+0x254ced)[0x7fae35588ced]
/usr/lib64/libgtk-3.so.0(+0x237191)[0x7fae3556b191]
/usr/lib64/libgobject-2.0.so.0(+0xfc07)[0x7fae33c94c07]
/usr/lib64/libgobject-2.0.so.0(g_signal_emit_valist+0x477)[0x7fae33cae757]
/usr/lib64/libgobject-2.0.so.0(g_signal_emit+0x8f)[0x7fae33caf3df]
/usr/lib64/libgtk-3.so.0(+0x386b8c)[0x7fae356bab8c]
/usr/lib64/libgtk-3.so.0(+0x2341cc)[0x7fae355681cc]
/usr/lib64/libgtk-3.so.0(gtk_main_do_event+0x736)[0x7fae3556a186]
/usr/lib64/libgdk-3.so.0(+0x372b5)[0x7fae3507f2b5]
/usr/lib64/libgdk-3.so.0(+0x69242)[0x7fae350b1242]
/usr/lib64/libglib-2.0.so.0(g_main_context_dispatch+0x159)[0x7fae339bb8f9]
/usr/lib64/libglib-2.0.so.0(+0x4ac58)[0x7fae339bbc58]
/usr/lib64/libglib-2.0.so.0(g_main_loop_run+0x6a)[0x7fae339bbf2a]
/usr/lib64/libgtk-3.so.0(gtk_main+0x85)[0x7fae35569395]
/usr/lib64/libwx_gtk3u_core-3.0.so.0(_ZN14wxGUIEventLoop5DoRunEv+0x25)[0x7fae383f5605]
/usr/lib64/libwx_baseu-3.0.so.0(_ZN15wxEventLoopBase3RunEv+0xa0)[0x7fae37926280]
/usr/lib64/libwx_baseu-3.0.so.0(_ZN16wxAppConsoleBase8MainLoopEv+0x4d)[0x7fae378e38ad]
/usr/lib64/libwx_baseu-3.0.so.0(_Z7wxEntryRiPPw+0x5d)[0x7fae37973c1d]
./boincmgr[0x425c18]
/usr/lib64/libc.so.6(__libc_start_main+0xf5)[0x7fae32da73d5]
./boincmgr[0x431150]
======= Memory map: ========
00400000-0065d000 r-xp 00000000 fd:02 14943046                           /home/knreed/test-boinc/boinc/boincmgr
0085c000-0085d000 r--p 0025c000 fd:02 14943046                           /home/knreed/test-boinc/boinc/boincmgr
0085d000-0087a000 rw-p 0025d000 fd:02 14943046                           /home/knreed/test-boinc/boinc/boincmgr
0087a000-00899000 rw-p 00000000 00:00 0 
0094f000-01311000 rw-p 00000000 00:00 0                                  [heap]
7fad90000000-7fad90022000 rw-p 00000000 00:00 0 
7fad90022000-7fad94000000 ---p 00000000 00:00 0 
7fad98000000-7fad98021000 rw-p 00000000 00:00 0 
7fad98021000-7fad9c000000 ---p 00000000 00:00 0 
7fad9c000000-7fad9c022000 rw-p 00000000 00:00 0 
7fad9c022000-7fada0000000 ---p 00000000 00:00 0 
7fada0000000-7fada0021000 rw-p 00000000 00:00 0 
7fada0021000-7fada4000000 ---p 00000000 00:00 0 
7fada4000000-7fada4021000 rw-p 00000000 00:00 0 
7fada4021000-7fada8000000 ---p 00000000 00:00 0 
7fada8000000-7fada8021000 rw-p 00000000 00:00 0 
7fada8021000-7fadac000000 ---p 00000000 00:00 0 
7fadadbd5000-7fadaf7ff000 rw-p 00000000 00:00 0 
7fadaf7ff000-7fadaf800000 ---p 00000000 00:00 0 
7fadaf800000-7fadb0000000 rw-p 00000000 00:00 0 
7fadb0000000-7fadb0021000 rw-p 00000000 00:00 0 
7fadb0021000-7fadb4000000 ---p 00000000 00:00 0 
7fadb4000000-7fadb4021000 rw-p 00000000 00:00 0 
7fadb4021000-7fadb8000000 ---p 00000000 00:00 0 
7fadb84f8000-7fadb8530000 r-xp 00000000 fd:02 19407955                   /usr/lib64/libcroco-0.6.so.3.0.1
7fadb8530000-7fadb872f000 ---p 00038000 fd:02 19407955                   /usr/lib64/libcroco-0.6.so.3.0.1
7fadb872f000-7fadb8732000 r--p 00037000 fd:02 19407955                   /usr/lib64/libcroco-0.6.so.3.0.1
7fadb8732000-7fadb8733000 rw-p 0003a000 fd:02 19407955                   /usr/lib64/libcroco-0.6.so.3.0.1
7fadb8733000-7fadb8767000 r-xp 00000000 fd:02 19408498                   /usr/lib64/librsvg-2.so.2.40.16
7fadb8767000-7fadb8966000 ---p 00034000 fd:02 19408498                   /usr/lib64/librsvg-2.so.2.40.16
7fadb8966000-7fadb8967000 r--p 00033000 fd:02 19408498                   /usr/lib64/librsvg-2.so.2.40.16
7fadb8967000-7fadb8968000 rw-p 00034000 fd:02 19408498                   /usr/lib64/librsvg-2.so.2.40.16
7fadb8968000-7fadb896a000 r-xp 00000000 fd:02 19673457                   /usr/lib64/gdk-pixbuf-2.0/2.10.0/loaders/libpixbufloader-svg.so
7fadb896a000-7fadb8b69000 ---p 00002000 fd:02 19673457                   /usr/lib64/gdk-pixbuf-2.0/2.10.0/loaders/libpixbufloader-svg.so
7fadb8b69000-7fadb8b6a000 r--p 00001000 fd:02 19673457                   /usr/lib64/gdk-pixbuf-2.0/2.10.0/loaders/libpixbufloader-svg.so
7fadb8b6a000-7fadb8b6b000 rw-p 00002000 fd:02 19673457                   /usr/lib64/gdk-pixbuf-2.0/2.10.0/loaders/libpixbufloader-svg.so
7fadb93fb000-7fadb93fc000 ---p 00000000 00:00 0 
7fadb93fc000-7fadb9bfc000 rw-p 00000000 00:00 0 
7fadb9bfc000-7fadb9ffc000 ---p 00000000 00:00 0 
7fadb9ffc000-7fadb9ffd000 ---p 00000000 00:00 0 
7fadb9ffd000-7fadba7fd000 rw-p 00000000 00:00 0 
7fadba7fd000-7fadba7fe000 ---p 00000000 00:00 0 
7fadba7fe000-7fadbaffe000 rw-p 00000000 00:00 0 
7fadbaffe000-7fadbafff000 ---p 00000000 00:00 0 
7fadbafff000-7fadbb7ff000 rw-p 00000000 00:00 0 
7fadbb7ff000-7fadbb800000 ---p 00000000 00:00 0 
7fadbb800000-7fadbc000000 rw-p 00000000 00:00 0 
7fadbc000000-7fadbc021000 rw-p 00000000 00:00 0 
7fadbc021000-7fadc0000000 ---p 00000000 00:00 0 
7fadc009c000-7fadc0141000 r--p 00000000 fd:02 20450751                   /usr/share/fonts/dejavu/DejaVuSans-Bold.ttf
7fadc0141000-7fadc01f1000 r--p 00000000 fd:02 20450755                   /usr/share/fonts/dejavu/DejaVuSans.ttf
7fadc01f1000-7fadc01f2000 ---p 00000000 00:00 0 
7fadc01f2000-7fadc09f2000 rw-p 00000000 00:00 0 
7fadc09f2000-7fadc09f3000 ---p 00000000 00:00 0 
7fadc09f3000-7fadc11f3000 rw-p 00000000 00:00 0 
7fadc11f3000-7fadc11f4000 ---p 00000000 00:00 0 
7fadc11f4000-7fadc19f4000 rw-p 00000000 00:00 0 
7fadc19f4000-7fadc28d4000 r-xp 00000000 fd:02 19663971                   /usr/lib64/flash-plugin/libflashplayer.so
7fadc28d4000-7fadc2ad4000 ---p 00ee0000 fd:02 19663971                   /usr/lib64/flash-plugin/libflashplayer.so
7fadc2ad4000-7fadc2b93000 r--p 00ee0000 fd:02 19663971                   /usr/lib64/flash-plugin/libflashplayer.so
7fadc2b93000-7fadc2bc7000 rw-p 00f9f000 fd:02 19663971                   /usr/lib64/flash-plugin/libflashplayer.so
7fadc2bc7000-7fadc2cd2000 rw-p 00000000 00:00 0 
7fadc2cd2000-7fadc2cf8000 r-xp 00000000 fd:02 19408271                   /usr/lib64/libjson-glib-1.0.so.0.200.6
7fadc2cf8000-7fadc2ef8000 ---p 00026000 fd:02 19408271                   /usr/lib64/libjson-glib-1.0.so.0.200.6
7fadc2ef8000-7fadc2ef9000 r--p 00026000 fd:02 19408271                   /usr/lib64/libjson-glib-1.0.so.0.200.6
7fadc2ef9000-7fadc2efa000 rw-p 00027000 fd:02 19408271                   /usr/lib64/libjson-glib-1.0.so.0.200.6
7fadc2efa000-7fadc2efe000 r-xp 00000000 fd:02 19804146                   /usr/lib64/mozilla/plugins/libgnome-shell-browser-plugin.so
7fadc2efe000-7fadc30fd000 ---p 00004000 fd:02 19804146                   /usr/lib64/mozilla/plugins/libgnome-shell-browser-plugin.so
7fadc30fd000-7fadc30fe000 r--p 00003000 fd:02 19804146                   /usr/lib64/mozilla/plugins/libgnome-shell-browser-plugin.so
7fadc30fe000-7fadc30ff000 rw-p 00004000 fd:02 19804146                   /usr/lib64/mozilla/plugins/libgnome-shell-browser-plugin.so
7fadc30ff000-7fadc3129000 r-xp 00000000 fd:02 20071149                   /usr/lib/jvm/java-1.8.0-oracle-1.8.0.181-1jpp.2.el7.x86_64/jre/lib/amd64/libnpjp2.so
7fadc3129000-7fadc3328000 ---p 0002a000 fd:02 20071149                   /usr/lib/jvm/java-1.8.0-oracle-1.8.0.181-1jpp.2.el7.x86_64/jre/lib/amd64/libnpjp2.so
7fadc3328000-7fadc332b000 rw-p 00029000 fd:02 20071149                   /usr/lib/jvm/java-1.8.0-oracle-1.8.0.181-1jpp.2.el7.x86_64/jre/lib/amd64/libnpjp2.so
7fadc332b000-7fadc3331000 r-xp 00000000 fd:02 28705477                   /opt/Citrix/ICAClient/npica.so
7fadc3331000-7fadc3530000 ---p 00006000 fd:02 28705477                   /opt/Citrix/ICAClient/npica.so
7fadc3530000-7fadc3531000 r--p 00005000 fd:02 28705477                   /opt/Citrix/ICAClient/npica.so
7fadc3531000-7fadc3532000 rw-p 00006000 fd:02 28705477                   /opt/Citrix/ICAClient/npica.so
7fadc3532000-7fadc3569000 r-xp 00000000 fd:02 28721199                   /opt/google/talkplugin/libnpo1d.so
7fadc3569000-7fadc3768000 ---p 00037000 fd:02 28721199                   /opt/google/talkplugin/libnpo1d.so
7fadc3768000-7fadc376a000 r--p 00036000 fd:02 28721199                   /opt/google/talkplugin/libnpo1d.so
7fadc376a000-7fadc376b000 rw-p 00038000 fd:02 28721199                   /opt/google/talkplugin/libnpo1d.so
7fadc376b000-7fadc377b000 r-xp 00000000 fd:02 19795583                   /usr/lib64/mozilla/plugins/libfreegpws.so
7fadc377b000-7fadc397a000 ---p 00010000 fd:02 19795583                   /usr/lib64/mozilla/plugins/libfreegpws.so
7fadc397a000-7fadc397b000 r--p 0000f000 fd:02 19795583                   /usr/lib64/mozilla/plugins/libfreegpws.so
7fadc397b000-7fadc397c000 rw-p 00010000 fd:02 19795583                   /usr/lib64/mozilla/plugins/libfreegpws.so
7fadc397c000-7fadc3df1000 r-xp 00000000 fd:02 19408198                   /usr/lib64/libgtk-x11-2.0.so.0.2400.31
7fadc3df1000-7fadc3ff0000 ---p 00475000 fd:02 19408198                   /usr/lib64/libgtk-x11-2.0.so.0.2400.31
7fadc3ff0000-7fadc3ff7000 r--p 00474000 fd:02 19408198                   /usr/lib64/libgtk-x11-2.0.so.0.2400.31
```","```
(gdb) set print elements 0
(gdb) set print frame-arguments all
(gdb) thread apply all backtrace
```
may help in getting more data from GDB backtrace",30260406
761,./configure --enable-pkg-client doesn't work without libraries?,open,2018-08-27T06:23:45Z,2019-10-30T13:53:50Z,,CONTRIBUTOR,"Prereqs:
Brand new Ubuntu 18.04 installation
sudo apt install git build-essential m4 pkg-config ubuntu-dev-tools autoconf libtool libcurl4-openssl-dev libssl-dev libxss-dev zlib1g-dev
(btw, zlib1g-dev isn't detected by ./configure)

./configure --enable-pkg-client (same with --disable-server --enable-client --disable-libraries
 --disable-manager) results in failure:
  CXXLD    boinc_client
libtool:   error: cannot find the library '../lib/libboinc_crypt.la' or unhandled argument '../lib/libboinc_crypt.la'
Makefile:716: recipe for target 'boinc_client' failed

workaround, use this instead to build the client:
./configure --disable-server --enable-client --enable-libraries --disable-manager",,30260406
762,Add IDN support to our static libcurl build,open,2018-08-26T15:14:33Z,2019-10-30T13:53:27Z,,MEMBER,"International Domain Names (Umlaut-Domains among others) may be a thing in the future. `libcurl` already supports [libidn2](https://gitlab.com/libidn/libidn2) which translates the domain name into a string that can be resolved. Since we build a static version of libcurl we also need to build a static version of libidn2 too.

Currently this would need to be implemented for Windows and Mac since we don't really have a static Linux build.",In the future? IDNs have been live for a while.,30260406
763,Mac screensaver app can not be built on Travis CI,open,2018-08-26T14:32:01Z,2020-02-26T16:22:09Z,,MEMBER,"The Xcode ss_app target can not be built on Travis CI because of missing symbols. The error message seems to point to freetype or FTGL or some other missing static library. Since this is not release critical (it only happens on the Travis OSX VM) I just disabled the target but it would be nice if someone can investigate and fix it.

```
Undefined symbols for architecture x86_64:
  ""_png_error"", referenced from:
      _read_data_from_FT_Stream in libfreetype.a(sfnt.o)
  ""_png_create_info_struct"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_read_fn"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_read_info"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_palette_to_rgb"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_get_error_ptr"", referenced from:
      _error_callback in libfreetype.a(sfnt.o)
      _read_data_from_FT_Stream in libfreetype.a(sfnt.o)
  ""_png_create_read_struct"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_longjmp_fn"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
      _error_callback in libfreetype.a(sfnt.o)
  ""_png_set_tRNS_to_alpha"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_expand_gray_1_2_4_to_8"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_strip_16"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_read_image"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_packing"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_gray_to_rgb"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_interlace_handling"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_read_update_info"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_read_end"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_get_io_ptr"", referenced from:
      _read_data_from_FT_Stream in libfreetype.a(sfnt.o)
  ""_png_get_valid"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_get_IHDR"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_read_user_transform_fn"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_set_filler"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
  ""_png_destroy_read_struct"", referenced from:
      _Load_SBit_Png in libfreetype.a(sfnt.o)
```","Following those references I ended up [here](https://github.com/BOINC/boinc/blob/master/mac_build/buildfreetype.sh#L143) and `--without-png` is already included there. As Christian [said](https://github.com/BOINC/boinc/issues/2662#issuecomment-416055772), he already tried.",30260406
764,Make attach process the same for Client and Manager,open,2018-08-09T19:35:34Z,2019-10-30T13:53:07Z,,MEMBER,"Currently the Manager uses a 4 step process to attach the Client to a project:

1. The User selects or enters a `URL` in the Attach project Wizard. The Manager makes a GET request to `URL/get_project_config.php` to fetch the `master_url`
1. The Manager tells the Client to attach to `master_url`
1. The Client fetches the content of `master_url` to scrape the `scheduler_url` from the HTML
1. The Client uses the `scheduler_url` to actually attach to the project

This is a problem when a User uses `boinccmd` to attach to a project on the commandline since he/she has to use the ` master_url` or there will be an annoying message in the Client logfile each time the Client connects to the project. The process using `boinccmd` instead of the Manager looks like this:

1. The User calls `boinccmd --project_attach URL account_key`
1. The `boinccmd` tool tells the Client to attach to `URL`
1. The Client treats `URL` as the `master_url` and fetches the content of `URL` to scrape the `scheduler_url` from the HTML
1. The Client uses the `scheduler_url` to actually attach to the project

The problem is that there are projects that have a different `master_url` and `website_url`. For example Einstein@home redirects all web requests to `https://einsteinathome.org` but for historical reasons the `master_url` in `https://einsteinathome.org/get_project_config.php` is still `http://einstein.phys.uwm.edu/` (because it is not possible to just change this without all users re-attaching to the project).

For consistency either the `boinccmd` tool or the Client itself should do a request to `URL/get_project_config.php` like the Manager does.","This issue came up in https://boinc.berkeley.edu/forum_thread.php?id=12722, opened by Gary Roberts. Gary is one of our most experienced helpdesk volunteers (a moderator at Einstein@Home since the year dot), but even he was stumped by account manager problems and error messages.

I've had a good prod around all three current account managers, and I'd suggest the following.

* Going forward, both boinccmd and BOINC Manager should validate project urls via get_project_config.php, including urls supplied by an account manager.
* Existing account manager administrators should verify periodically that they are properly issuing MASTER urls as specified in https://boinc.berkeley.edu/trac/wiki/AccountManagement. I found one that wasn't, in the first session of testing. The documentation needs strengthening to reinforce this point.
* It's very unclear on the face of BOINC Manager that an account manager is in use, and I can't find a url to link back to the management page anywhere in BOINC Manager. The activity of an account manager is only explicit in the Advanced View version of the Tools menu.

Given that we lack a mechanism for recalling old versions of BOINC, I'd suggest that both the first two points are necessary. I'll leave the third point in the hands of the interface designers and translators, but I'd suggest an additional 'Visit AM site' button on the Projects tab, between the 'Command' and 'Project' button frames.",30260406
765,[Website] Update the compiling documents,open,2018-08-08T18:09:31Z,2019-10-30T13:52:58Z,,MEMBER,"When I searched how to compile the BOINC, I found 3 different webpage about it, with few differences:
- [https://boinc.berkeley.edu/trac/wiki/BuildSystem](https://boinc.berkeley.edu/trac/wiki/BuildSystem)
- [https://boinc.berkeley.edu/trac/wiki/CompileClient](https://boinc.berkeley.edu/trac/wiki/CompileClient)
- [http://boinc.berkeley.edu/wiki/Compiling_the_core_client](http://boinc.berkeley.edu/wiki/Compiling_the_core_client)

Only one page should exist and these pages should be merged to don't confuse people.",The rewards section could be connected to a proper Blockchain and have value too ,30260406
766,[Feature] Add more task properties on the Tasks tab,open,2018-08-07T15:12:22Z,2019-10-30T13:52:48Z,,MEMBER,"I'd like to see more properties per task in the table on the Tasks tab.
Specifically:
- CPU time
- CPU time since checkpoint
- Virtual memory size
- Working set size

The most important to me is the ""CPU time since checkpoint"", because I run a lot of tasks and some of them make checkpoints fairly rarely, and I don't want to waste the computation with a restart, but checking them one bye one is tedious.

The columns could be added or removed with a pop-up menu by clicking on the table with the mouse right button.","@CharlieFenton Thanks the info, I didn't know that option. IMO the popup version would be much more user friendly and intuitive. I show an example:
![capture](https://user-images.githubusercontent.com/16503773/43820735-5dd27566-9ae7-11e8-8404-c5fac813ccd1.PNG)

@a80b8829 Because you don't see need of these functions, it doesn't mean the function is useless for others. 

On the subject of BOINC Tasks , IMO I shouldn't use 3rd party app to get a fairly simple functionality.  Plus, I'm on the side that BOINC Tasks should have been merged with BOINC a long time ago.",30260406
767,"VBoxWrapper performance is worse - Poll function in PR 1946, likely",open,2018-08-06T13:34:23Z,2019-10-30T13:52:32Z,,NONE,"Here is a screenshot of the negative performance impact I noticed, comparing 26200 vs 26197. It might be attributed to the poll function changes in PR 1946. Not sure, but I assume it is, based on timing.
[https://github.com/BOINC/boinc/pull/1946](https://github.com/BOINC/boinc/pull/1946)

To summarize the screenshot:
- BOINC was crunching 4 RNA Tasks for 5450 minutes CPU Time. All using COM, based on Logs. Using VirtualBox v5.1.38.
- vboxwrapper 26200 used 40.2 additional minutes CPU Time, for a running task, during that time.
- vboxwrapper 26197 used 1.2 additional minutes CPU Time, for a running task, during that time.
- So, performance of 26200 is 33 times worse.

Could we investigate if this can be improved or reverted?
Thanks,
Jacob Klein

![boinc vboxwrapper 26200 vs 26197](https://user-images.githubusercontent.com/13267799/43719339-6f29d210-995b-11e8-998d-719c45fb7551.png)","> Do you have steps for an end user to easily re-test it?

No, I asked you because I still have to start studying the situation from the scratch. Any suggestion is welcomed",30260406
768,Look Into Porting BOINC to KaiOS,open,2018-07-24T20:48:38Z,2019-10-30T13:52:18Z,,NONE,"KaiOS is looking to be a possible third contender in the mobile market. Google just invested 22 Million USD in them, see the article from [TechCrunch.](https://techcrunch.com/2018/06/27/google-kaios/) As stated in that article there are more than 40 million phones running KaiOS at the moment
>KaiOS tells us that there have been more than 40 million KaiOS phones shipped to-date

An important thing to know about KaiOS is that it is designed for low-end feature phones that might not make this worth any effort until we see large adoption. This is probably something to look into in the future and maybe keep in mind.


Regardless, I think it could be worth reaching out to them about BOINC now so that we good relations with them and hear their thoughts. 

Here is their official [website](https://www.kaiostech.com)",I think adding support here might be possible through #3086 since it looks like it runs apps only through the browser,30260406
769,Expected role of account managers,open,2018-07-19T15:27:55Z,2019-10-30T13:51:37Z,,CONTRIBUTOR,"From discussion of #2565, the question has been raised as to what the role of an account manager (AM) is and what the nature of interaction should be between users of AMs and BOINC projects. For the historical perspective, see the original design and purpose of [Account management systems](http://boinc.berkeley.edu/trac/wiki/AccountManagement) as documented in the BOINC wiki.

Streamlining the process of attaching and managing BOINC clients on multiple projects seems to be the established purpose both in documentation and in discussion. So the topic at hand now is the user experience: Should users of AMs be required to interact directly with BOINC projects, or should all necessary interaction take place directly with the AM and client software?","For most effective reach of non-technical users, my stance is that the AM website and client software should be the only points of interaction that are required for an AM user to participate. Non-technical users of an AM may not even understand that BOINC projects exist independently of the AM website that they used to join, so it could be quite confusing if any direct interaction is required that bypasses the AM. ",30260406
770,Show subscribed threads in forum preferences with option to unsubscribe,open,2018-07-11T10:48:01Z,2019-10-30T13:51:17Z,,CONTRIBUTOR,"Now when a user is subscribed to threads in the forums, they show at the bottom of the main forum index. To unsubscribe, the user has to go to each thread and click the unsubscribe button, which depending on the thread length can be quite cumbersome and uses some data.  

I request that subscriptions are shown on (a sub-page of) the user's forum preferences page, with check boxes in front, and a unsubscribe button at the bottom. Just so the user can check which threads he wants to unsubscribe from and click the unsubscribe button, without having to load potential large threads just to do so. ",,30260406
771,BOINC Manager whites out due to 01/01/1970 01:00 ,open,2018-06-27T16:12:17Z,2019-10-30T13:48:34Z,,CONTRIBUTOR,"An effect I found over the past days. Is perhaps easily tested by just setting all projects to NNT, then setting the http_debug flag in Event Log Options. Stay on the Projects tab. Next open the Event Log (CTRL+SHIFT+E). 

Now exit BOINC, wait 10 seconds, then restart it. 
If all is well, BOINC will start to contact all projects for updated Notices, you can see that in the Event Log.

Eventually BOINC may come across one or more projects which have some trouble with this contact, which will 'empty' the Projects tab. Either all text is gone, or only Work Done/Avg.work done/Resource share show and those all with values 0 (zero). 

The Event Log meanwhilst will also start to flicker. 
It'll have entries like this 01/01/1970 01:00:00 |  |  and be stuck on that for a while. It also happens that when waiting long enough on this screen, that it ends with BOINC Manager telling me the password was incorrect. 

I can reproduce this with 7.10.2 all the time, at every start of BOINC. At every 'whiteout' the 01/01/1970 is showing, like so:

```
27/06/2018 18:06:11 |  | [http] HTTP_OP::init_get(): http://boinc.bakerlab.org/rosetta/notices.php?userid=64&auth=64_043f925d622deb5bb6040b70c180a916
27/06/2018 18:06:11 |  | [http] HTTP_OP::libcurl_exec(): ca-bundle set
27/06/2018 18:06:12 |  | [http] [ID#0] Info:    Trying 128.95.160.156...
27/06/2018 18:06:13 |  | [http] [ID#0] Info:  Connected to boinc.bakerlab.org (128.95.160.156) port 80 (#15)
27/06/2018 18:06:13 |  | [http] [ID#0] Sent header to server: GET /rosetta/notices.php?userid=64&auth=64_043f925d622deb5bb6040b70c180a916 HTTP/1.1
27/06/2018 18:06:13 |  | [http] [ID#0] Sent header to server: Host: boinc.bakerlab.org
27/06/2018 18:06:13 |  | [http] [ID#0] Sent header to server: User-Agent: BOINC client (windows_x86_64 7.10.2)
27/06/2018 18:06:13 |  | [http] [ID#0] Sent header to server: Accept: */*
27/06/2018 18:06:13 |  | [http] [ID#0] Sent header to server: Accept-Encoding: deflate, gzip
27/06/2018 18:06:13 |  | [http] [ID#0] Sent header to server: Content-Type: application/x-www-form-urlencoded
27/06/2018 18:06:13 |  | [http] [ID#0] Sent header to server: Accept-Language: en_GB
27/06/2018 18:06:13 |  | [http] [ID#0] Sent header to server:
27/06/2018 18:06:13 |  | 
01/01/1970 01:00:00 |  | 
```
![empty bm on scheduler contact](https://user-images.githubusercontent.com/10869587/41986001-272b2580-7a35-11e8-81bf-5e581e48e08f.jpg)

","From Jord's description, my first guess would be that there is either a bad pointer somewhere or a buffer overflow that is, among other things, setting the variable containing the date of the event to zero (since zero represents 1/1//1970) and overwriting other locations in memory.",30260406
772,"Clarify what is meant by ""runtime""",open,2018-06-27T05:39:14Z,2019-10-30T13:48:24Z,,CONTRIBUTOR,"The BOINC scheduling logic, both client and server,
revolves around estimates of the runtime of jobs, both unstarted and in progress.
But there are different notions of runtime:
1) the time the job would take running continuously
2) the time it would take given fractional CPU and/or GPU availability
3) 2) + taking CPU throttling into account.

The logic, for the most part, doesn't distinguish between these,
and often uses one of them when it should be using another.

So the task is:
- assign names to the different forms of runtime
- go through all the scheduling code (client and server),
  figure out what form we should be using everywhere, and fix accordingly.",,30260406
773,update GeoIP files,open,2018-06-25T18:47:30Z,2019-10-30T13:47:59Z,,CONTRIBUTOR,"https://dev.maxmind.com/geoip/geoip2/geolite2/

The files in BOINC show Serbia and Montenegro as 1 country; no longer the case.","Maxmind has deprecated GeoIP Legacy. The code is [still available](https://github.com/maxmind/geoip-api-php/blob/master/src/geoip.inc), but the databases [are not](https://support.maxmind.com/geolite-legacy-discontinuation-notice/), so we should upgrade to [GeoLite2](https://dev.maxmind.com/geoip/geoip2/geolite2/).",30260406
774,Web RPC am_set_info should not email user when email changes.,open,2018-06-21T14:13:48Z,2019-10-30T12:53:20Z,,CONTRIBUTOR,"Web RPC am_set_info.php was altered in PR #2500 to add the following [lines of code](https://github.com/BOINC/boinc/blob/master/html/user/am_set_info.php#L220-L222).

```
if ($send_changed_email_to_user) {
    send_changed_email($user);
}
```

The function `send_changed_email()` sends the two emails to the current and previous email addresses for the user.

I think this code should be removed/reverted, because this ‘layer’ of the code is the wrong place to send emails to a user. A good example is if someone is using an Account Manager, is attached to ten projects, and then uses the Account Manager interface to change his/her email. (Assuming all 10 projects are using this code;) The user will receive ten emails from all the projects. The AM should be the layer which handles the email exchange.

Likewise, in the Drupal-BOINC implementation, Drupal is handing the emails to the user, with a different link for recovering one’s email address. If a someone uses this RPC to change his/her email, then s/he will receive an email with the incorrect link for the Drupal run Website.

I would ask @Uplinger and @TheAspens to chime in because they developed this functionality.","Hey Shawn,

Thanks for writing this up.  Let me make sure I'm understanding your thoughts from an account manager perspective.

When someone changes their email in the account manager, they only receive a single email to their old email address from EACH project that their email address has been changed by an account manager.  You do not wish to have the new email address getting an email from each project clarifying that they changed the email address.  

I personally think the new email still needs to be sent, this is because not every project will be updated to the email notification scheme.  This means that if they change their email today and try again tomorrow, only projects running the old code will update the email address again.  The projects running the new code will respond with an error to the account managers.  This could be confusing to the members if they didn't receive the email letting them know they can only change their email after 7 days. 

I think the extra information is valuable from an individual project perspective, as it clarifies things to the end user as well as helps protect them from malicious attempts.  The subjects will be uniform which would help them see that a mass change has happened, but that is just a minor delete of extra messages if they warranted the change. 

Note:  We can change the subject to let them know that instead of the subject being:

PROJECT."" email address change""

Something like this can be set when coming from the account manager (unfortunately I dont' think account managers pass their name to the server when connecting):

""An Account Manager has changed your email address on "".PROJECT

Thanks,
-Keith",30260406
775,client: clean up files,open,2018-06-18T21:28:06Z,2019-10-30T11:22:29Z,,CONTRIBUTOR,"A user points out that the client disk usage increases without bound (see below).
We need to:
- prune job_log
- delete old app_version files.  The simplest way to do this might be to
   delete files that haven't been used in a month or two.

---------------------
""Does the project job_log have a maximum size or does it get pruned automatically by the client? I have a client that pretty much runs continuously for years. There is a job_log that has grown to almost 50MB in size. This is a 32 thread machine that runs a lot of jobs per day. I suspect it will continue to grow unless there is a specific action taken to delete it. Additionally, there are executable files for applications that have accumulated in the ../project/<project_url> directory that are years old and the application no longer runs. Shouldn't the project clean those up from the server side when the application ends? I know I can clean the ""mess"" up by doing a periodic project reset but that seems like overkill when the BOINC eco-system should clean up after itself. For clients that are not ""actively"" monitored (I thought the nature of BOINC was to install, start, run in background, forget), this amounts to a disk storage leak. Over time this one client has amassed almost 500MB of unused files with the majority being orphaned executables from long extinct apps. These also include executables from beta tests that don't get cleaned up. This also causes the client_state.xml and client_state_prev.xml to grow. Current sizes for these files are almost 3MB.""",Those don't mention job logs,30260406
776,Apple to deprecate OpenGL and OpenCL in MacOS 10.14,open,2018-06-06T07:48:44Z,2019-10-30T11:21:42Z,,CONTRIBUTOR,"[Apple deprecates OpenGL and OpenCL support](https://www.phoronix.com/scan.php?page=news_item&px=Apple-Deprecates-OpenGL-OpenCL)

> With today's announcement of macOS 10.14 Mojave, Apple quietly confirmed they are deprecating OpenGL and OpenCL within macOS.

> Apple deprecating OpenCL and OpenGL hardly comes as a surprise given in the past few years they have been pushing their Metal API for graphics and compute across macOS and iOS. Additionally, their OpenGL stack hasn't been updated well in years and has lagged behind the OpenGL 4.x upstream advancements out of The Khronos Group.

> Apps built using OpenGL and OpenCL will continue to run in macOS 10.14, but these legacy technologies are deprecated in macOS 10.14. Games and graphics-intensive apps that use OpenGL should now adopt Metal. Similarly, apps that use OpenCL for computational tasks should now adopt Metal and Metal Performance Shaders.

This means that BOINC needs code enabling it to detect GPUs using Metal. ","MoltenVK (Vulkan on Metal from Khronos) seems relevant: https://github.com/KhronosGroup/MoltenVK

There's also a paid OpenGL on Metal version from the same set of developers who wrote MoltenVK called MoltenGL.",30260406
777,[Android] Boinc doesn't utilize all the cores,open,2018-06-05T13:59:48Z,2019-10-30T11:21:19Z,,MEMBER,"I have a LeEco Le Max 2 phone with LineageOS 15.1 (Android 8.1). It has Snapdragon 820 SOC with 4 cores. The power management is set to performance.

I set the boinc to use all the 4 cores, and if I check the CPU utilization with the Kernel Auditor, it shows that only 1 CPU is used. I've tried multiple scenarios with different options like use 1, 2 or 3 cores, but I got the same results.

![screenshot_kernel_adiutor_20180605-155029](https://user-images.githubusercontent.com/16503773/40980664-4a8c61c8-68d9-11e8-993b-4a6ab73d5bc7.png)
","For me, heat is not a concern on my device. I just want it to run consistently as advertised :) Unfortunately, until this issue is resolved, it's broken for me.",30260406
778,vboxwrapper should use ncpus = physical cores,open,2018-05-30T12:59:16Z,2019-10-30T11:21:10Z,,MEMBER,"From https://www.virtualbox.org/manual/ch03.html:

> You should not, however, configure virtual machines to use more CPU cores than you have available physically (real cores, no hyperthreads).

However currently vboxwrapper uses the number logical cores. There is some discussion of this in https://github.com/BOINC/boinc/issues/1501 and https://github.com/marius311/boinc-server-docker/issues/33.

I had thought this was some minor sub-optimality, but thanks to @zonca getting a totally reproducible scenario working, I recently made the connection that this could be causing a significant number of the failures we see at C@H which I had previously not understood. 

I don't have a solution, see https://github.com/BOINC/boinc/issues/1501 for discussion of the issues involved in getting the number of physical cores (anyone know a good API?). I'm opening this though so we can brainstorm / track progress on this specific issue.","Work is being done on branch [phys_cores](https://github.com/BOINC/boinc/tree/phys_cores).

- [ ] Client detection
  - [x] Linux
  - [ ] Windows
  - [ ] Mac
- [x] Receive info on server, update DB, and display in host info pages
- [ ] Add planclass (?) to allow limiting to physical cores
- [ ] Use this info in scheduling",30260406
779,Boinccmd: Linux failure on headless machine,open,2018-05-26T08:40:17Z,2019-10-30T11:20:59Z,,CONTRIBUTOR,"Late report on v7.10.2 at [SETI@Home](https://setiathome.berkeley.edu/forum_thread.php?id=82973&postid=1937231#1937231).

> I have some issue with boinc 7.10.2 on linux when running on a computer without a monitor
> connected to (I use costamagnagianfranco version from https://launchpad.net/~costamagnagianfranco/+archive/ubuntu/boinc)
> 
> I get 'Authorization failure: -102' when launching commands with boinccmd
> No problem when a monitor is connected.
> 
> Version 7.2.42 have no problem.
> Version 7.8.2 have the same problem with or without a monitor.

Any Linux gurus able to reproduce, or better yet fix?","Forget it, I am just experiencing the same problem on a machine with a running graphic environment... No way to use `boinccmd` without getting a `Authorization failure: -155` error",30260406
780,web-only signup option,open,2018-05-21T06:52:31Z,2019-10-30T11:20:40Z,,CONTRIBUTOR,"All projects to specify (in config.xml, get_project_config.php)
that they allow signup only on the web.
When you select that project in the Attach Wizard,
it asks for your credentials,
with a link to the web site to create an account if needed,
with an explanation that you can only sign up on the web site.",,30260406
781,GPU access from VMs,open,2018-05-14T17:03:25Z,2019-10-30T11:20:17Z,,CONTRIBUTOR,"GPU access from VMs would be huge.

I've seen lots of papers in the last few years about virtualizing GPUs.
I don't think this has found its way in to VirtualBox yet,
but maybe there's some other VM system that supports it.","Why close this issue?  Although there may be no solution at this point, we need to keep looking for one.  Issues don't have to be implementation items.",30260406
782,"client message: Can't detect VirtualBox because this is a 32-bit version of BOINC; to fix, please install a 64-bit version",open,2018-05-08T19:11:39Z,2019-11-04T21:51:25Z,,CONTRIBUTOR,"```
04-May-2018 13:07:16 [---] Can't detect VirtualBox because this is a 32-bit version of BOINC; to fix, please install a 64-bit version.
```
That's on a system that doesn't have VirtualBox installed.
It was even the maiden time I installed BOINC on it. For testing the client/OS start-up message, I installed the 32bit client on 64bit Windows 7 SP1. 
It gives it on the version WITHOUT VirtualBox. Not only as a message in the start-up messages, but also as a notice.

Referencing comment given by @TheAspens in https://github.com/BOINC/boinc/pull/2388#issuecomment-376938278

> I think I didn't state my opinion clearly enough above. I think that only one of the two following is the correct thing to do:
>
> *  No message should be displayed
> *  A simple generic message is displayed. I recommend the message
>```
> VirtualBox not detected. If you want BOINC to be able to use VirtualBox, then please see https://useful.wiki.page.on.boinc.site
> ```
> You cannot give users a very prominent message stating that something is wrong and that they must take action when in fact that may not be true.

","You're not getting a message about needing to install VirtualBox. You're getting a message about needing to install BOINC - the 64-bit version - so that BOINC can assess the situation properly.

So, we have two problems.

1) It's a clumsy and badly-written message, and a notice as well - that might be overkill. But it comes from bc2ea6ede9a61e6d549d7dce36521cd8617b534f of Sep 23, 2014 - not from 7.8, but from [7.4.24](http://boinc.berkeley.edu/dev/forum_thread.php?id=8378&postid=57198)

2) We are likely to be advising more people to be running 32-bit BOINC on 64-bit Windows because of the as-yet unresolved #2470 - and that isn't going to be resolved until v7.12, as both @JuhaSointusalo and @TheAspens have said in 2470.

So, is a bad message which has survived through two previous versions a show-stopper this time? In my judgement. no, but I hope other people will express an opinion too.",30260406
783,EU-GDPR - Right to Erasure - Account Managers,open,2018-05-08T17:02:13Z,2019-10-30T11:19:50Z,,MEMBER,"In issue #2447 there is a discussion about the interaction between account managers and projects with regards to the right to erasure.  The discussion starts here:  https://github.com/BOINC/boinc/issues/2447#issuecomment-383603716

I am creating this ticket to seperate the discussion and planning for this part of the issue sperate from the rest of the implementation.

There are two proposals provide in the discussion (with comments on both so please review the other ticket before before commenting here).

From Kevin Reed:

> 
> - Account managers should monitor the new user_deleted.xml that will be exported in the stats - this will list users deleted on a project in the past 60 days. See https://boinc.berkeley.edu/trac/wiki/RightToErasure#DataExports and https://boinc.berkeley.edu/trac/wiki/RightToErasure#FinalRemoval
> - Account managers need to be able to invoke the delete operation on a project. A new am_delete_account RPC is needed that would align with the other RPC's defined here: https://boinc.berkeley.edu/trac/wiki/WebRpc
> - In order to secure the delete web rpc, projects will have a list of trusted Web RPC users (i.e. https://boincstats.com/, https://scienceunited.org/, https://www.gridrepublic.org/)
> - TheWeb RPC's (need to decide which) will be updated to include a section that contains:
> ```
> <signature>
>        <signer>https://boincstats.com/</signer>
>        <hash>1234afd123asdf1234asdf134asdf.....</hash>
> </signature>
> 
> ```
> 
> - The Web RPC users will provide a public key at a standard location like /public.key (i.e https://boincstats.com/public.key
> - The Web RPC user will use their private key to sign the message and send the signature with the request
> - The RPC will verify that the signer is a trusted signer and will then obtain the public key (either from local cache or from the remote server - but if the signature fails, it needs to refetch the public key to allow the signer to update their key) and then verify that the signature matches the content.
> - Only after that processing is complete and successful will it perform the actions of the RPC.

From Eric Korpela:

> I don't think that the account manager needs to be able to run the project
> delete without intervention.  The account manager should redirect the user
> to the project delete function.  Then the delete would propagate back to
> the project manager in the next stats export.

In addition to resolving the design and approach, someone needs to take on the implementation.","I'm trying to determine the state of GDPR in BOINC: how about this issue? I presume there hasn't been any progress, right?

Thanks",30260406
784,Email notifications of nearly empty work queue,open,2018-05-04T18:58:51Z,2019-10-30T11:19:32Z,,CONTRIBUTOR,"Some time ago I was browsing my stats at WUProp, and noticed that some of my hosts switched from project X to backup project. When I checked this more closely, it turned out that project X is shut down for maintenance since 3 days.
Please add option to BOINC Client to automatically sent email when waiting work queue will drop to some configured value. Make sure it will work properly if it will be set to 0 and backup project is available - in such case email should be sent when BOINC requests work from backup project.
BOINC has two work queues (for CPU and GPU), so they should be monitored independently, and 2 emails sent if necessary.
There also should be 2nd option, how often these emails should be repeated (e.g. every 24h). This limit should be applied separately to CPU and GPU queues.
","If you want something a bit easier, you could also get a slightly less featureful version of this with minimal effort if you are on Linux or some other UNIX platform by installing [Netdata](http://my-netdata.io/) and configuring it to monitor BOINC and deliver notifications via email.  It ships configuration by default to send notifications on non-zero numbers of upload or compute errors, an empty work queue, and a non-empty queue with no active tasks.",30260406
785,Clarify credit options,open,2018-05-01T20:16:31Z,2019-10-30T11:19:14Z,,CONTRIBUTOR,"A proposal designed to eliminate the need for project-specific customizations related to credit:
https://boinc.berkeley.edu/trac/wiki/CreditOptions","Thanks Eric - I thought I was loosing the plot here so I'd cludged a minimum value for delta into my simulation code and it ran smoothly on my first (and most simple) data set.

Next job is to throw one of the more vicious data sets at it to see how well it behaves, and after that some of the data culled from my own results....  (Each data set has at least two thousand points, so it can take a while to set up and run)",30260406
786,[Client] Network interruption causes other side-effects and failures,open,2018-04-30T17:01:12Z,2019-10-30T11:18:57Z,,CONTRIBUTOR,"On the morning of Monday 30 April 2018, network problems on the UCB campus [*] are causing project and website communication delays for both BOINC and SETI@Home. (UCB campus incident reference INC0660327).

The client (v7.10.1 local build, VS2013 under Windows 7/64) is backing off and retrying appropriately when network comms to SETI@Home are interrupted - at the moment, primarily uploads are affected - but other local problems are apparent at the same time.

1) local network connections between the local client and a remote Manager are also slow and intermittent

2) Client elapsed time recording is inaccurate. This task:

30/04/2018 17:10:26 | SETI@home | Starting task 29au17aa.3277.4517446.7.34.63.vlar_1
30/04/2018 17:36:36 | SETI@home | Computation for task 29au17aa.3277.4517446.7.34.63.vlar_1 finished

ran without interruption for a wall time of 26:10, but only recorded an elapsed time of 13:09 according to the remote Manager, and is showing

Run time	13 min 9 sec
CPU time	24 min 35 sec

on the task report https://setiathome.berkeley.edu/result.php?result_name=29au17aa.3277.4517446.7.34.63.vlar_1

(task is single CPU/single GPU with expected near-equal run and CPU times: local networking is via hardwired LAN cable, gigabit switch, and shared fibre router. Local problems as described are extremely rare, which is why I'm associating them with the UCB incident)

[*] Now reported as dropped packets due to instabilities introduced by a newly-commissioned firewall",,30260406
787,Waiting for memory issue,open,2018-04-28T13:05:29Z,2019-10-30T11:18:47Z,,MEMBER,"My grandma has an old laptop with only 2GB of RAM.
Here are the Memory options which I use:
- When computer is in use, use at most 25%.
- When computer is not in use, use at most 90%.

It runs the Rosetta@home and the World Community Grid for backup project.
Rosetta@home WUs usually really demanding in terms of RAM, and stops working when the computer is used, because it exceeds the RAM limit, which is fine.
The problem is, the laptop doesn't crunching, while it is used, and I think the workflow should be modified.

If the the main project's WU has exceeded the RAM limit, then the BOINC should download WUs from the the backup project.","I know that currently the BOINC doesn't know beforehand that how much memory needed for a wu. I see two solutions:

1. The BOINC should download a wu from the backup project and run it. If the backup wu is still too big, than return it and don't download another wu.
2. The project server should send the wu's estimated memory demand, based on statistics or predefined value.",30260406
788,Flatpak/Snap support,open,2018-04-27T10:22:33Z,2020-03-24T13:31:29Z,,NONE,"Dear developers,

could you provide Flatpak and/or Snap support for Linux distributions? Both are new packaging formats that work distribution independently and offer sandbox isolation and other other neat features.


Regards

Jakob","BTW I personally would prefer flatpak. In contrast to snap, you can also self-host it, so you stay in control, and it is widely supported. (snap is not so nice to setup in many distros and snap's security depends on AppArmor, which is not always available in many distros)
Also – in contrast to snaps – flatpaks do not only claim to be distro-independent, but [actually are](https://kamikazow.wordpress.com/2018/06/08/adoption-of-flatpak-vs-snap-2018-edition/).",30260406
789,Possible security vulnerability in Windows installer,open,2018-04-25T03:39:19Z,2019-10-30T11:18:13Z,,CONTRIBUTOR,"(from a user email)

boinc_7.8.3_windows_intelx86.exe, available from
<https://boinc.berkeley.edu/dl/boinc_7.8.3_windows_intelx86.exe>
via <http://boinc.berkeley.edu/download.php>, is vulnerable to DLL
hijacking: it loads multiple Windows system DLLs from its
""application directory"", typically the user's ""Downloads"" directory
""%USERPROFILE%\Downloads"", instead from Windows ""system directory""
%SystemRoot%\System32\

On a fully patched Windows 7 SP1, these are at least the following
DLLs:
    uxtheme.dll or dwmapi.dll, version.dll, ntmarta.dll, msi.dll

The vulnerability and attack are well-known and well-documented:
see <https://cwe.mitre.org/data/definitions/426.html>
and <https://cwe.mitre.org/data/definitions/427.html>
plus <https://capec.mitre.org/data/definitions/471.html>

Also see
<https://insights.sei.cmu.edu/cert/2008/09/carpet-bombing-and-directory-poisoning.html>
and
<http://blog.acrossecurity.com/2012/02/downloads-folder-binary-planting.html>


The embedded ""application manifest"" specifies ""requireAdministrator"",
so boinc_7.8.3_windows_intelx86.exe and all the DLLs it loads are
run with administrative privileges: an attacker who is able to place
one of the above named DLLs in the user's ""Downloads"" directory, for
example per ""drive-by download"", gains elevation of privilege.


Proof of concept/demonstration:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1- Follow the instructions from
   <https://skanthak.homepage.t-online.de/minesweeper.html>
   and build forwarder DLLs for the DLLs named above.

2- Place these DLLs in your ""Downloads"" directory.

3- Download
   <https://boinc.berkeley.edu/dl/boinc_7.8.3_windows_intelx86.exe>
   and save it in your ""Downloads"" directory.

4- Start boinc_7.8.3_windows_intelx86.exe: notice the message boxes
   displayed from the DLLs!


FIX:
~~~~

DUMP the vulnerable executable installer, provide an .MSI instead!

See <https://skanthak.homepage.t-online.de/!execute.html>
",,30260406
790,tools/upgrade: doesn't stop on DB update failures,open,2018-04-24T19:50:34Z,2019-10-30T11:17:53Z,,CONTRIBUTOR,"I accidentally told `upgrade` to use non-existent DB user account when upgrading a project. `upgrade` logged errors but didn't stop and updated `project_root/db_revision` with the new DB version even though it had not updated the DB schema. A second run then incorrectly said there were no DB updates to do.

```
juha@mint ~/dev/boinc/src $ tools/upgrade --web_only ~/projects/tah2
Overwrite files in /home/juha/projects/tah2 [y/N]  y
Upgrading files...
Upgrading files... done
Updating translations
update_translations finished
Checking for DB updates
DB version: 27020
need update update_3_8_2018
need update update_4_5_2018
need update update_4_6_2018
need update update_4_18_2018
Do you want to apply these updates? (y/n) y

Warning: you are upgrading only web or server code,
but these updates may affect the other code as well.
We recommend that you run 'upgrade' again to upgrade both parts of the code.

Database username (default: owner of mysqld process): wrong_account
Database password (if any): password
performing update update_3_8_2018
Doing query:
alter table user
        modify column login_token char(32) not null default '',
        modify column login_token_time double not null default 0

PHP Warning:  mysqli_query(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 57

Warning: mysqli_query(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 57
PHP Warning:  mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106

Warning: mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106
Failed:

PHP Warning:  mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106

Warning: mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106
performing update update_4_5_2018
Doing query:
create table token (
        token                   varchar(255)    not null,
        userid                  integer         not null,
        type                    char            not null,
        create_time             integer         not null,
        expire_time             integer,
        primary key (token),
        index token_userid (userid)
        ) engine=InnoDB

PHP Warning:  mysqli_query(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 57

Warning: mysqli_query(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 57
PHP Warning:  mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106

Warning: mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106
Failed:

PHP Warning:  mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106

Warning: mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106
performing update update_4_6_2018
Doing query:
alter table team
        modify column total_credit double not null default 0.0,
        modify column expavg_credit double not null default 0.0,
        modify column seti_id integer not null default 0

PHP Warning:  mysqli_query(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 57

Warning: mysqli_query(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 57
PHP Warning:  mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106

Warning: mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106
Failed:

PHP Warning:  mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106

Warning: mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106
performing update update_4_18_2018
Doing query:
alter table token
        modify column create_time integer not null

PHP Warning:  mysqli_query(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 57

Warning: mysqli_query(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 57
PHP Warning:  mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106

Warning: mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106
Failed:

PHP Warning:  mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106

Warning: mysqli_error(): Couldn't fetch mysqli in /home/juha/projects/tah2/html/inc/db.inc on line 106
All done.
```

And second run
```
juha@mint ~/dev/boinc/src $ tools/upgrade --web_only ~/projects/tah2
Overwrite files in /home/juha/projects/tah2 [y/N]  y
Upgrading files...
Upgrading files... done
Updating translations
update_translations finished
Checking for DB updates
DB version: 27024
No updates needed
```
","I spent 10 minutes looking through the PHP code and made the following observations.

Your issue is an interesting edge case. If you had entered a incorrect username or password, or a DB user who did not exist, then the PHP script would [`die` with an error](https://github.com/BOINC/boinc/blob/master/html/inc/util_ops.inc#L117-L119). If there is a mysql error, the PHP script would [break out of the loop, and not write to the db_revision file](https://github.com/BOINC/boinc/blob/master/html/ops/upgrade_db.php#L67-L77)

But in your edge case, PHP correctly creates a mysqli connection, but doesn't correctly understand that it cannot use it. As a result when it tries to [return an error it cannot](https://github.com/BOINC/boinc/blob/master/html/inc/db.inc#L104-L106) (`Couldn't fetch mysqli in (...)/db.inc on line 106`). Thus the script continues to the loop, and eventually writes to the db_revision file.

I don't know enough about PHP and mysqli to be able to come up with a fix, but perhaps someone else can.",30260406
791,Port to Python3,open,2018-04-19T15:29:12Z,2019-10-30T11:17:47Z,,CONTRIBUTOR,"Python(2) is only supported until 2020, maybe a good time to make a plan to port to python3.  I'm not sure if it's important to support both 2 and 3 at once.  I noticed because boinc-client depends on python2 in Ubuntu 18.04 (and python2 is no longer installed by default)

My initial check indicates python is used quite a bit.
py/*
samples/vm_wrapper/
sched/
test/
tools/

Any obvious places to start porting?","With a bit of luck, my https://github.com/BOINC/boinc/pull/3259 also closes (most of?) this.",30260406
792,Contacting BOINC server will crash client in libeay32.dll,open,2018-04-15T23:34:59Z,2020-01-24T11:31:37Z,,CONTRIBUTOR,"Reported in two separate threads at the BOINC forums. 
In both cases the crash happens when using an Intel Pentium CPU N4200, on Windows 10, with BOINC 7.8.3, but also with earlier versions. 
Juha checked and thinks this may be a problem with this specific CPU and the version of OpenSSL we use in present BOINC and that it may need updating. 

Workaround is to set <no_info_fetch> in cc_config.xml, or use it as command line attribute, or to install the 32bit client. 

Error message:
>Faulting application name: boinc.exe, version: 7.8.3.0, time stamp: 0x59d3f202
>Faulting module name: LIBEAY32.dll, version: 1.0.2.7, time stamp: 0x56d5fc8e
>Exception code: 0xc0000005
>Fault offset: 0x00000000000e6543
>Faulting process id: 0x824
>Faulting application start time: 0x01d3d32c4b8e41e1
>Faulting application path: C:\Program Files\BOINC\boinc.exe
>Faulting module path: C:\Program Files\BOINC\LIBEAY32.dll
>Report Id: 00636b25-2b84-41af-9d92-9d0b0c5e47ed

[Thread 1](https://boinc.berkeley.edu/dev/forum_thread.php?id=12383)
[Thread 2](https://boinc.berkeley.edu/dev/forum_thread.php?id=11870)","Confirmed v7.16.4 x64 installs and runs correctly on the Celeron.

14 watts running 4x SSE cores, no iGPU (Einstein) - for the person who was asking yesterday.",30260406
793,update BOINC Manager icon,open,2018-04-13T20:01:46Z,2019-10-30T11:17:30Z,,NONE,"The older ""aero""/glossy icon is still used for the BOINC Manager icon. I really like the newer ""flat"" icon style. Here's a cropped-by-hand version of the logo to try to match the proportions of the old one (OK to use this if deemed ""good enough"", but I think working with the SVG will yield something slightly more accurate and maintainable):
![boinc_flat_cropped](https://user-images.githubusercontent.com/7941193/38755151-4956f8c8-3f2a-11e8-8343-3d1204dc57d1.png)


Here's a few mockups of what that would look like on recent macOS:
![boinc_tinyselect](https://user-images.githubusercontent.com/7941193/38755395-2e239e0c-3f2b-11e8-9fc7-8daeed3e3ce2.png)
![boinc_tiny](https://user-images.githubusercontent.com/7941193/38755396-2e368206-3f2b-11e8-8663-8b3d78145980.png)

![boinc_darkdock](https://user-images.githubusercontent.com/7941193/38755063-fb3e12ca-3f29-11e8-922d-0e2bb4ab6088.png)
![boinc_darkswitcher](https://user-images.githubusercontent.com/7941193/38755064-fb5259ec-3f29-11e8-81ac-d5a6bd497d65.png)
![boinc_lightdock](https://user-images.githubusercontent.com/7941193/38755065-fb6343c4-3f29-11e8-871e-0e70b084d403.png)
![boinc_lightswitcher](https://user-images.githubusercontent.com/7941193/38755066-fb738054-3f29-11e8-8a13-9d1f4a7151a6.png)
","> I now see where the newer icon is already in clientgui/res/templates/icons/…

Actually, those files are artwork for the BOINC icon on MS Windows, though some of them could be used to create a _BOINCMgr.icns_ file for the Mac, (preferably with the addition of  512X512 and 1024X1024 renderings.) There are instructions for converting a set of PNG files to a single .icns file [here](http://applehelpwriter.com/2012/12/16/make-your-own-icns-icons-for-free/) and [here](https://stackoverflow.com/questions/12306223/how-to-manually-create-icns-files-using-iconutil), though you probably already know how to do this. You may need to have _Xcode_ installed to have the _iconutil_ utility;  I'm not sure.

By the way, I have been the Mac programmer for BOINC for many years, but have now retired. I've continued volunteering on a limited basis to keep BOINC working on the Mac, but am phasing out my  efforts. Would either of you like to get more directly involved in writing BOINC code for the Mac? As an open source project, BOINC has a number of people writing / maintaining code who are familiar with MS Windows, Linux and Android, but I seem to be the only person on the project familiar with Macintosh programming.

The areas where most of the Mac-specific code is needed are the BOINC Manager, the BOINC Mac installer and uninstaller and the screensaver. Over the years, BOINC has evolved into a somewhat complex project with quite a bit of code, so there is a significant learning curve to get familiar with all its details, but I am happy to answer questions and provide guidance to a new set of Mac BOINC programmers. BOINC sure could use the help.",30260406
794,BOINC on OnePlus 5 Android 8 stops running by itself after 10 minutes,open,2018-04-13T13:18:58Z,2020-02-26T16:14:31Z,,CONTRIBUTOR,"As reported in the [BOINC forums](https://boinc.berkeley.edu/dev/forum_thread.php?id=12396):

>After getting the latest release of Android 8 I've noticed that BOINC no longer continues to run.
>After about 10 minutes it just stops running, and when I unplug the power supply and re-plug it in it will not start automatically.
>If I reset the auto-start it starts no matter how many times I unplug or plug it in, until it just stops running - then it will never auto start!!!
>
>Has anyone else run into this issue?
>I'm running the COLLATZ SIEVE only - maybe it's that that's causing the issue? I will try another project to see what happens.

BOINC 7.4.53","I'm talking about **this** issue here. The latest BOINC version I built worked just fine as long as you disable the battery optimization (see https://github.com/BOINC/boinc/issues/2467#issuecomment-422390244). The version on the PlayStore doesn't as far as I know. Thus, IMHO, this issue shouldn't be closed until a new PlayStore release happened. That said, it can and will eventually be resolved. Android doesn't preclude BOINC from working as expected (at least that's true for Oreo, haven't yet tested Pie).

Point is, the community thankfully works on fixing Android-related issues and even improves the Android version. Yet, all of those efforts are useless unless there's a new PlayStore release. It's a shame that there's apparently still no progress on that front.",30260406
795,Android P will require apps to use TLS to encrypt all connections,open,2018-04-13T13:15:03Z,2019-10-30T11:15:48Z,,CONTRIBUTOR,"As per https://android-developers.googleblog.com/2018/03/previewing-android-p.html and https://security.googleblog.com/2018/04/protecting-users-with-tls-by-default-in.html

_If your app uses TLS for all connections then you have nothing to do. If not, update your app to use TLS to encrypt all connections._
_Android considers all networks potentially hostile and so encrypting traffic should be used at all times, for all connections. Mobile devices are especially at risk because they regularly connect to many different networks, such as the Wi-Fi at a coffee shop. All traffic should be encrypted, regardless of content, as any unencrypted connections can be used to inject content, increase attack surface for potentially vulnerable client code, or track the user._

So this will mean that present BOINC for Android won't work under Android 9. ","I fail to see any reasonable excuse for not using TLS in [2018](https://letsencrypt.org). Thus it shouldn't stop us from what we are required to do anyway, e.g. by GDPR.",30260406
796,Screensaver not working correctly on Windows.,open,2018-04-11T20:54:55Z,2019-10-30T11:15:24Z,,CONTRIBUTOR,"From a user email.  This is consistent with my own experience on Win7.  There seem to be many problems with the BOINC screensaver on Win.
------------
I’ve been using BOINC since its release & their has always been issues with the screensaver Not Loading &/or it just loads to a Blue Screen/Blank Screen? That the only way to escape it is to ctrl-alt-delete.

These things have been happening forever now threw different operating systems, BOINC versions, different computers/components Etc. Etc. Etc., so its nothing but BOINC fault.",Is it still an issue?,30260406
797,[Manager] Make F1 (on focused button) consistent across Simple View and Advanced View,open,2018-04-09T15:22:28Z,2019-10-30T11:15:12Z,,NONE,"On Windows, F1 is supposed to open Web Help.

However, it was discovered that, when on Advanced View, with focus on a button, F1 shows an old-style yellow tooltip first, and Web Help requires pressing F1 a second time. Simple View does not exhibit this behavior, and instead will open Web Help right away when pressing F1 with a button in focus.

My presumption is that this might be intentional, to show a tooltip for keyboard-only users, though I'm not 100% sure.

Mousing over the buttons, for both views, shows a modern tooltip (for me on Windows 10).

I propose that we make the behavior consistent for both views. Furthermore, it is my opinion that we shouldn't show the old-style yellow tooltip, and instead, F1 should immediately launch Web Help when in Advanced View focused on a button.

Thanks,
Jacob","When F1 is pressed in various areas of the Advanced View, it will launch a URL that has a Control ID.
That eventually results in the following:
https://boinc.berkeley.edu/wiki/Advanced_view

My expectation, especially in your 2 listed cases, would be to do the same.

But I understand if the ""reasoning"" for the current behavior (Pressing F1 while button is in focus generates a yellow tooltip) is because of accessibility reasons or screen readers.

However, that being said, I'd like some consistency - ie: Even Simple View buttons should function the same way. Look at Synchronize, Notices, Suspend, Help -- they don't exhibit the same behavior, but should, in my opinion.

That's my take. Thanks for asking/listening.

________________________________
From: Jord van der Elst <notifications@github.com>
Sent: Monday, April 16, 2018 3:28 AM
To: BOINC/boinc
Cc: JacobWKlein; Author
Subject: Re: [BOINC/boinc] [Manager] Make F1 (on focused button) consistent across Simple View and Advanced View (#2458)


But I still wonder what Jacob expects of the behaviour of pressing F1 and what should open here.
Especially in the cases of:

  *   Projects tab: tab to selecting the Remove button under the Commands section.
  *   Projects tab: tab to selecting the Account button under the Project web pages section.

Are you expecting in both cases that the Advanced View wiki page opens?
Even if for the second case there is nothing on those pages?

@charlie<https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fcharlie&data=02%7C01%7C%7C4d7ed3d2da124f7cd07508d5a36b9a85%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636594604901807175&sdata=n6b8UERmKXy3WlCKul9Q4hsWrmTDXWCsJfWMAYok1Tg%3D&reserved=0>, what does the second case do on the Mac with the Help button?

I have downloaded NV Access<https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvaccess.org%2Fdownload%2F&data=02%7C01%7C%7C4d7ed3d2da124f7cd07508d5a36b9a85%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636594604901807175&sdata=Q4eNz2nCI6drfXhRQhZu%2BASWo%2B23oYNWnXBnOUzjE0M%3D&reserved=0>, an open source screen reader for Windows which will try to read out loud anything you type, point at or F1 onto. I can tell you that it will NOT read out loud the mouse-over balloons, but it WILL read out loud the text in the yellow box.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FBOINC%2Fboinc%2Fissues%2F2458%23issuecomment-381505036&data=02%7C01%7C%7C4d7ed3d2da124f7cd07508d5a36b9a85%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636594604901807175&sdata=IBYccg4Zq2bnPP0kALevMuf%2BkURBZ658V6H3ssEcG9A%3D&reserved=0>, or mute the thread<https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAMpzVyRuHhvrkh6EVr05Vnhl1-X9POfmks5tpEgIgaJpZM4TMu-r&data=02%7C01%7C%7C4d7ed3d2da124f7cd07508d5a36b9a85%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636594604901807175&sdata=mE2d3bxqWp5CV%2BlP5N9sUz%2Bc%2BJG5UHy7JvkNzQGotYY%3D&reserved=0>.
",30260406
798,No automatic benchmarks upon CPU switch,open,2018-04-07T09:38:21Z,2019-10-30T11:14:59Z,,CONTRIBUTOR,"Yesterday I've been busy switching from CPU on my main system, one that also runs BOINC every now and then. I switched from an i5-2500K to an i5-3470 on the same motherboard, with the same Windows, with a whole lot of trouble (all neatly registered on the BOINC forums :)). 

So just prior to switching the CPU, I was at:
```
03-Apr-2018 21:57:29 [---] Host name: i5-2500K
03-Apr-2018 21:57:29 [---] Processor: 4 GenuineIntel  Intel(R) Core(TM) i5-2500K CPU @ 3.30GHz [Family 6 Model 42 Stepping 7]
03-Apr-2018 21:57:29 [---] Processor features: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss htt tm pni ssse3 cx16 sse4_1 sse4_2 popcnt aes syscall lm avx vmx tm2 pbe
```
After switching the CPU, I was at:
```
07-Apr-2018 01:08:37 [---] Host name: i5-2500K
07-Apr-2018 01:08:37 [---] Processor: 4 GenuineIntel  Intel(R) Core(TM) i5-3470 CPU @ 3.20GHz [Family 6 Model 58 Stepping 9]
07-Apr-2018 01:08:37 [---] Processor features: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss htt tm pni ssse3 cx16 sse4_1 sse4_2 popcnt aes f16c rdrandsyscall nx lm avx vmx smx tm2 pbe fsgsbase smep
```
You can see that the system name is still the same, I had not thought of changing it yet. 
However, I noticed this morning that BOINC had not done a new benchmark for the new CPU. We do new benchmarks when BOINC changes, so why don't we do them when the actual hardware it benchmarks changes? 

","However, I would agree that this is something that could be changed and that you have correctly set its priority as ""trivial"" as it is much lower priority than other bugs out there.",30260406
799,Remove code related to browser cookies from manager,open,2018-04-06T20:01:10Z,2018-10-24T13:32:40Z,,CONTRIBUTOR,"At one point we experimented with auto-attach features using web browser cookies.  We abandoned this because the way that browsers store cookies changes all the time and is undocumented.

Remove this code (e.g. browser.cpp) from the manager, and remove related stuff from GUI RPCs and the client.","@davidpanderson Did Matt Blumberg ever respond to your email of June 29 where you wrote:
> Is anyone (WCG?  Gridrepublic? Charity Engine?) using this feature

If not, I suggest asking him again before removing this code. 

> it doesn't work anymore, at least on Chrome.

I don't have Chrome, so I can't test it here. You might want to ak Matt Blumberg about that, too.
",30260406
800,EU-GDPR - Privacy by design and default,open,2018-04-05T12:45:23Z,2019-10-30T11:10:18Z,,CONTRIBUTOR,"Let's use this issue for privacy-by-design-related topics that need to be addressed. I start with this one:

* On account creation [the user's computers are publicly visible](https://github.com/BOINC/boinc/blob/master/html/inc/user_util.inc#L76) by default. Needs to be changed/inverted (`show_hosts`).
* FYI, we don't consider `send_email` to be problematic is this doesn't cause personal data to be shared.","@davidpanderson opened a ticket about host name and IP address. #2836

@drshawnkwang:
> I will ask at the next contributors call whether there are other privacy options people care about.

Hmm, maybe better ask volunteers instead.",30260406
801,client download error Permanent HTTP error,open,2018-04-02T13:04:36Z,2018-10-24T13:30:40Z,,NONE,"hello

when the client try to download application and inputs files. i got 

Permanent HTTP error

so i check the server and i find there is no application under the PROJECT/download/ . i also check the log. i find client send a request   http://xxxx.com/download/APP_NAME. just copy the application from PRJECT/apps/APP_NAME/VERSION/x86_app_darwin/APP_NAME to PROJECT/download/ APP_NAME and it's working fine.


sorry...im not good at english .....

i want to know is this a bug??
if client always access http://xxxx.com/download/APP_NAME. how can i deal with the architecture? like linux windows mac ....

can somebody help me.

thanks.","is the server. i find this:

<file_info>
    <-name>test</name>
    <-url>http://server.xxxxs.com/xxxx/download/test</url>
    <-executable/>
    <file_signature>
ccc8948fb1b199cad84767c1a9be1b1fa791b8c1387a92ca70de0d7363918144
608c9296570adb1af266812ac5588211f42b66b5e60024b5099716e0e39ef356
5c01dbcc26ab5510e67cff3de90668425f5ecf112f6c17808d3d0e57d48a349d
d2f429cc9e17a3c127a9bd9f38fd72ce3645af2dedd8ce106e708ffb70f9e937
.
    </file_signature>
    <nbytes>404360</nbytes>
</file_info>
<app_version>
    <app_name>test</app_name>
    <version_num>100</version_num>
    <api_version>7.11.0</api_version>
<file_ref>
    <file_name>test</file_name>
    <main_program/>
</file_ref>
</app_version>


at this line 

  <url>http://server.xxxxs.com/xxxx/download/test</url>


it is some mistake? 


",30260406
802,[Linux] Unix Domain Socket permissions prevent regular user from running boinccmd,open,2018-03-31T21:56:50Z,2018-10-24T13:29:10Z,,NONE,"The setup is the following:

1. `cd /var/lib/boinc-client`
2. `sudo -u boinc /usr/bin/boinc --no_priority_change --gui_rpc_unix_domain`
3. `boinccmd --unix_domain --get_host_info`

This returns `can't connect to Unix domain socket`. A quick investigation reveals that:
```
$ ls -lh boinc_socket
srwxr-xr-x 1 boinc boinc 0 Mar 31 17:51 boinc_socket
```
So nobody but the owner (user `boinc`) can write to the socket. This seems to be relevant, since running `sudo -u boinc boinccmd --unix_domain --get_host_info` succeeds. I am in the `boinc` group:
```
$ groups
chiraag tty cdrom floppy sudo audio dip video plugdev netdev proc bluetooth lpadmin scanner boinc
```
I suspect the default umask should be changed when the socket is created.

[update]
Running `sudo chmod 775 boinc_socket` indeed fixes the problem and `boinccmd` works as expected.",Bump. Do you need more information? Is there anything else I can provide to help with debugging?,30260406
803,Specify plan class for single job (boinc-server),open,2018-03-29T15:32:53Z,2018-10-25T02:35:53Z,,NONE,"Hi everyone, i need to run single job, which uses a gpu. But the command does not have a parameter 'plan-class' so I can pinpoint who will get this task. Is it possible to somehow send a job specifying that it uses gpu like cuda?","The single-job mechanism currently works only for CPU apps.
Extending it to handle GPU apps would be a fair amount of work (maybe a day or two).

I'm not sure this is worth doing; the single-job mechanism is clunky,
and the Docker-based system now fills the role.",30260406
804,scheduler: Infinite loop possible,open,2018-03-29T09:05:21Z,2019-10-30T10:23:40Z,,CONTRIBUTOR,"This [loop](https://github.com/BOINC/boinc/blob/90f8a1d4b4280ead1d7046efdcaa28eb603ca763/sched/handle_request.cpp#L261) is dangerous as it can run infinitely (until CGI timeout) while potentially maxing out the database server.

We witnessed at least two cases where the ""linked"" new host ID also had a `userid=0` and linked back to the original host via its `rpc_seqno`. IOW, both hosts were ""zombies"" and cross-referenced each other, resulting in an infinite loop. While this appears to be caused by an unrelated bug in Drupal's host merging code, the possibility itself should be avoided.

A potential solution could be to check that `host.lookup_id(host.rpc_seqno).rpc_seqno` doesn't match the host ID of the original request `g_request->hostid`. Not sure how to handle that case, though.



",,30260406
805,[Manager] Show no name when connecting to local client,open,2018-03-27T11:46:24Z,2019-10-30T10:22:48Z,,MEMBER,"Sometimes when Manager starts client it shows no name of client in status bar.
If I remember correctly it shows 'Connected to local (client.version) before.
Correct me if I'm wrong.

![image](https://user-images.githubusercontent.com/3234209/37965393-9a897e60-31cd-11e8-8ae3-73faa320a485.png)

Seen on version 7.8.2 and 7.8.3","It's hard to debug this issue. After reconnecting to client Manager shows correct info but after some period of time starts to show empty host name.
Will try to debug it later",30260406
806,[Feature] Option to disable autostart only the Manager,open,2018-03-15T19:21:56Z,2019-10-30T10:22:25Z,,MEMBER,"Hi,

I'd appreciate an option to disable/enable autostarting the BOINC Manager. I have several machines which run BOINC, but I managed them remotely therefore the Manager uses unnecessary refources.

Moreover the current solution is misleading, because if I disable the ""Run manager at login?"", I assume the Client will work just fine in the background at startup, but it won't start.
This situation is only on Windows, the Linux version works just fine without the Manager, which is the default option.

<img width=""270"" alt=""manager autostart"" src=""https://user-images.githubusercontent.com/16503773/37486318-70edcc02-288e-11e8-88c1-2ba8e05a0fd1.PNG"">
","I could see this being part of the Windows installer, but maybe as a [command line property](http://boinc.berkeley.edu/wiki/Creating_custom_installers#Installer_properties) only, so as not to clutter the graphical installer interface?",30260406
807,EU-GDPR - User consent,open,2018-03-15T16:11:47Z,2019-03-11T10:52:23Z,,CONTRIBUTOR,"On May 25th, 2018 a new [General Data Protection Regulation (GDPR)](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation) is going to be effective in the EU. While most of its legal requirements only affect the projects themselves, at least one part affects BOINC directly - the sign-up process. As of that day users who want to sign-up to a project need to give their informed explicit consent to how the project processes their data, before any personally identifiable information about that new user gets processed/stored by the project. This opt-in statement of consent then needs to tracked by the project as proof.

Given the current way the BOINC Manager/Client project sign-up workflow is done, I think that (at least) all EU projects are basically forced to disable client-based login and only allow a (customized!) web sign-up process until the missing features got added. In order for BOINC being GDPR compliant I think the following needs to be changed (first rough sketch):

* During sign-up, request the project specific declaration of consent and checkbox label (see below) from the project (additional RPC) and display it to the user (multi-language support required, i.e. add language code to request)
* That dialog should support HTML such that links to, say, the external privacy policy could be included in the text
* Add an opt-in checkbox showing the label requested above (again, with link support)
* Allow sign-up only when checkbox is selected and transmit that as an additional boolean attribute in the account creation RPC
* Add an additional `date` type attribute `privacy_consent_dt` to the `user` table
* Store the datetime of the user's consent in the new attribute

The standard BOINC web interface also needs to support equivalently the above workflow. It might even be the case that we're required to get this consent from existing users as well, so the web interface should make the consent from equally available outside of the sign-up process (for a logged-in user in this case). We're already working on incorporating the necessary changes to the Drupal web interface.

Note to all account managers: you should be affected in the same way as the BOINC client!
","This is infeasible because then the team founder is unable to manage the team on that project.

When someone creates a BWT, the name and email address they use become public; it spells this out on the BWT site.  Deleting them later doesn't accomplish anything.",30260406
808,BOINC 7.9.2 releases security information,open,2018-03-15T11:28:01Z,2018-10-24T13:20:55Z,,NONE,"With BOINC now providing the Glibc version information it is now available for users and hackers to exploit. Below is a Raspberry Pi that was updated and now shows this information on the project server (Einstein in this case).

Linux Raspbian
Raspbian GNU/Linux 9.3 (stretch) [4.9.59-v7+|libc 2.24 (Debian GLIBC 2.24-11+deb9u1).

That shows the version, but also says its had deb9u1 applied which is a security fix. The lack of that information could be used in a targeted attack that the security fix was patching.

We should only return the version number portion for security reasons (ie the 2.24).","> While including the patch level of libc is not massively bad it's not good either.

I think the point here is: the patch level is just part of how Debian does its versioning and we're just retrieving version information. That said, the version information is very useful during debugging, like any other similar host details. If that version detail might contain sensitive information then we need to consider removing it from the **public** host pages, like the IP or domain name. The project admins and the user would still be able to see it.

Isn't it that simple or what am I missing?",30260406
809,Upload server: reject incoming files if backing storage can't accept them,open,2018-03-15T11:07:30Z,2018-11-21T09:37:30Z,,CONTRIBUTOR,"In most cases, BOINC upload servers which fail, fill up, are undergoing maintenance, or are unreachable, reject uploads immediately - either during http header negotiation, or via an http timeout.

But in some cases, the upload server accepts the entire file - which can be 100 MB or more - and only then responds with a failure message. This means that the entire file has to be retried at a later stage, and for individuals with metered connections, or for users in countries where metering is customary, costs may become higher than they budgeted for.

My understanding is that the problem arises when the BOINC upload server acts as a gateway to a separate background data storage facility. If the storage facility is rejecting transfers ('Disk quota exceeded' in a current case), the BOINC server should reject offered uploads at the header negotiation stage to avoid wasted bandwidth and expense.

Source reference: http://boinc.berkeley.edu/dev/forum_thread.php?id=12349","Adding another data point: Einstein@Home is (21 November 2018) accepting the full content of upload files to the upload server, but then responding with

504 Gateway Time-out

and backing off the upload for further retries at a later date.",30260406
810,"""Sync with account manager"" does an unnecessary get_project_config RPC",open,2018-03-10T05:03:30Z,2019-10-30T10:20:36Z,,CONTRIBUTOR,"... because it seems to use the same Attach Wizard logic as attaching to an AM.

TODO: document the flow of the Attach Wizard.  I wouldn't know how to start fixing this bug because I don't understand the structure.","Sync is without 'h', so changed title. ",30260406
811,Shipping BOINC for Android in F-Droid,open,2018-03-03T13:05:48Z,2019-10-30T10:20:11Z,,CONTRIBUTOR,"Hi there!

I've been using BOINC for Android for the last couple of years on my no-Google phones from various sources and was somewhat confused that an Open-Source project like BOINc wasn't available from F-Droid. (I'm also [not the first person asking about this](https://boinc.berkeley.edu/dev/forum_thread.php?id=12005), btw.)

So recently I took a stab at compiling BOINC for Android using the F-Droid Server system and was pleasantly surprised at how simple it was – even when compiling the NDK binaries! You can view my request for inclusion into the F-Droid main repo [here](https://gitlab.com/fdroid/fdroiddata/merge_requests/2861).

This raises two questions of course:

  * What is your (upstream's) policy on this kind of “3rd-party distribution” (it's not a fork: [no source code is modified](https://gitlab.com/fdroid/fdroiddata/blob/32bc4ff4b921083737df6fbbd5e60c9839d4f89a/metadata/edu.berkeley.boinc.txt)!)? Would you be willing in participating in [reproducible builds](https://f-droid.org/en/docs/Reproducible_Builds/) (allowing the app to carry your official upstream signature)?
  * Would you be willing to add the translations of your Google Play app description to your repository? I'm currently scraping these off Google's website using a script and add them to F-Droid's repository data, but this approach tends to result in stale description data and bloats the repository, so we prefer to keep it upstream if at all possible.

Thanks for awesome work on this!","@AenBleidd, @SETIguy, @alexander255, &c:  Is this sufficient for y'all re: F-Droid's binary-downloading policy (or any other questions of the sort)?  Now's an _awesome_ time to ask!",30260406
812,"No hardware resources selected in project preferences, means no back-off time either",open,2018-03-03T09:59:02Z,2019-10-30T10:20:00Z,,CONTRIBUTOR,"Prior to using a test version of BOINC with auto-attach, I set Seti@Home to not use the CPU or any of the GPUs via Seti's project preferences. In the account_setiathome.berkeley.edu.xml file this shows as:
```
<no_cpu>1</no_cpu>
<no_ati>1</no_ati>
<no_cuda>1</no_cuda>
<no_intel_gpu>1</no_intel_gpu>
```
Having no resources selected does not set a back-off time in BOINC. 
```
    <rsc_backoff_time>
        <name>CPU</name>
        <value>0.000000</value>
    </rsc_backoff_time>
    <rsc_backoff_interval>
        <name>CPU</name>
        <value>0.000000</value>
    </rsc_backoff_interval>
    <no_rsc_apps>CPU</no_rsc_apps>
    <no_rsc_pref>CPU</no_rsc_pref>
    <rsc_backoff_time>
        <name>ATI</name>
        <value>0.000000</value>
    </rsc_backoff_time>
    <rsc_backoff_interval>
        <name>ATI</name>
        <value>0.000000</value>
    </rsc_backoff_interval>
    <no_rsc_pref>ATI</no_rsc_pref>
    <scheduler_url>http://setiboinc.ssl.berkeley.edu/sah_cgi/cgi</scheduler_url>
```
So when the user now sets a hardware resource at the project, BOINC will not automatically get it. They will have to go by the computer(s) and manually update it (/them). 

Didn't we have a 24 hour back-off time for cases like this? ",,30260406
813,"No hardware resource selected in project preferences, on initialization BOINC gets work anyway",open,2018-03-02T09:09:51Z,2019-10-30T10:19:46Z,,CONTRIBUTOR,"Prior to using a test version of BOINC with auto-attach, I set Seti@Home to not use the CPU or any of the GPUs via Seti's project preferences. In the account_setiathome.berkeley.edu.xml file this shows as:
```
<no_cpu>1</no_cpu>
<no_ati>1</no_ati>
<no_cuda>1</no_cuda>
<no_intel_gpu>1</no_intel_gpu>
```

When BOINC next started on its first initialization run, it asked for 1 task for my AMD GPU. 
Later, after lots of uninstalling & reinstalling the client multiple times, and switching between 32bit and 64bit clients, one of the initialized work requests was for 518400.00 seconds for the AMD GPU. 

All this time my project preferences were still the same as above, do not use any hardware resource. 
What the event log also shows, after every scheduler contact is _Your current settings do not allow tasks from this project.  To fix this, you can change Project Preferences on the project's web site._ It showed this after the scheduler contact that gave it the 518400 seconds worth of work. And since that initialization BOINC will hold true to the preferences set, it won't ask for work because no resources checked. 

Just never on the first request. Code added through https://github.com/BOINC/boinc/commit/71c7e7a74b82e49b2da73869442e13bf06cb73c3 says:
 - client/scheduler/web: add per-project preferences for whether to accept CPU, NVIDIA and ATI jobs.
    These prefs are shown only where relevant: e.g., only for processor types for which the project has app versions, and if it has versions for only one type, no pref is shown.

    These prefs affect both client and scheduler. The client won't ask for work for a device blocked by prefs,
    and the scheduler won't send it.

    This replaces earlier optional project-specific prefs for ""no CPU jobs"" and ""no GPU jobs"". (However, these prefs continue to be honored on the server side).

Can we make BOINC read and use the project preferences if they are already available in the account_*.xml file, even when it's in init mode? And can the scheduler be made more intelligent that when the user has set his preferences up like this, that no work is sent, even when work is asked? ",,30260406
814,"Windows + VirtualBox installer, when errors encountered, or some part didn't install, show that to the user",open,2018-03-02T08:17:24Z,2019-10-30T10:19:25Z,,CONTRIBUTOR,"Referencing #2389.
When installing VirtualBox from the combined installer with BOINC, and any errors are encountered through which VirtualBox isn't installed, the user isn't told about this. 

I think we need a screen at the end of the installer, prior to the screen or perhaps even on the screen where BOINC Manager can be started, what the outcome was of the installers, if all parts have been installed correctly. 

Is this an option in the presently used installer technology? ",,30260406
815,"Can't detect VirtualBox because this is a 32-bit version of BOINC; to fix, please install a 64-bit version.",open,2018-03-02T08:08:05Z,2019-11-04T21:59:55Z,,CONTRIBUTOR,"_Can't detect VirtualBox because this is a 32-bit version of BOINC; to fix, please install a 64-bit version._ is an error message in BOINC that one encounters when one has tried to install a 32bit version of BOINC + 32bit Vbox on a 64bit OS. 

As I found out through installing boinc_7.9.2_windows_intelx86_vbox.exe on my Windows 7 x64, a 32bit version of VBox will not even install on a 64bit OS. It is silently dropped. No warning messages, no errors during the installation. For the untrained eye the installation was done perfectly. Things were unpacked, the VBox installer closed without errors, the BOINC installer started without problems. 

Perusing through Windows Event Viewer, Windows Logs->Application, I found mostly informational messages and one error: 

```
Information: Beginning a Windows Installer transaction: C:\Users\Ageless\AppData\Local\Temp\{2A0AADE7-93C3-4409-8E38-FE0CA571F879}\{99360D9A-6C66-4E9E-B1DA-CFBECEBD9678}\VirtualBox-5.2.6-r120293-MultiArch_x86.msi. Client Process Id: 5980.
Error: Product: Oracle VM VirtualBox 5.2.6 -- This application only runs on 32-bit Windows systems. Please install the 64-bit version of Oracle VM VirtualBox 5.2.6!
Information: Product: Oracle VM VirtualBox 5.2.6 -- Installation failed.
Information: Windows Installer installed the product. Product Name: Oracle VM VirtualBox 5.2.6. Product Version: 5.2.6. Product Language: 1033. Manufacturer: Oracle Corporation. Installation success or error status: 1603.
Information: Ending a Windows Installer transaction: C:\Users\Ageless\AppData\Local\Temp\{2A0AADE7-93C3-4409-8E38-FE0CA571F879}\{99360D9A-6C66-4E9E-B1DA-CFBECEBD9678}
\VirtualBox-5.2.6-r120293-MultiArch_x86.msi. Client Process Id: 5980.
```
Starting BOINC after this failed VBox installation yields the error _Can't detect VirtualBox because this is a 32-bit version of BOINC; to fix, please install a 64-bit version._. 

The message implies that VirtualBox is installed, but just cannot be found. It hasn't been installed. 
It also implies that by installing a 64bit version of BOINC, the 32bit version of VBox will work... which it still won't. So the message needs to include that VirtualBox didn't install correctly and that for everything to work, a 64bit version of VBox is needed on a 64bit OS. 

Whether BOINC is 32bit or 64bit here, I don't think matters. 32bit BOINC can run 64bit applications on a 64bit system/OS, so why wouldn't it be able to work with a 64bit VBox? (I am not testing that scenario out.)
Calling this critical and not a blocker, because in essence, BOINC runs fine. It just won't work with VBox in this scenario. 
I'll be putting the error logging of the installer in a separate issue. ",,30260406
816,Make About BOINC Manager copyright year increase automatically,open,2018-02-27T21:26:35Z,2019-10-30T10:18:29Z,,CONTRIBUTOR,"Since we collectively forget each year to increase the copyright on BOINC Manager's About window, how about having it increase automatically? Is that a possibility, have it read it from the computer's clock or something? That way we don't need to redo translations on it every year either.  

Super trivial issue, until it's January again and someone's about to release 7.14...",,30260406
817,"""make install"" target doesn't seem to respect configured program-suffix",open,2018-02-26T15:21:13Z,2019-10-30T10:17:14Z,,NONE,"Steps to reproduce:

- `autoregen && mkdir boinc-build`
- `../configure [maybe some other switches] --program-suffix=.foo`
- `make`
- `make install`

Expected result: Install target completes
Actual result:

> make  install-exec-hook
make[4]: Entering directory '/home/erikjwaxx/git/boinc/build/client'
rm -f /usr/local/bin/boinc
/bin/ln /usr/local/bin/boinc_client /usr/local/bin/boinc
/bin/ln: failed to access '/usr/local/bin/boinc_client': No such file or directory
Makefile:1904: recipe for target 'install-exec-hook' failed
make[4]: *** [install-exec-hook] Error 1
make[4]: Leaving directory '/home/erikjwaxx/git/boinc/build/client'
Makefile:1822: recipe for target 'install-exec-am' failed
make[3]: *** [install-exec-am] Error 2
make[3]: Leaving directory '/home/erikjwaxx/git/boinc/build/client'
Makefile:1768: recipe for target 'install-am' failed
make[2]: *** [install-am] Error 2
make[2]: Leaving directory '/home/erikjwaxx/git/boinc/build/client'
Makefile:646: recipe for target 'install-recursive' failed
make[1]: *** [install-recursive] Error 1
make[1]: Leaving directory '/home/erikjwaxx/git/boinc/build'
Makefile:945: recipe for target 'install' failed
make: *** [install] Error 2

Looks like it's trying to link the default program names instead of the suffixed ones?",,30260406
818,"Disallow the ""account key"" or ""authenticator"" to be used as a security credential",open,2018-02-21T16:16:16Z,2019-10-30T10:16:11Z,,MEMBER,"Quoting @nicolas17 in ticket #2353 

> The client's normal requests authenticate with the ""account key"" or ""authenticator"", which is stored directly in the database as clear text. If the database is compromised, the attacker gets the account key and can do RPC requests or [login to the project website](https://github.com/BOINC/boinc/blob/master/html/user/login_auth.php) with it. To make things worse, unlike the password, there is no way for a user to change the authentication key.

The BOINC website and [web rpc's](https://boinc.berkeley.edu/trac/wiki/WebRpc) should not rely on the ""account key"" or ""authenticator"" as a security credential.  Additional items this change would effect:
- [login to the project website](https://github.com/BOINC/boinc/blob/master/html/user/login_auth.php) with the account key should not be allowed.
- The session cookie and token should not be the authenticator (see the auth cookie in https://github.com/BOINC/boinc/blob/master/html/user/login_action.php)

We need a design and plan to change this.  It will need to take into consideration how the account managers work and interact with the BOINC project websites (although it will almost certainly require them to make changes).","It may not be shown anymore that you can log in with it on the get_passwd.php page, but when you put your authenticator key in instead of the email address, you still go to your page. ",30260406
819,Forums not secure due to mixed content,open,2018-02-21T13:33:44Z,2019-10-30T10:15:56Z,,CONTRIBUTOR,"A lot of the BOINC forum threads are not secure, due to images in the thread or in people's signatures and other links being loaded via http instead of https. Browsers will then show that this link isn't secure, even though the forums do use https. 

For example: https://boinc.berkeley.edu/dev/forum_thread.php?id=11763
This shows for my browser (Firefox):
![not-secure 3](https://user-images.githubusercontent.com/10869587/36482519-df5ca8d4-1713-11e8-9f6d-bd7bb19f098e.png)
![not-secure 2](https://user-images.githubusercontent.com/10869587/36482518-df5b6e6a-1713-11e8-9d02-944a51b16b32.png)



",,30260406
820,Android BOINC uses <suppress_net_info>? ,open,2018-02-20T21:35:11Z,2019-10-30T10:15:40Z,,CONTRIBUTOR,"It's more of a question really. This weekend I added a new Android device to Seti, and found in my account -> my computers that it was working fine, but wasn't showing its internal IP address. Then I checked my other Android devices, and none of those show the internal IP address, only the external IP address. 
My PC shows both the internal and external IP addresses. 

On my LAN I give all devices a static IP address, so for the project this internal IP address should show and should always be the same for that device. But in this case it's totally blank. So the only things I can come up with is that the Android BOINC client comes with a cc_config.xml with <suppress_net_info> default on, or that Android doesn't allow sending of its IP address to internet servers.  

I've checked the source code and can't find any evidence of that <suppress_net_info> being used.
I've asked Oliver Bock about it, he doesn't know. 
I've asked Eric Korpela, just in case, but he doesn't know either. 

Who does? :)

Apropos, seen this happen on both BOINC 7.4.43 and 7.4.53","Some background: the host's network details are looked up [here](https://github.com/BOINC/boinc/blob/master/client/hostinfo_network.cpp#L62). As you can see the domain name of an Android device is modified randomly in case its original hostname is ""localhost"". That in turn should pretty much cause the actual [IP lookup](https://github.com/BOINC/boinc/blob/master/lib/network.cpp#L171) to fail.

Note: I think the above code is buggy anyway as I don't understand why the [hostname randomization](https://github.com/BOINC/boinc/blob/master/client/hostinfo_network.cpp#L65) is done before the hostname gets [fetched](https://github.com/BOINC/boinc/blob/master/client/hostinfo_network.cpp#L84) further down below.

Apart from that, getting the local IP of an Android device might not be trivial to begin with since it's not clear which network interface (WLAN, cell, bluetooth, USB?) you should query. In fact you'd probably have to contact an external server first and use that's socket's local IP to be sure as that would use the currently active routing table to get the currently used interface.",30260406
821,Create a setting to automatically remove expired tasks.,open,2018-02-20T18:37:33Z,2019-10-30T10:08:59Z,,NONE,"I started up Boinc for the first time in over a month and was surprised to see that instead of removing my overdue tasks, it started computing them instead.  I believe at lease one of these tasks had not been started but I have no way to prove that.

The tasks are identified as overdue in the logs.

    20-Feb-2018 18:17:47 [Citizen Science Grid] Task exact_genome_1515759602_61_12650_1 is 17.16 days overdue; you may not get credit for it.  Consider aborting it.
    20-Feb-2018 18:17:47 [Citizen Science Grid] Task exact_genome_1515759602_58_12641_1 is 10.69 days overdue; you may not get credit for it.  Consider aborting it.
    20-Feb-2018 18:17:47 [Citizen Science Grid] Task exact_genome_1511526933_31_18545_3 is 9.17 days overdue; you may not get credit for it.  Consider aborting it.
    20-Feb-2018 18:17:47 [Citizen Science Grid] Task exact_genome_1515759602_60_12665_1 is 9.93 days overdue; you may not get credit for it.  Consider aborting it.
    20-Feb-2018 18:17:47 [Collatz Conjecture] Task collatz_sieve_3949741896312092098560_52776558133248_0 is 19.06 days overdue; you may not get credit for it.  Consider aborting it.
    20-Feb-2018 18:17:47 [Einstein@Home] Task LATeah0021F_504.0_173800_0.0_0 is 19.06 days overdue; you may not get credit for it.  Consider aborting it.
    20-Feb-2018 18:17:47 [Collatz Conjecture] Task collatz_sieve_3949741949088650231808_52776558133248_0 is 19.06 days overdue; you may not get credit for it.  Consider aborting it.

It would be good to have a preference which allowed these to automatically be removed.  I would have expected this to be the default behaviour however just the option would be good.  The ability to remove once expired by a set time would be even better.

Examples:

    <max_overdue_hours>1</max_overdue_hours> # -1 for disabled

or

    <remove_expired />",,30260406
822,[Linux] move lockfile to /run,open,2018-02-19T20:40:51Z,2019-10-30T10:04:12Z,,CONTRIBUTOR,"Concerning systemd unit file, what do you think about the comment of systemd creator, about the position of BOINC lock file?
https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/message/7YIZQ45SEHP3YXWMFDXWRCW6SZJGGR7D/

in particular concerning
`ExecStopPost=/bin/rm -f /var/lib/boinc/lockfile `
he said
> If this file is not supposed to survive a daemon restart it really should be placed in /run somewhere.","> If this file is not supposed to survive a daemon restart it really should be placed in /run somewhere.

BOINC has code for the Mac specifically to deal with this problem. It is in _clientguiI/BOINCGUIApp.cpp_ lines 368 - 380 but could be generalized for other platforms. It currently uses Mac-Specific code at lines 90 - 104 in _clientgui/mac/BOINCGUIApp.mm_.",30260406
823,Advanced view tabs don't fit text,open,2018-02-19T16:07:22Z,2019-10-30T10:01:31Z,,NONE,"In Advanced view tabs labels don't expand to fit their text. Example with Polish translation:

![zakladki](https://user-images.githubusercontent.com/14052154/36386148-163cd142-1595-11e8-8acd-684c73177748.png)

Problem appears in Manager for Windows (Windows 7), on Debian with Gnome it works properly:
![zakladki_linux](https://user-images.githubusercontent.com/14052154/36386413-f629b81a-1595-11e8-98e0-0565eb54f2bf.png)
","> All to do with #1554 and what Charlie Fenton commented on in #2363

This is in effect a duplicate of #1554",30260406
824,Command buttons too short on Linux,open,2018-02-19T06:04:56Z,2019-10-30T10:00:45Z,,NONE,"Command names are being cutoff and replaced with ""..."" in BOINC Manager 7.6.31, running on Linux Mint.

![boinc](https://user-images.githubusercontent.com/13253146/36364201-af5e6960-1510-11e8-8427-dd304781778a.png)

",Problem seems to appear in very large resolutions... Image attached was when monitor was running in 3840x2160.  When I switch to 1920x1080 everything looks fine.,30260406
825,Advanced view -> View menu has options indented,open,2018-02-17T14:41:24Z,2019-10-30T10:00:01Z,,CONTRIBUTOR,"In English, and therefore in all the translations, the Advanced view -> View menu has all the key combos after Notices one or more pixels indented. I've made a screen shot and added a straight line to it that shows it:
![still indented](https://user-images.githubusercontent.com/10869587/36342052-be2c37b0-13f8-11e8-8b0c-d5e23bf776cd.png)

It's not an optical illusion. ","As AenBleidd's code shows, the two chunks of text on each menu line are separated by a \t special character. I suspect that this means 'right justify the rest of the string':

![bm view menu](https://user-images.githubusercontent.com/14886436/36536844-4bfe37ac-17c6-11e8-8e90-369c62f8ed18.png)

The trouble is that some of the characters, notably P and S, are narrower in this font than N and D.

NNNNNNNNNN
PPPPPPPPPP
SSSSSSSSSS
DDDDDDDDDD

The usage seems correct, according to http://docs.wxwidgets.org/3.0/classwx_menu_item.html#a8b0517fb35e3eada66b51568aa87f261 (ugh, I'm glad I don't have to work with that documentation) - the right alignment seems to be associated with the tab character, without the option.",30260406
826,boincmgr killed by SIGSEGV after switching from simple view to advanced view,open,2018-02-14T21:14:34Z,2019-01-17T11:02:57Z,,CONTRIBUTOR,"Upstream clone of Fedora bugreport https://bugzilla.redhat.com/show_bug.cgi?id=1545407
Attached log in previous URL


Description of problem:
boincmgr is killed by a SIGSEGV when switching from simple view to advanced view.

Version-Release number of selected component (if applicable):
boinc-client-7.8.4-4.fc27.x86_64
boinc-manager-7.8.4-4.fc27.x86_64

How reproducible:
Very

Steps to Reproduce:
1.  From the command line, start boincmgr in advanced view.
2.  Switch to simple view
3.  Switch back to advanced view

Actual results:
boincmgr aborts on step 3.

Expected results:
boincmgr switches back to advanced view

Additional info:
After the abort, execute boinmgr again.  It starts in simple view, and this time the switch to advanced view works OK.  This abort is not captured by the normal Fedora abrt processing, and does not show up in the journal, but only in ~/.BOINC/stderrgui.txt","By reproducing the crash and reading Valgrind output and GDB output, I think that it can be a WebKit bug. I opened bugreport https://bugs.webkit.org/show_bug.cgi?id=193532",30260406
827,reference .desktop file,open,2018-02-05T21:10:21Z,2019-10-30T09:52:41Z,,CONTRIBUTOR,"Different Linux distros have different ideas of what should be in .desktop file (the file behind the entry in start menu) for Manager. All the different versions could be upstreamed so that there is one reference file.

The most important part of the file is using `Path` to set working directory to BOINC's data directory so that Manager can find `gui_rpc_auth.cfg`. It would be nice if translation of the file was integrated with the rest of the BOINC translation system.

[Arch Linux](https://git.archlinux.org/svntogit/community.git/tree/trunk/boinc.desktop?h=packages/boinc)
[Debian](https://anonscm.debian.org/cgit/pkg-boinc/boinc.git/tree/debian/boinc-manager.desktop)
[Fedora](https://src.fedoraproject.org/rpms/boinc-client/blob/master/f/boinc-manager.desktop)
[Mageia](http://sophie.zarb.org/rpms/a43a82667e3b928558f576b6007cfc5b/files/3)
[Suse](https://build.opensuse.org/package/view_file/network/boinc-client/boinc-gui.desktop?expand=1)",I don't understand your post.  Which moderators?  I have been asking everywhere for help solving this issue.  This bug was referenced by Ageless as the likeliest place to get some response.,30260406
828,BOINC client sends incorrect max cpu count in scheduler request.,open,2018-02-03T17:33:47Z,2018-10-24T13:00:04Z,,CONTRIBUTOR,"I have configured PrimeGrid's PPS Mega app to use 4 threads by adding following to its app_config.xml:

```xml
    <app_version>
        <app_name>llrMEGA</app_name>
        <cmdline>-t 4</cmdline>
        <avg_ncpus>4</avg_ncpus>
        <max_ncpus>4</max_ncpus>
    </app_version>

```

I also configured PrimeGrid as a backup project (resource usage set to 0). My goal was to eliminate tasks waiting for crunching on my machine, and download new one(s) as necessary after previous ones will end. However it turned out that client always downloads 4 new tasks instead of 1, so this is even worse than running with buffer set to 0. I have checked sched_request_www.primegrid.com.xml file and found this. Looks that client sends 1 for max cpu. I know that max_cpu tag in app_config.xml is ignored, but I thought that BOINC client will set its internal value to the same as avg_cpu one. Looks that this is not the case. I suspect that because of this server assumes that client would need 4 new tasks instead of 1.

```xml
<app_version>
    <app_name>llrMEGA</app_name>
    <version_num>800</version_num>
    <platform>x86_64-pc-linux-gnu</platform>
    <avg_ncpus>4.000000</avg_ncpus>
    <max_ncpus>1.000000</max_ncpus>
    <flops>4323964981.302576</flops>
    <api_version>7.7.0</api_version>
    <cmdline>-t 4</cmdline>
</app_version>

```","I agree, any dead stuff should be removed.

Edit: so if this field is not used, it looks that server to not use avg cpu count too and sends as many tasks as number of idle cpus. This also should be fixed.",30260406
829,Confirmation dialog on Defaults button (in Event log options...),open,2018-01-26T08:23:30Z,2019-10-30T09:51:53Z,,CONTRIBUTOR,"When clicking the Defaults button in the Event Log options... menu, there is now no confirmation dialog asking if I want to reset everything to default. We have confirmation dialogs on every other decision window, so there should be one here as well. ",">
> The Options->Select columns->Defaults has the warning, while it also has
> the Save button. So why do it like that there then?
>
Select columns -> Defaults implicitly clicks Save for you. You don't get a
chance to change your mind so there needs to be the confirmation.

> I do think that in all these scenarios the Defaults button should have the
> warning dialog, plus on clicking OK on the warning dialog should
> immediately set the defaults, no need to separately click the Save button.
>
If the Event Log -> Defaults is changed to implicitly click Save and close
the dialog then it makes sense to have confirmation there too. That would
unify user experience for the dialogs so that would work for me.
",30260406
830,Restrict web RPCs to AMs,open,2018-01-17T23:21:03Z,2019-10-30T09:51:34Z,,CONTRIBUTOR,"Projects export a set of RPCs for creating and modifying accounts.
These are intended for use by account managers.

But as it stands now they can also be used by spammer scripts,
and have been used to create thousands of fake accounts.
Unlike web registration, Recaptcha is not available.

So I propose allowing RPCs only from vetted account managers.
Initially we could do this based on info in the HTTP request header.
This can be faked, so if we see continued spamming we could add
a stronger mechanism.","Another way would be if the account managers were to create a private key and then publish their public key.  They could then sign the message and send the hash along with the message.  The projects then verify the signature.

The public keys could either be distributed by BOINC or there could be a list of ""official"" account managers and a script on the project server could run every X hours and fetch the latest keys from each account manager. 

http://php.net/manual/en/function.openssl-sign.php and http://php.net/manual/en/function.openssl-verify.php could be used for this.

This is a bit larger effort than originally seen, but it would have the advantage of being much harder to work around and it won't be dependent on BOINC infrastructure.",30260406
831,Restrict web RPCs to AMs,open,2018-01-17T23:21:03Z,2019-10-30T09:51:34Z,,CONTRIBUTOR,"Projects export a set of RPCs for creating and modifying accounts.
These are intended for use by account managers.

But as it stands now they can also be used by spammer scripts,
and have been used to create thousands of fake accounts.
Unlike web registration, Recaptcha is not available.

So I propose allowing RPCs only from vetted account managers.
Initially we could do this based on info in the HTTP request header.
This can be faked, so if we see continued spamming we could add
a stronger mechanism.","Another way would be if the account managers were to create a private key and then publish their public key.  They could then sign the message and send the hash along with the message.  The projects then verify the signature.

The public keys could either be distributed by BOINC or there could be a list of ""official"" account managers and a script on the project server could run every X hours and fetch the latest keys from each account manager. 

http://php.net/manual/en/function.openssl-sign.php and http://php.net/manual/en/function.openssl-verify.php could be used for this.

This is a bit larger effort than originally seen, but it would have the advantage of being much harder to work around and it won't be dependent on BOINC infrastructure.",30260406
832,Don't use GPU while watching movie,open,2018-01-14T02:21:34Z,2019-10-30T09:51:06Z,,CONTRIBUTOR,"While watching a movie (streaming or DVD) the computer looks idle and BOINC starts using the GPU, which glitches the movie display.

CE's user testing found this to be a common and major issue.

How to tell if we're watching a movie?  I'm not sure.  I don't see any relevant APIs in Windows.  Maybe if we monitor read I/O and it remains above a certain level.","@adamradocz It's not just GPU tasks though.  There are a lot of games out there that are extremely demanding on the CPU and don't tolerate latency very well.  Minecraft is a pretty good trivial example, it's essentially unplayable on systems that have other things pushing the CPU utilization to 100%, even if those things are at a low priority in the scheduler.

I'm not saying that we shouldn't have the ability to suspend GPU usage if things are running in full-screen, just that we should also have an equivalent setting for CPU apps (or, alternatively, an arrangement like the current 'computer in-use' option, one option that suspends all computation when something is running full screen, and another one (possibly enabled by default) that suspends only GPU computation).",30260406
833,Use hypens instead of underscores for command line options,open,2018-01-10T00:52:53Z,2019-10-30T09:50:55Z,,NONE,"Would it be possible for any BOINC program's command line options with multiple words to use hyphens (""dashes"") instead of underscores? Those seem to be the more popular and easier to type approach. E.g.:
```
boinccmd --get-project-status
```
instead of
```
boinccmd --get_project_status
```
And for compatibility, would it be possible to accept either way (i.e. documentation uses hyphens, but programs support both)?",,30260406
834,Backup\restore feature,open,2017-12-31T12:19:27Z,2019-10-30T11:57:20Z,,MEMBER,"Hi,

It would be nice a backup\restore feature which backup all the options, the connected projects and the account manager.","It would probably be nice to use on Android for getting access to those
pesky log files. :-)

-- Jord van der Elst.


On Wed, Oct 30, 2019 at 11:16 AM Vitalii Koshura <notifications@github.com>
wrote:

> Ok, I'll keep it open then
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/2281?email_source=notifications&email_token=ACS5WU64KA26LQNXENB5KY3QRFNJJA5CNFSM4EJ7LCJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECTTUVY#issuecomment-547830359>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACS5WU3KRD7H2VVQM6STN6TQRFNJJANCNFSM4EJ7LCJA>
> .
>
",30260406
835,New Feature Request: Use SSL for MYSQL connections,open,2017-12-24T12:27:36Z,2019-10-30T09:47:52Z,,NONE,"It would be useful if we could use SSL for connections from the various programs to the MSQL DB.  Using a CA in particular would be great for AWS RDS connections.

SSL connections can be handled by using mysql_ssl_set.  https://dev.mysql.com/doc/refman/5.7/en/mysql-ssl-set.html

I imagine this may have to be implemented in multiple areas to support the various C/Python/PHP that connect to the MySQL DB",,30260406
836,Option to completely disable checkpointing,open,2017-12-18T15:05:09Z,2019-10-30T09:41:42Z,,CONTRIBUTOR,"I find that for users of laptops (who just suspend everything) and for those who run BOINC on their 24/7 servers, the concept of checkpoints is a bit like from the past. In my mind, today this mostly disturbs by spinning up hard drives, especially (as it just happened for me) when checkpointing was done too frequently becaues of bad parameter settings.

You may argue that just by putting a large number as a checkpoint interval, this may be sufficient to disable it. To me, this would be a way for a quick internal implementation, but the user should be offered that ""disable checkpointing"" box to be very sure to avoid the overhead.","As I see this is an option in job.xml.
But from my POV it is rather better to have this option in cc_config.xml to
be able to disable checkpoints for all VMs of all projects.
",30260406
837,Windows VirtualBox Process Priority support,open,2017-12-18T09:06:38Z,2019-10-30T09:41:14Z,,CONTRIBUTOR,"Cross-referencing this issue submitted to the VirtualBox developers. 

https://www.virtualbox.org/ticket/13500",,30260406
838,Bugs: boinccmd --get_old_tasks,open,2017-12-06T12:14:57Z,2019-10-30T09:40:47Z,,CONTRIBUTOR,"1) online help at https://github.com/BOINC/boinc/blob/master/client/boinc_cmd.cpp#L80 shows ""show reported tasks from last 24 hours"": this was changed to 1 hour in 9024f8cc18fe06b4c9d8bededda80355e4aaceb6

2) final line of output from https://github.com/BOINC/boinc/blob/d15d0b95de6db61fc9eac3445ec280be823134bf/lib/gui_rpc_client_print.cpp#L413 is labelled ""reported time"": I would take this to be the time of reporting to the project server. Instead, it's the create_time of the OLD_RESULT record, which is identical to the ""completed time"" of the task.

From report by user naptimelabs at https://boinc.berkeley.edu/dev/forum_thread.php?id=12158",,30260406
839,Default minimal invasiveness for new BOINC installations,open,2017-12-05T12:10:59Z,2019-10-30T09:40:09Z,,CONTRIBUTOR,"Hello,

A new installation by default takes all the processors and all the cores that the machine has to offer.  The simple view does not allow to change that behaviour.

The effect is that the machine will be noisy or hot or (likely) both very soon. Invasiveness is not only about disturbing other computes. It is also e.g. about noise in a working environment when you had tried to concentrate off-screen or you want to talk to a customer.

Whenever I urge someone to contribute, the first thing we do is to set the number of cores to contribute to one, so the machine is rendered undifferent to before. Everything else is considered unacceptable. It is like a web browser taking 100% CPU time and users don't notice that either.

I should have suggested this years ago and deeply apologize for not having done so. Maybe this is less obvious when one is not ""out in the field"" so much. Please kindly rest assured that power users who want to contribute more will find those settings.

Summary: Please change default settings to ask only for a minimum. It will pay off.

Cheers,

Steffen","The settings at WCG only sort of work.  We still get reports of laptops cranking the fan high.  We have played around with the settings a lot and we have not been able to find a reasonable default between preventing fans from spinning up and still getting significant contribution.  What we really need here is to find a way to extend Peter Hanappe's work on low energy volunteer computing (http://peter.hanappe.com/#project-low-energy-boinc) and avoid triggering either the fan or the higher cpu energy states.  At one point there were some settings in Linux that he was looking at, but I don't know where that progressed.  I haven't looked into them in 4-5 years (on any platform) so I don't know if anything new is provided by the operating systems but this would be a interesting project for someone to dig into.",30260406
840,Exclusive applications feature on Android,open,2017-12-03T18:28:47Z,2020-01-10T10:25:44Z,,MEMBER,"Hi,

I'd like to see the Exclusive applications feature on Android, just like on PC. The ""Pause computation when screen is on"" is off for me, which is OK most of the times, but sometimes I don't want to run the BOINC, when I use a certain app, and it's a little bit annoying to stop it manually.

Thanks!",@AenBleidd You're right. This thread has a solution for API 19 and above: [https://stackoverflow.com/questions/28066231/how-to-gettopactivity-name-or-get-currently-running-application-package-name-i](https://stackoverflow.com/questions/28066231/how-to-gettopactivity-name-or-get-currently-running-application-package-name-i),30260406
841,Fails occaisonally with X error,open,2017-11-30T03:14:54Z,2019-10-30T09:38:59Z,,CONTRIBUTOR,"This may be related to issue #2256.  It errors out about once a week like this.

$ sudo systemctl status boinc-client.service 
● boinc-client.service - Berkeley Open Infrastructure Network Computing Client
   Loaded: loaded (/lib/systemd/system/boinc-client.service; enabled; vendor preset: enabled)
   Active: failed (Result: exit-code) since Tue 2017-11-28 19:39:04 EST; 1 day 2h ago
     Docs: man:boinc(1)
  Process: 6277 ExecStopPost=/bin/rm -f /var/lib/boinc-client/lockfile (code=exited, status=0/SUCCESS)
  Process: 3306 ExecStop=/opt/client/boinccmd --quit (code=exited, status=0/SUCCESS)
  Process: 3325 ExecStart=/opt/client/boinc (code=exited, status=1/FAILURE)
 Main PID: 3325 (code=exited, status=1/FAILURE)
      CPU: 3w 3d 14h 18min 56.265s

Nov 28 19:37:41 desktop boinc[3325]: No protocol specified
Nov 28 19:37:41 desktop boinc[3325]: No protocol specified
Nov 28 19:37:42 desktop boinc[3325]: 28-Nov-2017 19:37:42 [---] Suspending GPU computation - computer is in use
Nov 28 19:37:54 desktop boinc[3325]: 28-Nov-2017 19:37:54 [---] Suspending computation - CPU is busy
Nov 28 19:38:34 desktop boinc[3325]: 28-Nov-2017 19:38:34 [---] Resuming computation
Nov 28 19:39:04 desktop boinc[3325]: XIO:  fatal IO error 11 (Resource temporarily unavailable) on X server "":1""
Nov 28 19:39:04 desktop boinc[3325]:       after 0 requests (0 known processed) with 0 events remaining.
Nov 28 19:39:04 desktop systemd[1]: boinc-client.service: Main process exited, code=exited, status=1/FAILURE
Nov 28 19:39:04 desktop systemd[1]: boinc-client.service: Unit entered failed state.
Nov 28 19:39:04 desktop systemd[1]: boinc-client.service: Failed with result 'exit-code'.","Client tries to get user's idleness from X screensaver extension.

systemd-logind based idle detection would be good addition but I don't think we can remove XSS idle detection because not everyone uses systemd.

XSS might stop working permanently once everyone has moved to Wayland but I think we are still far from that happening.",30260406
842,proposal: earning rewards for new contributors,open,2017-11-29T14:37:36Z,2019-10-30T09:38:44Z,,CONTRIBUTOR,"Hello, the WCG allows you to earn badges for bringing new contributors to the project.  This is a splendid idea to reward the spreading of the science. My personal preference would be to have not the number of (fake?) email addresses rewarded but to reward the factual credits that have been accumulated by the acquired new contributors.
Either way, it would be nice to have such a concept integrated in the regular BOINC server code.  Once could discuss if any such ""I heard about the (BOINC) project from"" interlinking of participants should be implemented across projects or if only within a given project this information should be stored and used. My personal prefernce is to have it implemented only on the project level.","These two reward schemes are not mutually in conflict and both seem straight forward to implement once the recruited-by relationship is established, right? Then let us go for them both. 
The focus on eyeballs is certainly interesting for the WCG to strengthen then the supporty by IBM, and all other projects also benefit from an opportunity to get a bit of extra money from web ads.",30260406
843,Provide SHA256 of stat files,open,2017-11-26T16:24:00Z,2019-10-30T09:38:34Z,,CONTRIBUTOR,"When generating stat files, would it also be possible to provide a `.sha256` for each file, containing the hash of the file in question? This can be used to determine the authenticity of the file and to ensure that it has not been tampered with along the way.",,30260406
844,Suspended GPU but new workunits are requested,open,2017-11-23T22:37:13Z,2019-10-30T09:38:11Z,,CONTRIBUTOR,"Hello,
because of my GPU issues (https://github.com/BOINC/boinc/issues/2119) I have suspended GPU computing. But still, tasks also for the GPU are requested. I propose to only download new material for resources that are enabled at the time of the download.
Cheers,
Steffen

```
Thu 23 Nov 2017 11:00:01 PM CET |  | max disk usage: 8.00 GB
Thu 23 Nov 2017 11:00:01 PM CET |  | max CPUs used: 1
Thu 23 Nov 2017 11:00:01 PM CET |  | don't compute while active
Thu 23 Nov 2017 11:00:01 PM CET |  | don't use GPU while active
Thu 23 Nov 2017 11:00:01 PM CET |  | suspend work if non-BOINC CPU load exceeds 25%
Thu 23 Nov 2017 11:00:01 PM CET |  | (to change preferences, visit a project web site or select Preferences in the Manager)
Thu 23 Nov 2017 11:00:01 PM CET |  | gui_rpc_auth.cfg is empty - no GUI RPC password protection
vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
Thu 23 Nov 2017 11:00:01 PM CET |  | Suspending GPU computation - user request
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Thu 23 Nov 2017 11:26:27 PM CET |  | General prefs: from http://www.worldcommunitygrid.org/ (last modified 28-Aug-2017 20:38:57)_
Thu 23 Nov 2017 11:26:27 PM CET |  | Host location: none
Thu 23 Nov 2017 11:26:27 PM CET |  | General prefs: using your defaults
Thu 23 Nov 2017 11:26:27 PM CET |  | Reading preferences override file
Thu 23 Nov 2017 11:26:27 PM CET |  | Preferences:
Thu 23 Nov 2017 11:26:27 PM CET |  | max memory usage when active: 7983.17 MB
Thu 23 Nov 2017 11:26:27 PM CET |  | max memory usage when idle: 7983.17 MB
Thu 23 Nov 2017 11:26:27 PM CET |  | max disk usage: 8.00 GB
Thu 23 Nov 2017 11:26:27 PM CET |  | max CPUs used: 1
Thu 23 Nov 2017 11:26:27 PM CET |  | don't compute while active
Thu 23 Nov 2017 11:26:27 PM CET |  | don't use GPU while active
Thu 23 Nov 2017 11:26:27 PM CET |  | suspend work if non-BOINC CPU load exceeds 25%
Thu 23 Nov 2017 11:26:27 PM CET |  | (to change preferences, visit a project web site or select Preferences in the Manager)
Thu 23 Nov 2017 11:26:31 PM CET | Einstein@Home | work fetch resumed by user
Thu 23 Nov 2017 11:26:36 PM CET | Einstein@Home | Sending scheduler request: To fetch work.
Thu 23 Nov 2017 11:26:36 PM CET | Einstein@Home | Requesting new tasks for CPU and NVIDIA GPU
Thu 23 Nov 2017 11:26:44 PM CET | Einstein@Home | Scheduler request completed: got 6 new tasks
Thu 23 Nov 2017 11:26:46 PM CET | Einstein@Home | Started download of einstein_O1Spot1Lo_1.00_x86_64-pc-linux-gnu__AVX
Thu 23 Nov 2017 11:26:46 PM CET | Einstein@Home | Started download of 20161121_O1MD1_4m_G3473489h_segmentList.seg
Thu 23 Nov 2017 11:26:48 PM CET | Einstein@Home | Finished download of 20161121_O1MD1_4m_G3473489h_segmentList.seg
Thu 23 Nov 2017 11:26:48 PM CET | Einstein@Home | Started download of skygrid_0262.40Hz_O1Spot1.dat
```","In Northern Europe most energy is produced with water and to use it for heating is common.  I am much with @sirzooro here. Seasonal contributions seem very natural, outside California this is. That said, I still do not get why there are electric heaters that do not also compute :o/",30260406
845,"""No protocol specified"" message",open,2017-11-23T15:24:57Z,2019-10-30T09:36:57Z,,CONTRIBUTOR,"I seem to get an error ""No protocol specified"" when I'm not logged into them machine- locked or logged off.  Doesn't seem to be 100% reproducible, but most of the time.  I'm using my systemd unit file from #2255.

When it occurs it logs every second.  This also happens if redirected to boinc error log (but then it has no timestamps..)  - the default in Ubuntu/Debian.

I've also had it actually stop boinc from running, but that could be a separate issue.

The first occurrence is sometimes followed by this dir_open fail.
Nov 22 15:57:36 desktop boinc[15002]: No protocol specified
Nov 22 15:57:36 desktop boinc[15002]: dir_open: Could not open directory '/dev/input/mice' from '/var/lib/boinc-client'.
Nov 22 15:57:37 desktop boinc[15002]: No protocol specified
Nov 22 15:57:38 desktop boinc[15002]: No protocol specified

sudo systemctl status boinc-client.service 
● boinc-client.service - Berkeley Open Infrastructure Network Computing Client
   Loaded: loaded (/lib/systemd/system/boinc-client.service; enabled; vendor preset: enabled)
   Active: active (running) since Wed 2017-11-22 14:33:35 EST; 19h ago
     Docs: man:boinc(1)
  Process: 14972 ExecStopPost=/bin/rm -f /var/lib/boinc-client/lockfile (code=exited, status=0/SUCCESS)
  Process: 14971 ExecStop=/usr/bin/boinccmd --quit (code=exited, status=0/SUCCESS)
  Process: 14960 ExecReload=/usr/bin/boinccmd --read_cc_config (code=exited, status=0/SUCCESS)
 Main PID: 15002 (boinc)
    Tasks: 21 (limit: 4915)
   Memory: 467.4M
      CPU: 1w 5d 2h 48min 4.613s
   CGroup: /system.slice/boinc-client.service
           ├─15002 /usr/bin/boinc
           └─30949 ../../projects/sech.me_boinc_Amicable/amicable_v_2_03 /from 2^2*5^2*17*15500813 /to 2^2*5^2*17*15554447 /task_size 410954439845 --nthreads 16

Nov 23 10:15:04 desktop boinc[15002]: No protocol specified
Nov 23 10:15:05 desktop boinc[15002]: No protocol specified
Nov 23 10:15:06 desktop boinc[15002]: No protocol specified
Nov 23 10:15:07 desktop boinc[15002]: No protocol specified
Nov 23 10:15:08 desktop boinc[15002]: No protocol specified
Nov 23 10:15:09 desktop boinc[15002]: No protocol specified
Nov 23 10:15:10 desktop boinc[15002]: No protocol specified
Nov 23 10:15:11 desktop boinc[15002]: 23-Nov-2017 10:15:11 [---] Suspending GPU computation - computer is in use
Nov 23 10:16:40 desktop boinc[15002]: 23-Nov-2017 10:16:40 [---] Suspending computation - CPU is busy
Nov 23 10:17:40 desktop boinc[15002]: 23-Nov-2017 10:17:40 [---] Resuming computation","I have noticed that this message gets printed once a second even if I have specifically allowed my BOINC client to use the CPU and GPU while the computer is in use. The log messages still appear at the end of the value of the `idle_time_to_run` setting (Computing allowed / Only after computer has been idle for ... minutes) in my global preferences; if I change that setting to 0, the messages appear immediately after reloading `global_prefs_override.xml`.

For now it seems to be sufficient to set that value to a (very) large number to avoid the check entirely, but the client definitely should not be sensitive to this value at all if I have configured it to compute continuously. That might well be a separate issue.",30260406
846,Allow any remote desktop client OSX,open,2017-11-12T11:26:39Z,2019-10-30T09:33:41Z,,NONE,"On my Mac, when I click ""Show VM Console"" then it asks me to download CoRD, however in my case I have the the microsoft remote desktop application.  Could it be genericized so that any RDP capable program can be used?","> So BOINC Manager might have to use something like `VBoxManage` to query what the resolution is (assuming it has a subcommand with this information)

Found it: `VBoxManage showvminfo --machinereadable`, then look for `VideoMode=""X,Y,colordepth""`. Example:

```
# Video mode during BIOS splash screen
$ VBoxManage showvminfo --machinereadable test | grep ""^VideoMode=""
VideoMode=""640,480,32""@0,0 1

# Video mode during 80x25 text mode
$ VBoxManage showvminfo --machinereadable test | grep ""^VideoMode=""
VideoMode=""720,400,0""@0,0 1
```",30260406
847,Windows Manager: multiple monitor persistence,open,2017-11-07T20:28:26Z,2019-10-30T09:31:38Z,,CONTRIBUTOR,"Feature request: allow memory of window positions to persist across multiple monitors.

From https://lists.ssl.berkeley.edu/pipermail/boinc_dev/2017-November/023032.html","The Macintosh Manager code already does remember window positions across multiple monitors. And it also automatically moves the window so the title bar is at least partially on screen if needed because the layout of the monitors has changed (for example, if a second monitor is removed from the system.) It is important to do this.

The Mac uses different code for this than other platforms; for other platforms, the code uses the wxWidgets API`wxSystemSettings::GetMetric( );` to return the width of the screen. This uses the main screen by default, but one can pass it the window so it works with any screen the window is on. However, in this code it needs to be called before the window exists, so that feature of this API can't be used here.",30260406
848,Throttle computing based on fan speeds,open,2017-11-06T06:02:12Z,2019-10-30T09:31:18Z,,CONTRIBUTOR,"Volunteers often stop running BOINC if it makes their fans go to high speed.

By default, the client should throttle computation in a way that avoids this.","Being able to limit computation based on temperature in general would be nice.  It's not going to solve this issue though, because how temperature correlates to fan speed, and then fan speed to noise, is not something the software itself can reliably figure out (It's perfectly normal for many modern Intel CPU's to run over 70 degrees Celsius in an OEM system without a huge amount of noise, but in a DIY gaming system, running that hot will almost always sound like you're on a airport tarmac), and because the type of people who wouldn't look for ways to limit resource usage to make things quieter are also not likely to look for ways to limit system temperature to make the system quieter.",30260406
849,screensaver translations not working,open,2017-11-04T07:17:34Z,2019-10-30T09:30:32Z,,CONTRIBUTOR,"According to Andy Bowery, translations are not working with the BOINC screensaver.",,30260406
850,garbage-collect apps in client,open,2017-11-04T07:15:04Z,2019-10-30T09:29:28Z,,CONTRIBUTOR,"Currently the client doesn't garbage-collect apps, so files for old apps remain on client.

How to do this:
if sched reply includes any apps, mark other apps as deprecated.
In garbage collection, if an app is deprecated and not referenced by any WUs,
delete it and all APP_VERSIONs that refer to it.
Do this before garbage-collecting files",This is a duplicate of #1386,30260406
851,user-reported Android issue,open,2017-11-04T06:13:37Z,2020-02-25T16:48:54Z,,CONTRIBUTOR,"This is what happens when I try to run bonic on my Asus ZenFone 2 - please fix!


GUI Messages

```
11-04 09:21:51.625 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:51.625 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:51.625 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:51.625 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:51.625 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:51.625 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:51.625 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:51.625 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:51.625 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:51.625 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:51.625 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:51.625 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:51.625 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:51.625 E/BOINC_GUI(17156): IOException
11-04 09:21:51.610 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:50.621 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:50.621 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:50.621 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:50.621 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:50.621 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:50.621 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:50.621 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:50.621 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:50.621 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:50.621 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:50.621 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:50.621 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:50.621 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:50.621 E/BOINC_GUI(17156): IOException
11-04 09:21:50.608 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:49.641 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:49.641 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:49.641 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:49.641 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:49.641 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:49.641 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:49.641 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:49.641 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:49.641 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:49.641 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:49.641 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:49.641 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:49.641 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:49.641 E/BOINC_GUI(17156): IOException
11-04 09:21:49.623 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:48.674 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:48.674 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:48.674 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:48.674 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:48.674 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:48.674 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:48.674 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:48.674 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:48.674 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:48.674 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:48.674 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:48.674 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:48.674 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:48.674 E/BOINC_GUI(17156): IOException
11-04 09:21:48.657 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:47.613 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:47.613 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:47.613 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:47.613 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:47.613 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:47.613 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:47.613 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:47.613 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:47.613 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:47.613 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:47.613 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:47.613 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:47.613 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:47.613 E/BOINC_GUI(17156): IOException
11-04 09:21:47.600 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:46.596 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:46.596 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:46.596 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:46.596 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:46.596 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:46.596 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:46.596 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:46.596 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:46.596 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:46.596 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:46.596 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:46.596 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:46.596 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:46.596 E/BOINC_GUI(17156): IOException
11-04 09:21:46.581 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:45.627 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:45.627 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:45.627 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:45.627 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:45.627 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:45.627 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:45.627 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:45.627 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:45.627 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:45.627 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:45.627 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:45.627 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:45.627 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:45.627 E/BOINC_GUI(17156): IOException
11-04 09:21:45.612 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:44.612 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:44.612 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:44.612 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:44.612 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:44.612 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:44.612 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:44.612 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:44.612 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:44.612 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:44.612 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:44.612 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:44.612 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:44.612 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:44.612 E/BOINC_GUI(17156): IOException
11-04 09:21:44.596 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:43.636 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:43.636 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:43.636 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:43.636 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:43.636 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:43.636 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:43.636 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:43.636 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:43.636 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:43.636 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:43.636 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:43.636 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:43.636 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:43.636 E/BOINC_GUI(17156): IOException
11-04 09:21:43.616 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:42.610 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:42.610 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:42.610 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:42.610 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:42.610 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:42.610 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:42.610 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:42.610 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:42.610 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:42.610 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:42.610 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:42.610 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:42.610 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:42.610 E/BOINC_GUI(17156): IOException
11-04 09:21:42.596 W/BOINC_GUI(17156): Launching '/data/data/edu.berkeley.boinc/client/boinc' from '/data/data/edu.berkeley.boinc/client/'
11-04 09:21:41.584 E/BOINC_GUI(17156): ... 7 more
11-04 09:21:41.584 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:209)
11-04 09:21:41.584 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(Native Method)
11-04 09:21:41.584 E/BOINC_GUI(17156): Caused by: java.io.IOException: No such file or directory
11-04 09:21:41.584 E/BOINC_GUI(17156): at java.util.Timer$TimerImpl.run(Timer.java:284)
11-04 09:21:41.584 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor$StatusUpdateTimerTask.run(Monitor.java:340)
11-04 09:21:41.584 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.access$5(Monitor.java:349)
11-04 09:21:41.584 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.updateStatus(Monitor.java:352)
11-04 09:21:41.584 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.clientSetup(Monitor.java:525)
11-04 09:21:41.584 E/BOINC_GUI(17156): at edu.berkeley.boinc.client.Monitor.runClient(Monitor.java:594)
11-04 09:21:41.584 E/BOINC_GUI(17156): at java.lang.Runtime.exec(Runtime.java:173)
11-04 09:21:41.584 E/BOINC_GUI(17156): at java.lang.ProcessManager.exec(ProcessManager.java:211)
11-04 09:21:41.584 E/BOINC_GUI(17156): java.io.IOException: Error running exec(). Command: [/data/data/edu.berkeley.boinc/client/boinc, --daemon, --gui_rpc_unix_domain] Working Directory: /data/data/edu.berkeley.boinc/client Environment: null
11-04 09:21:41.584 E/BOINC_GUI(17156): IOException

```


",The original issue that is shown with the log file is still happens if BOINC client was killed by OS core and user tries to reopen BOINC Manager without complete application shutdown. So I'd keep this to fix later,30260406
852,Multihost boinc reports incorrect host in server status for scheduler,open,2017-11-03T12:20:44Z,2018-10-24T12:49:45Z,,NONE,"I am running multi host boinc.
My main `<host>` in the config file is a host that does not run the feeder (or scheduler).
When I look at the server status page, the scheduler is incorrectly reported to be running on the same host as the main `<host>` in config.xml
I tested this by stopping the feeder on the feeder host which stops the scheduler too.  I can see both processes stopped in the server_status page.  the feeder is reported on one host as stopped, and the scheduler is reported on the `<host>` as stopped

So this is a problem with the html server_status page or a bug in remote_server_status.php https://boinc.berkeley.edu/trac/wiki/ServerStatus",,30260406
853,[Feature Request] continuation of computation on another computer/OS,open,2017-10-31T10:15:39Z,2018-10-24T12:49:33Z,,NONE,"I (and not only me) have more than one place where I can compute project units and each computer has different OS (Windows, Linux, MacOS). This leads to necessity to run different units on each machine but I have not enough time till deadline on some of them. So, the idea for boinc-client feature: user can press some button (for project) and this will lead to packing current results, marking by <user ID> and sending package to the project server[-s]. Server stores this data and user can retrieve it with all needed processing cores (algorithms) in another boinc-client by <user ID> but only once for the sake of limiting duplication of information (and calculations).","I think sharing is possible and in fact it's somewhat working for me...
Dual booting between Windows and Linux (WINE), BOINC continues to work accordingly by pointing to the same ProgramData/BOINC directory. I suppose BOINC is thinking it is still in a Windows environment under WINE...

The only problem lies on the projects. Unless the project managers say so, I don't think they can be shared with one another natively to their own platform/OS. And I think BOINC aborts all the task immediately when it sees the BOINC data directory is from a foreign OS.",30260406
854,Cannot change process priority on Android.,open,2017-10-30T14:47:25Z,2018-10-24T12:49:23Z,,NONE,"I am using MIUI, where I discovered that 8 computing processes only share 1 slowest core, with other 7 cores idle. However, just one normal process can use up 1 fast core.  
So could you please add a property to configure the process priority?","Thats great,thanks!",30260406
855,boinc_client gets blocked on XOpenDisplay () when 2 Xorg sessions are open,open,2017-10-30T03:29:16Z,2018-10-24T12:49:04Z,,NONE,"When two graphic sessions are opened on my system with only one active, boinc_client gets eventually stuck forever waiting on XOpenDisplay(). Killing the non-active Xorg session or switching back to it will unstuck boinc_client.

```
% ps aux|grep boinc
boinc      616  0.1  0.1  76512 16276 ?        SNsl oct.25  11:27 /usr/bin/boinc_client --dir /var/lib/boinc --redirectio
boinc     6469 27.0  2.7 375236 227700 ?       SNl  oct.28 471:12 ../../projects/boinc.bakerlab.org_rosetta/minirosetta_3.78_x86_64-pc-linux-gnu @G187193_1-50_MAP_hyb_TMP_v02_i01_t000__krypton.flags -in:file:boinc_wu_zip G187193_1-50_MAP_hyb_TMP_v02_i01_t000__krypton.zip -run:protocol jd2_scripting -silent_gz -mute all -out:file:silent default.out -nstruct 10000 -cpu_run_time 28800 -boinc:max_nstruct 600 -checkpoint_interval 120 -database minirosetta_database -in::file::zip minirosetta_database.zip -boinc::watchdog -run::rng mt19937 -constant_seed -jran 1240652
boinc     8431 15.0  0.0      0     0 ?        ZN   oct.28 238:57 [minirosetta_3.7] <defunct>
boinc     9481 18.7  0.0      0     0 ?        ZN   oct.28 267:20 [wcgrid_mcm1_7.3] <defunct>
boinc     9770  7.4  0.0      0     0 ?        ZN   oct.28 102:17 [minirosetta_3.7] <defunct>
boinc     9822 18.9  0.0      0     0 ?        ZN   oct.28 257:39 [wcgrid_mcm1_7.3] <defunct>
boinc    14127  0.0  0.0      0     0 ?        ZN   oct.28   0:06 [primegrid_genef] <defunct>
boinc    14128 17.8  0.0      0     0 ?        ZN   oct.28 226:39 [wcgrid_mcm1_7.3] <defunct>
paul     21201  0.2  1.4 2969360 120644 ?      Sl   19:10   0:00 /usr/bin/boincmgr
paul     21295  0.0  0.0  11204   896 pts/1    S+   19:12   0:00 grep --color=auto boinc
```
Backtrace (incomplete) but we can see boinc_client is stuck waiting on XOpenDisplay () to return.
```
Attaching to process 616
[New LWP 827]
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib64/libthread_db.so.1"".
0x00007f50e53a7a3d in poll () from /lib64/libc.so.6
(gdb) bt
#0  0x00007f50e53a7a3d in poll () from /lib64/libc.so.6
#1  0x00007f50e50aea9b in ?? () from /usr/lib64/libxcb.so.1
#2  0x00007f50e50ac693 in xcb_connect_to_fd () from /usr/lib64/libxcb.so.1
#3  0x00007f50e50b03d9 in xcb_connect_to_display_with_auth_info () from /usr/lib64/libxcb.so.1
#4  0x00007f50e63d674a in _XConnectXCB () from /usr/lib64/libX11.so.6
#5  0x00007f50e63c7702 in XOpenDisplay () from /usr/lib64/libX11.so.6
#6  0x0000000000494494 in ?? ()
#7  0x00000000004948cf in ?? ()
#8  0x000000000042090e in ?? ()
#9  0x00000000004732c0 in ?? ()
#10 0x00000000004084a0 in ?? ()
#11 0x00007f50e52e9640 in __libc_start_main () from /lib64/libc.so.6
#12 0x0000000000408a59 in ?? ()
```
I suspect that is because the non active Xorg server doesn't answer requests until it gets active again. As a result the boinc_client process gets fully stuck and unit processing eventually stops, as computing processes eventually get into zombie state when they finish their work.",,30260406
856,Please start using ngettext() to translate plural forms in BOINC projects website,open,2017-10-24T08:14:50Z,2018-10-24T12:47:48Z,,CONTRIBUTOR,"I have started updating Polish translation and noticed that there are some strings which needs singular and plural forms, but po input file allows to enter only one form, what makes translation more difficult. This problem is described in more details: here: [https://www.gnu.org/savannah-checkouts/gnu/gettext/manual/html_node/Plural-forms.html](https://www.gnu.org/savannah-checkouts/gnu/gettext/manual/html_node/Plural-forms.html).

Linked article mentions that gettext po files allows to enter multiple translations for one string, and there is ngettext function which chooses proper one. As I checked, BOINC website code uses its own po file parser written in PHP, and this functionality is missing there. Please add it and start using it in code, to make translations better.

You also may want to use part of all of Wordpress' pomo code [https://github.com/markjaquith/WordPress/tree/master/wp-includes/pomo](https://github.com/markjaquith/WordPress/tree/master/wp-includes/pomo), which is also part of Glotpress: [https://bitbucket.org/thanyawzinmin/glotpress/src/bb31d8abc7ae/pomo/?at=default](https://bitbucket.org/thanyawzinmin/glotpress/src/bb31d8abc7ae/pomo/?at=default). If its license would allow to use it in BOINC, it would save you some work here.

**Edit:** there are few more alternatives available:
[PHP gettext module](https://secure.php.net/manual/en/book.gettext.php)
[Dynamic MO Loader](https://github.com/aucor/dynamic-mo-loader)
[ginger-mo](https://github.com/dd32/ginger-mo)","This also applies for Drupal translations. I did not check other translation files, but most probably they are affected too.",30260406
857,[Feature] optirun for gpu task,open,2017-10-19T22:14:36Z,2018-10-24T12:46:39Z,,NONE,"Please add an configruation option to optirun only the GPU tasks. This makes things much easier. The systemd service files doesn't need to be changed for optirun. If GPU task is suspended, the optirun instance is removed. That means the card switches off automatically. 

You can add optirun support by adding an option for optional task spawn command and/or detect if there is a bumblebee service running. If it is running, prepend optirun.",,30260406
858,"Misleading ""Another instance of BOINC is running"" with wrong ownership of boinc dir",open,2017-10-05T07:46:18Z,2019-11-04T22:04:24Z,,NONE,"Just resolved this by ~~picking the boinc pid from `watch -d -n 0.2 systemctl status boinc-client.service`,~~ tracing the process within the `        retval = wait_client_mutex(""."", 10);` 10 second timeout with `strace -fe trace=file -p $(pidof boinc)` and that gave me the real issue:

```
command: ""strace -fe trace=file -p 19601"" (strace is /usr/bin/strace)
@dir: /home/empee584 @time: Thu  5 Oct 09:27:53 CEST 2017

strace: Process 19601 attached
open(""./lockfile"", O_WRONLY|O_CREAT, 0664) = -1 EACCES (Permission denied)
open(""./lockfile"", O_WRONLY|O_CREAT, 0664) = -1 EACCES (Permission denied)
open(""./lockfile"", O_WRONLY|O_CREAT, 0664) = -1 EACCES (Permission denied)
open(""./lockfile"", O_WRONLY|O_CREAT, 0664) = -1 EACCES (Permission denied)
open(""./lockfile"", O_WRONLY|O_CREAT, 0664) = -1 EACCES (Permission denied)
open(""./lockfile"", O_WRONLY|O_CREAT, 0664) = -1 EACCES (Permission denied)
open(""/etc/localtime"", O_RDONLY|O_CLOEXEC) = 3
+++ exited with 108 +++

""strace -fe trace=file -p 19601"" exited after 6 seconds
exit status 0 at Thu  5 Oct 09:27:59 CEST 2017

```
which was caused by the fact that my older version of ranger doesn't transfer file permissions when copying a dir (ranger/ranger#734).
So the presented error message is wrong and totally misleading. Obviously, this is a corner case, but still.  :smile: ",,30260406
859,CTRL+A shortcut doesn't work,open,2017-10-04T12:52:46Z,2018-10-24T12:38:27Z,,MEMBER,"The CTRL+A shortcut doesn't work on the Projects and Tasks tab. Assuming also this is the case on the Transfers tab.
The CTRL+Left Click and the SHIFT+UP/DOWN/HOME/END shortcuts are working.","#MeToo - had to help a user where this would have been useful. Wherever the focus happens to be on the tab controls, CTRL+A convention implies 'select all lines in list'.",30260406
860,"make_project fails on setting up database with exception: _mysql_exceptions.OperationalError: (1071, 'Specified key was too long; max key length is 767 bytes')",open,2017-09-23T19:16:59Z,2018-10-17T14:20:46Z,,NONE,"I want to get sorry for a possibly stupid issue, but I really can't find the solution. 

My system configuration: 
Debian 9 on VirtualBox 
50Gb storage
2Gb RAM 

Also I have installed all dependencies with boinc-server-maker, but then compiled the server from sources. 

When I try to start make_project script it always fails on setting up database with:
gleb@BOINCserver:~/boinc-src/tools$ ./make_project firstTest
Creating project 'firstTest' (short name 'firstTest'):
   PROJECT_ROOT = /home/gleb/projects/firstTest/
   PROJECT_HOST = BOINCserver
       URL_BASE = http://BOINCserver.vorfolomeevboinc.com/
  HTML_USER_URL = http://BOINCserver.vorfolomeevboinc.com/firstTest/
   HTML_OPS_URL = http://BOINCserver.vorfolomeevboinc.com/firstTest_ops/
        KEY_DIR = /home/gleb/projects/firstTest/keys/
        DB_NAME = firstTest
        DB_HOST = 

Continue? [Y/n]  y
Creating directories
Generating encryption keys
Copying files
Setting up database
Traceback (most recent call last):
  File ""./make_project"", line 234, in <module>
    project.install_project()
  File ""/home/gleb/boinc-src/py/Boinc/setup_project.py"", line 592, in install_project
    drop_first = options.drop_db_first
  File ""/home/gleb/boinc-src/py/Boinc/database.py"", line 290, in create_database
    _execute_sql_script(cursor, os.path.join(srcdir, 'db', file))
  File ""/home/gleb/boinc-src/py/Boinc/database.py"", line 277, in _execute_sql_script
    cursor.execute(query)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 226, in execute
    self.errorhandler(self, exc, value)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
    raise errorvalue
_mysql_exceptions.OperationalError: (1071,  'Specified key was too long; max key length is 767 bytes')","This is probably explained here:
https://stackoverflow.com/questions/1814532/1071-specified-key-was-too-long-max-key-length-is-767-bytes

Try editing boinc/db/schema.sql, change all the 255s to 254, see if that works",30260406
861,Support for Windows 10 on ARM CPUs,open,2017-09-21T11:35:35Z,2019-12-29T11:03:52Z,,CONTRIBUTOR,"Recently I have read that MS is working on Windows 10 for ARM, which most probably will launch in last quarter this year:
[Microsoft's Windows 10 ARM-twist comes closer with first demonstration](https://www.theregister.co.uk/2017/05/12/microsofts_windows_10_armtwist_comes_closer_with_first_demonstration/)
[Windows 10 on ARM still on track for Q4 2017, but not all OEMs will make it](https://mspoweruser.com/windows-10-arm-still-track-q4-2017-not-oems-will-make/)

Sooner or later someone will try to run BOINC on it, so some changes in BOINC will be required to support it. This Windows version will run 64-bit ARM CPUs, so new platform string will be required. It will also have x86 emulator (32-bit), so windows-x86 should be reported as an alternative platform. SSE instructions most probably also will be supported, so BOINC Client will have to run some helper x86 module to execute x86 CPUID instruction, and somehow report results to server. This probably will require update to client-server protocol too, mixing x86 CPU capabilities with ARM ones is not a good idea.","https://en.wikipedia.org/wiki/List_of_Qualcomm_Snapdragon_systems-on-chip#SQ1
2.1TFLOPS for a fan-less design is definitely something to consider
I would also like to see the Hexagon DSP with 9 TOPS being supported also as a new class of computing device",30260406
862,Develop credit simulator,open,2017-09-17T19:32:53Z,2018-10-17T14:07:06Z,,CONTRIBUTOR,"The credit system doesn't work well in some situations.

To figure out why, and improve the policy, it would help to have a simulator.
Inputs:
- a set of ""hosts"" (synthetic, or taken from a project DB)
- a stream of job descriptors (host, runtime, when sent, when received)
  could be synthetic or from project
State:
- host_app_version records, other DB
Output:
  figures of merit, e.g. device neutrality","Here are a few selected graphs from our research. All are with freshly updated server code in the first week of June 2014; freshly reset app-version and host-app-version tables; and with Einstein apps which have fixed <rsc_fpops_est> and very consistent runtimes. Watch out for logarithmic scales on some graphs.
![albert creditnew brp4g gpu5](https://user-images.githubusercontent.com/14886436/30600822-58c64b48-9d58-11e7-9ee2-5257a71ffeb1.PNG)
![albert creditnew brp4g gpu9](https://user-images.githubusercontent.com/14886436/30601088-f44905ec-9d58-11e7-8797-1a41d3bff36c.PNG)
![albert creditnew perseus linearb](https://user-images.githubusercontent.com/14886436/30601164-2e88c85a-9d59-11e7-852e-8c5f7a3fc7bd.PNG)
",30260406
863,Allow to configure different resource usage for CPU and GPU,open,2017-09-16T13:55:10Z,2020-02-09T23:33:08Z,,CONTRIBUTOR,"Use case: I would like to use CPU to crunch tasks from PrimeGrid, and at the same time set it as a backup project for GPU. Now it is not possible, I have to keep PG in backup mode for CPU even if it is main project for it, or choose another project as a backup for GPU.",This can be easily handled by the project's preferences page.,30260406
864,Please add compression to GUI RPC protocol,open,2017-09-16T12:06:00Z,2018-10-17T14:03:08Z,,CONTRIBUTOR,"When BOINC Client has 1000 WUs in the queue, communication between Client and Manager becomes slow. This is especially visible when link between them is slow - e.g. when managing Boinc installed on some remove server. Protocol between Client and Manager is XML-based, so compression should help a lot. Of course this should be backward-compatible, so communication should start as non-compressed, and Manager should explicitly ask for it.

Note: 1000 WUs seems a lot. However with 2-minute WUs and 32 CPUs all of them will be finished in about 1 hour, so real work buffer is not that big.","Reviewing for next release - we need a cost benefit analysis of compression vs. comms savings. I would also urge a configurable refresh rate for gui rpcs (instead of fixed 1 per second), for use on WAN links.",30260406
865,Android Boinc shows 0 tasks,open,2017-09-14T20:30:25Z,2019-10-30T00:59:08Z,,NONE,"If there are too much tasks (more than 100?) are running but the android client shows 0 tasks.

This happens on a Galaxy S5 Neo with Android 6.0.1.

See also https://boinc.berkeley.edu/dev/forum_thread.php?id=11640
",The limit to trigger this bug can now be easily hit on moderately high chipsets (like Snapdragon 855) and a project that likes to give lots of smaller tasks all at once (such as Einstein@Home),30260406
866,Fields To Show Task Estimated Completion & Relative To Deadline,open,2017-09-12T23:57:07Z,2019-12-13T00:46:58Z,,NONE,"It would be really useful if two extra columns were available on the Advanced View --> Tasks pane (I'm using the Windows BOINC Manager 7.8.2). These would help visualising if a task was likely to be completed before it's deadline and help users micro-manage priorities to ensure they are, if and as required. It would be especially useful for the larger, longer-running tasks.

# New Field 1: Estimated Completion

This would show the estimated date/time that a task will complete. It would be calculated as `current_datetime + remaining_estimated`

# New Field 2: Relative To Deadline

I'm not sure what the best name for this would be, but basically it would show the difference (in `days hours:minutes:seconds`) between the deadline and the ""estimated completion"" (as described above). This would be shown as negative if the task is expected to complete early or positive if the task is expected to overrun. This column should be sortable so you can see the tasks most likely to overrun at the top of the list and the tasks with most time in hand at the bottom - or vice-versa.

## Example

If Task1 is expected to complete on 15th September at 13:00 and the deadline is 17th September at 14:30, then this field would show something like `-2d 01:30:00` (i.e. it is expected to complete 2 days 1 hour and 30 minutes before the deadline - plenty of time to spare, no problem).

If Task2 was expected to complete at on 19th September at 14:00 but the deadline was the same day at 10:00, then it would show something like `+ 04:00:00` (i.e. it's expected to overrun by 4 hours - might want to take action to try to get this to complete before the deadline).

These new fields could look something like this (based on current time being 12/09/2017 23:00 and ignoring most existing fields for the sake of the example):

Task | Deadline | Remaining | Estimated Completion | Relative To Deadline
-----|----------|------------|------------------------|----------------------
Task1 | 17/09/2017 14:30:00 | 2d 14:00:00 | 15/09/2017 13:00:00 | -2d 01:30:00
Task2 | 19/09/2017 10:00:00 | 6d 15:00:00 | 19/09/2017 14:00:00 | +0d 04:00:00","I don’t mind either way. It was just a suggestion, I’m happy with whatever anyone decides",30260406
867,Web Interface For the BOINC Client,open,2017-09-12T15:07:21Z,2019-04-12T13:12:13Z,,CONTRIBUTOR,It would be great if the BOINC client could be controlled via a HTML page. This would enable a platform independent GUI to be created and hence avoid some of the issues with the current manager. This would also help to improve usability by allowing for more dynamic content that could guide volunteers through some more complex activities such as running VirtualBox applications.,This is being discussed further in PR #2275,30260406
868,Please add option to allow migration to new URL,open,2017-09-12T12:05:52Z,2018-10-17T13:59:13Z,,CONTRIBUTOR,"Projects sometimes have to change their URL, usually when they adds SSL certificate. There are also cases when project starts with IP address and then gets domain name. Now official way to handle this (from client perspective) is to disconnect from project and connect again using new URL. This also requires to finish all downloaded WUs and report them back to server, otherwise any progress will be lost. Please add new option to client which will allow to switch project to new URL, so all downloaded WUs and work progress would be preserved.

It would be also nice to have some redirect support on server side, so whole process could happen with no or little user interaction - some kind of confirmation from user probably should be there for security purposes.","Also, when you want to change url and add the new one first, the 'delete project' button doesn't work on the url that you want to delete.",30260406
869,Network activity radio button not following status in BOINC 7.8.0,open,2017-09-10T19:35:45Z,2019-10-30T09:21:33Z,,NONE,Wish I could some attention to this problem.  I'm being ignored.,Has this issue been forgotten?  I've received no response to my last post. Is anybody working on the problem?,30260406
870,Potential op script for identifying botnets,open,2017-09-10T16:28:40Z,2018-10-17T13:58:51Z,,CONTRIBUTOR,"We've recently had issues with pseudo-botnets (no c&c infrastructure AFAIK) running silent BOINC clients (installed via game cracks) in order to earn GRC.

It's difficult to identify botnets, especially if the user has hidden their hosts from the public. All the Gridcoin community has to go with is their earning potential & often results in false accusations and witch hunts.

What would be great would be if there was an ops PHP script which would flag users whom have many unique hostnames (rather than many similar hostnames - which indicate an institution such as an university running BOINC whilst idle) for further manual analysis. That way we can rapidly identify potential botnets and take action sooner.

Account managers such as grcpool.com which redistributes work out to its userbase may flag within such a script, so manual verification steps rather than automatic banning would be appropriate.

Any thoughts? Is comparing the hostname of hosts sufficient, or should additional information be included (such as IP block)? If the user hides their host details, is that hidden from the project too or just the users?

Thanks",,30260406
871,BOINC venues need to be able to be assigned to projects instead of computers,open,2017-09-01T17:44:17Z,2018-10-17T13:58:39Z,,NONE,On computers running multiple projects simultaneously it is impossible to run a specific venue assigned to a project without having its configuration affect other projects on that computer.  Global disk space/cache sizes and memory limits the main case in point. The venue needs to be able to be assigned to the project itself and not to the computer ID as presently is the case.,,30260406
872,Need to be able to show reported tasks sorted by task name and date/time on Seti website,open,2017-09-01T17:37:59Z,2018-10-17T13:58:26Z,,NONE,Need to be able to show reported tasks sorted by name in ascending order along with date/time on Seti website instead of just by either date/time OR ascending order on the Tasks page.  Would make it much easier to find a suspect task so the stderr.txt output could be examined.,Even that method would be preferable to what is current.,30260406
873,Improve support for universal apps,open,2017-08-28T19:10:06Z,2018-10-17T22:57:54Z,,CONTRIBUTOR,"A BOINC ""universal app"" is a VM player.
The VM images are part of the workunit.
Umbrella projects will typically have a single universal app
that is used for dozens or hundreds of science applications.
(Call these ""science apps"", and assume that they have integer IDs and version numbers).

BOINC has a number of mechanism that, in essence, equate apps and science apps,
and that don't work optimally in the presence of univeral apps.

1) The BOINC mechanism for estimating job runtime uses statistics
that are maintained at the level of (host, app_version).
What we'd like is (host, science_app_version).

2) Same for reliability (host_app_version.consecutive_valid).

3) Job submission permissions (user_submit_app table).
These control what jobs a user can submit.
Currently permissions are the level of app;
they could be at the level of science app.
(Not a high priority since this mechanism is not currently used AFAIK.)

4) The admin web interface has tools for looking at error rate and other stats
on the level of app and (app, platform).
We could increase the resolution to science app.

5) The user web interface shows what app a job is associated with.
It should also show the science app.

It remains to be seen how severe each of these problems is.

----------------

How to address these potential problems?
Some general approaches:

- Add new tables for science app and science app version.
This would greatly increase code size and complexity.
and it would require DB access when adding new science apps.
I'd prefer to avoid it.

- Generalize existing structures:
	Add a ""universal app"" flag to app
	Add ""science app ID"" and ""science app version"" fields to workunit,
		populated for universal app jobs.
		Also possibly ""science app name"".
	For universal apps, host_app_version.app_version_id encodes
		science app ID and version.
This would increase code complexity but not size.
","I'm not sure what a ""Universal app"" is meant to be. It seems to be another abstraction above what we now know as ""app"". I'm not sure I understand what is meant by ""VM player"" also. Here are my thoughts on the points in the opening post:

1. runtime estimation is depending on the combination of science payload (input data) and a particular app_version. I don't see how a science_app_version relates to a current app_version so I don't agree that the runtime statistics should use (host, science_app_version).

1. see above

1. RNA World uses the job submission interface and we already introduced our notion of ""universal app"". In our case we have have one science application (cmsearch) that we had to split up in three BOINC applications (cmsearch S, cmsearch XL, cmsearch VM) which have different runtime characteristics and because the last also uses VBox different input files. The user who is able to submit user jobs only sees the science application and our work generator selects the correct BOINC app to be used based on some tests on the input data. Currently this uses hardcoded app id's.

Comments about how to address this:

- I prefer to add a new table for such a ""universal"" science app which holds the associations to the BOINC apps and other meta data. This is based on my interpretation that a ""universal"" app is just a container to abstract a set of BOINC applications for the user. That's also why I don't see what a science_app_version can be. Maybe an example from an existing project may help.

- Reusing existing DB fields for different purposes and giving it different meaning based on other context is bad in my opinion and should be avoided at all costs. It is ok to increase the size of the codebase if it helps to separate things from each other.

Just to be clear: I vote -1 to the current design because of the reasons given above mainly the proposed mangling of BOINC apps with this new science apps.",30260406
874,Clean up remote job submission interface,open,2017-08-25T20:44:22Z,2018-10-17T13:56:54Z,,CONTRIBUTOR,"The remote job submission interface
https://boinc.berkeley.edu/trac/wiki/RemoteJobs
has grown by accretion and has some redundancy and ambiguity:

- a job's rsc_fpops_est can be specified either directly or as part of its input template.
- if rsc_fpops_est is specified directly it's used to estimate batch resource usage but doesn't make it into the WU DB record.
- input file descriptors and command line can be specified directly or as part of the input template.
- the doc doesn't say what takes precedent of what

Proposal: get rid of the rsc_fpops_est, command_line, and input_files attributes of ""job"".  If you want to set any of these, pass them in an input template (which includes all of them).  Per-job templates have precedence over batch-level templates.",,30260406
875,Replace custom XML parser with an existing implementation,open,2017-08-25T12:50:48Z,2019-03-14T20:02:15Z,,MEMBER,"This issue is a spin-off of issue number #2041 
Please review the discussion there about replacing the custom XML parsing with a standard library.",,30260406
876,Protected application execution no longer used in favor of Service Install,open,2017-08-24T09:41:12Z,2019-10-30T09:20:04Z,,CONTRIBUTOR,"Going through the source code, I find:.
In https://github.com/BOINC/boinc/blob/f83c2527db0be7c111ccdb042e0ba512648ecb0d/sched/handle_request.cpp#L1149 it says ""Unable to send work to Vista with BOINC installed in protected mode.  Please reinstall BOINC and uncheck 'Protected application execution'""

The 'Protected application execution' part has been 'Service Install' for all of BOINC 7. 
As an aside, the check only happens for Windows Vista? Not 7, 8, 8.1, or 10? ",,30260406
877,"Show language options in actual language, not all in English (only)",open,2017-08-22T11:15:21Z,2019-08-27T00:10:57Z,,CONTRIBUTOR,"This is a request to make the Options->Other options->Language drop down menu show the optional languages in those languages instead of all in English. Or show them in dual language. For example Deutsch (German), Nederlands (Dutch) , Français (French) etc. 

For consistency this is also needed at the web sites. ","I think flag is unnecessary.

Best regards,
Vitalii Koshura

2017-09-03 10:13 GMT+03:00 Adam Radocz <notifications@github.com>:

> Flag for each language also would be nice.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/2061#issuecomment-326788576>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ADFZoWkiTCs6WRQhOoqL3SGCQbNIeFWtks5selGvgaJpZM4O-eFp>
> .
>
",30260406
878,Move 2 prefs,open,2017-08-21T02:44:53Z,2018-10-17T13:53:58Z,,CONTRIBUTOR,"The ""OK to send email"" and ""OK to show hosts"" prefs are currently part of Project Prefs.
I think they belong in Community Prefs instead.
That's where other communication and privacy prefs are.

Except for these 2 items, Project Prefs involve computing: which apps to run, resource share, etc.
In fact it would be clearer to call them Project Computing Prefs.",,30260406
879,Eliminate line-oriented XML parsing,open,2017-08-15T20:43:51Z,2018-10-17T13:53:40Z,,CONTRIBUTOR,"There are a few functions like copy_element_contents() that use fgets() instead of fgetc().  This means they work correctly only the the close tag is on a line by itself.
Change these functions to use fgetc().","@davidpanderson,

I do not think that it is bad to rewrite existing functionality to make in
easier and more stable.
For example, I want to spend some time working on this task. Because I want
and I like such kind of jobs.
I want to cover all of this with unit tests too.
So i think it will very good for a project to use some existing libraries
that can make our life better and easier.
I do not force anyone to do this. It is just my choice, for example.
So why this is bad?
This will not break any existing functionality. This is just replacing
custom and potentially buggy code with better one that is already tested in
many projects. And this is good, I think.

This is what I think about such tasks.

Thanks

Best regards,
Vitalii Koshura

2017-08-26 3:18 GMT+03:00 David Anderson <notifications@github.com>:

> There are currently 3,542 lines of code that interface to the XML parser.
> These would all have to be replaced. Opportunity cost.
> The current parser handles attributes (like <global_preferences
> venue=""home"">).
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/2041#issuecomment-325062055>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ADFZoUmJ0P0yMpfjEk6onL89YnPvs9EOks5sb2RigaJpZM4O4Fua>
> .
>
",30260406
880,improve network animation in Manager,open,2017-08-13T07:36:26Z,2018-10-17T13:51:45Z,,CONTRIBUTOR,"The network animation (dots going between 2 computers) looks bad;
jerky, and the computers have partial outlines.","Doesn't that depend on your local resolution and monitor?
I'm on 1920x1080 pixels, the max my 23"" monitor can do and don't see the
problem so much. But perhaps that it'll show on higher resolutions (4K), or
larger monitors.


-- Jord van der Elst.

On Sun, Aug 13, 2017 at 9:36 AM, David Anderson <notifications@github.com>
wrote:

> The network animation (dots going between 2 computers) looks bad;
> jerky, and the computers have partial outlines.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/2028>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AKXbUzKQa_iL2GOb0cvQ98DaEkkoHr59ks5sXqd7gaJpZM4O1nnF>
> .
>
",30260406
881,configure log lies when some OpenGL packages are missing,open,2017-08-10T20:31:29Z,2018-10-25T01:17:16Z,,CONTRIBUTOR,"When there are no OpenGL packages installed configure reports the following:

```
checking for OpenGL library... -L/mingw64/lib -lopengl32
checking for OpenGL Utility library... -L/mingw64/lib -lglu32
...
checking for GLUT library... no
configure: WARNING:
================================================================================
WARNING: Development libraries and headers (""-dev"") of {openGL, GLU, glut} needed!
```

Even though the later message gets it right the earlier log messages are confusing.

If I install freeglut I get no warning but of the files that configure ""found"" only freeglut_static.a and GL/glut.h exist:
```
checking for OpenGL Utility library... -L/mingw64/lib -lglu32
checking for fopen in -lXmu... no
checking for fopen in -lXi... no
checking for fopen in -lgdi32... yes
checking for fopen in -lwinmm... yes
checking for GLUT library... -L/mingw64/lib -lfreeglut_static
checking gl.h usability... no
checking gl.h presence... no
checking for gl.h... no
checking glu.h usability... no
checking glu.h presence... no
checking for glu.h... no
checking glut.h usability... no
checking glut.h presence... no
checking for glut.h... no
checking glaux.h usability... no
checking glaux.h presence... no
checking for glaux.h... no
checking GL/gl.h usability... yes
checking GL/gl.h presence... yes
checking for GL/gl.h... yes
checking GL/glu.h usability... yes
checking GL/glu.h presence... yes
checking for GL/glu.h... yes
checking GL/glut.h usability... yes
checking GL/glut.h presence... yes
checking for GL/glut.h... yes
checking GL/glaux.h usability... yes
checking GL/glaux.h presence... yes
checking for GL/glaux.h... yes


```","After looking around a bit more carefully turns out the files do exist but not in /mingw64/lib as the log says. libopengl32.a and libglu32.a are in /mingw64/x86_64-w64-mingw32/lib/ .

On Linux it's similar:
```
checking for OpenGL library... -L/usr/local/lib -lGL
checking for OpenGL Utility library... -L/usr/local/lib -lGLU
checking for GLUT library... -L/usr/local/lib -lglut
```
The files are not in /usr/local/lib but in /usr/lib/x86_64-linux-gnu/.

Even though configure does correctly report if it found or did not find OpenGL libs, I'm leaving this open. I think it would be better if configure doesn't report path or add extra linker flags if the libraries are found in default library directory.",30260406
882,Standardize compute preference pre-sets,open,2017-08-08T21:08:30Z,2018-05-23T16:28:18Z,,CONTRIBUTOR,"GR and Drupal have preferences ""pre-sets"": named packages of preferences like
Maximum computing, Standard, Power saver
This should be part of the standard distribution, and should be the default interface (with an ""Advanced"" button for volunteers who want more control).

So we need to decide what the pre-set names are, and the associated pref sets.
I put some very sketchy ideas here:
https://boinc.berkeley.edu/trac/wiki/PrefsPresets","> I added some implementation notes:

https://boinc.berkeley.edu/trac/wiki/PrefsPresets contains the suggestion

![proposed presets](https://user-images.githubusercontent.com/14886436/40437599-aa8a75b4-5ead-11e8-8b29-5772b95f397f.PNG)

implying that %CPUs is more important than the inbuilt throttling implied by %CPU time. I'd agree with that: I use the former, but not the latter, and advised a user accordingly on the BOINC site message board recently.

But another user has pointed out that the dialog for Computing Preferences accessed from the Simple View in BOINC Manager has it the other way round: %CPU time is listed, but %CPUs is nowhere to be seen. Shouldn't we be consistent and swap them over - and should I raise a separate issue to handle it, or can we include it here?
",30260406
883,NCI and non-NCI apps on same project,open,2017-07-21T11:28:00Z,2017-07-27T08:30:40Z,,NONE,"I have this problem mentioned previously but now it is more important
and I really need to solve it.

Basically, I'm trying to put both: CPU intensive and non-intensive apps
on same project but I found that if you do this, all apps in Manager are
visible as CPU intensive and non-NCI work units takes one thread for
each of it.

The problem can be easily reproduced by installing server with default
values and adding example_app and sleeper to same project - I had
created project with --test_app then added sleeper as second application
for test purposes. Obviously sleeper is marked as NCI in Ops.

I strongly suspect that problem is not in server itself but in Manager.",,30260406
884,"Requesting a rewrite of the PM system, make it email-style, add an outbox, additional ignore options",open,2017-07-07T16:43:38Z,2017-07-22T21:29:52Z,,CONTRIBUTOR,"The PM system as it is now is a shambles. All PMs are on top of each other, they're all immediately in your face. 

What I would like to get is the email system:
- opening my inbox I get a column with names of people who PMed me. 
- Only when I click on a name does a wider column get populated showing the titles of the PMs this person sent me. 
- Bright blue for unread PMs, dark blue for read PMs. 
- Clicking a title opens the PM screen wide as it does now. 
- Needs a back button, and/or home button to be able to get back to the previous index, or main index.
- Have an outbox (https://github.com/BOINC/boinc/issues/234) where copies of the PMs we sent are being kept. 
- Outbox messages cannot be deleted by the user, the PMs in here delete themselves after a specific time (say 30 days), this so there is an easier paper trail in case an admin does need to check if a user is abusing the PM system. 

We can leave the ignore function enabled here. 
- Ignore function may require increased size, especially now with larger user IDs. 

Additionally, show by colouring the user's name red that they have us on filter, so we don't have to find that out when we try to send them something. 
Show the user's name in a different colour to show we have them on filter.","With regards to the outbox, I got the code from Primegrid's Michael Goetz who have this in their forums. I'll add them to this thread:
https://www.dropbox.com/s/k2orjgwxq5voin3/forum_pm.php?dl=0 (although the code here may not have changed, Michael didn't know for sure)
https://www.dropbox.com/s/j8yv1ayly1gll1e/pm.inc?dl=0 
https://www.dropbox.com/s/2nmd536jvwx3xhg/pm.php?dl=0

Michael Goetz writes:
> It's one very small schema change (one column added to `private_messages`), and a very small number of PHP files that change. It definitely should be added to BOINC. It's a really nice improvement. All it does it set two bits in the private_messages table. One bit is set when the sender deletes the message, and the other when the recipient deletes the message. The PM isn't physically deleted until both parties delete it. It doesn't store separate copies of the PMs, so the overhead is only one byte per PM.
> 
> ... Looking at the code, it looks like only html/user/pm.php and html/inc/pm.inc are affected.
> 
> Besides those two files, you may need to also modify the main private user account page as well as the forum page to call the pm_header function that displays ""Inbox | Outbox | Inbox & Outbox | Write "". I don't remember if I put that in, or if it was already there beforehand. The two files that should call pm_header are html/user/forum_pm.php (three places) and html/user/pm.php (two places). If you include those, it's a total of 4 files.

He also gave me the schema needed for the database:
```
CREATE TABLE `private_messages` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `userid` int(10) unsigned NOT NULL,
  `senderid` int(10) unsigned NOT NULL,
  `date` int(10) unsigned NOT NULL,
  `opened` tinyint(1) unsigned NOT NULL DEFAULT '0',
  `deleted_bits` tinyint(1) NOT NULL DEFAULT '0' COMMENT '1=Deleted by recipient, 2=Deleted by sender',
  `subject` varchar(255) NOT NULL,
  `content` text NOT NULL,
  PRIMARY KEY (`id`),
  KEY `userid` (`userid`)
) ENGINE=MyISAM AUTO_INCREMENT=63862 DEFAULT CHARSET=latin1
```",30260406
885,BSD build/run issues,open,2017-07-06T12:52:43Z,2019-06-04T12:41:56Z,,CONTRIBUTOR,"With freebsd,

In ""client/mac_address.cpp"":
I seem to have SIOCGIFCONF defined but neither SIOCGIFHWADDR or SIOCGIFARP which leads to hw_addr never being initialised, which causes the call to ether_ntoa(hw_addr) to segfault.
There is no #else to catch as a fall through.

Building needs to link to -lexecinfo for backtrace calls.

In ""sched/db_dump.cpp""
WEXITSTATUS needs sys/wait.h

It also fails to build with clang - for some reason a number of system headers never get included.

On netbsd,
lib/diagnostics.cpp and lib/diagnostics.h

""boinc_catch_signal(int signal, struct siginfo *siginfo, void *sigcontext)""

on netbsd siginfo seems to be defined as a union, and not a struct.


","I have a bit of time to look into fixing things again. One issue we found was that our AVX FreeBSD executables were not being sent out.

https://github.com/BOINC/boinc/blob/master/client/hostinfo_unix.cpp#L715-L762

It seems the code here checks for just mmx, 3dnow, sse1-3, and discards everything else.

With respect to the earlier solaris comment, it does not build for me at all either; again the MAC address extraction needs looking at.

I will start to make some PRs.",30260406
886,Win screensaver doesn't exit properly,open,2017-07-05T03:36:50Z,2018-10-17T22:56:07Z,,CONTRIBUTOR,"In 7.8 on Win7, if the BOINC screensaver kicks in while BOINC is not running, it correctly shows that.
But when you move the mouse, a BOINC icon remains in the system tray.
Left-clicking brings up a black screen.
Right-clicking shows only ""Close window"", but that has no effect.","Reading https://answers.microsoft.com/en-us/windows/forum/windows_10-hello/wake-up-from-screen-saver-is-only-by-ctrl-alt-del/6e772e86-8403-4034-8b19-aea31db60400, it may be possible that this is a problem in Windows 10 itself, by chance due to it being installed on older (all-of-a-sudden-not-supported) hardware. ",30260406
887,Many CPU cores and scheduler request delays do not play well together for backup projects,open,2017-06-14T18:01:25Z,2020-03-04T21:23:23Z,,CONTRIBUTOR,"When BOINC Client cannot download new WUs from projects with non-zero resource share, it tries to download them from projects with resource share set to 0. However when it does this, it requests only minimum required number of tasks to start them immediately and does not try to build any buffer. When host has many CPU cores and backup project has minimum scheduler request delay set to few minutes, host is not able to immediately download new WUs as necessary, it has to wait until this request delay time ends. Please change this. In such case BOINC Client should at least work as if work buffer size was set to zero. You can also consider using configured value or some fraction of it, but this sometimes may not be desirable - BOINC may download a lot of work from backup project, when the main one will stop providing work during maintenance period.","This parameter is used by Boinc Manager. Project servers only store it and send to manager. I use it with PrimeGrid - in fact in this project it is recommended to do this in order to increase change to be first discoverer of a new prime. I also tried to use this with about 2 other project but there were some issues with downloading WUs - one of them ran out of WUs for some time, and another one required long delay (something like 1 minute) between every connection to server.
I also recall problem with yoyo project which uses some ancient server version and does not allow to set 0. Other projects are more up to date and do not have such limitation.",30260406
888,Race condition in the scheduler,open,2017-06-12T12:12:18Z,2017-06-13T15:57:32Z,,NONE,"I believe there might a race condition in the scheduler when using targeted job.

Below is the scheduler log sorted by timestamp, as you can see the same WU get assigned multiple time despite using min quorum 1, target results 1, max success results 1. 

It happens when the scheduler has to send multiple WU before the assignment table can be updated.

I don't think we were having this issue before using targeted job. 

It is not critical for us, it doesn't causes any issue but some computation time is wasted computing the same WU twice. We are using trusted nodes, we don't need to replicate result.

scheduler.log:2017-05-26 07:01:30.6088 [PID=29263]    [version] get_app_version(): getting app version for WU#34631 () appid:5
scheduler.log:2017-05-26 07:01:30.6089 [PID=29265]    [version] get_app_version(): getting app version for WU#34631 () appid:5
scheduler.log:2017-05-26 07:01:30.6089 [PID=29267]    [version] get_app_version(): getting app version for WU#34631 () appid:5
scheduler.log:2017-05-26 07:01:30.6589 [PID=29265]    [send_job] est. duration for WU 34631: unscaled 297.96 scaled 298.26
scheduler.log:2017-05-26 07:01:30.6589 [PID=29267]    [send_job] est. duration for WU 34631: unscaled 294.54 scaled 294.54
scheduler.log:2017-05-26 07:01:30.6591 [PID=29263]    [send_job] est. duration for WU 34631: unscaled 289.13 scaled 289.30
scheduler.log:2017-05-26 07:01:30.6592 [PID=29265]    [assign] [WU#34631] [RESULT#34904] [HOST#22] send assignment 34631
scheduler.log:2017-05-26 07:01:30.6592 [PID=29267]    [assign] [WU#34631] [RESULT#34905] [HOST#15] send assignment 34631
scheduler.log:2017-05-26 07:01:30.6594 [PID=29263]    [assign] [WU#34631] [RESULT#34903] [HOST#18] send assignment 34631

WU settings:
min quorum 	1
target results 	1
max success results 	1
max error results 	5
max total results 	5

",,30260406
889,Creating an account with MacOS and Windows uses the users' computer username.,open,2017-06-03T20:32:57Z,2018-10-05T18:56:44Z,,CONTRIBUTOR,"https://einsteinathome.org/content/how-can-i-delete-my-account

Users report that when creating their account, BOINC client uses their computer's username. Account name should be a field the user fills in.

~~PS- I have not experienced this behavior with the Linux client.~~

Update: This does happen in the Linux client as well.","Adding an extra text field for the user name is easy enough but what to do with the web page you go to after the account is created? It would be a bit silly to ask for a user name on the web page just after asking for a user name in the Manager.

![image](https://user-images.githubusercontent.com/15806192/46554522-44159380-c8e9-11e8-9d4c-36d1ea21fbe2.png)",30260406
890,"Bootstrap: have forum style (background, font colour etc.) user-selectable",open,2017-05-27T23:36:27Z,2017-06-13T15:53:27Z,,CONTRIBUTOR,"Have a user-selectable choice on how they want the forum style (background colour, font colour, font style) to look like. Now the bootstrap software has the administrator set the colours, but if the users don't like that, they should be able to set them to something they like without having to resort to third party programs that may not be available for all platforms. ",,30260406
891,Do not allow moderators and administrators to be filtered by the user,open,2017-05-26T21:53:22Z,2020-02-09T23:57:32Z,,CONTRIBUTOR,"At the moment it is possible for a user to put everyone else that he doesn't like on ignore, this includes moderators and administrators. When a user is on ignore his posts are filtered and he cannot private message me. I feel that moderators and administrators need always have the ability to PM the user to be able to ask them to tone down when needed. On phpBB boards it's not possible to put moderators and administrators on ignore. I think we should adopt that. 

phpBB's https://github.com/phpbb/phpbb/blob/c571d99c0f4108ec9136b601c9a57b4878597470/phpBB/includes/ucp/ucp_zebra.php lines 162 and further shows how it is done for them. Can this be adapted for BOINC? ","Obnoxious moderators at SETI are the most important reason to be able to block moderators.
If an administrator needs blocking then their whole project should be avoided. I agree no blocking for administrators.",30260406
892,New feature: Running in Hi-priority (user request),open,2017-05-20T10:15:29Z,2017-06-14T17:24:34Z,,NONE,"The situation: After BOINC Pentathlon has finished, _imagine_ You have several computers still running a few very long **camb_legacy 2.17** tasks from Cosmology@Home project.
Those long-running WUs **checkpoint** only once every hour or even **once in every 2 hours** (on old mobile core i7 CPU). So, You want to let them run **uninterrupted and unsuspended in memory** to let them finish as SOON as possible.
While those (for example 3) tasks are running, i.e.12 up to 24 hours, the **5 remaining CPU logical cores are hungry and sitting IDLE**!!

Because: **IF** You allow any work from any other project (especially vBox tasks, from LHC or even from the Cosmology itself), **it will** take over the cores and suspend those task which You **NEED** to finish ASAP (also because they occupy 0.5 GB RAM each).

This is the reason we need this feature.
We need **FULL control** of BOINC, so we can better tune-up the volunteer computations on our PCs.

With this feature, I could just **set** those three task to **Hi-Priority mode** and re-enable New Tasks from a backup project with single-core tasks, for example Rosetta@home ... or even from a main project - like for example **Asteroids**. And all would run just fine - without the need for me to **manually suspend or abort** _ALL OTHER TASKS_ - which is just **insane amount of needed user-intervention, inadequate** to the effect (just to feed those 5 hungry cores!). :-) :-) :-)

So, I propose to add this feature to the _Tasks Tab_ of Boinc Manager, to improve the amount of control:
**Run in Hi-priority**","Similar requests appears from time to time on BOINC forum, e.g. [https://boinc.berkeley.edu/dev/forum_thread.php?id=11259](https://boinc.berkeley.edu/dev/forum_thread.php?id=11259). Sometimes people would like to specify which tasks should be completed first. It would be good to finally implement such feature, this requests gets big +1 from me.",30260406
893,Android BOINC Mgr switches to Notices tab when screen orientation changes,open,2017-05-04T22:33:06Z,2019-10-02T19:18:41Z,,CONTRIBUTOR,"One time I noticed that when I changed device orientation from vertical to horizontal, BOINC Mgr was both changing screen orientation and switching from Tasks to Notices tab. This happened every time I changes device orientation, I always ended on Notices tab. After BOINC restart problem disappeared.",I stuck on point 3. I've never gotten any notification on android.,30260406
894,stderr output from VirtualBox is not saved when task is aborted,open,2017-05-04T22:16:21Z,2018-10-17T13:50:13Z,,CONTRIBUTOR,"Recently I had VirtualBox task which was running for very long time. After aborting it and sending it back to server I wanted to check if there was something interesting logged to stderr, but there was only ""aborted by user"" message. Other successful tasks had many lines of stderr output logged, so this one also should have something more. Please retrieve stderr output from VM and send it to server in such case, it will help project developers with fixing bugs in their apps.","Actually, I think this was fixed (for standard tasks) by 0355298b0deac03a5f42ddb211ac055c44d130b7 - unless there's a separate issue for VBox?",30260406
895,"Please add new option ""Run NCI tasks when on battery"" to Android app",open,2017-05-04T22:05:41Z,2018-10-17T13:49:31Z,,CONTRIBUTOR,"As in title. NCI tasks consumes little CPU, so user may want to run them when device is on battery too.","After some time I came to conclusion that this new option could be more general - namely ""Run NCI tasks when suspended"". BOINC Clients on Windows and Linux would benefit even more from it, as they usually do not run on battery.",30260406
896,Number of required CPUs/GPUs for app is not restored to default after deleting app_config.xml,open,2017-05-04T22:00:40Z,2017-06-01T02:19:13Z,,CONTRIBUTOR,"I have reconfigured app to use 0.5 CPU instead of default 1 and reloaded config - BOINC correctly started additional instances of app. Then I removed app_config.xml and reloaded config again, but number of running apps did not change - looks that BOINC still used modified value. I had to create app_config.xml with CPU count set to 1 and reload config one more time in order to change this value back to 1.","Boinc does store the CPU/GPU settings in the client_state xml file. If you override the defaults set by the website using app_config or app_info xml then that is what Boinc uses from then on. You either have to edit your app_config or app_info to set them back and have the manager issue the read config files after the edit or use multiple app_configs. Example:

app_config.xml <-----CPU/GPU setting for one task at a time on the GPU
app_config_two.xml <----- CPU/GPU settings for two tasks at a time on the GPU
app_config_three.xml <---- CPU/GPU settings for three tasks at a time on the GPU

so to go from one task at a time to two tasks at a time, rename app_config to app_config_one and app_config_two to app_config and have the manager issue the read config files command.

There is no code at this time to check for the removal of an app_config and switch back to another (i.e. profiles). Boinc does store the original amount of CPU to use set by the website in the <max_ncpus> tag. And uses this value if the app_config xml is removed.",30260406
897,Displayed CPU/GPU count for app is not updated when app_config.xml changes them,open,2017-05-04T21:54:33Z,2017-05-29T11:10:15Z,,CONTRIBUTOR,"When you change number of required CPUs and/or GPUs for app using app_config.xml, GUI is not updated - it still shows old values. 3rd party software like BoincTasks also shows it in this way, so BOINC Client sends them incorrect values. This is display-only issue, number of running tasks is properly adjusted. New tasks downloaded later shows correct value. If I remember correctly, BOINC restart also fixes this.",This has existed since the start of GPU usage and has never been acted upon.,30260406
898,power issues on Android 5.0,open,2017-05-01T02:44:00Z,2018-09-19T21:14:31Z,,CONTRIBUTOR,"from a user email:

Tablet: RCA
Model #RCT6773W22B
Android version 5.0
BOINC Version 7.4.53

Though I have preferences set to run on charger or with USB power
connection, and tablet is connected to charger (verified by seeing battery
is charging), BOINC continues with message ""Connect your device to a charger
to continue computing."" I've tried couple chargers, and connected via USB to
couple computers. However, continue with above message. 

Also note that when I add ""allow use with battery (set to be 90% or
greater)"" to preferences, I'll then get the message ""Computing will resume
when battery charge reaches 90% (currently 100%)."" Strange!  It MAY crunch
for a short time (no longer than 2-3 days) and then stop again after I
restart BOINC Mgr. or abort a WU, or maybe at its own whim.
 
Tablet otherwise works well on all other apps installed. Note that above
BOINC version is working well on phones with Android 4.1.1 and 6.01 (though
very slow crunching on 4.4.1 phone with 1 processor, as expected). 
 ","By trial and error, of late I've noted the following: I've been running Seti@Home and Rosetta@Home on this device.  Of late, when I've noticed the device has stopped crunching due to noted power issues, I've aborted a Rosetta WU, and usually this will start the device crunching again.  For now I've selected no new tasks for Rosetta, and it has continued to process the Seti WUs for last couple days at least.  Don't know _why_ this would be, but wanted the folks involved with this issue to be aware.  Thanks again.",30260406
899,Comparative advantage optimization based on floating point MIPS and integer MIPS,open,2017-04-26T22:43:36Z,2017-08-23T15:06:33Z,,MEMBER,"Hi,

I compared my laptop and my desktop PC performance and I got the following result:
![image](https://cloud.githubusercontent.com/assets/16503773/25460103/ac7621ce-2ad9-11e7-8ca6-e7fc09bb781b.png)

My desktop has almost 50% performance advantage at floating point calculation, but barely 5% advantage at integer calculation.

So I was wandering, if it was possible to estimate which working unit is how much floating point calculation heavy, than those working units could be served to my desktop and the less floating point calculation heavy WUs could be served to my laptop.

I think in bigger scale, like the World Community Grid, could have a big impact in terms of calculation time.
I'm not sure the idea is feasible or totally BS.
What do you think?","Almost all science applications are dominated by floating point.

But it's an interesting idea: if a user has equal resource shares for projects A and B,
currently BOINC tries to divide each of the user's computers 50/50 between the projects.
It might increase throughput to use one computer entirely for A and another entirely 
for B.
I couldn't figure out an easy way to do this.

-- D

On 4/26/2017 3:43 PM, Adam Radocz wrote:
>
> Hi,
>
> I'm compared my laptop and my desktop PC performance and I get the following result:
> image 
> <https://cloud.githubusercontent.com/assets/16503773/25460103/ac7621ce-2ad9-11e7-8ca6-e7fc09bb781b.png>
>
> My desktop has almost 50% performance advantage at floating point calculation, but 
> barely 5% advantage at integer calculation.
>
> So I was wandering, if it was possible to estimate which working unit is how much 
> floating point calculation heavy, than those working units could be served to my 
> desktop and the less floating point calculation heavy WUs could be served to my 
> laptop.
>
> I think in bigger scale, like the World Community Grid, could have a big impact in 
> terms of calculation time.
> I'm not sure the idea is feasible or totally BS.
> What do you think?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub 
> <https://github.com/BOINC/boinc/issues/1887>, or mute the thread 
> <https://github.com/notifications/unsubscribe-auth/AA8KgQeKdw-BwIAqWj9THSWHFWJ-GV75ks5rz8iYgaJpZM4NJhAe>.
>

",30260406
900,Adjust scrutinizer-CI for different coding styles,open,2017-04-24T08:03:30Z,2018-10-17T13:44:56Z,,MEMBER,"[Scrutinizer-CI](https://scrutinizer-ci.com) is a static code analyzer for PHP and JS which supports, among other things, codestyle checking. Based on that the tool provides patches to achieve the desired coding style for the existing code.

Since the Drupal part is coded using the preferred Drupal style which is different from the BOINC style there needs to be a separate configuration for Drupal. According to a Scrutinizer developer we can use the [multiple-build environment](https://scrutinizer-ci.com/docs/build/multiple_test_environment) to achieve this.

Please contact me directly if you want to test this as you might need access to the scrutinizer project.",,30260406
901,Replace autotools build system,open,2017-04-24T07:43:16Z,2018-10-28T09:28:02Z,,MEMBER,"The current autotools centered build system is very fragile and hard to maintain (if you are not a autotools expert). It is also Unix/Linux centered and only half heartedly supports cross compilation (e.g. mingw).

I propose to replace it gradually with a [cmake](https://cmake.org/) based build system which offers multi-platform and multi-compiler support and an easier language to write build configuration files. It can for example build Visual Studio project files as well as regular Makefiles as well as control files for other build tools.

I already did some initial testing and can create config.h to replace autoheader and have some simple rules to build basic libraries. The challenging part is backwards compatibility (what special checks are still relevant and what can be omitted) and creating/using static libraries for release builds which I have not yet explored.",In a recent PR discussion the idea to use [meson](https://mesonbuild.com) to replace the build system was floated. I'm not sure we can migrate all our use cases mainly because I don't know all our use cases. Meson is written in python3 and uses the [ninja build](https://ninja-build.org/) system. This would move us away from make and into new territory.,30260406
902,"""Page/swap file: use at most"" setting can't be set to 0",open,2017-04-23T10:06:29Z,2018-10-17T13:44:05Z,,MEMBER,"Hi,

The ""Page/swap file: use at most"" setting must be between 1 and 100 and the 0 value is not allowed. Some cases I don't want to allow BOINC to use page/swap file and I can't set 0 value. It doesn't make sense especially, when I don't have page/swap file.","In that case swap-size detection would return 0 and the logic would be skipped.
But like I say the current code is wrong, and probably a no-op.
See cpu_sched.cpp.",30260406
903,plan_class without <max_threads> uses always only 1 thread,open,2017-04-22T18:00:50Z,2017-07-14T10:40:00Z,,CONTRIBUTOR,"This plan_class:
```
     <plan_class>
         <name>                  16t          </name>
         <min_ncpus>             16           </min_ncpus>
         <nthreads_cmdline/>
     </plan_class>
```
Uses only 1 thread. It sends to the client 
```
     <platform>windows_x86_64</platform>
     <plan_class>16t</plan_class>
     <avg_ncpus>1.000000</avg_ncpus>
     <max_ncpus>1.000000</max_ncpus>
     <flops>3718491507.830066</flops>
     <cmdline> --nthreads 1</cmdline>
```
But if I extend the plan_class with
```
<max_threads>           512          </max_threads>
```
It sends 16 threads to the client.

max_threads should be considered as very big or at least min_ncpus if missing. But it shouldn't be 1 as default.","If <max_threads> is present, BOINC will always set --nthreads to the maximum number of threads available to the BOINC client at the time the task is requested and allocated - so an 8-core machine with BOINC restricted with <max_ncpus_pct> 75 will receive tasks with --nthreads 6

So there should be no downside to setting a high default value as suggested: it will only come into play with extreme workstations or possibly Xeon Phi coprocessors.

More practically, there is probably an upper limit to the benefits of multithreading for each application: a tipping point where the overheads of synchronisation are greater than the benefits of an extra thread. Administrators deploying MT apps might choose to set lower <max_threads> values like 32 or 64, depending on the performance of their applications, but that's a matter for individual projects, not BOINC.",30260406
904,SIGSEGV segmentation violation crashes clean boinc-client install,open,2017-04-21T14:55:21Z,2019-08-27T00:07:02Z,,NONE,"This error occurred on a clean install of Boinc from the main openSUSE Tumbleweed repositories. The stack trace output is as follows:
```
SIGSEGV: segmentation violation
Stack trace (10 frames):
boinc-client[0x49a560]
/lib64/libpthread.so.0(+0x12230)[0x7f2341d8f230]
/lib64/libc.so.6(fputs+0x18)[0x7f234118df78]
boinc-client[0x4645dd]
boinc-client[0x464d53]
boinc-client[0x41e8fd]
boinc-client[0x473033]
boinc-client[0x408b80]
/lib64/libc.so.6(__libc_start_main+0xf1)[0x7f2341144541]
boinc-client[0x40910a]

Exiting...
```
The full program output is visible [here as a gist](https://gist.github.com/Pbtflakes/b88f0890dce7c0d9b3ffdda632a53195).

System information:
- Linux 4.10.9-1-default
- BOINC 7.6.33-2.1","10-31-18; FWIW, a clean install of Linux BOINC Manager 7.12.0 (x64) went smoothly and various client apps (SETI, Einstein, WCG) completed WUs without incident.  See issue 1556 for further comment. ",30260406
905,Computing Preferences window is too big under 1366*768 resolution,open,2017-04-18T13:08:29Z,2017-04-24T06:54:40Z,,CONTRIBUTOR,"![screenshot from 2017-04-18 21-00-33](https://cloud.githubusercontent.com/assets/12658589/25131992/84be3a50-247a-11e7-9ff6-4e38b80f59e1.png)

Fedora 25 Workstation
GNOME 3
BOINC version 7.6.22
wxWidgets version 3.0.2

I don't understand why recent BOINC Managers keep growing the size of that window","![screenshot from 2017-04-18 21-22-38](https://cloud.githubusercontent.com/assets/12658589/25132761/2d65851c-247d-11e7-851f-8b93f2885559.png)

EDIT: Yes and these are the normal defaults on Fedora I guess...",30260406
906,The BOINC manager should display the build SHA1 in its About dialog,open,2017-04-11T12:43:15Z,2019-03-14T20:01:58Z,,CONTRIBUTOR,Showing the arbitrary build version isn't enough to know exactly the source code version used for a build. Show the abbreviated SHA1 of the HEAD used for building the app in the dialog as well.,"The Android app can be changed in a pretty easy way. I'm going to open a PR for it soon.

Alternatively the client itself could have the SHA1 compiled in (use `git describe` to fetch the details) and a new RPC (?) added such that the manager can query it.  This assumes that manager and client are compiled from the same source though.",30260406
907,"Quoting adds white lines, making a quote after text removes a white line",open,2017-04-06T10:22:28Z,2017-04-07T06:20:27Z,,CONTRIBUTOR,"With the new bootstrap code implemented in the forum code, when quoting, one has to write their answer directly after the closing quote tag, as the quote box gives an extra white line. When quoting someone and starting the text on the next line, two white lines are added. 

Further, when posting a new quote immediately after a bit of text, that quote is squashed directly against the previous text. 

I have made a new thread at Seti (as that is using the newest code, which the BOINC forums don't even yet have) that shows the problems: https://setiathome.berkeley.edu/forum_thread.php?id=81261",,30260406
908,Implement alert messages on Linux,open,2017-04-05T13:50:26Z,2017-04-07T06:21:15Z,,MEMBER,"Linux currently doesn't show any alerts issued by the Manager. While on Windows and Mac such alerts are shown via the tray (Balloontips) there is no code implemented for Linux.

The idea of the tray icon is to not bother the user with a modal dialog but sometimes it is necessary to have one. A good example and starting point is [CBOINCBaseFrame::OnAlert()](https://github.com/BOINC/boinc/blob/master/clientgui/BOINCBaseFrame.cpp#L244) which e.g. displays a message about a wrong password when communicating with the client on Windows and Mac but not on Linux.",,30260406
909,Android release management,open,2017-04-04T09:23:52Z,2019-10-14T08:42:08Z,,CONTRIBUTOR,"What's the current state of the Android release management? With Rom's departure it seems as if there simply are no further Android releases at the moment, right? *If* we want to retain Android as a compute *and* outreach platform we have to fix this. Here are a few questions that I think need to be answered in order to establish a new working process:

* Is there anyone currently responsible for Android releases?
* Who has access to the Berkeley SSL Play Store account and thus *could* release a new version?
* Should we continue to use that account or create a new community account? Are we entitled to do the latter under the BOINC brand?
* Who can/should take over control over that account
* Is there any documentation on the old release workflow/procedure for someone willing to step in?
* Which client version should we release?
* Which Android app issues are release critical?
* Should we also target [F-Droid](https://f-droid.org) as a release channel for all free software enthusiasts rejecting the Play Store?","Almost. @TheAspens wants to document the release management process for Android which should answer the process-related questions I asked. The issue can be closed as soon as that's done.

Thanks",30260406
910,automatic Release Notes generation,open,2017-03-29T10:31:17Z,2018-10-19T06:34:51Z,,MEMBER,"Hi,

I would be grateful if you could make some release notes for the development version as well, not just the stable. It would be easier to identify the potential bugs if we knew what did you changed and what are the new features.","Users expect to be able to see both

* Release Notes - summary of major changes/fixes only
* Change log - detailed, hopefully comprehensive, list of all changes made, ideally including changes made during testing.

I don't think Github is an appropriate tool for non-developers: Milestones are also more often forward-looking planning tools, rather than retrospective archive records.

@computersalat - try https://boinc.berkeley.edu/forum_thread.php?id=11539",30260406
911,Improve email address validation,open,2017-03-26T07:32:27Z,2017-12-01T11:57:32Z,,CONTRIBUTOR,"The current check of email addrs is syntactic; ""1@1.1"" passes.
It's easy for spammers or people suspended on the forums to create new accounts.
Possible changes (non-exclusive):
- Use a 3rd party validater like http://validateemailaddress.org/
  (there are many; most of them charge)
- Add a project option to require new accounts to be email-validated
  (the code for this already exists) and to hide unvalidated accounts.","Could be done as a suggestion is, to have Spam suspect posts, we've seen quite a few in recent weeks with garbage titles, to mark/move these into a moderation place, 1 time, make them invisible to the general public. Then moderators, whenever present, can decide on final outcome i.e. if deemed not spam/inflammatory, the mods move them back in view as final decision, this to prevent a back and forth on such a 'not compliant' post.",30260406
912,New upload directory requried for remote submission,open,2017-03-17T09:38:13Z,2017-04-12T12:42:05Z,,CONTRIBUTOR,"The download directory should not be used for staging input files when using remote submission. At least a subdirectory should be used on the Web server so that write access can be limited. Ideally, an external storage solution such as Ceph could be used and the volunteers would download the input file directly from there. This would avoid data throughput on the Web server. ","Couple of comments:
- the remote file management system doesn't let job submitters overwrite existing files; they can only create new files.
- The remote job submission has an option (the ""remote"" file type) that lets you put files on any server.",30260406
913,"basic DB function inconsistency (count, max, sum, ...)",open,2017-03-16T09:37:10Z,2017-04-12T12:41:38Z,,MEMBER,"See implementations for `count()`, `max()`, and `sum()` in `html/inc/db_conn.inc` in particular the `$clause` argument which needs to start with a DB specific keyword for the latter two functions but must not start with a keyword for `count()`. This is inconsistent when using the functions.

Make this consistent as in: Do not require the keyword to be part of the argument. ",,30260406
914,DB layer enum functions are inconsistent,open,2017-03-16T09:30:10Z,2017-04-12T12:41:21Z,,MEMBER,"The basic `enum*` functions in `html/inc/db_conn.inc` support an argument for _where_ and _order_ clauses and the _where_ clause must not start with ""where"". The table specific `enum` functions in `html/inc/boinc_db.inc` sometimes have one sometimes two arguments. See `BoincProfile::enum` and `BoincCreditUser::enum`.

This should be made consistent, as in: All table specific `enum` implementations should support two arguments. In addition a third argument for _limit_ clauses should be added to the basic functions and the class implementations. This will make the DB layer more generic and will allow switching to something other than mysql in the future.",,30260406
915,More cores used than user limit,open,2017-03-15T22:47:07Z,2017-04-23T21:41:22Z,,NONE,"I have a mixture of single-threaded and multi-threaded applications running from PrimeGrid.  My computing preferences are set so I have 4 cores available:

```
Wed 15 Mar 22:23:50 2017 |  | max CPUs used: 4
```
The multi-threaded apps are set to use 4 threads and have the avg_ncpus and max_ncpus attributes set to 4.

In the initial configuration, 4 single-threaded tasks are running and 2 multi-threaded tasks are idle:

<img width=""390"" alt=""a"" src=""https://cloud.githubusercontent.com/assets/7913631/23973998/c9c75268-09d0-11e7-962b-ea24b86d0bcc.png"">

If I suspend one of the running tasks (so only 3 cores are being used), BOINC starts up one of the multithreaded tasks (thus using 7 cores, more than the limit of 4!)

<img width=""392"" alt=""b"" src=""https://cloud.githubusercontent.com/assets/7913631/23973999/c9cada82-09d0-11e7-98be-97272e28202f.png"">

If I resume the suspended single-threaded task, BOINC then pauses all the other single-threaded tasks, taking the total number of cores back down to 4 as expected.
<img width=""398"" alt=""c"" src=""https://cloud.githubusercontent.com/assets/7913631/23973997/c9bb1c00-09d0-11e7-8cc8-7144d56be843.png"">

I think that the correct behaviour should be that the multi-threaded is started, the other 3 single-threaded tasks should go into the waiting state, such that the total number of cores being used is always less than the specified limit.


","@ibethune , nice catch! Pretty easy to reproduce and thus to track and to fix. **Thank You!**",30260406
916,"Port Win client to UWP, put in MS app store",open,2017-03-06T21:48:36Z,2019-05-30T16:12:45Z,,CONTRIBUTOR,"MS is moving toward having users download apps only from their app store.
Such apps must be built as Universal Windows Platform apps.
This requires VS2015.

Info: https://docs.microsoft.com/en-us/windows/uwp/porting/desktop-to-uwp-root
Rom thinks that CreateProcess() may be missing from UWP.
However, see
https://social.msdn.microsoft.com/Forums/en-US/537da783-f8af-4ce4-853e-d68bd97e2e88/uwpdesktop-bridgecreateprocess-not-working?forum=wpdevelop","FYI
https://www.theverge.com/2019/5/30/18645609/microsofts-universal-windows-app-dead-microsoft-store-windows-store",30260406
917,Server status page inconsistency,open,2017-02-14T00:22:01Z,2017-04-12T12:34:11Z,,MEMBER,"Minor bug, but the server status page says ""Runtime of last 100 tasks in hours"" but if you look at the query:

https://github.com/BOINC/boinc/blob/479cbb61544ab6ce482349cdc74c860086309354/html/user/server_status.php#L390

its looking at all tasks, not just the last 100.",,30260406
918,BOINC+VirtualBox bundle installer should warn that it will temporarily disconnect the host from network,open,2017-02-03T21:33:51Z,2017-04-21T05:32:54Z,,CONTRIBUTOR,"VirtualBox' own installer has the following warning:

> Warning:
> 
> Network Interfaces
> 
> Installing the Oracle VM VirtualBox 5.0.32 Networking feature will reset your network connection and temporarily disconnect you from the network.
> 
> Proceed with installation now?

The bundle installer should have a similar warning so that people installing BOINC can decide if this is a good time for the network reset.",On the LHC project you don't need the vbox advanced networking components for it to work so I just dissable these options at installaion.  I don't know if other projects are the same but it could be possiable to just change the launch options for the installer.,30260406
919,Vboxwrapper uses Vbox 4.2 API even if have later version,open,2017-02-02T21:20:56Z,2018-10-17T13:40:18Z,,CONTRIBUTOR,"vbox_mscom_impl.cpp has the following:

```
    // Tweak the VM's USB Configuration
    //
    vboxlog_msg(""Disabling USB Support for VM."");
#ifdef _VIRTUALBOX42_
    rc = pMachine->get_USBController(&pUSBContoller);
    if (SUCCEEDED(rc)) {
        pUSBContoller->put_Enabled(FALSE);
    }
#else
    rc = pMachine->GetUSBControllerCountByType(USBControllerType_OHCI, &lOHCICtrls);
    if (SUCCEEDED(rc) && lOHCICtrls) {
        pMachine->RemoveUSBController(CComBSTR(""OHCI""));
    }
#endif
```

`_VIRTUALBOX42_` is always defined, so we're always going to use that API,
even if the host has a later Vbox version.
The choice of API should be run time, not compile time.",,30260406
920,BOINC may not use all CPUs in some cases,open,2017-01-29T09:06:10Z,2017-09-18T05:24:00Z,,CONTRIBUTOR,"I am cleaning up my work queue before next PrimeGrid challenge, and found case when BOINC Client does not run tasks on all available cores. Now I have 3 rosetta@home tasks running on 3 out of 8 available CPUs. There are also some ATLAS@Home and Cosmology@Home tasks waiting, but they require 7 or 8 CPUs per WU. Most projects are now set to not download new tasks, except for one with zero resource usage set. It looks that BOINC only checks if there are some other tasks available in the queue and do not try to download new ones from project with zero resource usage set when there are some downloaded tasks waiting. This is wrong, it should also check required CPU count for them and compare it with current free CPU count to eliminate cases like this.

I suspect that other similar cases may also exists, e.g. when some tasks are waiting but there is not enough memory to run them, please take a look on them too.

Windows 10 64bit,  BOINC 7.6.33

Edit: there is one more case. I suspended rosetta project and BOINC started crunching one Cosmology WU. It finished it and started ATLAS WU. It required more memory so it stopped working (status is Waiting for memory). Now BOINC does not use any CPU (except for small fraction reserved for GPU and NCI tasks), even if there are other Cosmology tasks ready to start.","My PC became VM unmanagable, here is sim with the required files, I didn't look to see if the SIM was blocked? https://boinc.berkeley.edu/dev/sim_web.php?action=show_simulation&scen=154&sim=0

Here is one with 24 job limit
https://boinc.berkeley.edu/dev/sim_web.php?action=simulation_form&scen=155",30260406
921,"Newer Nvidia GPUs with more than 4GB memory, report max 4GB memory under CUDA",open,2017-01-28T14:27:33Z,2020-03-22T00:47:04Z,,CONTRIBUTOR,"As reported on the [BOINC forums](https://boinc.berkeley.edu/dev/forum_thread.php?id=11430), with newer Nvidia GPUs with more than 4 gigabyte video memory, BOINC reports they have a max of 4096MB for CUDA. OpenCL detects the memory correctly. 

Checking https://github.com/BOINC/boinc/blob/master/client/gpu_nvidia.cpp, line 195, total memory is checked with an int. An int is 32bit, thus the maximum memory it can address is 4GB. In this case I think it needs a long long int here (referencing http://en.cppreference.com/w/cpp/language/types) to be able to detect over 4GB of memory (64bit). ",I added a workaround for this problem on Macs in 2013. See commits 631e236b08 and 4d74c5abbd (26 and 27 June 2013.) I hope you have found a better way to do this.,30260406
922,Please send notification to user that his/her host started producing invalid results,open,2017-01-25T18:59:26Z,2018-10-12T07:44:23Z,,CONTRIBUTOR,"It will be good to implement on server side some mechanism which will look for hosts which started producing invalid results. When this will happen, it should automatically send email or private message with information about this and kindly ask user to check that machine because it may be overheating or have some hardware issue. You could also resend this notification periodically if host will keep producing these bad results for longer period of time. This should help reduce number of invalid WUs returned to projects servers.

**Edit:** it would be good to show some warning on user's home page too, this page is visited more often than task or computers list. You could also show some warning or special icon on computers list, this would be helpful for users with multiple machines to quickly identify ones which produces bad results.",Detection of invalid results and user notification is a general issue. We can decide on the best way to implemented the notification. ,30260406
923,Add possibility to connect from one manager instance to multiple client instances simultaneously,open,2017-01-13T10:44:17Z,2017-04-14T23:10:17Z,,MEMBER,"It would be nice to have an ability to connect several client instances to one manager instance because it is quite hard to monitor several devices (7 in my case). 
Also it would be nice to have some kind of connection managing system which can automatically connect manager to known devices with saved passwords.
Of course this will require a huge changes both in architectural and interface parts.
So as from my POV this task can be splitted into several smaller tasks:
1 - Allow manager to be connected to several clients simultaneously.
2 - Change GUI to show all devices (e.g. in a tree view).
3 - Allow manager to configure all devices simultaneously.
4 - Allow manager to configure every device separately (e.g. different update task status time).
5 - Allow manager to save list of devices to connect them automatically with or without (on users choice) asking password to each of them (maybe add some kind of master password which can be used to encrypt/decrypt file with devices and their passwords) and manager start.
6- It would be nice to allow manager to connect to android clients also but this will require changes on Android client part too.

",This is possiable with BoincTasks from eFMer,30260406
924,combine power saving settings of Android,open,2017-01-12T10:31:19Z,2017-04-12T12:26:37Z,,MEMBER,"Request from Raistmer on [BOINC dev](http://boinc.berkeley.edu/dev/forum_thread.php?id=11401&postid=75138#75138).

There currently is a discrepancy of available preference settings between Android and non-Android in regard to power management. There are some pros and cons to this and reasons why it was implemented this way. See the discussion in the forum.

the gist is that it would be nice to have the same preferences on all platforms (because non-Android devices also have batteries). A possible structure could look like this:

- Suspend when computer is on battery (yes/no)
- Suspend computation if battery level is below (value)
- Suspend computation above battery temperature of (value)
- Start computation again if battery level is above (value)
- Start computation again if battery temperature is below (value)

If the first one is set to yes it would override every other setting. If it is set to no the other settings would be in effect. Therefore a better naming seems appropriate (always, thresholds).

The other settings control the hysteresis behavior we need on Android and can be configured to other behaviors. This would solve the currently hardcoded limit on battery power on Android. They should be clearly marked ""Advanced"" settings and maybe hidden away for non-expert users as wrong settings may damage the device.","API availability on non-Android OS:
Windows: [SYSTEM_POWER_STATUS structure](https://msdn.microsoft.com/en-us/library/windows/desktop/aa373232(v=vs.85).aspx) (BatteryLifePercent member)

Linux: newer kernels provide this info in /sys/class/power_supply/ see: [How to check battery status using terminal?](http://askubuntu.com/questions/69556/how-to-check-battery-status-using-terminal)

MacOS: Using the `ioreg` tool (need to check if this is part of a default installation). See: [How to get the battery life of mac os x macbooks programatically?](http://stackoverflow.com/questions/1432792/how-to-get-the-battery-life-of-mac-os-x-macbooks-programatically)",30260406
925,Android computing pref request,open,2017-01-12T00:19:52Z,2017-04-12T12:25:48Z,,CONTRIBUTOR,"(from a user email; I'm not sure this feature is desirable)

Is there any way that you could possibly implement profiles in the mobile client for charging vs battery?  I like to set BOINC to use 2 cores at 100% while charging but then set it down to 1 core at 20% while on battery.  I think it could potentially lead to more computation time if people can set it to run at a reduced rate while on battery and have it switch on it's own instead of having to go in and change the settings all the time.",,30260406
926,Liven up the forums with emoticons/smileys,open,2017-01-10T21:03:58Z,2017-04-12T12:25:07Z,,CONTRIBUTOR,"Is it possible to liven up the BOINC forums with its own group of emoticons/smileys built into the software? About all social media has smileys and emoticons, so why don't we? 

Possibly have them at the thread title (Post icon) to show the intent of the thread, but also have them in-line in the post. Not unlike the setup at the WCG forums. ","+1 for inline emoticons, they are used often. Emoticons for post title are not used often (people usually use default one), so they may be skipped.",30260406
927,stderr ouput is truncated because of problem with character encoding,open,2017-01-08T17:42:39Z,2017-04-12T12:24:37Z,,CONTRIBUTOR,"I have just tried to troubleshot problem with optimized TN-Grid app which could not start [(link)](http://gene.disi.unitn.it/test/result.php?resultid=4971772). Here is stderr output copied from that link:
```
<core_client_version>7.6.33</core_client_version>
<![CDATA[
<message>
couldn't start app: CreateProcess() failed - Odmowa dost
</message>
]]>
```
This message it truncated, it should be as below. It is in Polish language. it means ""access denied"".
```
couldn't start app: CreateProcess() failed - Odmowa dostępu
```
Such text truncation usually are sign of encoding problem, e.g. some invalid UTF-8 char was encountered, and app discarded this char and everything past it. I am not sure if problem is in BOINC itself, or it is related to Polish translation. Polish version of Windows uses CP-1250 encoding, maybe this is not specified somewhere?

I also wonder if DB is configured properly to store such chars.

If for some reason this would be hard to fix, you can change all native chars to their similar US-ASCII counterparts. In this case message would look in following way (ę is changed to e):
```
couldn't start app: CreateProcess() failed - Odmowa dostepu
```
",,30260406
928,BOINC Manager mistreats its windows on Windows 8/10,open,2016-12-28T11:31:22Z,2019-03-14T20:01:18Z,,NONE,"using 7.6.33;
how to reproduce:
1) open Manager
2) open Event log window
3) drop focus; click othe application's window
4) click BOINC window or select it from task-bar or whatever it is called
5) the other BOINC window than the one clicked/selected gets focus

right window may get focus when main window is selected,
but it always fail when selected Event log window.",https://drive.google.com/open?id=0B8jzqKlhXhyMT2I3cXZSSV9kazQ,30260406
929,"vboxwrapper Win 10 ""VM failed to enter an online state..."" errors",open,2016-12-18T22:02:43Z,2018-10-17T13:39:20Z,,MEMBER,"Hi folks, trying to track down this error which seems to be plaguing a number of users across BOINC projects running VM apps. Based on the jobs in our database at Cosmology@Home (which includes two weeks worth of results), the symptoms appear to be:

* User receives the message ""Postponed: VM Hypervisor failed to enter an online state in a timely fashion""
* stderr_log contains ""VM failed to enter an online state"" message
* About 50% of the time, the job is retried and finished successfully later 
* Not all jobs will have this error on a given host (on average, for hosts that see this error at all, 25% of their jobs will have it)
* This seems to be largely happening on Win 10 (especially 10.00.14393.00) 
* There's no super obvious client version / VBox version correlation (more data points might reveal something)

The error logs are a bit all over the place (plus many are truncated so stuff is often missing), but below is a list of some of the most common lines which appear amongst these logs (beyond the ""failed to enter an online state"" one which identifies jobs in the first place):

```
Status Report: virtualbox.exe/vboxheadless.exe is no longer running.
ERROR [COM]: aRC=E_ACCESSDENIED (0x80070005) aIID={f30138d4-e5ea-4b3a-8858-a059de4c93fd} aComponent={MachineWrap} aText={The object functionality is limited}, preserve=false aResultDetail=0
Power up failed (vrc=VERR_MODULE_NOT_FOUND, rc=E_FAIL (0X80004005)) 
ERROR [COM]: aRC=E_ACCESSDENIED (0x80070005) aIID={480cf695-2d8d-4256-9c7c-cce4184fa048} aComponent={Machine} aText={The object functionality is limited}, preserve=false                                                                                                                                 180
VMSetError: D:\tinderbox\win-4.3\src\VBox\VMM\VMMR3\PDMLdr.cpp(703) int __cdecl pdmR3LoadR0U(struct UVM *,const char *,const char *,const char *); rc=VERR_LDR_MISMATCH_NATIVE                                                                                                                            163
ERROR [COM]: aRC=VBOX_E_INVALID_OBJECT_STATE (0x80bb0007) aIID={480cf695-2d8d-4256-9c7c-cce4184fa048} aComponent={Machine} aText={The given session is busy}, preserve=false                                                                                                                               56
```



Here's the distribution of these errors across OS version (the way to read these tables is, e.g. the first row below says ""17% of the 750 hosts running Windows 10.00.14393.00 saw this error at least once)

OS|frac|tot
-|-|-
Windows (10.00.14393.00)|0.17|750
Linux 4.4.0-47-generic|0.17|6
Windows (06.02.9200.00)|0.08|12
Windows (10.00.10240.00)|0.08|13
Windows (06.03.9600.00)|0.06|99
Windows (10.00.10586.00)|0.05|86
Windows (06.01.7601.00)|0.03|543

Across Vbox version:

VBox|frac|tot
-|-|-
4.2.4|1.00|1
4.3.8|0.50|4
4.3.12|0.32|194
5.0.4|0.29|7
4.3.12_ZZZZ|0.26|19
4.2.12|0.25|4
4.2.16|0.24|63
4.3.10|0.20|5
5.0.0|0.20|5
5.1.10|0.10|175
5.0.24|0.08|12
5.0.30r112061|0.08|13
5.0.28|0.08|26
5.0.12|0.07|321
5.0.18|0.07|159
5.0.30|0.06|36
5.1.8|0.04|79
5.0.10|0.04|57
5.1.6|0.02|91

And client version:

Client|frac|tot
-|-|-
7.4.42|0.39|41
7.0.28|0.33|3
7.2.47|0.33|3
7.6.9|0.31|83
7.4.36|0.26|23
7.4.27|0.15|20
7.6.33|0.09|437
7.6.22|0.06|1041
7.2.42|0.05|82




See also some project specific threads:
* https://www.cosmologyathome.org/forum_thread.php?id=7443
* https://www.cosmologyathome.org/forum_thread.php?id=7439
* http://lhcathome2.cern.ch/vLHCathome/forum_thread.php?id=1754
* http://lhcathome2.cern.ch/vLHCathome/forum_thread.php?id=1880
* https://www.rechenkraft.net/forum/viewtopic.php?f=75&t=15980
* https://lhcathome.cern.ch/lhcathome/forum_thread.php?id=3967
* http://atlasathome.cern.ch/forum_thread.php?id=544","> You may even consider filtering those versions out of your searches. 

Right, for example, in my original post I show that 10% of 175 hosts running 5.1.10 are still seeing these errors. So again, its not as simple as I think you're trying to make it.",30260406
930,Please add new setting option - maximum numbers of vbox instances,open,2016-12-05T20:21:27Z,2019-03-14T20:01:06Z,,CONTRIBUTOR,"VirtualBox tasks needs more memory than other ones. Additionally they starts slowly, especially when few of them starts at the same time. I would like to limit number of vbox tasks which can run in parallel. Now it is only possible to apply such limit per app type using app_config.xml, but there is no way to set global limit. Please add new config option (probably to cc_config.xml) which will allow to set such limit.",,30260406
931,Checking conditions for a message display by BOINC manager,open,2016-11-28T13:58:38Z,2017-04-12T10:28:25Z,,NONE,"I have looked at [the implementation of the member function “CBOINCBaseFrame::OnAlert”](https://github.com/BOINC/boinc/blob/e6607a63402a2b4ce2d88588efa2c011ffae9b86/clientgui/BOINCBaseFrame.cpp#L244 ""Update candidate: CBOINCBaseFrame::OnAlert()"") once more.

1. A condition could be written a bit shorter there: `if (IsShown() && !(event.m_notification_only && pTaskbar->IsBalloonsSupported())) {`
2. I wonder about the check ""`!IsShown()`"" there. How should this statement ever fit when the opposite condition was checked before?","This seems to be a copy&paste leftover. One needs to find out what the intended purpose of the function is and adjust the whole function accordingly.

Although your version of the condition is a bit shorter it is not so clear on the first glance what it does. I rather like the current one where it is more clear.",30260406
932,Task stuck in uploading state after power outage,open,2016-11-06T19:39:53Z,2019-09-12T21:52:25Z,,CONTRIBUTOR,"I've just had an unexpected power outage. After recovering from it I found than one of SRBase tasks is stuck in ""uploading"" state, and there are no files listed on Transfers tab. I tried to abort it but this did not have any effect.

How can I remove this task, or somehow unstuck it?

Boinc 7.7.0, Linux CentOS 7

Edit: I removed project and added it again using new URL as I saw in logs, and after doing this that task is gone. However BOINC client should check for such stuck tasks during startup and do something with them.","This is a bug.
On startup, the client checks whether the files it thinks are there actually are
(i.e. the file exists and has the right size).
For downloaded files, if they're not there it just starts another download.
For uploaded files (output files) it should either re-run the job
or mark the job as an error (preferable).
It doesn't currently do either of these, and the job is in limbo
(this is my theory at least... I need to check it)",30260406
933,Android remote monitoring,open,2016-11-06T11:02:45Z,2019-07-27T12:22:56Z,,CONTRIBUTOR,"The communication between the BOINC client and the Android UI is done through a UNIX socket instead of the optional TCP socket as is the case on desktop devices. This prevents you from using tools such as BoincTasks to monitor an entire crunching farm. An option would be to switch to TCP sockets and allow the user to enable remote monitoring support.

I have briefly looked into this and ran into some issues with creating a TCP socket in the native BOINC application. If there is an interest for this feature I can pursue the work and issue a PR when/if I get something working.","Currently all platforms except Android use TCP/IP by default. Unix sockets
could be enabled on Linux but again, by default TCP/IP is used.

сб, 27 июля 2019 г. в 10:34, Adam Radocz <notifications@github.com>:

> Ok, Unix socket has some performance benefits too.
> One more question. How does to IPC implemented in Linux, Android, Mac, and
> Windows?
> Linux and Android use Unix socket. What do Mac and Windows use?
>
> —
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/1699?email_source=notifications&email_token=AAYVTIIUDRSP3ZKJV2SUOVLQBP3BZA5CNFSM4CVKDCOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD26GA2A#issuecomment-515661928>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAYVTIJAPOSEKCIFTQL4O53QBP3BZANCNFSM4CVKDCOA>
> .
>
-- 
Best regards,
Vitalii Koshura

Sent via iPhone
",30260406
934,BOINC RPC protocol is not encrypted,open,2016-10-31T14:00:06Z,2019-11-11T10:35:47Z,,CONTRIBUTOR,"I have just found that BOINC RPC protocol commands are sent in clear text. This is bad, it should be encrypted. We have to keep backward compatibility, so RPC clients like boinccmd should start session in clear mode and send special command to initiate TLS handshake, e.g. following one. RPC server after receiving it should reply with `<success/>` as usual and then start TLS handshake.  Something like this is implemented in SMTP protocol.

```xml
<boinc_gui_rpc_request>
    <start_tls/>
</boinc_gui_rpc_request>
```

Additionally new config option to disallow clear-text connections should be added. When it would be enabled, any RPC client which will connect and does not upgrade connection to secure before sending commands would get error like this:
```xml
<boinc_gui_rpc_reply>
    <error>TLS connection required</error>
</boinc_gui_rpc_reply>
```

Additionally you can also implement support for RPC clients which would automatically initiate TLS handshake after connecting, without need to send `<starttls>` command first. You could do this by checking if command starts with `<boinc_gui_rpc_request>`. If not, assume that it is beginning of a TLS handshake and call appropriate OpenSSL (or other) functions which will handle it.",Please make encryption for all BOINC related traffic mandatory!,30260406
935,make Mali GPUs usable (Odroid XU4),open,2016-10-24T19:13:04Z,2018-10-23T14:05:51Z,,CONTRIBUTOR,"I have Odroid XU4, which has OpenCL-capable GPU. It is detected by BOINC client:

```
5           2016-10-24 21:09:05 OpenCL: Mali-T628 0: Mali-T628 (driver version 1.2, device version OpenCL 1.2 v1.r9p0-05rel0.816303d14b549c8bed2bad5983436ff4, 1991MB, 1991MB available, 3 GFLOPS peak) 
6           2016-10-24 21:09:05 OpenCL: Mali-T628 0: Mali-T628 (driver version 1.2, device version OpenCL 1.2 v1.r9p0-05rel0.816303d14b549c8bed2bad5983436ff4, 1991MB, 1991MB available, 1 GFLOPS peak) 
```

I also see following entries in sched_request_*.xml files:

``` xml
    <coprocs>
<coproc>
   <type>Mali-T628</type>
   <count>1</count>
   <req_secs>0.000000</req_secs>
   <req_instances>0.000000</req_instances>
   <estimated_delay>0.000000</estimated_delay>
   <coproc_opencl>
      <name>Mali-T628</name>
      <vendor>ARM</vendor>
      <vendor_id>102760464</vendor_id>
      <available>1</available>
      <half_fp_config>63</half_fp_config>
      <single_fp_config>63</single_fp_config>
      <double_fp_config>63</double_fp_config>
      <endian_little>1</endian_little>
      <execution_capabilities>1</execution_capabilities>
      <extensions>cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_fp64 cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_fp16 cl_khr_gl_sharing cl_khr_icd cl_khr_egl_event cl_khr_egl_image cl_khr_image2d_from_buffer cl_arm_core_id cl_arm_printf cl_arm_thread_limit_hint cl_arm_non_uniform_work_group_size cl_arm_import_memory</extensions>
      <global_mem_size>2086998016</global_mem_size>
      <local_mem_size>32768</local_mem_size>
      <max_clock_frequency>600</max_clock_frequency>
      <max_compute_units>4</max_compute_units>
      <nv_compute_capability_major>0</nv_compute_capability_major>
      <nv_compute_capability_minor>0</nv_compute_capability_minor>
      <amd_simd_per_compute_unit>0</amd_simd_per_compute_unit>
      <amd_simd_width>0</amd_simd_width>
      <amd_simd_instruction_width>0</amd_simd_instruction_width>
      <opencl_platform_version>OpenCL 1.2 v1.r14p0-01rel0.0fe2d25ca074016740f8ab3fb451b151</opencl_platform_version>
      <opencl_device_version>OpenCL 1.2 v1.r14p0-01rel0.0fe2d25ca074016740f8ab3fb451b151</opencl_device_version>
      <opencl_driver_version>1.2</opencl_driver_version>
   </coproc_opencl>
</coproc>
<coproc>
   <type>Mali-T628</type>
   <count>1</count>
   <req_secs>0.000000</req_secs>
   <req_instances>0.000000</req_instances>
   <estimated_delay>0.000000</estimated_delay>
   <coproc_opencl>
      <name>Mali-T628</name>
      <vendor>ARM</vendor>
      <vendor_id>102760464</vendor_id>
      <available>1</available>
      <half_fp_config>63</half_fp_config>
      <single_fp_config>63</single_fp_config>
      <double_fp_config>63</double_fp_config>
      <endian_little>1</endian_little>
      <execution_capabilities>1</execution_capabilities>
      <extensions>cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_byte_addressable_store cl_khr_3d_image_writes cl_khr_fp64 cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_fp16 cl_khr_gl_sharing cl_khr_icd cl_khr_egl_event cl_khr_egl_image cl_khr_image2d_from_buffer cl_arm_core_id cl_arm_printf cl_arm_thread_limit_hint cl_arm_non_uniform_work_group_size cl_arm_import_memory</extensions>
      <global_mem_size>2086998016</global_mem_size>
      <local_mem_size>32768</local_mem_size>
      <max_clock_frequency>600</max_clock_frequency>
      <max_compute_units>2</max_compute_units>
      <nv_compute_capability_major>0</nv_compute_capability_major>
      <nv_compute_capability_minor>0</nv_compute_capability_minor>
      <amd_simd_per_compute_unit>0</amd_simd_per_compute_unit>
      <amd_simd_width>0</amd_simd_width>
      <amd_simd_instruction_width>0</amd_simd_instruction_width>
      <opencl_platform_version>OpenCL 1.2 v1.r14p0-01rel0.0fe2d25ca074016740f8ab3fb451b151</opencl_platform_version>
      <opencl_device_version>OpenCL 1.2 v1.r14p0-01rel0.0fe2d25ca074016740f8ab3fb451b151</opencl_device_version>
      <opencl_driver_version>1.2</opencl_driver_version>
   </coproc_opencl>
</coproc>
    </coprocs>
```

However this is not displayed on Computers page on BOINC project's websites. Please fix this.
","@sirzooro, is this still an issue?",30260406
936,Prefs flip back and forth when using account manager,open,2016-10-13T19:53:43Z,2017-08-18T07:21:54Z,,MEMBER,"If you have a host that is configured to use an account manager and you update your global preferences at a project website, then each time the host contacts the project servers, it will receive and start using the global preferences updated at the project website.  When it next syncs with the account manager, it will receive the prefs from the account manager and start using those.

This back and forth will continue until the preferences are updated at the account manager so that the mod_time field for the account manager prefs are more recent then the prefs from the project website.

This can be fixed in either of the following two ways:
- Modify the client in cs_scheduler.cpp CLIENT_STATE::make_scheduler_request() so that it sends a flag to the project indicating that the host is being managed by an account manager.  The server code in would also need to be modified in handle_request.cpp handle_global_prefs() so that this flag was read and if the flag was set, the project would not send its db prefs to the client.

or
- Modify the client so that if it is being managed by BAM and it receives global prefs from a project, it simply discards them and does not use them.
",This issue has plagued me since I signed up for BOINCstats BAM! to manage my work preferences almost 2 years ago. Any indication on when this might be addressed? I'm a bit tired of manually updating work preferences on each machine because my preferences keep getting overwritten by WCG.,30260406
937,Allow downloading new tasks when there are suspended ones in that project,open,2016-10-09T19:55:13Z,2017-04-12T09:49:03Z,,CONTRIBUTOR,"I found that BOINC does not try to download new tasks when there is some suspended task in project - I found following entry in log:

Not requesting tasks: some task is suspended via Manager

I would like to have an option to disable this. Few days ago I was participating in PrimeGrid challenge, and wanted to stop crunching PrimeGrid SoB task (it still needs over 5 days to complete), and resume it after challenge ends. Unfortunately BOINC does not allow to do so. Please add option which will allow to disable this behavior, sometimes it would be handy. Alternatively you can add some ""hard suspend"" command, this also would help in such case.

Suspended tasks should still count towards remaining work size, otherwise someone could exploit this to download tons of work from projects.
","Hmm, I did not know about this, I thought that BOINC Client sends this number in work request. So this issue should be resolved in different way. This could work in similar way as if resource share was set to zero - allow to download single task only if there is nothing to crunch, Or do it in a bit relaxed way, allow to keep 1 extra task per CPU if user wants to store some work in advance.

Update: people on my team's forum mentioned one more issue, suspended GPU tasks prevents downloading CPU ones and vice-versa. It would be good to enforce this limitation separately for CPU and GPU tasks.",30260406
938,Report Private Messages as abusive,open,2016-09-26T18:16:23Z,2017-04-12T09:47:47Z,,NONE,"When receiving an unsolicited private message, it is only possible to block an (ab)user from sending the receiver more PMs. The little red X should also allow for the option to report an abusive msg just like it is possible with posts. That way, cyber bullies and trolls will not be able to use PMs to continue their disrespectful ways since moderators will be capable of seeing a pattern of reported abusive PMs.
Just one of many ways to counter the growing Internet culture of hate.
",,30260406
939,BOINC stops processing on Fire 5th gen tablet,open,2016-08-19T20:24:53Z,2017-04-12T09:35:08Z,,CONTRIBUTOR,"(from a user):
For some reason BOINC stops crunching tasks in the Amazon Fire 7"" 5th Gen tablet. It seems that the tablet drops the Wifi connection and that is when BOINC stops crunching. What happens is that the task timers stops, and there are no progress for any tasks. The solution is to turn off the Wifi and turn it on again, and the tasks will continue crunching, but after a few hours, the problem starts again. This issue only happens in the Fire tablet as my other Android phones and tablets runs BOINC perfectly. I did a factory reset and it is only running BOINC but the problem still persist.

I am running BOINC 7.4.53
",,30260406
940,Run tasks natively on Docker,open,2016-08-18T08:57:28Z,2018-10-17T13:38:38Z,,NONE,"Hi everybody,
there's a way to run BOINC2DOCKER apps natively on Linux using Docker without installing Virtualbox?

Thanks.
","Those are just Docker containers running the BOINC client (which would be great to have something official nevertheless). This issue was about deliverying Docker workunits that would run natively on a volunteers computer who had Docker installed, as opposed to via Virtualbox as is currently possible. ",30260406
941,allow users to enable/disable proxy autodetection,open,2016-07-25T19:57:52Z,2017-04-12T09:12:22Z,,CONTRIBUTOR,"Some users have set up their systems to normally use a proxy but BOINC doesn't need to or shouldn't use the proxy. Currently on Windows, if the user has not set a proxy then BOINC will try to autodetect one.

BOINC should have an option to enable or disable proxy autodetection. It should be enabled by default to increase the chances of BOINC working automagically even in the presence of a proxy.

http://boinc.berkeley.edu/dev/forum_thread.php?id=11117
","I fixed the error.
Not sure it's worth polishing this feature; only 1 user requested it AFAIK.
",30260406
942,Manager: unify wxSingleInstanceChecker() lockfile,open,2016-07-19T12:22:51Z,2019-09-30T12:23:50Z,,MEMBER,"`wxSingleInstanceChecker()` uses a lockfile to determine if there is a already a Manager running. On Mac this lockfile has a custom name and is managed by BOINC. On Linux and Windows the file has a default name (""Boinc Manager"") and will be placed in the users home directory. If the Manager crashes this file is not deleted and prevents another instance from starting. This is observed on Linux mainly. The wx devlopers suggested to also use a custom filename and path for Linux and windows as is already there for Mac.

The relevant places are:
[BOINCGUIApp.cpp: CBOINCGUIApp::DetectDuplicateInstance()](https://github.com/BOINC/boinc/blob/8c11cad5b49952ec84309e1f5f9cd20fb41c82c0/clientgui/BOINCGUIApp.cpp#L742)
[BOINCGUIApp.cpp: CBOINCGUIApp::OnInit()](https://github.com/BOINC/boinc/blob/8c11cad5b49952ec84309e1f5f9cd20fb41c82c0/clientgui/BOINCGUIApp.cpp#L424)
[mac_installer/PostInstall.cpp: UpdateAllVisibleUsers](https://github.com/BOINC/boinc/blob/8604e8e0bdda499694e1040d5f0b86fe5f5fe4e1/mac_installer/PostInstall.cpp#L1534)

Also see the Debian bug: [#781789](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=781789)
","Then let's preserve the #3310 observations here.

https://boinc.berkeley.edu/forum_thread.php?id=13132 

**Steps To Reproduce**
1. Crash the system
2. or (sporadically) forget to use the menu to close the Manager window

**System Information (please complete the following information):**
 - OS: Linux - Ubuntu and Mint
 - BOINC Version: unstated and v7.16.1

Maybe someone will pay some attention to the backlog then.",30260406
943,Android: tethering,open,2016-07-10T18:28:38Z,2017-04-12T08:48:55Z,,CONTRIBUTOR,"I got a user email saying the BOINC doesn't support bluetooth tethering (i.e. routing IP traffic over bluetooth).  Is this an app-specific function?  Doesn't seem like it would be.
","> Is this an app-specific function?

You are correct, this is not an app-specific function, at least generally speaking. However, a badly written app can break this feature. It depends on what IP address an app binds to for outgoing connections. If it binds to `0.0.0.0` — as it should — then the IP stack should not only be able to figure out the route to the destination IP address but also which local NIC (in this case a tethered Bluetooth device) to use automatically. If the app binds to a specific local IP address of a NIC which does not happen to be a tethered Bluetooth device then obviously tethering over Bluetooth will not work.

I think that perhaps the user did not setup the router or NAT at the device tethered via Bluetooth. Tethering is not some sort of automagical technology. It still requires a working proxy router or NAT (in most cases).
",30260406
944,Minimised Manager window opens after closing last other app on desktop,open,2016-06-08T19:03:40Z,2019-08-27T00:07:11Z,,CONTRIBUTOR,"From [forums](http://boinc.berkeley.edu/dev/forum_thread.php?id=10995):

After using the Boinc Manager I can click on the close or minimize button and the window seems to close.

On closing the last other than BOINC window on my desktop; BOINC Manager window will open by itself. This will happen multiple times, seemingly without end.

OS: Ubuntu 14.04.3 LTS

---

Upgraded to Ubuntu 16.04 LTS yesterday w/ kernel 4.4.0

Still having the same prob, only now the BOINC Manager opens after any other window is closed.

---

14.04: only Advanced View (?)
16.04: both Simple and Advanced Views (?)

Seems to be some weird interaction between Manager and Unity; can't repro on Mint 17 (=Ubuntu 14.04) running Cinnamon.
","10-31-18: With all the machination going on with Linux DE types these days IMHO it is virtually impossible to fix  bugs that come and go with distro releases. Moreover, Ubuntu is moving away from Unity so I'm not sure anytime spent on debugging BOINC on Unity is warranted now.   I can verify that I cannot reproduce this behavior on a fresh install of Lubuntu 18.04 x64 with BOINC Manager 7.12.0 (x64) wxWidgets 3.0.4.  Note that Lubuntu 18.04 marks the intro of LXQt, replacing LXDE packaged with 17.10  and earlier.   It even displays project graphics correctly however  BOINC slideshow does not work.  

I suggest closing this out and limiting testing to whichever distros and versions the linux package manager is willing to support and debug. ",30260406
945,"""Show graphics"" option grayed out when gui_rpc_auth.cfg is set with password",open,2016-06-08T07:38:31Z,2019-06-28T18:40:56Z,,NONE,"When a password is set in /var/lib/boinc/gui_rpc_auth.cfg and I login to the BOINC Manager with host name 127.0.0.1 and the password set in gui_rpc_auth.cfg, the ""Show graphics"" option is grayed out in the Tasks tab. The ""Show graphics option is working when no password is set. Are there extra configurations needed to use a password and have the option to see the graphics?
# User/group associations

```
# id keesj
uid=1000(keesj) gid=1000(keesj) groups=1000(keesj),36(kvm),986(libvirt),978(boinc),135(mock),1001(docker)
```

```
# id boinc
uid=984(boinc) gid=978(boinc) groups=978(boinc)
```
# System info

```
# fpaste --sysinfo --printonly
=== fpaste 0.3.8.1 System Information (fpaste --sysinfo) ===
* OS Release (lsb_release -ds):
     ""Fedora release 23 (Twenty Three)""

* Kernel (uname -r ; cat /proc/cmdline):
     4.5.5-201.fc23.x86_64
     BOOT_IMAGE=/vmlinuz-4.5.5-201.fc23.x86_64 root=/dev/mapper/fedora_defiant-root ro rd.lvm.lv=fedora_defiant/root rd.lvm.lv=fedora_defiant/swap rhgb quiet pcie_aspm=force LANG=en_US.UTF-8

* Desktop(s) Running (failed: ""ps -eo comm= | grep -E
         '(gnome-session|startkde|startactive|xfce.?-session|fluxbox|blackbox|hackedbox|ratpoison|enlightenment|icewm-session|od-session|wmaker|wmx|openbox-lxde|openbox-gnome-session|openbox-kde-session|mwm|e16|fvwm|xmonad|sugar-session|mate-session|lxqt-session|cinnamon)'
         ""):
     N/A

* Desktop(s) Installed (ls -m /usr/share/xsessions/ | sed 's/\.desktop//g' ):
     gnome-classic, gnome

* SELinux Status (sestatus):
     SELinux status:                 enabled
     SELinuxfs mount:                /sys/fs/selinux
     SELinux root directory:         /etc/selinux
     Loaded policy name:             targeted
     Current mode:                   enforcing
     Mode from config file:          enforcing
     Policy MLS status:              enabled
     Policy deny_unknown status:     allowed
     Max kernel policy version:      30

* SELinux Error Count (failed: ""selinuxenabled && journalctl --since yesterday |grep avc: |grep -Eo
         ""comm=""[^ ]+"" |sort |uniq -c |sort -rn""):
     N/A

* CPU Model (grep 'model name' /proc/cpuinfo | awk -F: '{print $2}' | uniq -c |
         sed -re 's/^ +//' ):
     4  Intel(R) Core(TM) i7-4600U CPU @ 2.10GHz

* 64-bit Support (grep -q ' lm ' /proc/cpuinfo && echo Yes || echo No):
     Yes

* Hardware Virtualization Support (grep -Eq '(vmx|svm)' /proc/cpuinfo && echo Yes || echo No):
     Yes

* Load average (uptime):
      09:29:20 up 49 min,  1 user,  load average: 0.68, 0.87, 0.86

* Memory usage (free -m):
                   total        used        free      shared  buff/cache   available
     Mem:           7889        4796         384         982        2708        1718
     Swap:         10239          41       10198

* Top 5 CPU hogs (ps axuScnh | awk '$2!=21879' | sort -rnk3 | head -5):
          984  4228 12.2 15.5 1441372 1256588 ?     SNl  08:40   5:59 minirosetta_3.7
          984  3995 10.9  4.0 510528 325908 ?       SNl  08:40   5:21 minirosetta_3.7
          984  4154 10.7  4.5 551992 367736 ?       SNl  08:40   5:15 minirosetta_3.7
          984  4017 10.7  4.0 512312 326776 ?       SNl  08:40   5:17 minirosetta_3.7
         1000 32397  7.4  0.1 521140 11228 tty2     S+   08:54   2:38 chrome

* Top 5 Memory hogs (ps axuScnh | sort -rnk4 | head -5):
          984  4228 12.2 15.5 1441372 1256588 ?     SNl  08:40   5:59 minirosetta_3.7
          984  4154 10.7  4.5 551992 367736 ?       SNl  08:40   5:15 minirosetta_3.7
          984  4017 10.7  4.0 512312 326776 ?       SNl  08:40   5:17 minirosetta_3.7
          984  3995 10.9  4.0 510528 325908 ?       SNl  08:40   5:21 minirosetta_3.7
         1000 32379  3.9  3.3 1335372 269848 tty2   SLl+ 08:54   1:23 chrome

* Disk space usage (df -hT):
     Filesystem                      Type      Size  Used Avail Use% Mounted on
     devtmpfs                        devtmpfs  3.9G     0  3.9G   0% /dev
     tmpfs                           tmpfs     3.9G   18M  3.9G   1% /dev/shm
     tmpfs                           tmpfs     3.9G  2.1M  3.9G   1% /run
     tmpfs                           tmpfs     3.9G     0  3.9G   0% /sys/fs/cgroup
     /dev/mapper/fedora_defiant-root ext4       28G  8.2G   18G  32% /
     tmpfs                           tmpfs     3.9G  551M  3.4G  14% /tmp
     /dev/sda2                       ext4      283M  151M  114M  58% /boot
     /dev/sda1                       vfat      100M  8.3M   92M   9% /boot/efi
     /dev/mapper/fedora_defiant-home ext4      118G   49G   64G  44% /home
     /dev/mapper/fedora_defiant-var  ext4       79G   54G   22G  72% /var
     tmpfs                           tmpfs     789M   16K  789M   1% /run/user/42
     tmpfs                           tmpfs     789M   84K  789M   1% /run/user/1000

* Block devices (blkid):
     /dev/sda1: SEC_TYPE=""msdos"" LABEL=""efi"" UUID=""B22D-5C27"" TYPE=""vfat"" PARTLABEL=""EFI System Partition"" PARTUUID=""2b0c2b4d-c260-4ec0-a6d9-0bbcc7318e9f""
     /dev/sda2: LABEL=""boot"" UUID=""6101a78b-77f6-49cc-a917-aa43d25121e9"" TYPE=""ext4"" PARTUUID=""0bce51b4-2ef7-439f-906d-7eff5f82a0e3""
     /dev/sda3: UUID=""69Rq0Z-3ESj-Oosf-6hNW-4D4z-ZT16-wlGRYY"" TYPE=""LVM2_member"" PARTUUID=""8dd42424-2bca-44e3-9155-6b8054367f38""
     /dev/mapper/fedora_defiant-root: LABEL=""root"" UUID=""e286608a-f8f6-4ddf-8cc6-51c81a7fa6e0"" TYPE=""ext4""
     /dev/mapper/fedora_defiant-swap: LABEL=""swap"" UUID=""7d9696f0-d5a2-4a7d-a967-1f26537ceb71"" TYPE=""swap""
     /dev/mapper/fedora_defiant-home: LABEL=""home"" UUID=""6cd34bc3-4f32-43f2-a464-51a2a0265b5c"" TYPE=""ext4""
     /dev/mapper/fedora_defiant-var: LABEL=""var"" UUID=""5c7806d4-e9f2-40e4-9f20-1b526d6628ea"" TYPE=""ext4""
     /dev/loop0: UUID=""965760d6-0efa-494e-9fb4-6bf2e25a1bfb"" TYPE=""ext4""
     /dev/mapper/docker-253:3-264578-pool: UUID=""965760d6-0efa-494e-9fb4-6bf2e25a1bfb"" TYPE=""ext4""

* PCI devices (lspci):
     00:00.0 Host bridge: Intel Corporation Haswell-ULT DRAM Controller (rev 0b)
     00:02.0 VGA compatible controller: Intel Corporation Haswell-ULT Integrated Graphics Controller (rev 0b)
     00:03.0 Audio device: Intel Corporation Haswell-ULT HD Audio Controller (rev 0b)
     00:14.0 USB controller: Intel Corporation 8 Series USB xHCI HC (rev 04)
     00:16.0 Communication controller: Intel Corporation 8 Series HECI #0 (rev 04)
     00:16.3 Serial controller: Intel Corporation 8 Series HECI KT (rev 04)
     00:19.0 Ethernet controller: Intel Corporation Ethernet Connection I218-LM (rev 04)
     00:1b.0 Audio device: Intel Corporation 8 Series HD Audio Controller (rev 04)
     00:1c.0 PCI bridge: Intel Corporation 8 Series PCI Express Root Port 1 (rev e4)
     00:1c.3 PCI bridge: Intel Corporation 8 Series PCI Express Root Port 4 (rev e4)
     00:1c.4 PCI bridge: Intel Corporation 8 Series PCI Express Root Port 5 (rev e4)
     00:1d.0 USB controller: Intel Corporation 8 Series USB EHCI #1 (rev 04)
     00:1f.0 ISA bridge: Intel Corporation 8 Series LPC Controller (rev 04)
     00:1f.2 SATA controller: Intel Corporation 8 Series SATA Controller 1 [AHCI mode] (rev 04)
     00:1f.3 SMBus: Intel Corporation 8 Series SMBus Controller (rev 04)
     02:00.0 Network controller: Intel Corporation Wireless 7260 (rev 73)
     03:00.0 SD Host controller: O2 Micro, Inc. SD/MMC Card Reader Controller (rev 01)

* USB devices (lsusb):
     Bus 001 Device 003: ID 0a5c:5801 Broadcom Corp. BCM5880 Secure Applications Processor with fingerprint swipe sensor
     Bus 001 Device 002: ID 8087:8000 Intel Corp.
     Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
     Bus 003 Device 002: ID 413c:5534 Dell Computer Corp.
     Bus 003 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
     Bus 002 Device 006: ID 413c:81a3 Dell Computer Corp.
     Bus 002 Device 007: ID 046d:0823 Logitech, Inc.
     Bus 002 Device 005: ID 413c:2134 Dell Computer Corp.
     Bus 002 Device 004: ID 0c45:64d2 Microdia
     Bus 002 Device 003: ID 046d:c52b Logitech, Inc. Unifying Receiver
     Bus 002 Device 008: ID 24ae:2000
     Bus 002 Device 002: ID 413c:2513 Dell Computer Corp. internal USB Hub of E-Port Replicator
     Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub

* DRM Information (journalctl -k -b | grep -o 'kernel:.*drm.*$' | cut -d ' ' -f 2-
         ):
     [drm] Initialized drm 1.1.0 20060810
     [drm] Memory usable by graphics device = 2048M
     fb: switching to inteldrmfb from EFI VGA
     [drm] Replacing VGA console driver
     [drm] Supports vblank timestamp caching Rev 2 (21.10.2013).
     [drm] Driver supports precise vblank timestamp query.
     fbcon: inteldrmfb (fb0) is primary device
     [drm] Initialized i915 1.6.0 20151218 for 0000:00:02.0 on minor 0
     i915 0000:00:02.0: fb0: inteldrmfb frame buffer device

* Xorg modules (grep LoadModule /var/log/Xorg.0.log ~/.local/share/xorg/Xorg.0.log |
         cut -d \"" -f 2 | xargs):
     glx intel modesetting fbdev vesa fbdevhw dri2 present libinput

* GL Support (glxinfo | grep -E ""OpenGL version|OpenGL renderer""):
     OpenGL renderer string: Mesa DRI Intel(R) Haswell Mobile
     OpenGL version string: 3.0 Mesa 11.1.0 (git-525f3c2)

* Xorg errors (grep '^\[.*(EE)' /var/log/Xorg.0.log ~/.local/share/xorg/Xorg.0.log
         | cut -d ':' -f 2- ):
     /var/log/Xorg.0.log:[  2133.924] (EE) systemd-logind: ReleaseControl failed: Connection was disconnected before a reply was received

* Last few reboots (last -x -n10 reboot runlevel):
     runlevel (to lvl 5)   4.5.5-201.fc23.x Wed Jun  8 08:40   still running
     reboot   system boot  4.5.5-201.fc23.x Wed Jun  8 08:40   still running
     runlevel (to lvl 5)   4.5.5-201.fc23.x Tue Jun  7 08:24 - 16:56  (08:32)
     reboot   system boot  4.5.5-201.fc23.x Tue Jun  7 08:23 - 16:56  (08:32)
     runlevel (to lvl 5)   4.5.5-201.fc23.x Mon Jun  6 18:25 - 21:05  (02:40)
     reboot   system boot  4.5.5-201.fc23.x Mon Jun  6 18:25 - 21:05  (02:40)
     runlevel (to lvl 5)   4.5.5-201.fc23.x Mon Jun  6 16:11 - 16:18  (00:07)
     reboot   system boot  4.5.5-201.fc23.x Mon Jun  6 16:11 - 16:18  (00:07)
     runlevel (to lvl 5)   4.5.5-201.fc23.x Mon Jun  6 16:09 - 16:10  (00:01)
     reboot   system boot  4.5.5-201.fc23.x Mon Jun  6 16:09 - 16:10  (00:01)

     wtmp begins Mon Jun  6 10:06:34 2016

* DNF Repositories (dnf -C repolist):
     Last metadata expiration check: 0:45:17 ago on Wed Jun  8 08:44:03 2016.
     repo id                    repo name                                      status
     *fedora                    Fedora 23 - x86_64                             46,074
     google-chrome              google-chrome                                       3
     rpmfusion-free             RPM Fusion for Fedora 23 - Free                   692
     rpmfusion-free-updates     RPM Fusion for Fedora 23 - Free - Updates         268
     rpmfusion-nonfree          RPM Fusion for Fedora 23 - Nonfree                206
     rpmfusion-nonfree-updates  RPM Fusion for Fedora 23 - Nonfree - Updates      103
     *updates                   Fedora 23 - x86_64 - Updates                   19,400

* DNF Extras (dnf -C list extras):
     Last metadata expiration check: 0:45:18 ago on Wed Jun  8 08:44:03 2016.
     Extra Packages
     kernel.x86_64                           4.4.8-300.fc23             @updates
     kernel.x86_64                           4.4.9-300.fc23             @updates
     kernel-core.x86_64                      4.4.8-300.fc23             @updates
     kernel-core.x86_64                      4.4.9-300.fc23             @updates
     kernel-modules.x86_64                   4.4.8-300.fc23             @updates
     kernel-modules.x86_64                   4.4.9-300.fc23             @updates
     kernel-modules-extra.x86_64             4.4.8-300.fc23             @updates
     kernel-modules-extra.x86_64             4.4.9-300.fc23             @updates
     lightworks.x86_64                       12.6-1                     @@commandline
     tmux.x86_64                             2.2-1.fc23                 @@commandline

* Last 20 packages installed (rpm -qa --nodigest --nosignature --last | head -20):
     setroubleshoot-server-3.3.8.1-1.fc23.x86_64   Wed 08 Jun 2016 08:45:03 AM CEST
     setroubleshoot-3.3.8.1-1.fc23.x86_64          Wed 08 Jun 2016 08:45:03 AM CEST
     glib2-2.46.2-2.fc23.i686                      Tue 07 Jun 2016 02:42:00 PM CEST
     libcurl-7.43.0-7.fc23.i686                    Tue 07 Jun 2016 02:41:59 PM CEST
     glibc-2.22-17.fc23.i686                       Tue 07 Jun 2016 02:41:59 PM CEST
     ansible-lint-2.6.2-1.fc23.noarch              Tue 07 Jun 2016 02:41:59 PM CEST
     xemacs-filesystem-21.5.34-14.20160603hga561e02bb626.fc23.noarch Tue 07 Jun 2016 02:41:58 PM CEST
     libxkbcommon-x11-0.6.1-1.fc23.x86_64          Tue 07 Jun 2016 02:41:58 PM CEST
     libxkbcommon-devel-0.6.1-1.fc23.x86_64        Tue 07 Jun 2016 02:41:58 PM CEST
     ibus-libzhuyin-1.7.7-1.fc23.x86_64            Tue 07 Jun 2016 02:41:58 PM CEST
     glibc-devel-2.22-17.fc23.x86_64               Tue 07 Jun 2016 02:41:58 PM CEST
     glib2-devel-2.46.2-2.fc23.x86_64              Tue 07 Jun 2016 02:41:58 PM CEST
     fedpkg-1.23-2.fc23.noarch                     Tue 07 Jun 2016 02:41:58 PM CEST
     cryptsetup-1.7.2-1.fc23.x86_64                Tue 07 Jun 2016 02:41:58 PM CEST
     python3-urllib3-1.15.1-3.fc23.noarch          Tue 07 Jun 2016 02:41:57 PM CEST
     python3-requests-2.10.0-2.fc23.noarch         Tue 07 Jun 2016 02:41:57 PM CEST
     python2-urllib3-1.15.1-3.fc23.noarch          Tue 07 Jun 2016 02:41:57 PM CEST
     python2-requests-2.10.0-2.fc23.noarch         Tue 07 Jun 2016 02:41:57 PM CEST
     pyrpkg-1.44-1.fc23.noarch                     Tue 07 Jun 2016 02:41:57 PM CEST
     kmod-libs-22-4.fc23.x86_64                    Tue 07 Jun 2016 02:41:57 PM CEST

```
",graphics only work when the apps are running on the same machine as the manager,30260406
946,Display Buttons contents with three little dots,open,2016-06-07T16:40:13Z,2018-03-20T09:17:39Z,,NONE,"All the Boinc Manager buttons are non dynamically resized taking into account the text length. Consequently, it is even sometimes difficult to guess the missing words.

Here is a French example but all languages are probably impacted:

![capture d ecran 2016-06-07 a 18 04 15](https://cloud.githubusercontent.com/assets/19802944/15866497/29f6c31e-2cdf-11e6-890e-b7b702b724a2.png)

![capture d ecran 2016-06-07 a 18 04 43](https://cloud.githubusercontent.com/assets/19802944/15866499/2b51b4bc-2cdf-11e6-9d90-8335b16f5e95.png)
",Fixed in PR #2418. @DPhilippe4 thank you for reporting this.,30260406
947,Hide or gray out projects already added from the BOINC Manager Add Project wizard,open,2016-05-31T14:34:53Z,2019-11-04T22:14:34Z,,CONTRIBUTOR,"Is it possible to add to the Add Project wizard that it hides, or grayes out those projects that are already added.

Would that be easy to add? 
",,30260406
948,"Android problems: display orientation, no project list",open,2016-05-27T22:22:34Z,2018-10-17T13:37:26Z,,CONTRIBUTOR,"I just installed it on a Odroid, which is not a phone.  It fires off rotated 90degrees.  I even tried to pickup the little gizmo and spin it.  No joy.  Is there someway for me to fix that?  It also stops and asks me what projects I want to run, it doesn't seem to provide a list.  I guess I need to run some sort of a configuration for it.  I've been running nativeboinc for several years, but suddenly it refused to connect.
","@GITNE this is most likely a request via mail or from Google Play Store. No easy way to contact the OP.
",30260406
949,Client: commandline arguments do not override cc_config.xml,open,2016-05-09T12:43:22Z,2018-10-12T10:14:17Z,,MEMBER,"On Debian there is a default cc_config.xml that contains all options. When starting the client with commandline arguments that are also in the cc_config.xml they get ignored.

See: https://github.com/BOINC/boinc/blob/2581ac688d600423d64ce3e50a50d38439e3b8fc/client/log_flags.cpp#L290

Commandline arguments should override everything in cc_config.xml but should not get saved to cc_config.xml
","I get the problem, but don't know at the moment in what order these are read and executed. 
I think cc_config.xml is read during start-up, but after any command line entries are parsed, so its entries will be the last that are run.  Isn't is easier here to delete the cc_config.xml file? ",30260406
950,permanent upload error is not detected by Client,open,2016-04-04T13:52:08Z,2017-10-24T16:50:25Z,,MEMBER,"If the file upload handler reports a permanent error it seems that the Client does not recognize it. I think I tracked it down to [client/client_state.cpp#L1754](https://github.com/BOINC/boinc/blob/22678a4076f2c2aff6f8ca92f3df8d3ae3f6eaf8/client/client_state.cpp#L1754) where the result state is set to `RESULT_FILES_UPLOADED` regardless of the state of the uploads. The scheduler only checks this and the exit_status ([sched/sched_result.cpp#L393](https://github.com/BOINC/boinc/blob/22678a4076f2c2aff6f8ca92f3df8d3ae3f6eaf8/sched/sched_result.cpp#L393)) to determine if a result is good or bad. The first daemon to notice the missing result files is the validator.

Shouldn't the Client report a problem with the result so it gets marked as a problem before validation?
",Any progress on this bug? ,30260406
951,"Feeder is keeping ""didn't need"" results in shmem",open,2016-03-31T10:10:14Z,2017-04-11T15:55:36Z,,MEMBER,"The feeder does not purge results that where marked as ""didn't need"" (server_state=5,outcome=5) from the shmem array. This lead to the case where one result was unsent (and in shmem) but no client was assigned because a lot of other results for this appid where in the shmem segment but they were all in state ""didn't need"" and thus infeasible to be send out. The fastest way to clear this was to restart the feeder.

A better and more automatic solution would be that the feeder (or the scheduler) delete results from the shmem when it detects them as ""didn't need"" analogue to the detection of workunits with a non zero error_mask in the feeder. This does not have to happen often (once an hour?) as it is only really needed if there are ""didn't need"" results for an app.
",,30260406
952,VBoxWrapper not quickly updating status info anymore,open,2016-03-15T02:12:24Z,2018-10-17T13:37:18Z,,NONE,"I crunch long-running tasks for RNA World. Currently, I've got 2 different application versions (which use 2 different vboxwrapper versions), and they behave quite differently! The versions are:
- old: vboxwrapper v26167
- new: vboxwrapper v26184

The new application, appears to have some regressions. It is quite a bit more sluggish, as compared to the old.

Specifically, the new vboxwrapper:
- Only sends an < app_msg_receive > message every 10 seconds, instead of every 1 second like the old app
- Only updates the < current_cpu_time > and < fraction_done > within the < app_msg_receive >, about every 90 seconds, instead of every 10 seconds like the old app, thus making the UI not update often.
- Takes about half a minute longer to get the < remote_desktop_addr > message, to show the ""Show VM Console"" button as available.
- Takes about 10 seconds longer to exit the processes, when exiting BOINC.

Below is a snippet from my Event Log. Slots 17 and 11 are running the old vboxwrapper v26167, while slots 2 and 4 are running the new vboxwrapper v26184.

Can we please look into why the new vboxwrapper is so much more sluggish, and maybe fix it?

Thanks,
Jacob Klein

[20160314 VBoxWrapper Sluggishness.txt](https://github.com/BOINC/boinc/files/173163/20160314.VBoxWrapper.Sluggishness.txt)
",Yes. This is still an issue.,30260406
953,XML team exports contain mangled non-ASCII characters,open,2016-03-09T15:25:17Z,2019-02-27T18:57:28Z,,CONTRIBUTOR,"Current XML parsing, namely XML exports, uses iso-8859-1. Since UTF-8 is used for team information in BOINC projects, any non-ASCII characters become mojibake.

On a side note: switching to libxml2 would remove any XML problems once and for all including this one.
","The issue is caused by this line: [db_dump.cpp#L366](https://github.com/BOINC/boinc/blob/3d9538ca78a77c1866ed12b7698e179cb62a166b/sched/db_dump.cpp#L366). The encoding is set to iso-8859-1 without actually writing that file in that encoding. Quick fix is to change it to utf-8, as most servers in 2019 operate in utf-8. More permanent solution is to detect the encoding from mysql or  system locale.",30260406
954,Suggestion: improving forum posting and quoting ,open,2016-02-25T00:02:43Z,2017-04-11T15:48:23Z,,NONE,"When posting to the forums - currently there are three options (four if you include edit) when contributing to a thread.

A Reply does not read easy, it is not clear who you are replying to.

Quotes are often lazy used, and this is irksome - especially when reading on small displays.

Is it possible to
*1 add

>   [quote]In reply to: Agentb's message of 29 Feb 16 12:34:56UTC [/quote]

at the start of a reply or quoted message - The poster can of course edit this.

*2 replace the label on the current Quote button with ""QuoteAll"".

and

*3 An new ""QuoteLast"" button between Reply and QuoteAll buttons. 
This is like QuoteAll but instead only quotes only the remainder from last [/quote] found in the message being quoted.  

*4 A link to ""Netiquette"" somewhere obvious - possibly below Rules - where each forum can give their guidelines to good and bad posting.
","Spoiler tags (to reduce walls of text) would be a pretty decent addition to the forums too.
",30260406
955,BOINC doesn't work on latest Fire OS,open,2016-02-24T05:13:46Z,2019-07-28T15:58:20Z,,CONTRIBUTOR,"(user email)
My kindle fire hdx tablet computer has been running boinc with no problem, but today it was upgraded to the latest version of fire OS. Now boinc no longer works, and it is not available in the amazon app store. Since I can only download apps from the amazon app store, I cannot run boinc.
",Should be re-verified after new release update on Amazon store,30260406
956,Clients upload result files more than once,open,2016-02-08T13:48:57Z,2017-04-11T15:46:02Z,,MEMBER,"Some Clients are uploading result files more than once. Even after they reported the result as complete and it was validated. Those uploads I will call reuploads. This leads to data corruption on the project server as the BOINC server should make sure that result files that are validated should not change afterwards.

The initial fix to this was to make a result file readonly as soon as it is uploaded completely (be50103751667952b11793ffbadc354232e0b01d). This has triggered some other issues and there already was a discussion about the symptoms at ce00365f866e97ae48b1f41ba9608dff4c459ffe 

I'm currently investigating this and I'm in the process of gathering more data on those reuploads. The goal is to find the root cause for those reuploads especially those where in the reupload the Client reports a different filesize than in the first upload.
",We ave a fixed file upload handler running on Einstein@Home running for some time. I'm going to push that upstream when I have time.,30260406
957,parts of global_prefs get reported as variety in trickle up messages,open,2016-01-31T10:03:41Z,2018-07-03T17:19:18Z,,MEMBER,"I just found 26 entries in msg_from_host that accumulated over the last weeks that didn't have the correct variety of ""cpu_time"" instead they had the beginning of the global prefs XML or a something else stored (I deleted the newlines as they were messing with the layout).

```
| id     | create_time | hostid | variety                                                                                                                                                                                                                                                       | handled | xml                                                                                                                                                                                          |
| 433101 |  1435373824 |  33244 | <source_project>http://bam.boincstats.com/</source_project><mod_time>1406799579</mod_time>                                                                                                                                                                   |       0 | <result_name>cmsvm_GA-p[e30-50MB_Lin64f]_1_Oryza-sativa-Japonica-Group_CM000138.lin.EMBL_RF00177_SSU_rRNA_5_1330438623_83865_63</result_name><time>1435370520</time>         |
| 443986 |  1436319757 |  27843 | <source_project>http://bam.boincstats.com/</source_project><mod_time>1432043467</mod_time><venue name=""New: 2010-12-07 07:24:28""><ram_max_used_idle_pct>0</ram_max_used_idle_pct><work_buf_min_days>0.01</work_buf_min_days><cpu_usage_limit>100</cpu_us |       0 |       <result_name>cmsvm2_GA-p[e30-50MB_Lin64f]_1_Anopheles-gambiae-str.PEST_CM000358.lin.EMBL_RF00028_Intron_gpI_1330438623_1400_14</result_name>      <time>1436319500</time>         |
| 447532 |  1436637105 |  13748 | <source_project>http://gerasim.boinc.ru/</source_project>    <source_scheduler>http://gerasim.boinc.ru/sched</source_scheduler>    <mod_time>1404481426</mod_time>   <cpu_scheduling_period_minutes>120</cpu_scheduling_period_minutes>    <idle_time_to |       0 |       <result_name>cmsvm2_GA-p[e20-30MB_Lin64f]_1_Oryza-sativa-Japonica-Group_AP008214.lin.EMBL_RF00028_Intron_gpI_1349111823_52164_33</result_name>      <time>1436636371</time>        |
| 451898 |  1436980689 |  35203 | <source_project>http://bam.boincstats.com/</source_project><mod_time>1427008505</mod_time><venue name=""home""><ram_max_used_idle_pct>90</ram_max_used_idle_pct><run_on_batteries/><work_buf_min_days>0.2</work_buf_min_days><cpu_usage_limit>100</cpu_us |       0 |       <result_name>cmsvm_GA-p[e20-30MB_Lin64f]_1_Drosophila-melanogaster-(fruit-fly)_AE014297.lin.EMBL_RF00028_Intron_gpI_1349111823_13748_13</result_name>      <time>1436977561</time> |
| 478215 |  1439012242 |  36761 | windows_intelx86                                                                                                                                                                                                                                             |       0 |       <result_name>cmsvm_GA-p[e30-50MB_Lin64f]_1_Oryzias-latipes-(Japanese-medaka)_DG000007.lin.EMBL_RF00028_Intron_gpI_1330438623_94696_58</result_name>      <time>1439011888</time>   |
```

I don't know where this originates from but think this is another reason for a better XML parser like already discussed in #1470. It could be the client sending it wrong or the scheduler storing it wrong I didn't investigate.
","I fixed this by changing [sched/trickle_handler.cpp#L69](https://github.com/BOINC/boinc/blob/master/sched/trickle_handler.cpp#L69) like this:
```cpp
        retval = handle_trickle(mfh);
        if (!retval) {
            mfh.handled = true;
            mfh.update();
        } else {
            mfh.handled = 10;
            mfh.update();
        }
```
This will not delete the messages but at least the trickle handler will not choke every time this happens.",30260406
958,problem running on Acer Aspire One,open,2016-01-28T22:31:33Z,2018-10-17T13:36:58Z,,NONE,"The screen text for the Boinc screensaver does not display an some of the graphics do not show. This in no way affects the work done.

Hugh S. Myers
","I'll give that a shot—thanks for the pointer!

On Tue, Oct 24, 2017 at 9:33 AM, Jord van der Elst <notifications@github.com
> wrote:

> You can use the Icecream Screen Recorder
> <https://icecreamapps.com/Screen-Recorder/> to record the screen saver,
> then use something like https://handbrake.fr/ <http://Handbrake> to
> recompile the video to a smaller size and add it here.
>
> Or just describe in detail what you see, what you expect to see etc.
> Or you could try to use the camera on a (smart)phone to make a picture of
> the monitor when the screen saver shows and add that.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/1481#issuecomment-339051572>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AA5PvnFkteCfqrtoWrp8cMdcL_0F_YNaks5svhFXgaJpZM4HOwt1>
> .
>
",30260406
959,BOINC Monitoring daemon,open,2016-01-22T18:20:52Z,2017-04-11T15:37:33Z,,MEMBER,"We need to create a new daemon that can be run with an elevated account.  The purpose of this daemon is to collect various pieces of information not available to the BOINC client when run from an unprivileged account.

Basically this daemon would start and periodically check for updated information like CPU Temperatures and whether or not virtualization extensions have been enabled in the BIOS or not.  All the information should be exposed to the BOINC client via memory mapped files.

Basically we will need access to the rdmsr CPU instruction which is generally restricted to executing in ring 0.

Windows References:
http://www.securiteam.com/windowsntfocus/5TP0B2KC0K.html (Use of ZwSystemDebugControl with DebugSysReadMsr)

Generic References:

Determine if the virtualization instruction set is enabled in the BIOS:
http://bazaar.launchpad.net/~cpu-checker-dev/cpu-checker/trunk/view/head:/kvm-ok
","I added code to the client quite a long time ago which measures the temperature on Macs, but it has never actually been used, so it has not been tested recently. It is get_max_cpu_temperature() in hostinfo_unix.cpp lines 1004 - 1223 and does not require elevated privileges on the Mac.

Cheers,
--Charlie

On Jan 22, 2016, at 10:20 AM, Rom Walton notifications@github.com wrote:

> We need to create a new daemon that can be run with an elevated account. The purpose of this daemon is to collect various pieces of information not available to the BOINC client when run from an unprivileged account.
> 
> Basically this daemon would start and periodically check for updated information like CPU Temperatures and whether or not virtualization extensions have been enabled in the BIOS or not. All the information should be exposed to the BOINC client via memory mapped files.
> 
> Basically we will need access to the rdmsr CPU instruction which is generally restricted to executing in ring 0.
> 
> Windows References:
> http://www.securiteam.com/windowsntfocus/5TP0B2KC0K.html (Use of ZwSystemDebugControl with DebugSysReadMsr)
> 
> Generic References:
> 
> Determine if the virtualization instruction set is enabled in the BIOS:
> http://bazaar.launchpad.net/~cpu-checker-dev/cpu-checker/trunk/view/head:/kvm-ok
> 
> �
> Reply to this email directly or view it on GitHub.
",30260406
960,Add support for Windows 10 Battery Saver,open,2016-01-18T07:58:55Z,2020-01-20T13:28:14Z,,NONE,"Windows 10 OS contains feature called [**Battery Saver**](http://www.windowscentral.com/how-optimize-battery-life-windows-10-settings). It can be enabled when running on battery to conserve battery usage by lowering CPU/GPU frequency, stopping background tasks or lowering screen brightness.
It would be useful to add switch to the BOINC Manager to enable/disable computation when the device is on Battery Saver.
The Battery Saver status can be detected [using this API](https://msdn.microsoft.com/en-us/library/windows/desktop/dd405537).
",UPower solution is implemented in #3425 ,30260406
961,Suspend individual GPUs,open,2015-12-30T19:38:03Z,2019-03-14T20:00:22Z,,CONTRIBUTOR,"Add a feature for suspending individual GPUs
- manually
- when an exclusive app is running
- when computer is in use

This will involve both job scheduling and work fetch.
Eventually there should be a GUI; initially it could be done w/ config file.
",Yes yes yes! I want to snooze the intel GPU for web browsing and the nvidia for heavyweight stuff like games.,30260406
962,Make boinccmd output machine-readable,open,2015-12-22T22:33:35Z,2018-10-17T13:32:48Z,,CONTRIBUTOR,"If the output of boinccmd were machine-readable it would be easier to build systems for automating BOINC.  I suggest using pretty-printed JSON as the format; that would be human-readable too.  This would involve changing the various functions in lib/gui_rpc_client_print.cpp.
",,30260406
963,"Correct answer on a work request, when limit of tasks in progress is reached",open,2015-12-16T07:57:00Z,2017-04-11T15:25:40Z,,CONTRIBUTOR,"Just seen in 7.6.21

15/12/2015 02:10:10 | SETI@home | Requesting new tasks for AMD/ATI GPU
15/12/2015 02:10:10 | SETI@home | [sched_op] CPU work request: 0.00 seconds; 0.00 devices
15/12/2015 02:10:10 | SETI@home | [sched_op] AMD/ATI GPU work request: 167077.55 seconds; 0.00 devices
15/12/2015 02:10:13 | SETI@home | Scheduler request completed: got 0 new tasks
15/12/2015 02:10:13 | SETI@home | [sched_op] Server version 707
15/12/2015 02:10:13 | SETI@home | No tasks sent
15/12/2015 02:10:13 | SETI@home | No tasks are available for AstroPulse v7
15/12/2015 02:10:13 | SETI@home | Tasks for CPU are available, but your preferences are set to not accept them
15/12/2015 02:10:13 | SETI@home | Tasks for NVIDIA GPU are available, but your preferences are set to not accept them
15/12/2015 02:10:13 | SETI@home | Tasks for Intel GPU are available, but your preferences are set to not accept them
15/12/2015 02:10:13 | SETI@home | This computer has reached a limit on tasks in progress
15/12/2015 02:10:13 | SETI@home | Project requested delay of 303 seconds

The clincher is ""This computer has reached a limit on tasks in progress"", but instead of putting that out as answer to the work request, we first have the (confusing?) ""tasks are available for X but you don't want those"" messages.
Since this is a server decision to not send any further work because it has a maximum limit in place, can't we get a log alike this:

15/12/2015 02:10:10 | SETI@home | Requesting new tasks for AMD/ATI GPU
15/12/2015 02:10:10 | SETI@home | [sched_op] CPU work request: 0.00 seconds; 0.00 devices
15/12/2015 02:10:10 | SETI@home | [sched_op] AMD/ATI GPU work request: 167077.55 seconds; 0.00 devices
15/12/2015 02:10:13 | SETI@home | Scheduler request completed: got 0 new tasks
15/12/2015 02:10:13 | SETI@home | [sched_op] Server version 707
15/12/2015 02:10:13 | SETI@home | No tasks sent
15/12/2015 02:10:13 | SETI@home | This computer has reached a limit on tasks in progress
15/12/2015 02:10:13 | SETI@home | Project requested delay of 303 seconds
",,30260406
964,limit GPU usage to % value,open,2015-12-14T05:07:08Z,2018-11-01T21:10:04Z,,NONE,"Please add the ability to limit overall GPU usage to % value. I would contribute GPU if it didn't use 100% all the time. This would allow the GPU to perform tasks while watching netflix or such. I spend a great deal of time watching video with occasional gaming. I have 960 GTX that's close to idle most of the time, but I won't donate it's time if I can't enjoy my videos at the same time.

I've seen this request on multiple forums with no way to effectively control this.
","Although a separate GPU setting might be useful, CPU throttling should apply to GPUs as well.

However, it currently doesn't, because CPU throttling is disabled for apps that use a GPU.
The reason for this (commit notes from 2014) is:

- Don't throttle GPU apps.  GPU apps spend all their time in a
  critical section, during which they can't be suspended.
  They length of these critical sections (i.e. of GPU kernels)
  may be a significant part of a second, or more,
  so sub-second throttling isn't possible.

I'm not sure this is valid reasoning.
For starters, CPU throttling is not sub-second.
I think we should re-enable throttling of GPU apps,
see if there are problems, and if so try to solve them.",30260406
965,VM completion trigger file / VM shutdown race condition,open,2015-11-10T11:03:52Z,2019-05-23T00:00:44Z,,MEMBER,"If your VM both writes a completion trigger file _and_ shuts itself down shortly after, its possible to get vboxwrapper into a state where it hangs forever waiting to shutdown a VM which has already shut itself down. 

A simple workaround is just don't shut your VM down since the completion trigger file works fine (which is what I've done and hence don't have any more useful info to give on where in vboxwrapper the problem is). Leaving this issue here for record-keeping. 
",Is it still an issue?,30260406
966,Enhanced snooze features,open,2015-11-09T07:19:27Z,2019-03-14T20:00:03Z,,CONTRIBUTOR,"(from Jacob Klein)

1) How to set Snooze... User should be able to Snooze either all computing, or just GPU computing, or Network activity (so, can choose from all 3 activities)... with options ""1 hour"", ""3 hours"", ""6 hours"", ""12 hours"". There have been times when I wanted to snooze for a different time than default. The choices should be displayed as a cascading menu, that they must choose when setting snooze. They should be able to set it from the System Tray's right-click menu, OR set it the Advanced View Activity menu (as an option ABOVE ""Suspend"").

2) How to display Snooze times... For Advanced View, show it in the Status Bar at the bottom of the Window. It's possible that all 3 activities could have different snooze times, so you might need to show all 3 if they've been set. For Simple View... uhh.... Hmph. We should show the snooze times in the System Tray tooltip, while keeping the ""Suspend / Resume"" button the way it is currently on Simple View.
",Related to #1459 ,30260406
967,When in boinc only GPU is paused then I wish a corresponding tray icon,open,2015-10-27T17:23:46Z,2017-04-11T14:58:28Z,,NONE,"When in boinc only GPU is paused then I wish a corresponding tray icon similar to the one when boinc is completely paused. e.g. the letter G could appear before the pause sign ||
![unbenannt](https://cloud.githubusercontent.com/assets/6989214/10863157/47b39792-7fc4-11e5-8da2-3015ab787e51.png)
","We need someone who can decide the design of the icon. 
Then we can technically fit this in icon and xpm format.
",30260406
968,run while screensaver running,open,2015-10-21T16:04:39Z,2017-04-11T14:56:37Z,,CONTRIBUTOR,"(from boinc_android_testing):
I've grown rather fond of the Daydream screensaver in newer Android releases, but it has been eating into my BOINC credit, as I keep the ""Pause computations while screen is on"" setting enabled, to keep my device nice and snappy. However, I really don't need performance for Daydream mode, and it would be nice to see a setting that would allow computations while in Daydream mode, regardless of the current ""Pause computations while screen is on"" setting.
",,30260406
969,project preference to toggle virtualbox usage,open,2015-10-21T13:35:47Z,2017-07-02T20:30:20Z,,MEMBER,"On the project specific preference page there currently are checkboxes for:
- Use CPU  
- Use ATI GPU  
- Use NVIDIA GPU  
- Use Intel GPU

It would be nice to have a 
- Use VirtualBox (if installed)

checkbox too. This is only visible if the project has a virtualbox application and should contain a link where to download the recommended version of VBox. This will toggle the use of VirtualBox for this project only.

It would be nice to have this in the global preferences too.
","Another artifact of this mixed senario is I'm naged with messages:

Message from server: VirtualBox is not installed	

",30260406
970,project preference to set number of used cpu's,open,2015-10-20T08:24:49Z,2017-08-17T21:43:57Z,,CONTRIBUTOR,"Users of yafu@home and RNA World requests an option to limit the number of used CPU for multi core systems and multi threaded apps per project.
E.g. they want to use on a 8 core system only 6 cores for yafu@home and use the remaining 2 cores for other boinc projects.
",Is this still being asked for? ,30260406
971,Suggestion for a RPC for reporting invalid (and other status) tasks,open,2015-10-20T04:56:30Z,2017-04-11T14:54:10Z,,CONTRIBUTOR,"This proposal started from a thread posted on the boinc Q&P User Forum here
http://boinc.berkeley.edu/dev/forum_thread.php?id=10539&postid=64913#64913
## Current issues:
- Users only see invalid results if they _regularly_ visit _all_ their
  project websites and check their tasks
- Task are often purged from the user visible data so no long term
  record is visible
- Invalid tasks waste more host and server resources (power, bandwidth
  etc) on average than error tasks (they take longer)
- Project administrators may not have time to advise users about hosts
  which deliver bad results
- The boinc manager program(s) currently report nicely on historical RAC
  values but not so well on task reliability and host utilization.
- RAC values are slow to respond to changes (by design)
- There is an RPC for getting _pending_ tasks (project/pending.php)
  documented here https://boinc.berkeley.edu/trac/wiki/WebRpc
- There is no RPC for getting _errors_ or _invalid_ tasks
## Existing php which is similar: results.php

The existing project/results.php (and the only mechanism available for
users to see invalid tasks) accepts parameters

hostid=<host_id> or userid=<user_id>     # must match authentication
offset=<number_from_start>               # default 0
show_names=<0|1>                         #
state=4                                  # 4 is invalid, 0 default
matches all states
appid=0                                  # 0 default matches all apps

it then displays a maximum of 20 tasks one per row in a HTML table,
and the user has to follow links to the next page of 20 results.

Example
http://einstein.phys.uwm.edu/results.php?userid=<user_id>&offset=0&show_names=1&state=4&appid=0

Shows my first page of E@H invalids
## Suggestion:

Create a similar RPC to pending.php called say resultsRPC.php
         which would
     accept the parameters (except offset above) like results.php,
     then deliver an XML file like pending.php RPC with task data
## Benefit:

The resultsRPC could be used in a number of ways in the future

Future versions of boinc or the boincmgr could use it to
- put an alert notice if a new error detected recently
- log the event, like the job_<project>.txt files
- put a last week scorecard showing host/user efficiency
- report user/host health statistics (like RAC/Credit statistics tab)

Users could write scripts using for alerts on errors and invalids,
helpful for ""farmers"" with headless hosts to get a daily email with new
invalids and errors from the farm.

Project Managers could also use this RPC across users to monitor app
performance trends after changes.
## Detail

Select result records based on parameters
    state, appid, hostid, userid (like results.php)

and these new optional parameters

```
max_results=<maximum_number_results>     # default 1000, the project
                                         # should be able to set an
```

upper limit
    sent_end_time=<epoch_date_value>         # default to ""now""
    sent_time_interval=<number_of_seconds>   # default to 2419200 (28 days)

to filter results returned.

Then produce (like pending.php) an XML file with one or two tables based
on optional formatting parameters

format=xml                               # placeholder defaults to xml
report_results=<0|1>                     # default 1 = report results table
report_totals=<0|1>                      # default 1 = report totals table

-XML file-----------------------------------
<results_table>
   ... data values matching the results.php web page output,
       any time/date output in epoch seconds.
</results_table>

<results_total_table>
   ... which simply counts or sums the column, where that makes sense.
    (eg. count_of_results, Runtime, CPUtime, Claimedcredit, Grantedcredit)

</results_total_table>
",,30260406
972,evaluate a library to parse program options,open,2015-10-08T20:00:30Z,2017-04-11T14:48:23Z,,MEMBER,"Find a library that helps with parsing program options. Examples would be [GNU getopt](http://www.gnu.org/software/libc/manual/html_node/Parsing-Program-Arguments.html) or [Boost.Program_options](http://www.boost.org/doc/libs/1_59_0/doc/html/program_options.html) or something similar.

Requirements:
- must support short options with (-o value) and without (-v) arguments
- must support long options with (--option value) and without (--help) arguments
- long options do not need to have short options (but it could be possible in the future)
- must support detached sources like the trickle_handler framework: see [trickle_handler.cpp](https://github.com/BOINC/boinc/blob/master/sched/trickle_handler.cpp) and [trickle_deadline.cpp](https://github.com/BOINC/boinc/blob/master/sched/trickle_deadline.cpp)

Please comment on this issue for questions or comments.
",,30260406
973,Automatic classification of help requests/responses.,open,2015-10-07T18:31:03Z,2017-04-11T14:48:03Z,,CONTRIBUTOR,"Admins and moderators and ""help forums"" personnel get repeated requests to answer the same questions.  It would be feasible to use an existing bayesian classification software package to classify the questions by topic and provide automated replies or a pointer to the correct thread for the question.
",,30260406
974,Private message responses through email...,open,2015-10-07T18:26:21Z,2017-04-11T14:47:28Z,,CONTRIBUTOR,"It would be nice for Moderators/Admins to be able to respond to a PM via email.  A crypographic hash tag in the response would be sufficient to provide the authentication of the source.
","I guess there would need to be an sendmail/procmail/whatevermail filter that would parse the message and enter the message into the database.  I'd write it myself if I had the time.  I may even though I don't.  
",30260406
975,Memory Access & IO Priority,open,2015-09-25T22:11:56Z,2018-10-17T13:32:12Z,,NONE,"## Scenario

Usually i have boinc running at max 90% CPU core speed, entering idle mode when my other processes go up 30%. Sometimes, when i'm working with my vm (2 some times) the computer with boinc running gets really sluggish. I noticed that this was mainly because boinc was running applications that made use of vm's too. One time i had 4 vm's running at the same time, which is overkill for my 8GB of RAM, and mainly, the the Main Storage Drive (that couldn't handle all the IO).
## Idea

Knowing that boinc starts processes with CPU priority by default:low i don't know if this applies also to IO access priority.
- If it does not, it would be of interest to change this.
- If it does (most prob.), a mechanism to halt applications consuming absurd amounts of Bus Bandwidth (or all boinc applications) would be very useful, resuming when IO went back to normal.

This would be a mechanism similar to the IDLE/ACTIVE used for CPU usage.
I think that there is no thread like this.
",Reading disk IO utilization is very cumbersome on Linux. And even tools like ionice and cgroups do not work all the time. I'm keeping this in case someone wants to dig more into it but this is probably a time sink.,30260406
976,Client: remove files from deprecated apps,open,2015-08-31T17:56:17Z,2017-04-11T14:37:41Z,,CONTRIBUTOR,"Add a mechanism for removing files from deprecated apps.  This has 2 parts:

scheduler: send list of all non-deprecated apps, and a flag saying so

client: when process scheduler reply, mark app versions that don't belong to a current app, and garbage-collect their files.
",,30260406
977,"[pre][/pre] containers in [code][/code] containers on BOINC forum code execute, instead of show the container codes",open,2015-08-12T18:14:35Z,2017-04-12T18:47:39Z,,CONTRIBUTOR,"While teaching someone how to use all the tag containers, I added [pre] .. [/pre] in a code container ([code] .. [/code]) and found that it's executed inside the code container.

It shows as a code box, but then with  

```
<div class=""pre"">..</div>
```

in the post preview and in the post.
I expect that it shows [pre]..[/pre] in the code container.

Is someone able to fix this? 
","The same function (output_transform(), in text_transform.inc)
is used for preview and final version.

On 4/12/2017 5:57 AM, Christian Beer wrote:
>
> Ideally the same parser is used for preview and afterwards. I think that can be 
> fixed together with the [code] tag issue.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub 
> <https://github.com/BOINC/boinc/issues/1381#issuecomment-293567338>, or mute the 
> thread 
> <https://github.com/notifications/unsubscribe-auth/AA8KgbD7oCcqxDXjsBryYwO3HDRG19Irks5rvMpFgaJpZM4FqZ3o>.
>

",30260406
978,GPU Temperature Monitoring,open,2015-08-06T17:14:50Z,2017-10-08T04:25:50Z,,CONTRIBUTOR,"I'd like to see GPU temperature monitoring and temperature limits in the desktop client similar to the battery temperature limits seen in the Android client. This may require an additional package or library to be installed.  Response to overtemp could be a pause in processing, or underclocking the GPU.
","Maybe Windows can use this
`https://github.com/openhardwaremonitor/openhardwaremonitor`",30260406
979,CPU temperatures monitoring,open,2015-08-06T17:12:30Z,2020-02-10T00:31:48Z,,CONTRIBUTOR,"I'd like to see CPU temperature monitoring and temperature limits in the desktop client similar to the battery temperature limits seen in the Android client.  This would probably require an additional package or library to be installed (i.e. lm-senors on linux).
",I use `watch -n 5 sensors` in Linux Mint to monitor CPU temps in a terminal. I only do it when I'm troubleshooting a rig that's acting up. Sure would be nice to have it as a column in BoincTasks and even better the upcoming BOINC_Fleet_Manager :1st_place_medal: ,30260406
980,port to iOS,open,2015-08-02T20:40:48Z,2019-06-30T17:15:55Z,,CONTRIBUTOR,"Port the BOINC client to iOS, and develop a GUI similar to the Android GUI.
Before doing this we'd need to
1) get Apple's approval, since their App Store rules seem to disallow what BOINC does.
2) Figure out how to port project apps to iOS without requiring projects to
become registered Apple developers.
",Be sure to review [Apple's App Store review guidelines](https://developer.apple.com/app-store/review/guidelines/) and [this overview of the approval process](https://developer.apple.com/app-store/review/) before you put a lot of effort into this so you don't waste too much time. I am quite certain that you must be a member of the Apple Developer program to submit an app to the store.,30260406
981,Generalize exclusive apps,open,2015-07-31T19:23:41Z,2019-03-14T19:59:47Z,,CONTRIBUTOR,"Have a single list of exclusive apps.  For each app have checkboxes for
- whether to suspend all computing
- whether to suspend GPU computing
- whether to suspend file transfers

This would be a medium-size change (few 100 line) change to both manager and client.
",Might be nice to have in a next BOINC (7.10),30260406
982,bin/update_versions silently fails for app versions with explicit plan classes if other invalid versions exist in the same version folder,open,2015-07-07T21:17:15Z,2017-04-11T14:29:45Z,,MEMBER,"bin/update_versions correctly finds and adds `APPNAME/1.0/x86_64-pc-linux-gnu` in these cases:

```
APPNAME/
    1.0/
       A_NOT_VALID_APP_VERSION/
       x86_64-pc-linux-gnu/ 
```

and

```
APPNAME/
    1.0/
       x86_64-pc-linux-gnu__vbox_mt/ 
       Z_NOT_VALID_APP_VERSION/
```

but fails for, 

```
APPNAME/
    1.0/
       A_NOT_VALID_APP_VERSION/
       x86_64-pc-linux-gnu__vbox64_mt/ 
```

Here `A_NOT_VALID_APP_VESION` is just some folder which is not a valid app version either because the name is wrong or its contents is wrong, etc... Note the alphabetizing matters, and its only when there is an explicit plan class specified.  I have not checked with any version besides `x86_64-pc-linux-gnu` or plan class other than `vbox64_mt`. 

I'd say this might not even be worth a bug fix, but the fact that it actually works if the bad folder is alphabetically afterwards is at the very least extremely confusing and caused me to loose a lot of time figuring this out. 
",,30260406
983,NUMA-awareness,open,2015-03-19T16:10:39Z,2019-10-29T14:39:43Z,,NONE,"BOINC client needs to be NUMA-aware to support systems with multiple CPU Groups (NUMA nodes).
- Manage pool of all client processes and explicitly assign Group Affinity via SetThreadGroupAffinity.

See: http://boinc.berkeley.edu/dev/forum_thread.php?id=10124
","There's still an issue to be addressed: Windows assigns jobs to processor groups round-robin, which may be highly non-optimal, especially if some of the jobs are multithread.  If it turns out that Windows lets you assign jobs to processor groups (which I have yet to verify) then BOINC should do this in an intelligent way, as sketched above.",30260406
984,memory and priority idea,open,2015-02-26T15:52:15Z,2018-10-17T13:30:49Z,,NONE,"See http://lists.ssl.berkeley.edu/pipermail/boinc_dev/2015-February/021622.html
",Can you please reopen it?,30260406
985,Feature Request: additional Proxy behaviour,open,2015-02-11T10:09:31Z,2017-04-11T14:22:07Z,,NONE,"If proxy ist set, but the proxy cant be accessed try without proxy and if this does not work drop a error note like before.
","Any update?
",30260406
986,Manager window brought to foreground when focused,open,2015-02-04T06:29:14Z,2017-04-11T14:16:23Z,,MEMBER,"**Reported by Nicolas on 17 Mar 45021482 09:43 UTC**
I use focus follows mouse on Linux (KWin4 window manager).
Moving the mouse over a window makes it get the focus,
without bringing it to the foreground.

However, the BOINC Manager jumps to the foreground
when I give it focus that way,
ie. when I move the mouse over its unfocused window.
It's the only application I have that does this.
Audacity, which is also based on wxWidgets,
doesn't have this behavior.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1376
",,30260406
987,BOINC should push projects to use HTTPS for better security,open,2015-02-04T06:28:33Z,2019-11-11T10:35:37Z,,MEMBER,"**Reported by bryanquigley on 12 Nov 44964364 11:49 UTC**
Right now all downloads happen over HTTP and AFAICT the only protection is code signing and the sandbox.  HTTPS projects would have another layer protecting against MITM attacks.

Code signing won't protect against tampering with the data and/or results back to the project.  Tampering with the data as it goes to the client could lead to an exploit.

In all I'm just asking if BOINC can change to explicitly recommend using HTTPS for the project website and project URL. 

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1374
",Please make encryption for all BOINC related traffic mandatory!,30260406
988,account for max_concurrent in work fetch,open,2015-02-04T06:28:12Z,2017-08-17T21:18:41Z,,MEMBER,"**Reported by davea on 22 Sep 44922147 23:20 UTC**
Work fetch doesn't take max_concurrent into account, leading to idle devices in some situations.  Need to:
1) use max_concurrent in round-robin simulation
2) in work fetch, skip projects that are blocked on max_concurrent

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1373
",@davidpanderson any updates? ,30260406
989,clean up admin DB code,open,2015-02-04T06:27:52Z,2017-08-17T21:17:39Z,,MEMBER,"**Reported by davea on 16 Apr 44762362 03:33 UTC**
The admin web code (html/inc/db_ops.inc) is a mess.
Suggested changes:
- don't use $_GET to pass args
- eliminate hardwired constants (use common_defs.inc)
- eliminate the SqlQueryString class
- eliminate mysql_\* calls (use BoincDb)
- wherever we show a count of results, make it a link so you
  can drill down and see the results with those properties
- we already have code (in results.inc, user.inc etc.) to show entities
  as tables.  Use this instead of having separate versions
  (which are now somewhat out of date, and don't show all fields)
- The ""result summary"" stuff, rather than being a big matrix of links,
  should have menus for app, duration, and breakdown type.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1372
",Wasn't there a discussion about this not too long ago? ,30260406
990,proxy issue on UI,open,2015-02-04T06:24:26Z,2018-10-31T13:15:41Z,,MEMBER,"**Reported by Philip Lourandos on 4 Jun 44622500 23:22 UTC**
OS: windows 7 enterprise

I need to change my password due to security policies for Active Directory. I changed my password to: Qmv<OZ%Q
I updated the proxy settings via tools -> options -> HTTP Proxy.
I entered the full password above. I can see 8 masked characters in the text field. I press 'OK' and the dialog closes. I go back and I can only see 3 masked characters. The knock on effect my account was being locked out due to authentication failures by boinc.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1362
","10-31-18; I'll need some guidance on how to properly test this. The original bug filing included interaction with Active Directory (presumably MS?) and I do not have the ability to set that up, if it is required.  ",30260406
991,GPU-specific exclusive apps,open,2015-02-04T06:24:06Z,2017-04-11T13:59:55Z,,MEMBER,"**Reported by davea on 7 Aug 44506374 20:47 UTC**
Allow GPU exclusive apps to turn off computation on a subset of GPUs

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1361
",Needs #1149,30260406
992,use standard targets for unix build process,open,2015-02-04T06:22:23Z,2018-10-11T13:28:29Z,,MEMBER,"**Reported by ChristianB on 18 Jan 44414593 10:40 UTC**
Following the GNU convention for [makefile targets](http://www.gnu.org/prep/standards/html_node/Standard-Targets.html#Standard-Targets) I ran some tests with a fresh copy of the boinc-v2 source. I identified
several files that can be ignored and get deleted by clean/distclean and files that should be deleted by clean/distclean but are not.

Here is a short summary of the convention:

Files created by ./_autosetup need to be deleted in case of a system
upgrade because most likely the tools tested are updated. Those files
are not deleted via Makefile because it is a step before Makefile's exist.
Maybe adding an option to _autosetup that clears the files before
running the tests could help in this case.

What's important for building is that after running ""make clean"" we
should have the same files as after running ""./configure"" and after
running ""make distclean"" we should have the same files as after
""./_autosetup"".

The attached textfile contains all those files in gitignore style and
surely some of them should go into .gitignore.

Could someone who wrote the _autosetup and/or build stuff enhance clean
and distclean please?

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1357
","Partly implemented in 977b1414727acc142a02ec4a89e172c2ef831041

remaining files from the textfile in the original ticket:
```
## should be deleted by _autosetup itself
*.in
aclocal.m4
autom4te.cache/
compile
config.guess
config.sub
configure
depcomp
install-sh
ltmain.sh
m4/libtool.m4
m4/ltoptions.m4
m4/ltsugar.m4
m4/ltversion.m4
m4/lt~obsolete.m4
missing
test-driver

### files created by configure but NOT cleaned by 'make distclean':
clientgui/res/Makefile
packages/generic/sea/Makefile
packages/solaris/CSW/Makefile
packages/solaris/CSW/boincclient/Makefile
packages/solaris/CSW/boincclient/pkginfo
packages/solaris/CSW/boincclient/prototype
packages/solaris/CSW/boincdevel/Makefile
packages/solaris/CSW/boincdevel/pkginfo
packages/solaris/CSW/boincdevel/prototype
packages/solaris/CSW/boinclibs/Makefile
packages/solaris/CSW/boinclibs/pkginfo
packages/solaris/CSW/boinclibs/prototype
packages/solaris/CSW/boincmanager/Makefile
packages/solaris/CSW/boincmanager/pkginfo
packages/solaris/CSW/boincmanager/prototype

### files created by make but NOT cleaned by 'make clean':
api/libboinc_api.a
api/libboinc_graphics2.a
api/libboinc_opencl.a
client/boinc
py/lib.linux-x86_64-2.7/
sched/libsched.a
svn_version.h
zip/libboinc_zip.a
```",30260406
993,UI feature: statistics / achievements / sharable,open,2015-02-04T06:20:39Z,2019-06-27T14:14:26Z,,MEMBER,"**Reported by Joachim on 14 Sep 44351388 06:26 UTC**
The idea is to collect (or investigate whether the BOINC client is collecting this information already) run statistics (e.g. how many hours of computing) and have a easy to understand Statistics screen.

Based on this information, achievements could be issued like, ""silver Android"" for a certain number of hours.

Those achievements should be sharable on social media.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1353
","This could be done via adding Android specific badges. I'm thinking about adding something like this that is done for Apple Activity application:
![image](https://user-images.githubusercontent.com/3234209/60273511-f28db880-98fe-11e9-99c0-c854e0d481a5.png)
",30260406
994,Properly render web content in BOINC Notices,open,2015-02-04T06:20:19Z,2017-04-11T13:50:45Z,,MEMBER,"**Reported by Joachim on 21 Sep 44351381 08:52 UTC**
BOINC supports pictures and videos in notices. Those currently get not rendered correctly on the phones. Fix.

See e.g. notices of SETI or WCG alpha sites.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1352
","**Commented by Joachim on 4 May 44423717 18:23 UTC**
Web View in Android does not support plug in (`<options>` tag), asked David to have the youtube video link in the notice description.
",30260406
995,Make language a preference,open,2015-02-04T06:18:56Z,2017-04-11T13:49:21Z,,MEMBER,"**Reported by Joachim on 21 Oct 44307403 10:11 UTC**
Investigate if it is possible to change Android localization language through the preferences.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1349
","**Commented by Joachim on 2 Oct 44349566 02:59 UTC**
Replying to [Jitender](comment:2):

> Replying to [Joachim](ticket:1349):
> 
> > Investigate if it is possible to change Android localization language through the preferences.
> 
> Did you mean that in UI preference should have a option to change language? If so it can be done by changing the app Locale. 

Yes, users requested to change the UI language. We support many languages, but right now it can only be controlled by changing the language of the device. If we could have this as a preference in our UI, some users might appreciate it.
",30260406
996,Change Notice notification icon to make it recognizable,open,2015-02-04T06:16:21Z,2018-09-19T14:05:15Z,,MEMBER,"**Reported by Joachim on 22 Aug 44182133 23:56 UTC**
envelope icon is too generic. Asked WCG for a BOINC branded envelope icon

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1343
","Ah never mind, just noticed the Android Manager label there. :)",30260406
997,"Boinc manager can't connect to client using ""localhost""",open,2015-02-04T06:13:26Z,2019-10-29T16:18:29Z,,MEMBER,"**Reported by saldsl on 10 Sep 44020772 12:41 UTC**
Tested on Fedora 19 x86_64 (with RPMs from repository), boinc version 7.2.33.
Without linking the gui_rpc_auth.cfg file to my home directory, when I select Advanced -> Select Computer and I type ""localhost"" as target and put the password Boinc Manager can't connect to client. If I type ""127.0.0.1"" it connects.

This happens only with 7.2.33, if I downgrade boinc to 7.0.65 ""localhost"" works, so I don't think to a firewall/hostnames configuration problem.

[https://bugzilla.redhat.com/show_bug.cgi?id=1048545]

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1335
",@ChristianBeer I should test the fix in next 7 days,30260406
998,Avoid concurrent startup of tasks when the client is started,open,2015-02-04T06:07:33Z,2017-04-11T12:18:20Z,,MEMBER,"**Reported by knreed on 10 Oct 43589063 08:56 UTC**
Some research projects use significant disk or other resources and concurrent start up of tasks can cause an impact on the system or possible result in heartbeat non-response.  See http://www.worldcommunitygrid.org/forums/wcg/viewthread_thread,30688_offset,0#428458

It would be useful if the client would stagger the startup of tasks to avoid this issue or impact on the users machine.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1321
",vboxwrapper uses a random delay when started for exactly the same reason. One should add a random delay to starting apps in general especially when starting or coming out of hibernation.,30260406
999,GPU-tasks do not verify how many CPUs are used,open,2015-02-04T06:02:13Z,2020-02-10T01:52:37Z,,MEMBER,"**Reported by michel83 on 3 Mar 43556057 00:50 UTC**
When a system uses 100% of the resources available, the performance of the GPU tasks may drop significantly.
The GPU task does not check how many CPUs are already in use, so that eventually more CPU time is needed than available. (Example: System has 6 CPUs, all of them used by a task, the GPU needs an additional 0.5 CPU).

Using the Preference Settings and limiting the CPU usage to less than 100% is possible but it would be more accurate and convenient for the user if the system would check and adjust it itself.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1310
","@Aurum420, it is not a crossplatform solution. While it could work on linux, it still nice to find a way to do the same on Windows and MacOS. So before implementing it in this way it's nice to try to find more general solution, if possible",30260406
1000,"When switching Language, sometimes a ""new notice"" notification is generated, when there are no new notices",open,2015-02-04T05:27:47Z,2017-10-24T15:57:15Z,,MEMBER,"**Reported by JacobKlein on 23 Jan 43273407 23:33 UTC**
When switching Language, sometimes a ""new notice"" notification is generated, when there are no new notices

It might be necessary to already have some notices within the Notices tab, before switching language.

But still, this shouldn't be happening, and appears to be bug.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1241
","Using BOINC 7.8.3 I have switched between Dutch and English ten times to and fro. 
Even tried changing the Notice interval from never to a week to always. 

On each start up I still have only one notice. ",30260406
1001,Add Project - Adding a project that is down results in error dialog,open,2015-02-04T05:27:27Z,2019-03-14T19:59:32Z,,MEMBER,"**Reported by JacobKlein on 21 Oct 43272476 05:57 UTC**
Rom,

When I add a project with a made-up URL (like, for instance, http://www.madeupname.testin.com )...
The friendly dialog box says:
Project temporarily unavailable
The project is temporarily unavailable.
Please try again later.
... and the Event Log says:
4/9/2013 1:21:00 AM |  | Fetching configuration file from http://www.madeupname.testin.com/get_project_config.php
4/9/2013 1:21:02 AM |  | Project communication failed: attempting access to reference site
4/9/2013 1:21:04 AM |  | Internet access OK - project servers may be temporarily down.

BUT

When I add a project that is within the official list, which just happens to be down (like Superlink@Technion)...
The unfriendly dialog box says:
Failed to add project
An error has occurred;
check the Event Log for details.
Click Finish to close.
... and the Event Log says:
4/9/2013 1:23:20 AM |  | Fetching configuration file from http://cbl-boinc-server2.cs.technion.ac.il/superlinkattechnion/get_project_config.php
4/9/2013 1:23:27 AM |  | Project communication failed: attempting access to reference site
4/9/2013 1:23:28 AM |  | Internet access OK - project servers may be temporarily down.

Shouldn't both dialog boxes have been friendly here?
Especially since the Event Log entries are the same?

Because this happens in 7.0.28 also, I guess I'll create a ticket.

Thanks,
Jacob 

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1240
",Although the log messages look the same the logic behind that handles the error values differently. One just needs to find out what error cases are already handled and which need to be added. Switching the logic seems to be the best case. A made up or outdated URL usually means no DNS lookup which should prompt the error dialog. If the URL could be resolved but we get a 404 on `get_project_config.php` the error dialog should also be displayed since the URL does not seem to belong to a project. In any other case it seems to be a legit project URL but we can't connect so show the project temporarily down dialog.,30260406
1002,"Work Fetch - Leaves part of a GPU unused, when it should instead fetch work",open,2015-02-04T05:27:06Z,2018-10-02T13:41:27Z,,MEMBER,"**Reported by JacobKlein on 25 Mar 43272124 22:13 UTC**
If a project's GPU apps are setup to use only part of the GPU (ie: app_config.xml), then when the last remaining task(s) for that project are running and not utilizing the full GPU, work fetch should fetch more, but doesn't.

This issue was confirmed with both 7.0.60, as well as on 4/8/2013 in the  simulator (which has several unreleased work fetch changes).

It would seem that the prerequisites to reproducing the bug are:
- use an app_config.xml file (to set an app to use part of a GPU, so multiple tasks could run at the same time on the same device).
- use a small buffer setting

I'm not certain if GPU Exclusions are necessary to create the issue, but I believe that using GPU Exclusions makes this problem worse.

As a workaround, I had to increase my buffer settings way above what I would normally expect. It feels like, in addition to work fetch not realizing a portion of the GPU is idle, it might also not be realizing that the tasks run 2-at-a-time.

Details, including examples in a simulation, are in the email below:

---

From: jacob_w_klein@msn.com[davea@ssl.berkeley.edu[[BR]([BR]]To:)]Subject: RE: job scheduling[Mon, 8 Apr 2013 09:51:16 -0400[[BR]([BR]]Date:)][  Thank you. I really appreciate you looking at these issues, and I'll try to verify they work.[[BR]([BR]])]Your WCG project sounds interesting; maybe they're going to support Android?[wish we had a Windows Phone platform, I'd love to test on it.[[BR]([BR]]I)][you remember Ed (Beyond) reporting a GPU Exclusion Work Fetch issue?[[BR]([BR]]Do)]I might have found examples of what he was trying to explain...[noticing an issue, both on my computer (7.0.60's work fetch algorithm), as well as the simulator (new work fetch algorithm).[[BR]([BR]][[BR]]I'm)]If a GPU is only partially-loaded (ie: 0.5 GPU) by the last remaining task(s) for a project that has GPU-Exclusions,[get into a scenario where GPUs are left part-idle, and work fetch won't fetch more.[[BR]([BR]]We)][task scheduler (correctly) schedules the workload, which is scheduled in a way where a GPU is left part-idle,[[BR]([BR]]The)]But work fetch thinks we have plenty of work, and sees no fully idle instances, so it doesn't ask for any.[are some examples where that occurred, even with our work fetch changes:[[BR]([BR]][[BR]]Here)][[[BR]([BR]]http://boinc.berkeley.edu/dev/sim_web.php?action=show_simulation&scen=86&sim=26)]2 days 17:03:00[days 14:33:00[[BR]([BR]]3)]6 days 06:13:00[days 16:43:00[[BR]([BR]]8)]9 days 16:07:00[fix might involve evaluating the project's GPU apps to see if it has any that use partial GPU[[BR]([BR]][[BR]]The)]...  or maybe checking to see that all of its GPU apps use <= amount of  currently idle GPU (to ensure we don't keep asking/getting work we  cannot immediately use)[sounds to me like the fix for this one might be tricky instead of straight-forward, though I'm not sure.[[BR]([BR]][[BR]]It)]Do you plan on tackling this soon (fixed in short term), or should I create a ticket (fixed eventually, maybe months/years)?[[BR]][[BR]]Regards,[[BR]]Jacob

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1239
",Up,30260406
1003,"BOINC Manager crash - Statistics Tab - Clicking ""All projects (sum)"" while a project hasn't initialized",open,2015-02-04T05:26:45Z,2018-10-11T13:25:31Z,,MEMBER,"**Reported by JacobKlein on 19 Mar 43268893 00:52 UTC**
Note:[ticket is related to, but different from, the following unfixed tickets:[[BR]([BR]]This)]Ticket #659[#1053

From: jacob_w_klein@msn.com[[BR]([BR]]Ticket)]To: boinc_alpha@ssl.berkeley.edu[BOINC Manager crash - Statistics Tab - Clicking ""All projects (sum)"" while a project hasn't initialized[[BR]([BR]]Subject:)]Date: Sun, 7 Apr 2013 17:56:59 -0400[     I noticed this bug today, that crashes the BOINC Manager, on 7.0.60 x64.[[BR]([BR]][[BR]])]On  the older 7.0.28 version, I've confirmed similar behavior, where the  Manager appears to just freeze indefinitely; so I'll create a ticket for  this.[use an account manager to connect to several projects,  and the Superlink project hasn't been able to initialize for a long long  time; the project is down.[[BR]([BR]][[BR]]I)]Because of the status of the project, the Statistics tab's project list shows an empty string for that project.[is an issue (noted in Ticket 659), where it really should be showing the project's URL there, instead of empty string.[[BR]([BR]]That)][I've discovered another issue, a crashing issue.[[BR]([BR]]But)]While  the project is trying to initialize, if you click ""All projects (sum)""  on the Statistics Tab, the BOINC Manager will crash.[bug does not require the use of an account manager; you can trigger the bug immediately after adding 1 project.[[BR]([BR]]This)][to reproduce the crash:[[BR]([BR]]Steps)]1) Get a project in the status of ""Project Initialization"", using one of these methods:[Add a project, and watch for that status[[BR]([BR]]-)]- Add an account manager that auto-attaches to a project which is currently down[Click the Statistics tab, and notice that the project erroneously shows a blank in the Statistics project list[[BR]([BR]]2))]3) Click ""All projects (sum)""[Manager hangs/stalls, then closes itself.[[BR]([BR]]4))][is the relevant portion of the stderrgui.txt log file:[[BR]([BR]]Below)][Unhandled Exception Record -[[BR]([BR]]============================================================================================================[[BR]]-)]Reason: Breakpoint Encountered (0x80000003) at address 0x000007FB4DCF478A[Registers -[[BR]([BR]][[BR]]-)]rax=000000000000001a rbx=000000000427bb50 rcx=00000000ffffffff rdx=0000000000000002 rsi=000000000013eca0 rdi=0000000000000000[!r9=0000000000000000 !r10=0000000057bb0000 !r11=0000000000000200 !r12=0000000000000005 !r13=0000000001c735a0[[BR]([BR]]!r8=0000000000000000)]!r14=0000000001cc2c88 !r15=000000000421a3c0 rip=000000004dcf478a rsp=000000000013e5e8 rbp=000000000421a3c0[ss=002b ds=002b es=002b fs=0053 gs=002b efl=00000202[[BR]([BR]]cs=0033)][Callstack -[[BR]([BR]]-)]ChildEBP !RetAddr Args to Child[57bbb1bd 0427bb50 0421a3c0 0013eca0 00000000 KERNELBASE!!DebugBreak+0x0 [[BR]([BR]]0013e5e0)]0013eba0 400f10d7 01cc2ce8 01c8c720 0013eca0 04222100 MSVCR80!_invalid_parameter_noinfo+0x0 [ 400f5218 01c8c720 0013f120 00000005 00000000  boincmgr!CPaintStatistics::!DrawAll+0x17  (c:\src\boincgit\boinc_7.0b\clientgui!viewstatistics.cpp:1543) [[BR]([BR]]0013f090)]0013f440  4017cb71 00000507 0013f550 00000272 0013f250  boincmgr!CPaintStatistics::!OnPaint+0x0  (c:\src\boincgit\boinc_7.0b\clientgui!viewstatistics.cpp:1626) [ 4017d1e3 00000001 00000000 0013f500 4024712a  boincmgr!wxEvtHandler::!ProcessEventIfMatches+0x0  (c:\src\sdks\wx28x64\src\common!event.cpp:1233) [[BR]([BR]]0013f470)]0013f4a0 4017d2b0  0013f550 01c8c720 00000000 01c8c720  boincmgr!wxEventHashTable::!HandleEvent+0xf  (c:\src\sdks\wx28x64\src\common!event.cpp:907) [401e29ff  bc041863 bc041863 00000000 01c8c720  boincmgr!wxEvtHandler::!ProcessEvent+0x17  (c:\src\sdks\wx28x64\src\common!event.cpp:1293) [[BR]([BR]]0013f4d0)]0013f590 401e2cd0  40000000 01c8c720 00000002 00000070 boincmgr!wxWindow::!HandlePaint+0x12  (c:\src\sdks\wx28x64\src\msw!window.cpp:4602) [401e0d6f  01c8c720 007c05b0 00000000 00000000 boincmgr!wxWindow::MSWWindowProc+0x8  (c:\src\sdks\wx28x64\src\msw!window.cpp:2747) [[BR]([BR]]0013f850)]0013f880 4e2d3e95 00000001 0044068a 00000000 0000000f boincmgr!wxWndProc+0x0 (c:\src\sdks\wx28x64\src\msw!window.cpp:2623) [ 4e2d2a62 00000000 00000000 00000000 00000000  USER32!!UserCallWinProcCheckWow+0x0  (c:\src\sdks\wx28x64\src\msw!window.cpp:2623) [[BR]([BR]]0013f940)]0013f9a0 4e2d294d  00000000 00000000 00000000 57bb6dfb USER32!!DispatchClientMessage+0x0  (c:\src\sdks\wx28x64\src\msw!window.cpp:2623) [509f4b67 00000001 57bb6dfb fffffffe 00000000 USER32!!__fnEMPTY+0x0 (c:\src\sdks\wx28x64\src\msw!window.cpp:2623) [[BR]([BR]]0013fa00)]0013fa88  4e2d203a 4e2d204c 011d03d2 00000000 fffde000  ntdll!!KiUserCallbackDispatcherContinue+0x0  (c:\src\sdks\wx28x64\src\msw!window.cpp:2623) [4e2d204c  011d03d2 00000000 fffde000 4e2d19ec USER32!!ZwUserDispatchMessage+0x0  (c:\src\sdks\wx28x64\src\msw!window.cpp:2623) [[BR]([BR]]0013fa90)]0013fb10 4e2fe067  01c8be00 00907510 009071c0 0013fce8 USER32!!DispatchMessageWorker+0x0  (c:\src\sdks\wx28x64\src\msw!window.cpp:2623) [401e0c29 0013fc48 01c8be00 00000000 00000000 USER32!IsDialogMessageW+0x0 (c:\src\sdks\wx28x64\src\msw!window.cpp:2623) [[BR]([BR]]0013fba0)]0013fc50  4024c8af 01c8be00 00000000 0013fce8 00000000  boincmgr!wxWindow::MSWProcessMessage+0x10  (c:\src\sdks\wx28x64\src\msw!window.cpp:2407) [4024c39f  0013fce8 01c24d50 00000000 00000000  boincmgr!wxEventLoop::!PreProcessMessage+0xf  (c:\src\sdks\wx28x64\src\msw!evtloop.cpp:175) [[BR]([BR]]0013fc80)]0013fcb0 4024c702  00000000 00000000 01c24d50 00000010  boincmgr!wxEventLoop::!ProcessMessage+0x9  (c:\src\sdks\wx28x64\src\msw!evtloop.cpp:74) [4025a941  01c9eb70 00000000 01c24d50 01c24980 boincmgr!wxEventLoop::Dispatch+0x0  (c:\src\sdks\wx28x64\src\msw!evtloop.cpp:294) [[BR]([BR]]0013fd20)]0013fd60 40244f8e  01c9eb70 00000000 cb4edcf2 01c24cd0 boincmgr!wxEventLoopManual::Run+0x9  (c:\src\sdks\wx28x64\src\common!evtloopcmn.cpp:115) [ 401c02e3 01c9eb70 01c1c990 01c1c990 01c1c990  boincmgr!wxAppBase::!MainLoop+0x9  (c:\src\sdks\wx28x64\src\common!appcmn.cpp:312) [[BR]([BR]]0013fdb0)]0013fdf0 401b5219 01c1c990 00000000 00000000 00000000 boincmgr!wxEntryReal+0x10 (c:\src\sdks\wx28x64\src\common!init.cpp:460) [40244d04 00000008 00000000 003ec428 00000001 boincmgr!wxEntry+0xb (c:\src\sdks\wx28x64\src\msw!main.cpp:209) [[BR]([BR]]0013fe20)]0013fea0 401779cc 001b27e6 00000000 00000000 00000000 boincmgr!wxEntry+0xd (c:\src\sdks\wx28x64\src\msw!main.cpp:386) [ 4deb167e 00000000 00000000 00000000 00000000  boincmgr!!__tmainCRTStartup+0x26  (f:\dd\vctools\crt_bld\self_64_amd64\crt\src!crtexe.c:589) [[BR]([BR]]0013ff50)]0013ff80  50a13501 00000000 00000000 00000000 00000000  KERNEL32!!BaseThreadInitThunk+0x0  (f:\dd\vctools\crt_bld\self_64_amd64\crt\src!crtexe.c:589) [ 00000000 00000000 00000000 00000000 00000000  ntdll!!RtlUserThreadStart+0x0  (f:\dd\vctools\crt_bld\self_64_amd64\crt\src!crtexe.c:589) [[BR]([BR]]0013ffd0)]============================================================================================================[[BR]][[BR]]Regards,[[BR]]Jacob

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1238
",Can someone try to reproduce this?,30260406
1004,Upgrading during NVIDIA tasks crashes driver and interferes with hardware detection‏,open,2015-02-04T05:26:04Z,2017-04-10T15:53:19Z,,MEMBER,"**Reported by JacobKlein**
From: jacob_w_klein@…
To: boinc_alpha@…
Subject: Upgrading during NVIDIA tasks crashes driver and interferes with hardware detection
Date: Tue, 2 Apr 2013 13:48:41 -0400

When I perform an upgrade (say from 7.0.59 x64, to 7.0.60 x64), while the older version is running...
If there are NVIDIA tasks running (I usually have 4 tasks across 2 GPUs), the driver crashes 4 times.

The error is a balloon in the Windows system tray that says:
Display driver stopped responding and has recovered
Display driver NVIDIA Windows Kernel Mode Driver, Version 314.22 stopped
responding and has successfully recovered.

Each of the balloons takes something like 4 seconds to show and fade, and sometimes Windows flickers a bit while this happens.
But the installation wizard is already on the page saying ""Launch the BOINC Manager"", with the Finish button available.

If I click that, while the drivers are crashing/recovering, the end result is that a GPU is not detected properly by OpenCL detection in the new version.

Normally, the my detection sequence looks like:
4/2/2013 1:18:57 PM |  | CUDA: NVIDIA GPU 0: GeForce GTX 660 Ti (driver version 314.22, CUDA version 5.0, compute capability 3.0, 3072MB, 2859MB available, 3021 GFLOPS peak)
4/2/2013 1:18:57 PM |  | CUDA: NVIDIA GPU 1: GeForce GTX 460 (driver version 314.22, CUDA version 5.0, compute capability 2.1, 1024MB, 951MB available, 1025 GFLOPS peak)
4/2/2013 1:18:57 PM |  | OpenCL: NVIDIA GPU 0: GeForce GTX 660 Ti (driver version 314.22, device version OpenCL 1.1 CUDA, 3072MB, 2859MB available, 3021 GFLOPS peak)
4/2/2013 1:18:57 PM |  | OpenCL: NVIDIA GPU 1: GeForce GTX 460 (driver version 314.22, device version OpenCL 1.1 CUDA, 1024MB, 951MB available, 1025 GFLOPS peak)
4/2/2013 1:18:57 PM | Poem@Home | Found app_config.xml
4/2/2013 1:18:57 PM | GPUGRID | Found app_config.xml
4/2/2013 1:18:57 PM | World Community Grid | Found app_config.xml
4/2/2013 1:18:57 PM |  | Config: use all coprocessors
4/2/2013 1:18:57 PM | World Community Grid | Config: excluded GPU.  Type: all.  App: hcc1.  Device: 0
4/2/2013 1:18:57 PM | Poem@Home | Config: excluded GPU.  Type: all.  App: poemcl.  Device: 1

But, if I click ""Finish"" with ""Launch the BOINC Manager"" checked, while the drivers are crashing/recovering, I get:
4/2/2013 1:24:32 PM |  | CUDA: NVIDIA GPU 0: GeForce GTX 660 Ti (driver version 314.22, CUDA version 5.0, compute capability 3.0, 3072MB, 2775MB available, 3021 GFLOPS peak)
4/2/2013 1:24:32 PM |  | CUDA: NVIDIA GPU 1: GeForce GTX 460 (driver version 314.22, CUDA version 5.0, compute capability 2.1, 1024MB, 1024MB available, 1025 GFLOPS peak)
4/2/2013 1:24:32 PM |  | OpenCL: NVIDIA GPU 0: GeForce GTX 660 Ti (driver version 314.22, device version OpenCL 1.1 CUDA, 3072MB, 2775MB available, 3021 GFLOPS peak)
4/2/2013 1:24:32 PM | Poem@Home | Found app_config.xml
4/2/2013 1:24:32 PM | GPUGRID | Found app_config.xml
4/2/2013 1:24:32 PM | World Community Grid | Found app_config.xml
4/2/2013 1:24:32 PM |  | Config: use all coprocessors
4/2/2013 1:24:32 PM | World Community Grid | Config: excluded GPU.  Type: all.  App: hcc1.  Device: 0
4/2/2013 1:24:32 PM | Poem@Home | Config: excluded GPU.  Type: all.  App: poemcl.  Device: 1

Notice that one of the GPUs was not properly detected for OpenCL.

Now, normally when I exit BOINC (by right-clicking it in the System Tray, choosing Exit, and making sure the ""Stop running tasks"" box is checked)
... normally it closes just fine without any driver crashes.
Closing it this way, closes the Manager and the Client nicely.

So why is the installer not closing things nicely? Is it somehow closing things differently then normal?
We shouldn't see driver crashes when performing an upgrade, especially if they interfere with the new version's hardware detection.

Any ideas?

Note: I tested an upgrade from 7.0.27 to 7.0.28 (using the 7.0.28 installer), and it too exhibited driver-crashing behavior.
So, should I create a ticket for this?

Thanks,
Jacob 

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1236
",I updated the comments here so they are better readable. The request in the comment above still seems to be valid.,30260406
1005,"Consider directories for ""exclusive_apps"" setting",open,2015-02-04T05:24:21Z,2017-04-10T15:46:56Z,,MEMBER,"**Reported by LoB on 15 Jan 43200317 19:40 UTC**
Hey devs,

I have a lot of computer games on my PC. When playing, I like the exclusive_app configuration which saves me from deactivating and reactivating BOINC manually.
Thus, it's very timeconsuming to add all my exclusive applications. Since most of them are located in/below the same directory, a directory-wise exclusive mode would help alot.

So, instead of

```
    <exclusive_app>foo.exe</exclusive_app>
```

I would like to be able to add:

```
    <exclusive_app_dir>C:\bar\</exclusive_app_dir>
```

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1231
",,30260406
1006,"boinc_client does not notice (or does not care) that ""computer is in use""",open,2015-02-04T05:17:18Z,2019-10-30T01:01:17Z,,MEMBER,"**Reported by Bluefin Tuna on 3 Apr 42799116 09:47 UTC**
On Linux (Fedora 17 KDE spin), boinc_client (7.0.28 x86_64-pc-linux-gnu) is running as user ""boinc"", a user not used for anything else.

Settings as per ""Computing Preferences (apply to all BOINC projects in which you participate)"" are for Processor Usage:

```
Suspend work while computer is on battery power?    yes
Matters only for portable computers                 yes
Suspend work while computer is in use?              yes
Suspend GPU work while computer is in use?          yes
'In use' means mouse/keyboard activity in last      5 minutes
Suspend work if no mouse/keyboard activity in last  --- minutes
Suspend work if CPU usage is above                  50%
Do work only between the hours of                   ---
Leave tasks in memory while suspended?              no
Switch between tasks every                          120 minutes
On multiprocessors, use at most                     --- processors
On multiprocessors, use at most                     100% of the processors
Use at most                                         75% of CPU time 
```

The machine has 2 non-threaded CPUs. As per lscpu:

```
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
CPU(s):                2
On-line CPU(s) list:   0,1
Thread(s) per core:    1
Core(s) per socket:    2
Socket(s):             1
```

It also has a CUDA-capable graphics card, which is being used for BOINC projects.

Scenario:
- boinc runs while user is absent
- once the user activates the machine, boinc notices this and tunes down
  (the log shows nothing though, I think it should ... is there a ""verbose""
  option?)
  ps then shows only the ""boinc_client"" running, not the tasks themselves
- however, after a quarter of an hour or so, the tasks start up again; computer interactivity slows noticeably. More amazing, there are actually 3 tasks running [2 x einstein@home + 1 x seti](e.g.) even though the machine has just 2 CPUs (maybe it's done differently on CUDA machines); here too logging the decision logic would be informative
- boinc knows the machine is in use though, the log says ""Suspending network activity - computer is in use""
- to get the machine in hand, one has to ""kill boinc""

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1215
","dbus-send --print-reply --dest=org.gnome.Mutter.IdleMonitor /org/gnome/Mutter/IdleMonitor/Core org.gnome.Mutter.IdleMonitor.GetIdletime

The above call should give you idle time on gnome-shell (GNOME3) environments on top of Wayland. Not a perfect solution, but probably better than what we have today at least for Fedora default installation.",30260406
1007,"v7 hosts ""time out"" on new GPU app_version",open,2015-02-04T05:16:05Z,2017-08-17T20:34:11Z,,MEMBER,"**Reported by korpela on 27 Feb 42757159 01:17 UTC**
When a new app_version and while app_version pfc and host_app_version pfc are settling out, many hosts experience ""time out"" or ""resource bound exceeded"" errors after seconds or minutes of run time.  This is despite our delay bound being 14 days and our resource bound being 10x the typical number of flops.

http://setiweb.ssl.berkeley.edu/beta/workunit.php?wuid=4114163

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1212
","I think it's still an issue when releasing new app_versions if there's no
prior app_version of the same type to copy pfc_avg and pfc_scale info
from.  I thought about figuring out a way to estimate values but never
implemented.

On Thu, Aug 17, 2017 at 1:23 PM, Jord <notifications@github.com> wrote:

> Is this still an issue?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/1184#issuecomment-323183824>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AKXcsqNuHG8M3zBvMmoYTvDpcCbhRLNnks5sZKFDgaJpZM4DbZP8>
> .
>



-- 
Eric Korpela
korpela@ssl.berkeley.edu
AST:7731^29u18e3
",30260406
1008,md5_file: Too many open files,open,2015-02-04T05:12:29Z,2017-04-10T08:07:47Z,,MEMBER,"**Reported by smoe on 8 Jan 42640604 02:59 UTC**
I found the boinc-client to have stopped for no apparent reason. It was working only with a local self-built SETI client. I had seen this once a long time before, though, back then with the WCG.

From stderrdae.txt:

No protocol specified[protocol specified[[BR]([BR]]No)]No protocol specified[protocol specified[[BR]([BR]]No)]...

dir_open: Could not open directory 'slots/0'.[Could not open directory 'slots/18'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/17'.[Could not open directory 'slots/12'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/7'.[Could not open directory 'slots/4'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/22'.[Could not open directory 'slots/19'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/9'.[Could not open directory 'slots/16'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/14'.[Could not open directory 'slots/20'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/8'.[Could not open directory 'slots/3'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/23'.[Could not open directory 'slots/11'.[[BR]([BR]]dir_open:)]...

dir_open: Could not open directory 'slots/7'.[Could not open directory 'slots/7'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/7'.[can't open projects/einstein.phys.uwm.edu/einstein_S6LV1_1.10_i686-pc-linux-gnu!__SSE2[[BR]([BR]]md5_file:)]md5_file: Too many open files[Could not open directory 'projects/setiathome.berkeley.edu'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/24'.[can't open projects/setiathome.berkeley.edu/14ja12ac.18155.67.4.10.61_1_0[[BR]([BR]]md5_file:)]md5_file: Too many open files[Could not open directory 'slots/14'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/14'.[can't open projects/einstein.phys.uwm.edu/hsgamma_FGRP1_0.23_i686-pc-linux-gnu[[BR]([BR]]md5_file:)]md5_file: Too many open files[can't open projects/boinc.bakerlab.org_rosetta/minirosetta_3.26_x86_64-pc-linux-gnu[[BR]([BR]]md5_file:)]md5_file: Too many open files[Could not open directory 'projects/docking.cis.udel.edu'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'projects/spin.fh-bielefeld.de'.[Could not open directory 'projects/boinc.fzk.de_poem'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'projects/qah.uni-muenster.de'.[Could not open directory 'projects/www.rechenkraft.net_yoyo'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'projects/www.worldcommunitygrid.org'.[Could not open directory 'slots/21'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/21'.[Could not open directory 'slots/21'.[[BR]([BR]]dir_open:)]md5_file: can't open projects/www.worldcommunitygrid.org/wcg_faah_autodock_6.40_i686-pc-linux-gnu[Too many open files[[BR]([BR]]md5_file:)]dir_open: Could not open directory 'projects/lhcathomeclassic.cern.ch_sixtrack'.[Could not open directory 'slots/0'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/1'.[Could not open directory 'slots/2'.[[BR]([BR]]dir_open:)]....

dir_open: Could not open directory 'slots/21'.[Could not open directory 'slots/22'.[[BR]([BR]]dir_open:)]dir_open: Could not open directory 'slots/23'.[Could not open directory 'slots/4'.[[BR]([BR]]dir_open:)]md5_file: can't open projects/setiathome.berkeley.edu/30dc09aj.1678.25025.13.10.226_2_0[Too many open files[[BR]([BR]]md5_file:)]

From stdoutdae.txt:

21-Aug-2012 10:08:37 [Temporarily failed download of 23jn11ad.13583.17249.14.10.205: transient HTTP error[[BR](SETI@home])]21-Aug-2012 10:08:37 [Backing off 4 min 36 sec on download of 23jn11ad.13583.17249.14.10.205[[BR](SETI@home])]21-Aug-2012 10:08:37 [Temporarily failed download of 31oc10ac.1632.15183.4.10.58: transient HTTP error[[BR](SETI@home])]21-Aug-2012 10:08:37 [Backing off 5 min 27 sec on download of 31oc10ac.1632.15183.4.10.58[[BR](SETI@home])]21-Aug-2012 10:09:01 [Project communication failed: attempting access to reference site[[BR](---])]21-Aug-2012 10:09:02 [Internet access OK - project servers may be temporarily down.[[BR](---])]21-Aug-2012 10:13:40 [Started download of 05my12ad.31349.14382.3.10.249[[BR](SETI@home])]21-Aug-2012 10:13:40 [Started download of 05my12ad.31349.14382.3.10.255[[BR](SETI@home])]21-Aug-2012 10:13:53 [Finished download of 05my12ad.31349.14382.3.10.249[[BR](SETI@home])]21-Aug-2012 10:13:53 [Started download of 23jn11ad.13583.17249.14.10.241[[BR](SETI@home])]21-Aug-2012 10:13:54 [Finished download of 05my12ad.31349.14382.3.10.255[[BR](SETI@home])]21-Aug-2012 10:13:54 [Started download of 23jn11ad.13583.17249.14.10.205[[BR](SETI@home])]21-Aug-2012 10:14:02 [Finished download of 23jn11ad.13583.17249.14.10.241[[BR](SETI@home])]21-Aug-2012 10:14:02 [Started download of 05my12ad.31349.14382.3.10.224[[BR](SETI@home])]21-Aug-2012 10:14:12 [Finished download of 23jn11ad.13583.17249.14.10.205[[BR](SETI@home])]21-Aug-2012 10:14:12 [Finished download of 05my12ad.31349.14382.3.10.224[[BR](SETI@home])]21-Aug-2012 10:14:12 [Started download of 31oc10ac.1632.15183.4.10.58[[BR](SETI@home])]21-Aug-2012 10:14:12 [Started download of 30dc09aj.1678.25025.13.10.226[[BR](SETI@home])]21-Aug-2012 10:14:29 [Finished download of 30dc09aj.1678.25025.13.10.226[[BR](SETI@home])]21-Aug-2012 10:14:29 [Started download of 31oc10ac.1632.15183.4.10.64[[BR](SETI@home])]21-Aug-2012 10:14:30 [Finished download of 31oc10ac.1632.15183.4.10.58[[BR](SETI@home])]21-Aug-2012 10:14:34 [Finished download of 31oc10ac.1632.15183.4.10.64[[BR](SETI@home])]21-Aug-2012 10:17:46 [Started download of 30jn10ab.1159.23777.7.10.6.vlar[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 23jn11ad.13583.17249.14.10.241_1 using setiathome_enhanced version 612 in slot 0[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 05my12ad.31349.14382.3.10.247_0 using setiathome_enhanced version 612 in slot 1[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 23jn11ad.13583.17249.14.10.229_1 using setiathome_enhanced version 612 in slot 2[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 05my12ad.31349.14382.3.10.224_1 using setiathome_enhanced version 612 in slot 3[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 30dc09aj.1678.25025.13.10.226_2 using setiathome_enhanced version 612 in slot 4[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 23jn11ad.13583.17249.14.10.228_0 using setiathome_enhanced version 612 in slot 5[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 23jn11ad.13583.17249.14.10.248_0 using setiathome_enhanced version 612 in slot 6[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 30dc09aj.1678.25025.13.10.220_2 using setiathome_enhanced version 612 in slot 7[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 05my12ad.31349.14382.3.10.255_0 using setiathome_enhanced version 612 in slot 8[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 05my12ad.31349.14382.3.10.249_0 using setiathome_enhanced version 612 in slot 9[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 23jn11ad.13583.17249.14.10.205_1 using setiathome_enhanced version 612 in slot 10[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 05my12ad.31349.14382.3.10.246_0 using setiathome_enhanced version 612 in slot 11[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 27my10ac.18052.55637.5.10.1_2 using setiathome_enhanced version 612 in slot 12[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 31oc10ac.1632.15183.4.10.53_0 using setiathome_enhanced version 612 in slot 13[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 31oc10ac.1632.15183.4.10.41_1 using setiathome_enhanced version 612 in slot 14[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 31oc10ac.1632.15183.4.10.58_0 using setiathome_enhanced version 612 in slot 15[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 31oc10ac.1632.15183.4.10.49_0 using setiathome_enhanced version 612 in slot 16[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 31oc10ac.1632.15183.4.10.52_0 using setiathome_enhanced version 612 in slot 17[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 31oc10ac.1632.15183.4.10.64_0 using setiathome_enhanced version 612 in slot 18[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 31oc10ac.1632.15183.4.10.36_1 using setiathome_enhanced version 612 in slot 19[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 31oc10ac.1632.15183.4.10.28_1 using setiathome_enhanced version 612 in slot 20[[BR](SETI@home])]21-Aug-2012 10:17:55 [Starting task 31oc10ac.1632.15183.4.10.47_0 using setiathome_enhanced version 612 in slot 21[[BR](SETI@home])]21-Aug-2012 10:17:59 [Finished download of 30jn10ab.1159.23777.7.10.6.vlar[[BR](SETI@home])]21-Aug-2012 10:17:59 [Starting task 30jn10ab.1159.23777.7.10.6.vlar_3 using setiathome_enhanced version 612 in slot 22[[BR](SETI@home])]21-Aug-2012 10:18:26 [Started download of 19se10ac.457.271346.15.10.37.vlar[[BR](SETI@home])]21-Aug-2012 10:18:35 [Finished download of 19se10ac.457.271346.15.10.37.vlar[[BR](SETI@home])]21-Aug-2012 10:18:35 [Starting task 19se10ac.457.271346.15.10.37.vlar_3 using setiathome_enhanced version 612 in slot 23[[BR](SETI@home])]21-Aug-2012 10:48:33 [Can't get task disk usage: opendir() failed[[BR](SETI@home])]21-Aug-2012 10:48:33 [Can't get task disk usage: opendir() failed[[BR](SETI@home])]21-Aug-2012 10:48:33 [Can't get task disk usage: opendir() failed[[BR](SETI@home])]21-Aug-2012 10:48:33 [Can't get task disk usage: opendir() failed[[BR](SETI@home])]21-Aug-2012 10:48:33 [Can't get task disk usage: opendir() failed[[BR](SETI@home])]....

1-Aug-2012 11:38:37 [Can't get task disk usage: opendir() failed[[BR](SETI@home])]21-Aug-2012 11:38:37 [Can't get task disk usage: opendir() failed[[BR](SETI@home])]21-Aug-2012 11:38:37 [Can't get task disk usage: opendir() failed[[BR](SETI@home])]21-Aug-2012 11:38:37 [Can't get task disk usage: opendir() failed[[BR](SETI@home])]21-Aug-2012 11:45:55 [read_stderr_file(): malloc() failed[[BR](SETI@home])]21-Aug-2012 11:45:55 [Computation for task 30dc09aj.1678.25025.13.10.226_2 finished[[BR](SETI@home])]21-Aug-2012 11:45:55 [Can't open client_state_next.xml: fopen() failed[[BR](---])]21-Aug-2012 11:45:55 [Couldn't write state file: fopen() failed; giving up[[BR](---])]

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1203
","I made that change, but it doesn't involve leaking file descriptors.",30260406
1009,"ignored return values, gcc warning  in 7.0.28",open,2015-02-04T05:04:54Z,2018-10-11T13:00:33Z,,MEMBER,"**Reported by smoe on 4 Sep 42408185 23:09 UTC**
Hello, please consider addressing those warnings. 

Kind regards,

Steffen

app_control.cpp: In member function ‘void ACTIVE_TASK::read_task_state_file()’:

app_control.cpp:1374:27: warning: ignoring return value of ‘size_t fread(void*, size_t, size_t, FILE*)’, declared with attribute warn_unused_result [-Wunused-result]
app_control.cpp: In member function ‘bool ACTIVE_TASK::temporary_exit_file_present(double&, char*)’:
app_control.cpp:550:23: warning: ignoring return value of ‘char* fgets(char*, int, FILE*)’, declared with attribute warn_unused_result [-Wunused-result]
app_control.cpp:551:23: warning: ignoring return value of ‘char* fgets(char*, int, FILE*)’, declared with attribute warn_unused_result [-Wunused-result]
if g++ -DHAVE_CONFIG_H -I. -I. -I..  -I../api -I../db -I../lib -I../lib/mac -I../sched -I../tools -I../vda -pthread  -Wall -Wextra -Wshadow -Wredundant-decls -Wdisabled-optimization -Wpointer-arith -Wstrict-aliasing -g -Wall -D_FORTIFY_SOURCE=1 -fstack-protector -O2 -Wall -MT boinc_client-app_start.o -MD -MP -MF "".deps/boinc_client-app_start.Tpo"" -c -o boinc_client-app_start.o `test -f 'app_start.cpp' || echo './'`app_start.cpp; \
        then mv -f "".deps/boinc_client-app_start.Tpo"" "".deps/boinc_client-app_start.Po""; else rm -f "".deps/boinc_client-app_start.Tpo""; exit 1; fi
app_start.cpp: In member function ‘int ACTIVE_TASK::start()’:
app_start.cpp:841:45: warning: ignoring return value of ‘char* getcwd(char*, size_t)’, declared with attribute warn_unused_result [-Wunused-result]
app_start.cpp:985:43: warning: ignoring return value of ‘FILE* freopen(const char*, const char*, FILE*)’, declared with attribute warn_unused_result [-Wunused-result]

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1186
",The warnings are still there. ,30260406
1010,Have different <count> values for different coprocessors,open,2015-02-04T05:01:06Z,2017-04-10T07:16:12Z,,MEMBER,"**Reported by MarkJ on 6 Jun 42115089 08:43 UTC**
Currently you can only specify a single <count>.5</count> parameter within app_info.xml per app. It is applied across all coprocessors of that type and doesn't allow for different values for different devices.

 <coproc>
   <type>CUDA</type>
   <count>1</count>
 </coproc>

'''Example:''' Machine has a GTS250 and a GTX460 installed. User wants to run 1 wu at a time on GTS250 but 2 wu at a time on the GTX460.

Provide a mechanism where the user can have different <count> values per coprocessor device.

One possible way could be to allow the app_info to specify the device the <count> applies to in a similar fashion to the cc_config.xml <exclude_gpu> statements.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1176
","**Commented by davea on 7 Mar 42115472 08:52 UTC**
This will have to wait until we change BOINC's GPU handling to accommodate different GPU models of the same vendor.
",30260406
1011,Bugs in Advanced options in cooperation with screen readers,open,2015-02-04T04:50:57Z,2018-10-12T11:47:30Z,,MEMBER,"**Reported by krzyszp on 18 Dec 41858584 22:50 UTC**
In ""Advanced/Select computer"" window labels ""Host name"" and ""Password"" are not readable by screen readers

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1151
","Can someone with BOINC 7.8.2 or 7.8.3 check this? 
Or else point out a screen reader so we can test this ourselves? ",30260406
1012,Bugs in Local preferences in cooperation with screen readers,open,2015-02-04T04:50:36Z,2018-10-12T11:22:43Z,,MEMBER,"**Reported by krzyszp on 23 May 41858558 18:17 UTC**
In Local preferences (advanced view) labels for text fields are not readable by NVDA (screen reader). (Manager 6.10.13)

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1150
","Can someone check this with BOINC version 7.6.33 or newer, please? ",30260406
1013,Client stops working after system ran out of file descriptors,open,2015-02-04T04:46:28Z,2019-09-05T08:26:36Z,,MEMBER,"**Reported by scasady on 1 Dec 41721499 02:13 UTC**
osx  client spins on failing call to socketpair.  This results in everything stopping.

The error message says too many open files but that doesn't appear to be true so I will have
to look further.

It happens every day or so.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1141
",A nice overview on raising the limit for open files is on https://www.cyberciti.biz/faq/linux-increase-the-maximum-number-of-open-files/ . It may be appropriate add a check on the number of files that can still be opened analogously to how BOINC checks for disk space.,30260406
1014,"Memory Preference ""Use at most x[pct]"" should remove previously-suspended applications from memory if needed",open,2015-02-04T04:32:01Z,2018-10-12T22:46:06Z,,MEMBER,"**Reported by JacobKlein on 17 May 41326966 10:13 UTC**
Found this on a machine with limited memory. It is a Windows 7 x86 laptop with 2GB.

My settings were: Use 50% memory when computer in use, Use 100% memory when computer is idle, Leave applications in memory

At one point, I got a task that took 1.5 GB of memory, and another that took 300 MB. So, if I wasn't using the laptop, BOINC was free to run them both, and it would pretty much take up all the memory.

However, when I started using the laptop, I noticed that memory was not being freed up for me. Both tasks stayed in memory, even though I had a preference that said Use 50% memory when computer is in use. This made the laptop very unusable.

Another thing I believe I noticed is that, if applications had been suspended to memory because they were cycled out by scheduling periods, BOINC would not remove them from memory to make room for running applications. This may be a feature to preserve work for tasks that didn't have a checkpoint recently, I'm not sure. It felt wrong though.

BOINC should count previously-suspended applications as part of the total memory %... and it should remove previously-suspended applications from memory if needed, to meet a % memory limitation.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1104
",I looked at the code; it aborts the job only if its WSS is greater than both the in-use and idle memory limits.  So current behavior is probably OK.,30260406
1015,RPMLINT: shared-lib-calls-exit,open,2015-02-04T04:29:57Z,2018-10-12T09:15:43Z,,MEMBER,"**Reported by saigkill on 3 Mar 41325904 11:06 UTC**
libboinc6.x86_64: W: shared-lib-calls-exit /usr/lib64/libboinc_api.so.6.10.58 exit@GLIBC_2.2.5 

libboinc6.x86_64: W: shared-lib-calls-exit /usr/lib64/libboinc_graphics2.so.6.10.58 exit@GLIBC_2.2.5 

This library package calls exit() or _exit(), probably in a non-fork() context. Doing so from a library is strongly discouraged - when a library function calls exit(), it prevents the calling program from handling the error, reporting it to the user, closing files properly, and cleaning up any state that the program has. It is preferred for the library to return an actual error code and let the calling program decide how to handle the situation.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1098
","Yes it is:
```
[  182s] RPMLINT report:
[  182s] ===============
[  185s] libboinc7.x86_64: W: shared-lib-calls-exit /usr/lib64/libboinc_graphics2.so.7.8.2 exit@GLIBC_2.2.5
[  185s] This library package calls exit() or _exit(), probably in a non-fork()
[  185s] context. Doing so from a library is strongly discouraged - when a library
[  185s] function calls exit(), it prevents the calling program from handling the
[  185s] error, reporting it to the user, closing files properly, and cleaning up any
[  185s] state that the program has. It is preferred for the library to return an
[  185s] actual error code and let the calling program decide how to handle the
[  185s] situation.
```",30260406
1016,systray - please make one click to close Manager into systray and another to reopen it,open,2015-02-04T04:29:37Z,2019-03-14T19:59:13Z,,MEMBER,"**Reported by sandro tosi on 26 Mar 41315253 20:26 UTC**
Hi,
I noticed that when manager is minimized to the systray, a click on the tray icon re-opens it again.

It would be nice if a click on systray icon when manager is opened, would minimize it to systray.

Thanks,
Sandro

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1097
","Hello Jord,

No, this is not a function of Windows. You can implement any behavior on
any action with systray icon.
Also not so many applications have this function: in common case
application can be opened on doubleclick on systray icon. Other behaviors
are more rare.

Thanks

Best regards,
Vitalii Koshura

2017-08-18 13:44 GMT+03:00 Jord <notifications@github.com>:

> I also wonder if this isn't a function of Windows, as none of the icons in
> the system tray react to a single click. If I want to reopen my anti-virus,
> my Radeon settings, Speedfan, I need to all double click on the icons.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/BOINC/boinc/issues/1073#issuecomment-323321964>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ADFZodYqvz18Eu2ni3SlSjTv6mw-9po1ks5sZWsdgaJpZM4DbYjT>
> .
>
",30260406
1017,Removing an upload/download rate preference does not return speeds to maximum for in-progress uploads/downloads,open,2015-02-04T04:28:55Z,2018-10-12T08:50:06Z,,MEMBER,"**Reported by JacobKlein on 29 Jul 41291932 02:13 UTC**
If I change a download rate preference to something like 5KB/sec, I can see the speed drop down to match it.

However, if I then clear the preference (or type 0.00 in it), BOINC appears to keep the speed at 5KB/sec unless I restart BOINC.

If I change the preference to something like 9999 before restarting BOINC, I can see the speeds resume.

Essentially, the mechanism that sets BOINC to max speed is not being restored when a speed limit preference is removed.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1095
",I have just tested this. I can confirm that if I change the limit it affects existing transfers but if I remove the limit it does not affect existing transfers and they continue at the old limit.,30260406
1018,"BOINC does not always launch a second graphics window when ""Show graphics"" is clicked on a project's second task (RNA World)",open,2015-02-04T04:28:35Z,2017-04-07T14:47:15Z,,MEMBER,"**Reported by JacobKlein on 21 Oct 41291506 20:00 UTC**
I found this while testing graphics.

Scenario that works:[Have 2 or more RNA World tasks running[[BR]([BR]]-)]- Select them[Click ""Show graphics""[[BR]([BR]]-)]- See the multiple Adobe Flash 9 windows properly open

Scenario that is broken:[Have 2 or more RNA World tasks running[[BR]([BR]]-)]- Select only one of them[Click ""Show graphics""[[BR]([BR]]-)]- While those graphics are shown, select a different RNA World task[Click ""Show graphics"" for the different task[[BR]([BR]]-)]- The second graphics window does not appear, even though it should.

I know there are many problems with the way BOINC handles graphics and screensavers for RNA World, and it's possible that they're all related... but I figured I should report this as its minor bug.

Feel free to email me with any questions.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1094
",I would say this only happens with RNA World and is a problem of the Flash screensaver there. Would be interesting to know if someone could test this with YAFU@home and the HTML screensaver.,30260406
1019,"BOINC Screen Saver does not turn the display off at the right time, per Windows Power Management setting to turn off the display",open,2015-02-04T04:24:07Z,2019-02-07T11:04:43Z,,MEMBER,"**Reported by JacobKlein on 6 Apr 41257338 14:13 UTC**
I have Windows 7 x64.

I believe there is a miscalculation somewhere in the BOINC Screensaver,  which may be preventing the display from being turned off at the right time, per the Windows Power Management setting.

When I have the Blank Screensaver set to wait 1 minute before starting  (via the Windows 7 screensaver settings), and then I have Windows Power  Management set to turn off the display after 2 minutes, the Blank Screensaver will start after 1 minute, and then a minute later, the monitor will go into amber power save mode. Thus it takes 2 minutes of no activity to turn off the display.

When I have the BOINC Screensaver set to wait 1 minute before starting (via the Windows 7 screensaver settings), and then I have Windows Power Management set to turn off the display after 2 minutes, the BOINC Screensaver will start after 1 minute, but then it will run for 2 minutes before the monitor will go into amber power save mode. Thus it takes 3 minutes of no activity to turn off the display.

It should have only taken 2 minutes of no activity to turn off the display.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1082
","Still a thing.
I'm worried about my screen burn in as my computer is on 24/7/365.

Is there a legit workaround?

I'm not using BOINC screensaver and I'm not running as service. ",30260406
1020,"When adding a project, using an account manager URL does not halt gracefully.",open,2015-02-04T04:22:44Z,2019-03-14T19:58:51Z,,MEMBER,"**Reported by JacobKlein on 27 Jan 41247804 22:13 UTC**
Tools -> Add project or account manager... -> Add project -> Paste a project URL like http://bam.boincstats.com/ -> Next -> Identify account -> Receive the following error:

Failed to add project[error has occured;[[BR]([BR]]An)]check the Event Log for details.[Finish to close.

Event log says:[[BR]([BR]]Click)]3/31/2011 !12:31:10 PM | | Fetching configuration file from !http://bam.boincstats.com/get_project_config.php [!12:31:45 PM | | Project communication failed: attempting access to reference site[[BR]([BR]]3/31/2011)]3/31/2011 !12:31:48 PM | | Internet access OK - project servers may be temporarily down.

I propose this:

When the user is in ""add project"" mode, and first contact with the URL indicates that it is not capable of serving as a ""project"", the wizard should stop. It should not try to add it as an account manager. I should not have even gotten to the Username and password prompts.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1078
","Testing with BOINC 7.8.3:
Tools->Add project
Add https://bam.boincstats.com/ as 'project', Next.
After communicating, it shows me the ""Identify your account at BOINCStats BAM!"" window with the username + password boxes.
Meaning it's even nicer done, instead of exiting the add project wizard, we switch to the add account manager wizard. 

Mind, the ""Forgot your password?"" link goes to the general BOINCStats BAM! page (https://boincstats.com/en/bam/), but should probably go to https://boincstats.com/en/bam/accountRecovery (although this is accessible from the earlier page). ",30260406
1021,"When adding an account manager, using a project URL does not halt gracefully.",open,2015-02-04T04:22:23Z,2019-03-14T19:58:26Z,,MEMBER,"**Reported by JacobKlein on 16 May 41247778 08:26 UTC**
Tools -> Add project or account manager... -> Use account manager -> Paste a project URL like http://isaac.ssl.berkeley.edu/alpha/ -> Next -> Agree to terms if applicable -> Identify account for the project -> Receive the following error:

Failed to add project[error has occured;[[BR]([BR]]An)]check the Event Log for details.[Finish to close.

Event log says:[[BR]([BR]]Click)]3/31/2011 !12:13:48 PM | | Fetching configuration file from !http://isaac.ssl.berkeley.edu/alpha/get_project_config.php [!12:13:56 PM | | Contacting account manager at !http://isaac.ssl.berkeley.edu/alpha/ [[BR]([BR]]3/31/2011)]3/31/2011 !12:13:57 PM | | error: file not found

Notices (on 6.12.19) also throws the error as a notice:[from BOINC[[BR]([BR]]Notice)]error: file not found

I propose this:

When the user is in ""add account manager"" mode, and first contact with the URL indicates that it is not capable of serving as an ""account manager"", the wizard should stop. It should not try to add it as a project. I should not have even gotten to terms of use page, or the Email address and password prompts.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1077
","Just tested this with BOINC 7.8.1
In the communications window it shows that there was an error trying to add this account manager.
In the event log it shows:
17/08/2017 18:38:15 |  | Fetching configuration file from https://isaac.ssl.berkeley.edu/alpha/get_project_config.php
17/08/2017 18:38:17 |  | Project communication failed: attempting access to reference site
17/08/2017 18:38:20 |  | Internet access OK - project servers may be temporarily down.
",30260406
1022,"When adding an account manager, BOINC should not prompt for Usernames and Passwords for Account Managers that do not use them to authenticate, like BAM!",open,2015-02-04T04:21:21Z,2019-03-14T19:58:07Z,,MEMBER,"**Reported by JacobKlein on 5 Oct 41246501 04:53 UTC**
When attaching to BAM!, so long as I provide any password whose length is 6+, even if I use a blank username, BOINC appears to successfully attach to BAM!.

Further testing shows that I can use ANY Username, and ANY 6+ length password, and it still works.

When I first saw this, I was completely confused, and thought it was all completely broken. Now, I realize there must be some feature where my host_cpid and domain_name are used to automatically look up the information, but...[Username and Password are irrelevant to BAM! authentication, and BOINC is going to succeed in adding BAM! regardless of my inputs, then I conclude that:

''BOINC should not prompt for Usernames and Passwords for Account Managers that do not use them to authenticate.''

After giving a bogus name and bogus password, BOINC attached to BAM!, and here is the log:[[BR]([BR]]If)]3/31/2011 !12:49:47 AM |  | Fetching configuration file from !http://bam.boincstats.com/get_project_config.php [!12:50:00 AM |  | Contacting account manager at !http://bam.boincstats.com/ [[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Account manager: BAM! User-ID: 32925[!12:50:02 AM |  | Account manager: BAM! Host-ID: 271765[[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Account manager: Number of BAM! connections for this host: 54[!12:50:02 AM |  | Account manager contact succeeded[[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Attaching to !http://boinc.umiacs.umd.edu/ [!12:50:02 AM |  | Attaching to !http://boinc.bakerlab.org/rosetta/ [[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Attaching to !http://www.worldcommunitygrid.org/ [!12:50:02 AM |  | Attaching to !http://boincsimap.org/boincsimap/ [[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Attaching to !http://ralph.bakerlab.org/ [!12:50:02 AM |  | Attaching to !http://docking.cis.udel.edu/ [[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Attaching to !http://isaac.ssl.berkeley.edu/alpha/ [!12:50:02 AM |  | Attaching to !http://www.gpugrid.net/ [[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Attaching to !http://cbl-boinc-server2.cs.technion.ac.il/superlinkattechnion/ [!12:50:02 AM |  | Attaching to !http://boinc.fzk.de/poem/ [[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Attaching to !http://hydrogenathome.org/ [!12:50:02 AM |  | Attaching to !http://MindModeling.org/beta/ [[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Attaching to !http://www.freehal.net/freehal_at_home/ [!12:50:02 AM |  | Attaching to !http://boinc.drugdiscoveryathome.com/ [[BR]([BR]]3/31/2011)]3/31/2011 !12:50:02 AM |  | Attaching to !http://www.rnaworld.de/rnaworld/ [[BR]]

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1074
","Reproducible with BOINC 7.8.1
Added BAM with username ""HiptoHop"" and password ""1234567890"", it added:
 17/08/2017 18:35:21 |  | Account manager: BAM! User: 18, Ageless
 17/08/2017 18:35:21 |  | Account manager: BAM! Host: 703041
 17/08/2017 18:35:21 |  | Account manager: Number of BAM! connections for this host: 24
 17/08/2017 18:35:21 |  | Account manager contact succeeded
",30260406
1023,"When adding an already-added project, ""Already Added"" dialog is not always shown. [Superlink@Technion]",open,2015-02-04T04:18:36Z,2019-03-14T19:57:54Z,,MEMBER,"**Reported by JacobKlein on 5 Mar 41245674 16:26 UTC**
For the Superlink@Technion project, if I'm already attached to it, and I try to attach to it again, I do not get the ""already added"" dialog. BOINC should not let me add it again.

Instead, BOINC bypasses the ""Already Added"" check, and proceeds to this dialog:

---------------------------[Manager[[BR]([BR]]BOINC)]---------------------------[project may not have work for your type of computer. Do you want to add it anyway?[[BR]([BR]]This)]---------------------------[No [[BR]([BR]]Yes)]---------------------------

I believe the dialog[project may not have work for your type of computer. Do you want to add it anyway?""[[BR]([BR]]""This)]is preventing the dialog[already added this project. Please choose a different project.""[[BR]([BR]]""You)]from showing first properly.

Fortunately, I don't think there are severe consequences of adding the project again, as I'm not aware of any problems, but...[[BR]]The UI should not have allowed me to add this project twice.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1067
","That sounds more like a clash between an Account Manager and the basic BOINC GUI. (I assume you mean you're using BAM!, the account manager from the same stable as BOINCstats?). We should perhaps find a Science United user to check whether the problem is generic to all account manager interaction, or a specific BAM! bug.",30260406
1024,"When adding a project, some controls can improperly move and resize.",open,2015-02-04T04:18:15Z,2018-09-19T15:26:21Z,,MEMBER,"**Reported by JacobKlein on 13 Nov 41245624 10:13 UTC**
It appears that the placement of the ""Yes, existing user"" control is somehow based on the fields beneath it, but it shouldn't be.

When clicked (or when the text is clicked), it all moves leftward, which is slightly jarring to those that are OCD. This should be a quick fix. I apologize for it's near-insignificance.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1066
","I guess, to be clearer ... It's possible that the original post dealt with the position of the ""Yes, existing user"" radio button actually changing. That behavior doesn't happen any more for me.

But I still think it is jarring that, for the ""column of labels"" and the ""column of textboxes"", their widths change dependent upon which radio button is selected. It'd be nice to keep those widths the same.",30260406
1025,Installer Protected application execution section needs updated,open,2015-02-04T04:16:33Z,2017-08-17T16:11:35Z,,MEMBER,"**Reported by JacobKlein on 24 Mar 41245321 15:33 UTC**
In the Installer, the ""Protected application execution"" checkbox details should mention Windows 7, and be better described.

**I recommend:**[mention Windows 7 in addition to Windows Vista[[BR]([BR]]-)]- If the option is about the choice to install as a Windows Service, it should say so. The name should say ""Protected application execution - Install as a Windows Service"" to better indicate that checking this box would install as a Windows Service.[Saying ""This option is now disabled by default."" should be removed. It is redundant and unnecessary, since it's obvious that the option is unchecked when you first launch the installer.[[BR]([BR]]-)]- Instead of saying ""A reboot may be required."", say ""Selecting this option may require a reboot."", to better indicate that this option causes the need for the reboot.

So, to recap....

**On 6.10.58, the installer initially says:**[Protected application execution[[BR]([BR]][])]This option is now disabled by default.[reboot may be required.[[BR]([BR]]A)][6.10.58, if you click ""Advanced"", it then unlocks options and says:__[[BR]([BR]]__On)][Protected application execution[[BR](])]Run project applications under an unprivileged account. This provides protection from faulty applications, but it may cause graphics to not work with older applications, and on Windows Vista, it will prevent the use of applications that use graphics chips (GPUs)[reboot may be required.)

**I propose: the installer initially says:**[[BR]([BR]](A)] [Protected application execution - Install as a Windows Service[[BR](])]Selecting this option may require a reboot.[[[BR]([BR]])]**I propose: that if you click ""Advanced"", it then unlocks options and says:**[[]([BR]]) Protected application execution - Install as a Windows Service[Run project applications under an unprivileged account by installing as a Windows Service. This provides  protection from faulty applications, but it may cause graphics to not  work with older applications. On Windows Vista and Windows 7, it will also prevent the  use of applications that use graphics chips (GPUs).[[BR]([BR]])]Selecting this option may require a reboot.

This should be relatively easy to change, and would clarify things a bit. I must admit I was originally very confused about this option, but if it had said what I propose, it would have made more sense to me.

Note: Don't forget to update any localizations that may need translations.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1061
","On BOINC 7.8 the installer states:
""Service Install
Run project applications under an unprivileged account. This provides increased protection from faulty applications, and on Windows, it will prevent the use of applications that use the graphics chip (GPUs).
(A reboot may be required.)""

Is that all right? ",30260406
1026,"Installer: In ""Programs and Features"", ""Change"" should exist/work.",open,2015-02-04T04:15:51Z,2018-10-02T13:23:53Z,,MEMBER,"**Reported by JacobKlein on 19 Jan 41245008 01:46 UTC**
I noticed in my 6.12.x testing that all ""Change"" and ""Repair"" options were failing. I did some more testing, and confirmed they were failing for 6.10.58 as well.[3/29/2011, Rom said:[[BR]([BR]][[BR]]On)]I'm looking into it. It looks like the Change/Repair features were[for a while, sometime in the 6.6 timeframe. I opted to disable[[BR]([BR]]broken)]them until they could be fixed properly.[they work again, they both should be turned back on.[[BR]([BR]][[BR]]When)]This ticket is to fix, enable, test, and verify, the ""Change"" options.[[BR]][[BR]]Another ticket will handle the ""Repair"" options, just in case it becomes necessary to keep ""Change"" turned off.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1059
","Same comment as in #1034 - and the same to all installer issues, like my #824.",30260406
1027,"Installer: In ""Programs and Features"", ""Repair"" should exist/work.",open,2015-02-04T04:15:31Z,2018-10-02T13:19:59Z,,MEMBER,"**Reported by JacobKlein on 31 Mar 41245004 04:26 UTC**
I noticed in my 6.12.x testing that all ""Change"" and ""Repair"" options were failing. I did some more testing, and confirmed they were failing for 6.10.58 as well.

On 3/29/2011, Rom said:[looking into it. It looks like the Change/Repair features were[[BR]([BR]]I'm)]broken for a while, sometime in the 6.6 timeframe. I opted to disable[until they could be fixed properly.

When they work again, they both should be turned back on.[[BR]([BR]]them)]This ticket is to fix, enable, test, and verify, the ""Repair"" options.

Another ticket will handle the ""Change"" options, just in case it becomes necessary to keep ""Change"" turned off.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1058
","We need to do at least one of two things:

1) Recruit a volunteer with a thorough, professional knowledge of Installshield to finish off these little jobs that Rom left behind when the money ran out.

2) Recruit a volunteer with a thorough, professional knowledge of an open-source installer package to re-write the installer and include all the trac tickets which Rom had punted into the long grass long before the money ran out.",30260406
1028,Race condition when suspending tasks,open,2015-02-04T04:11:55Z,2018-10-18T14:14:18Z,,MEMBER,"**Reported by Martin Suchan on 21 Sep 41192538 07:06 UTC**
I've just noticed this issue when suspending tasks manually in BOINC Manager - situation:

Win7 x86, BM 6.12.15, only WCG project, Core2Duo - 2 cores
I got about 10 downloaded tasks, one is completed and reported, other two are running, the other tasks are not started yet, but allowed to be started once other task is finished.

'''I selected all not-started tasks PLUS one running task and clicked the Suspend button''' in the left command bar.

I expected, that all task will be marked at once as Suspended and the running will stop as well.

What actually happened? '''One not-yet-started task was started for about 1 second and after then it was suspended'''. I guess the task of '''""changing status to suspended"" is not done in transactional way'''. What actually happened, my guess, - some function got list of tasks to suspend, it started suspending one task each time. First it suspended the one running task. In this moment some other thread noticed there is one free slot for running, it found ready task and started it (typical race condition  ), in the meantime the first thread finished suspending the other tasks, 
including the one started by the other thread.

This should be fixed in my opinion. It could lead to bigger problems when running on 8+ core systems with lot of projects.

Event log:

task faah19421_ZINC17130909_xmdEq_1TW7_02_0 is running
task HFCC_L4_01202033_L4_0001_0 is in group for suspending, but it is started for 1 second

```
9.3.2011 9:50:46 |  | Suspending computation - user request
9.3.2011 9:50:50 |  | Resuming computation
9.3.2011 9:50:54 | World Community Grid | task faah19421_ZINC17130909_xmdEq_1TW7_02_0 suspended by user
9.3.2011 9:50:55 | World Community Grid | task oe781_00061_9 suspended by user
9.3.2011 9:50:55 | World Community Grid | task X0000065610008200603171636_1 suspended by user
9.3.2011 9:50:55 | World Community Grid | task X0000065621388200603241639_0 suspended by user
9.3.2011 9:50:55 | World Community Grid | Starting HFCC_L4_01202033_L4_0001_0
9.3.2011 9:50:55 | World Community Grid | Starting task HFCC_L4_01202033_L4_0001_0 using hfcc version 640
9.3.2011 9:50:56 | World Community Grid | task HFCC_L4_01202033_L4_0001_0 suspended by user
9.3.2011 9:50:56 | World Community Grid | task X0000065671034200603171856_1 suspended by user

```

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1048
","10-18-18 , BOINC 7.14.2:  On a Windows 7 Pro box, Intel Core Duo E8500 @ 3.16GHz, 4GB DRAM with nVIDIA GF 8400 GS as display adapter (GPU suspended in BOINC) , I was able to suspend multiple combinations of running and waiting (SETI and Einstein) tasks, resume them, and re-suspend them multiple times with instantaneous change to all highlighted tasks as viewed in Task window.

When an already suspended task was included in the list, the resume/suspend button was grayed out.

On a SuperMicro 2 x 6 Xeon X5650 `(12 physical cores) 16 GB ECC RAM, no GPU, running WIN10 Pro:  I was able to suspend and resume running and waiting (Einstein, MW 12-core WU and WCG) tasks multiple times with instantaneous change as viewed in Task window.  
When an already suspended task was included in the list, the resume/suspend button was grayed out.

No evidence of the race condition was observed.   

The above tests were done at the computer's console keyboard and mouse.",30260406
1029,BOINC projects' forum + PM notification e-mails in proper language,open,2015-02-04T04:07:35Z,2017-04-07T11:09:23Z,,MEMBER,"**Reported by Pepo on 12 Apr 41039882 22:13 UTC**
On any BOINC project forum pages, when user _'''A''' is sending a private message (PM) to user '''B'''_, the notification email is written in the _language, which is set as user '''A''''s preference, instead of user '''B''''s preferred (and understandable) language_.

Currently it is just  the e-mail subject, which gets translated. But if you imagine two users, which preferences are set to e.g. Slovak and Japanese, (and if the notification e-mails' contents would be thoroughly translated too,) both users would be actually getting some ''garbage'' e-mail they are not able to read and understand, except maybe the private message text itself, written in a language they both understand and agreed to use for communication.

I believe that this notification e-mail should be translated into the recipient's language instead of the sender's one. And if the PM is being addressed to multiple recipients, then each one correspondingly. Maybe with a fall-back to a particular (project-preferred) default language, e.g. now it is English.

(Response from davea: ""This would require adding a field to a DB table (probably forum_preferences) to store the language list passed in the last web access."")

PS: Forum posts' notification e-mails could possibly also be sent using user's preferred language.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1038
",I'm not sure if this is still an issue. I remember that I removed translation capability from emails a long time ago. Translating into an arbitrary language is currently not possible with the translation system. Keeping this open in case someone wants to do this.,30260406
1030,BOINC installation should not start the Manager under an admin account,open,2015-02-04T03:58:18Z,2018-02-05T23:45:34Z,,MEMBER,"**Reported by Pepo on 31 Aug 40749311 12:00 UTC**
After launching the BOINC installation exe (Win7, for compatibility install) as a plain user (pretty possibly the one, under which credentials the BOINC suite runs), the installation raises an UAC to be able to run under some admin account credentials.

After the installation, the wizard offers a checkbox to launch the Manager prior to terminating itself. But the Manager is launched under the same admin account with elevated rights, which in turn (in the case of compatibility install) starts the client with same elevated rights. The apps follow...

The installation process should start the Manager at least as the plain user, who launched the installation.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1015
",As I can see it is still an issue. There is a similar ticket #2107 ,30260406
1031,Localization of right-to-left languages,open,2015-02-04T03:56:34Z,2018-09-19T00:01:39Z,,MEMBER,"**Reported by mo v on 7 Nov 40709268 20:26 UTC**
Ido Schwartz, the Boinc Hebrew translator, has noticed problems in the Boinc manager Hebrew translation arising from the fact that this language is written right-to-left. He expects the same problems to arise with all RTL languages eg Arabic. 

In Boinc 6.11.6 and again in 6.11.7 he noticed that:
- the text in the Statistics and Disk tabs is inverted and unreadable
- the graphs are inverted from LTR to RTL 
- in the ""Simple View"" mode, the ""Advanced View"", ""Preferences"", ""Pause"" and ""Messages"" buttons do not translate. (I am not sure myself, however, whether this is related to the RTL problem)
- in the ""Preferences"" window the text is aligned to the left instead to the right

Ido has provided 3 screenshots from 6.11.6 showing the problem. In each case part of the BM is shown in its Hebrew and English versions.  

Statistics tab: [tab: [](]http://i15.photobucket.com/albums/a390/Dj_Leg0la5/Statistics.jpg[]

Disk)http://i15.photobucket.com/albums/a390/Dj_Leg0la5/Disk.jpg[view: [](]

Simple)http://i15.photobucket.com/albums/a390/Dj_Leg0la5/simple.jpg[]

A resolution of this problem would help our efforts to make Boinc more accessible world-wide. 

Thank you

Mo

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1011
",Still an issue. Please take a look at my comment in #911 ,30260406
1032,Hotkey suspend/resume,open,2015-02-04T03:51:56Z,2018-10-12T09:58:35Z,,MEMBER,"**Reported by davea on 27 Mar 40519687 23:06 UTC**
wxWidgets 2.8.11 supports hotkeys.
When we eventually upgrade to this,
add support for a configurable hotkey to suspend/resume.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/1001
",We are now at wxWidgets 3.0 and this still seems to be lacking.,30260406
1033,English-dependent word sequences in advanced prefs,open,2015-02-04T03:47:38Z,2017-04-07T10:27:23Z,,MEMBER,"**Reported by Nicolas on 8 Aug 40396573 22:40 UTC**
As reported on the boinc_loc mailing list:

The following msgids constitute English-dependent word sequence:

```
#: clientgui/DlgAdvPreferencesBase.cpp:353
msgid ""Transfer at most""
msgstr """"

#: clientgui/DlgAdvPreferencesBase.cpp:359
msgid ""Mbytes""
msgstr """"

#: clientgui/DlgAdvPreferencesBase.cpp:362
msgid ""every""
msgstr """"

#: clientgui/DlgAdvPreferencesBase.cpp:368
#: clientgui/DlgAdvPreferencesBase.cpp:382
msgid ""days""
msgstr """"

#: clientgui/DlgAdvPreferencesBase.cpp:374
msgid ""Connect about every""
msgstr """"
```

At least in Japanese, phrases that have similar meaning
to ""every 99 days"", cannot be used in this fixed
sequence of words:

  ""Transfer at most"" <99> ""M bytes"" ""every"" <99> ""days""

Sometimes Japanese translators can assign an irregular,
twisted mapping between the msgids and the msgstrs that
appear in a problematic sentence, that is, those words
(msgid-msgstrs pairs) have permuted (non-word-by-word)
mapping to mitigate this kind of issues. Then, of course
this trick is deeply dependent of the context and very
fragile to how these strings are used. In this case,
unfortunately msgid ""days"" is used twice,
so this kind of trick cannot be used here.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/991
",This is a rather technical problem as the current tools don't fully allow for such broken up sentences that contain user input boxes. I'm leaving this open maybe someone has a solution for this. It applies not only to the manager but also to the Web interface.,30260406
1034,Use color to identify work for CPU/GPU,open,2015-02-04T03:38:30Z,2018-09-19T14:10:31Z,,MEMBER,"**Reported by saldsl on 7 Sep 40007596 04:00 UTC**
In the advanced GUI, under Tasks tab, use different row background colors to identify WUs for CPU or GPU.
e.g. WU for CPU use blue, WU for GPU-nvidia use green, WU for GPU-ati use red

In this manner you could rapidly identify wich kind of work you have available in the queue.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/972
","@Ageless93 No, it's ok. I just have an unpredictable schedule so I can't tell for sure when I can do smth. Mostly it's related to my job :)",30260406
1035,add several scheduler profile against time,open,2015-02-04T03:31:57Z,2017-04-07T10:07:27Z,,MEMBER,"**Reported by adrien pre on 2 Sep 39869725 00:00 UTC**
Hello,

It would be great to have several time of day based scheduler settings.
For example:
- while I am in front of my computer, I could set one set of settings for the settings (low ressources usage)
- from one fixed hour, the scheduler automatically takes one other set of settings(night settings, full ressources usage)

Many thanks.

Adrien

Migrated-From: http://boinc.berkeley.edu/trac/ticket/958
",This would also affect the web preferences and should be considered when doing a Preference remodeling. ,30260406
1036,ignore list should apply to threads as well as posts,open,2015-02-04T03:31:37Z,2017-10-24T14:11:36Z,,MEMBER,"**Reported by davea on 10 Aug 39832964 14:40 UTC**
If you've ignored someone,
their threads should be hidden from you

Migrated-From: http://boinc.berkeley.edu/trac/ticket/957
",Is this very difficult to implement? ,30260406
1037,Add Sparkle Update framework to Mac OS Client for BOINC client updates,open,2015-02-04T03:23:09Z,2017-08-17T15:11:43Z,,MEMBER,"**Reported by lee mccartney on 27 Dec 39544275 10:40 UTC**
Please consider using the sparkle application update framework to the BOINC Mac OS client - its a free to use framework that seamlessly notifies users (and installs) program updates. General/Developer info can be found at the website: http://sparkle.andymatuschak.org/ 

I realised that the BOINC version I was running was quite old, I am sure that other Mac OS users are also unaware that they may be running an older version, this could be prevented fairly easily with some modifications to the client software.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/939
",Shouldn't this be solved with the auto-check for a new version on BOINC start that's now being tested? ,30260406
1038,configure --enable-bitness=32 doesn't work,open,2015-02-04T02:53:40Z,2019-11-04T22:19:16Z,,MEMBER,"**Reported by Nicolas on 4 Apr 39293706 04:00 UTC**
I tried to compile BOINC API for 32-bit Linux, being on a 64-bit Linux machine. Using `configure --enable-bitness=32` doesn't work; it compiles 64-bit code anyway (I didn't let `make` run to completion; I aborted it as soon as I saw a few files being compiled without any 32-bit flag).

I found the problem is the `SAH_OPTION_BITNESS` macro (defined in `m4/sah_select_bitness.m4`) adds `-m32` to `CFLAGS`, but not to `CXXFLAGS`, so C code would be compiled as 32-bit but C++ code wouldn't.

Another minor bug: if the macro detects `-m32` works, it prints ""Selecting 32 bit model... -m32"" but then prints ""failed"" on the next line anyway. The printing of ""failed"" is done always, instead of in the 'else' clause of checking if `-m32` works.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/876
",This is still an issue in https://github.com/BOINC/boinc/blob/master/m4/sah_select_bitness.m4,30260406
1039,BOINC Advanced GUI accessibility - larger text size option for visually impaired,open,2015-02-04T02:44:43Z,2018-10-13T19:50:53Z,,MEMBER,"**Reported by mo v on 3 Jan 39169959 12:53 UTC**
I originally brought up this topic in July 2007 on Trac ticket http://boinc.berkeley.edu/trac/ticket/147#comment:19. 

User Lightsttn1 who has somewhat impaired eyesight asked twice on the BOINC development forum for an option to increase the text size to be made available in the BM Advanced GUI. 

http://boinc.berkeley.edu/dev/forum_thread.php?id=2097#12310
http://boinc.berkeley.edu/dev/forum_thread.php?id=1908#11080

The Windows XP magnification tool, which I have tried in XP, works with the Boinc Manager but is inconvenient to use because a) the magnified view only occupies half of the screen and b) the minimum magnification it permits (level 1) is x4 which for some people is too much. With it one only sees one-eighth of the BM at any time. 

I know that screen readers can now be used to view Boinc Manager. However, many people have impaired eyesight that is not bad enough to require a screen reader. As the proportion of older people in the population increases, so will the number of BOINC users so affected. 

The font size in the Boinc Manager is of necessity pretty small. Would it be possible to create an option to view the BM in text one or two points larger? I realise that this would necessitate the use of the scroll bars, but they're already there. 

Migrated-From: http://boinc.berkeley.edu/trac/ticket/857
","> [Here's a hack](https://superuser.com/questions/29796/dual-monitors-on-windows-how-do-i-set-a-different-dpi-or-text-size-on-each-mon/632719#632719) for Windows 7 users. I don't know how well it works.

Doesn't work for me. For all applications where it matters (browsers, games, BOINC, essentially anything that can go full screen) the Settings options in Compatibility are greyed out. Seen that one in my travels as well. 
",30260406
1040,Automatic adjustment for CPU affinity,open,2015-02-04T02:42:49Z,2018-10-31T14:08:53Z,,MEMBER,"**Reported by Devaster on 21 Jul 39160755 11:33 UTC**
Can be implemented a mechanism to set automatic CPU affinity for every running project/task with selection plus when running multiple GPU clients on multiple GPUs  then they will occupy only one (the same) CPU  ?

Migrated-From: http://boinc.berkeley.edu/trac/ticket/853
","First stage of prototyping complete. Einstein FGRPopencl1K-nvidia application, GTX 970 card, free core for each app instance,

  | 1-up | 2-up | 3-up
-- | -- | -- | --
  |   |   |  
Average | 894.412 | 807.963 | 800.297
std_dev * | 4.014604 | 9.372873 | 22.92021
  |   |   |  
Sample | 64 | 114 | 72

Clear benefit from running multiple tasks per GPU, at cost of surrendering CPUs cores which could have been used for something else. Next to see if we can roll up all those wasted sleep cycles onto a single core with Process Lasso.

* std dev is on the raw timings (1616 / 2400 seconds), not the adjusted times for 2 / 3 concurrent",30260406
1041,Administrative installation adds inappropriate registry entries,open,2015-02-04T02:40:04Z,2017-12-25T09:43:21Z,,MEMBER,"**Reported by Richard Haselgrove on 19 Jun 39144534 20:26 UTC**
Running the installer with the '/a' (administrative installation) switch should not make changes to the local sysem registry.

Testing with both v6.4.5 and v6.6.10, I found that both INSTALLDIR and DATADIR were set to point to the network server image directory - this would break any running BOINC v6 on the workstation used for network upload. Fortunately I was running BOINC v5.

However, the administrative installer does not read back either registry key for subsequent installations - it defaults to the root of the first available mapped network drive.

An administrative installation should either make no local registry changes at all, or maintain and use its own ADMININSTALLDIR key.

Also, the final screen (attached) on the test machine (Windows XP SP3, workgroup) is contradictory - if the installation was successful (as it appears to have been), why am I being directed to run a repair tool from add/remove programs?

Migrated-From: http://boinc.berkeley.edu/trac/ticket/847
",Thanks for that - tested and works as advertised. I've added it to the user manual at http://boinc.berkeley.edu/wiki/Creating_custom_installers,30260406
1042,History of team's users,open,2015-02-04T01:56:34Z,2017-08-17T14:34:31Z,,MEMBER,"**Reported by Pepo on 20 Aug 38794575 21:20 UTC**
During and after the recent team hijack affair, a few ideas were said about how this could be prevented (among others, [here](http://boinc.berkeley.edu/dev/forum_thread.php?id=2860&nowrap=true#18440), [here](http://boinc.berkeley.edu/dev/forum_thread.php?id=2860&nowrap=true#18451), [here](http://boinc.berkeley.edu/dev/forum_thread.php?id=2860&nowrap=true#18455), [here](http://boinc.berkeley.edu/dev/forum_thread.php?id=2860&nowrap=true#18461), [here](http://boinc.berkeley.edu/dev/forum_thread.php?id=2860&nowrap=true#18625) and elsewhere). One of them was to create some publicly visible place containing the history of team's users and founders and their names.

I've noticed that such ''Team history'' page actually already exists, although not publicly visible, just for team admins (and possibly project admins, in case of problems or accident). These tables could be made publicly visible, because the team user data are already exported to statistics sites and nearly the same lists can be built over time. (Nearly.)

This page stores the users' join/leave history and currently looks like:

== Team history for !MyOwnTeam ==
||'''When'''||'''User'''||'''Action'''||'''Total credit at time of action'''||
||18 Aug 2008 17:32:56 UTC||[[http://boinc.berkeley.edu/dev/show_user.php?userid=1452 Lduhl@berkeley.edu]([Image%28http://boinc.berkeley.edu/dev/img/head_20.png%29]]) (ID 1452)||joined||0||
||10 Oct 2008 15:11:01 UTC||[[http://boinc.berkeley.edu/dev/show_user.php?userid=1452 Lduhl@berkeley.edu]([Image%28http://boinc.berkeley.edu/dev/img/head_20.png%29]]) (ID 1452)||quit||0||
||14 Nov 2008 22:37:04 UTC||[Pepo](http://boinc.berkeley.edu/dev/show_user.php?userid=1063) (ID 1063)||quit||13 752.23||

---

My proposal is to modify Team history page as following: Add one additional column (also into the database) named '''Initiator''', which would hold the ID of person, responsible for someone quitting a team: if the user went on his own, there would be his ID, if the team founder kicked him off as ''inactive member'', the founder's ID would go there.

(There were social features' proposals for letting a newcomer ''credit'' someone for having him join a team - such user would have the opportunity to name the inviting user and his name would be logged in the table as ''Initiator'' as well.)

One of the most vocal requests were for storing the ID of users, who requested team ownership transfers and especially those, who took it over (or were assigned a founder).

== Team history for !MyOwnTeam ==
||'''When'''||'''User'''||'''Action'''||'''Initiator'''||'''Total credit at time of action'''||
||18 Aug 2008 17:32:56 UTC||[[http://boinc.berkeley.edu/dev/show_user.php?userid=1452 Lduhl@berkeley.edu]([Image%28http://boinc.berkeley.edu/dev/img/head_20.png%29]]) (ID 1452)||joined||||0||
||10 Oct 2008 15:11:01 UTC||[[http://boinc.berkeley.edu/dev/show_user.php?userid=1452 Lduhl@berkeley.edu]([Image%28http://boinc.berkeley.edu/dev/img/head_20.png%29]]) (ID 1452)||quit||[Pepo](http://boinc.berkeley.edu/dev/show_user.php?userid=1063) (ID 1063)||0||
||12 Oct 2008 13:15:22 UTC||[[http://boinc.berkeley.edu/dev/show_user.php?userid=8 Ageless]([Image%28http://boinc.berkeley.edu/dev/img/head_20.png%29]]) (ID 8)||joined||[Pepo](http://boinc.berkeley.edu/dev/show_user.php?userid=1063) (ID 1063)||8 433.56||
||13 Oct 2008 23:10:54 UTC||[[http://boinc.berkeley.edu/dev/show_user.php?userid=8 Ageless]([Image%28http://boinc.berkeley.edu/dev/img/head_20.png%29]]) (ID 8)||takeover requested||||8 653.96||
||12 Nov 2008 23:10:54 UTC||[[http://boinc.berkeley.edu/dev/show_user.php?userid=8 Ageless]([Image%28http://boinc.berkeley.edu/dev/img/head_20.png%29]]) (ID 8)||tookover||||8 986.32||
||14 Nov 2008 22:37:04 UTC||[Pepo](http://boinc.berkeley.edu/dev/show_user.php?userid=1063) (ID 1063)||quit||[Pepo](http://boinc.berkeley.edu/dev/show_user.php?userid=1063) (ID 1063)||13 752.23||

Last but not least, it would be nice to additionally store then-valid users' names as text strings, not just their IDs (this way an additional event of team being renamed could be logged here too), but this would possibly inflate the tables in database. Although not that much...

,,Displayed user names and events are (mostly) imaginary and do not necessarily mirror real users or events.,,

Migrated-From: http://boinc.berkeley.edu/trac/ticket/757
",Is this still an issue that people want added? ,30260406
1043,Ability to see / reset to default (project) prefs,open,2015-02-04T01:55:12Z,2019-10-29T16:24:45Z,,MEMBER,"**Reported by jbk on 28 Nov 38783582 07:33 UTC**
Some users report that they have played around with the prefs and subsequently forgotten what the default was. They request a way to reset prefs and project specific prefs or some way to see the default values for items in them.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/754
","I agree that the web preferences could use this button. But the initial / default state of the local preferences (on the Manager's Computing preferences dialog, before the user modifies it) is the web preferences. in other words, if there is no  global_prefs_override.xml file, then the Manager's dialog is populated with the values from global_prefs.xml, which has the values from the user's web preferences.

This is explained at the top of the Manager's Computing Preferences dialog which says either:
     Using web-based preferences from <project>
     Set values and click OK to use local preferences instead.
when there is no global_prefs_override.xml file, or:
     Using local preferences.
     Click ""Use web prefs"" to use web-based preferences from <project> 
when there is a global_prefs_override.xml file.

Given this behavior, what else would the ""initial"" or ""default"" settings of this dialog be? ",30260406
1044,BOINC forum editor should not discard user's message upon sending,open,2015-02-04T01:54:20Z,2017-08-17T14:30:57Z,,MEMBER,"**Reported by Pepo on 15 Apr 38774523 23:33 UTC**
There are at least two cases, where user's typed in forum message gets inadvertedly lost, although maybe unnecessarily:
- Akismet: upon sending a message, if akismet is involved and argues about the message contents, it simply writes ''""'''Unable to handle request.''' Your post has been marked as spam by akismet.net anti-spam system. If you feel that this is wrong, please try editing your message.""'' What message? It is already lost!
- Sending (private) messages too often: ''""'''Unable to handle request.''' You are not allowed to send private'''''**S**''''' messages so often. Please wait some time before sending more messages.""'' But in that moment the message is already lost. This possibly applies (or will apply) also to normal forum messages, because at least in the team forums there are time limit settings available.
- I can imagine other scenarios like user ID being blacklisted, not enough total or average credit to post, etc...
  In all such cases, user should get (back) to the preview editor, facing the important message at the top of preview area.

(Sometimes, after having the message previewed prior to sending, it may be possible to get the browser one page back, refresh the expired page and see the (pre)previous preview again, but 1) this is not always the case, 2) not all messages are previewed first, 3) not every user has knowledge about this, 4) just the frustrated and negatively skilled  users copy their messages into clipboard prior to pressing ""Send message"" button.)

Migrated-From: http://boinc.berkeley.edu/trac/ticket/752
","Caching messages while writing/before sending, that would really be nice but I presume that it's a lot of work? ",30260406
1045,Manager connected remotely may show wrong information if clocks differ (Transfers tab),open,2015-02-04T01:47:47Z,2017-04-06T12:37:43Z,,MEMBER,"**Reported by Nicolas on 31 Dec 38736887 00:53 UTC**
1. Set the clock on computer A to one minute earlier than the clock on computer B.
2. Run the client on computer A.
3. Run the manager on computer B.
4. Wait for a file transfer to happen on computer A (for example, get more work).

The manager on computer B will say ""retrying transfer in 00:00:59"" (and counting down) even though the transfer is '''currently running''' with no problems.

Found the problem using BOINC 6.2.18 client, BOINC 5.10.45 manager; Windows XP on both.

The cause for this is that the client sends a Unix timestamp saying when the next attempt will be. I guess it's set to the current time if the transfer is currently running. If clocks differ, the manager may think the timestamp is in the future, so it shows the countdown.

I'm not sure how this could be fixed. Maybe the client could set it to a far-in-the-past timestamp instead of the present (but I'm not even sure how it currently works, so this is a wild guess). Ideally change the protocol so that there is an extra ""show retry countdown"" flag instead of using the timestamp for both purposes. Or maybe even try to compensate the clock offset by exchanging current clocks at the beginning.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/738
",This seems to be an inherent problem when sending unix timestamps around. Maybe we should just detect a time offset and inform the user that he needs to check NTP settings on both hosts.,30260406
1046,BOINC should check for errors in xml files without unparsed_xml flag on.,open,2015-02-04T01:31:15Z,2017-04-06T11:59:42Z,,MEMBER,"**Reported by Ageless on 23 Aug 38565018 23:33 UTC**
This bug came up through Michael Tughan, who found that his self-built Macintosh application for one of the projects was dropped by BOINC 6.2.14 as soon as it started. 

Both Rom and I checked his app_info.xml file and we could find nothing wrong with it at first. Eventually after checking all other possibilities, our attention returned to the app_info.xml file, where we did find a missing / in an end tag. 

So the question is, shouldn't BOINC be checking for things like this, especially when using user supplied applications? Perhaps <unparsed_xml> should be on all the time? 

Migrated-From: http://boinc.berkeley.edu/trac/ticket/703
",This can be solved by using a real XML parser library on the Client that checks for malformed XML before using it. Marking this as an Enhancement rather than a Defect. The high estimate is because this should be done to all XML parsing in the Client not only the described files.,30260406
1047,Use Windows (un)installer to fully uninstall WCG/BOINC,open,2015-02-04T01:28:50Z,2017-08-18T14:51:17Z,,MEMBER,"**Reported by Ageless on 23 Mar 38539966 15:33 UTC**
Is it possible that the Windows installer/uninstaller asks to remove all traces of BOINC? Now when uninstalling BOINC through Add/Remove programs, the BOINC directory and entries in the registry stay in place. 

What I would like is the option that some programs (games mostly) use, which ask you after you uninstalled the program if you want to delete the registry entries and 'saved files' as well. Of course this option should only be used when BOINC is uninstalled from the Add/Remove Programs program, not when upgrading it.

It gives that extra bit of finesse and service to the program

Migrated-From: http://boinc.berkeley.edu/trac/ticket/698
","I still do. It runs from a batch file. This is just an idea. I think it misses one or two registry entries. 
```
@ECHO OFF
Set CD=%CD%
Set SCRIPTDIR=%~dp0
Rem Removing of groups and accounts
ECHO Removing Groups and Accounts
net localgroup boinc_admins boinc_master /delete
net localgroup boinc_projects boinc_project /delete
net localgroup boinc_admins /delete
net localgroup boinc_users /delete
net localgroup boinc_projects /delete

Rem removing User Rights Assignments
ECHO Removing User Rights Assignments
%CD%:\ntrights -r SeNetworkLogonRight -u boinc_master
%CD%:\ntrights -r SeNetworkLogonRight -u boinc_project
%CD%:\ntrights -r SeDenyInteractiveLogonRight -u boinc_project
%CD%:\ntrights -r SeDenyInteractiveLogonRight -u boinc_master
%CD%:\ntrights -r SeServiceLogonRight -u boinc_master
%CD%:\ntrights -r SeServiceLogonRight -u boinc_project
%CD%:\ntrights -r SeDenyNetworkLogonRight -u boinc_project
%CD%:\ntrights -r SeDebugPrivilege -u 
%CD%:\ntrights -r SeIncreaseQuotaPrivilege -u boinc_users
%CD%:\ntrights -r SeIncreaseQuotaPrivilege -u boinc_admins
%CD%:\ntrights -r SeAssignPrimaryTokenPrivilege -u boinc_users
%CD%:\ntrights -r SeAssignPrimaryTokenPrivilege -u boinc_admins

Rem killing Boinctray.exe if that's still running
ECHO Killing Boinctray.exe if that's still running, else throw a message about that. 
tskill boinctray /A

Rem Removing left-over directories
ECHO Removing Left-Over Directories
FOR /F ""tokens=2* delims=	 "" %%A IN ('REG QUERY ""HKEY_LOCAL_MACHINE\SOFTWARE\Space Sciences Laboratory, U.C. Berkeley\BOINC Setup"" /v INSTALLDIR') DO SET ID=%%B
RMDIR /S /Q %ID%

FOR /F ""tokens=2* delims=	 "" %%A IN ('REG QUERY ""HKEY_LOCAL_MACHINE\SOFTWARE\Space Sciences Laboratory, U.C. Berkeley\BOINC Setup"" /v DATADIR') DO SET DD=%%B
RMDIR /S /Q %DD%

cd %homepath%
cd..
RMDIR /S /Q boinc_maste*
chdir /d %SCRIPTDIR%

Rem removing registry entries
ECHO Removing Registry Entries 
regedit /s cleaner.reg

:DONE
ECHO All done. Now all you need to do is remove this temporary directory and all files therein.

ECHO Thank you for using BOINC. We're sorry to see you go.
``` ",30260406
1048,All functions (buttons) in right click context menu,open,2015-02-04T00:53:18Z,2019-03-14T19:57:16Z,,MEMBER,"**Reported by rebirther on 23 Mar 38312230 21:46 UTC**
The BM is much too wide, to improve user activity (abort, suspend, resume etc.) it is better to handle with right mouse click, you remembering on 4.19?

Migrated-From: http://boinc.berkeley.edu/trac/ticket/624
","> setting both the Manager and the Event Log to a fixed width.

No, please don't do that. We all have our own differing preferences for how our screens should be arranged, and the mechanism for remembering where and what size each component was last placed seems to work well (until the screen geometry changes). On machines where I'm not doing other work, I like to set up BOINC to use the whole screen, with the Event Log at minimum width on the right, and the main Manager display to the left. Not everyone will want that, but the option should be there.",30260406
1049,Add a mechanism where joining a team or group requires approval of an admin.,open,2015-02-04T00:44:52Z,2017-08-17T13:44:55Z,,MEMBER,"**Reported by Didactylos on 25 Apr 38264736 22:13 UTC**
Add a mechanism where joining a team or group requires approval of an admin.

Source: DevProjects

Migrated-From: http://boinc.berkeley.edu/trac/ticket/605
",Is this still something people want? ,30260406
1050,Commas in usernames break private messages,open,2015-02-04T00:40:12Z,2018-11-04T20:27:56Z,,MEMBER,"**Reported by ToeBee on 8 Dec 38260853 16:00 UTC**
Commas are used on the ""Send Private Message"" page (pm.php?action=new) to allow you to send messages to multiple recipients. However this causes a problem if the user you are trying to send a message to has a comma in their username. It tries to find two users to send the message to. As a workaround you can remove the name and only use the user ID but when you click on a user's name from the forum both are included. 

Either a different delimiter needs to be used or the username needs to be enclosed in something or maybe commas in usernames should be escaped. Currently the username is enclosed in parens but the parsing of usernames apparently doesn't take them into account. Also, what if a username has parens in their name? Should some of these characters be disallowed?

Migrated-From: http://boinc.berkeley.edu/trac/ticket/594
","From the opening report:

> the username needs to be enclosed in something

It is enclosed in something: it's enclosed in brackets (and has been for the last 11 years).",30260406
1051,Support IPv6 on GUI RPCs,open,2015-02-04T00:24:09Z,2018-10-11T10:15:52Z,,MEMBER,"**Reported by dick groeneveld on 22 Sep 38148807 07:33 UTC**
Please provide support for IPv6 in a MS Vista environment

Migrated-From: http://boinc.berkeley.edu/trac/ticket/561
",This is essential for IPv6-only hosts. BOINC RPC is currently unusable unless a host is dual-stacked.,30260406
1052,Limited team types,open,2015-02-03T22:30:27Z,2018-10-11T10:11:43Z,,MEMBER,"**Reported by Didactylos on 5 Jan 37531830 20:00 UTC**
The current team types are
- Unclassified
- Company
- Primary school
- Secondary school
- Junior college
- University or department
- Government agency

This doesn't begin to cover the actual team types, which is presumably why this feature is mostly ignored. Also, the educational types seem overrepresented and are US-centric. Here are some addition team genres observed in the wild:
- religious groups
- non-profits
- national teams
- local, geographic teams
- teams centring around anything from sexual orientation to CPU preference
- teams centring around an existing community, website or webcomic
- personal teams

Migrated-From: http://boinc.berkeley.edu/trac/ticket/327
",Standard enumeration problem. More complicated than it sounds as they are defined in multiple places and translated. ,30260406
1053,Require unique username,open,2015-02-03T21:31:44Z,2019-05-22T22:33:52Z,,MEMBER,"**Reported by Eric Myers on 16 Jun 37375831 14:13 UTC**
Right now anybody can change their displayed username to anything, which allows for spoofing as someone else in the forums.   Displaying the id# under the name/avatar helps prevent this, but doesnt' stop it.

We should require that all usernames be unique.   They can still be changed, but whenever a name is chosen or changed it cannot be a name already in use.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/193
","**Commented by Nicolas on 22 Aug 40457912 15:06 UTC**
Or what about requiring a unique username only  to participate in forums? First time you try posting, if your name is also in use by other users, you're asked to change it.
",30260406
1054,sort order options for results.php,open,2015-02-03T20:41:29Z,2018-10-11T09:56:05Z,,MEMBER,"**Reported by AndyK on 11 Feb 37304275 22:13 UTC**

I would like to have the possibility to sort the output of the results.php by the column headers.

Would be great if we could specify more than one criteria.
For example:
- First click on a column header makes it 1st order criterion ascending
- Second click on that column header makes it 1st oder criterion descending
- First click on another column header makes it 2nd order criterion ascending
- Second click on that second column header makes it 2nd order criterion descending
- and so on...

And one link/button to eliminate the sorting

Migrated-From: http://boinc.berkeley.edu/trac/ticket/93
",There are some very useful JS libraries available now that can do table sorting on the client side. The DB just has to supply an unsorted list which might be very big.,30260406
1055,Wish: search function with RegEx and related to the current open forum level,open,2015-02-03T20:40:06Z,2018-10-11T09:53:50Z,,MEMBER,"**Reported by AndyK on 19 Nov 37304113 00:53 UTC**
Would be very nice to have a keyword search function on each forum level with RegEx capability.

With level I mean: Main forum index, 1st level subforum like 'Number Crunching' or 'Problems and Help' etc. and displayed thread
- At the main index it should search all the forum db. ''already done''
- At the 1st subforum level it should search only that forum (including any subforums), etc.
- And when a thread is displayed the search function should be related only to that thread including all posts which are not displayed due to forum settings for that user.

Migrated-From: http://boinc.berkeley.edu/trac/ticket/90
","The advanced search already allows to search by forum but the search box stays as _search forums_. It should be possible to change the behavior of the search box so that it changes the scope depending on the view e.g forum_index.php,  forum_forum.php or forum_thread.php. This would be a nice enhancement for anyone who wishes to start contributing.  Adding RegEx capability to the searches would be more complicated and is most probably not required. ",30260406
1056,Miscalculating Disk Size and Free Space,open,2015-02-03T20:39:44Z,2018-10-12T11:41:12Z,,MEMBER,"**Reported by KSMarksPsych on 31 Oct 37303931 23:33 UTC**
Posted by: 
Date: 9:54 AM 02-09-2007
Messages tab says ""No disk space""

Message tab says 256 TB total, 0 free space; pie chart says 'free disk space 17206.29 TB'; while in reality the C: drive has 6GB free and the D: has 229GB free.

All processing stops on this machine b/c it thinks there is no disk space.

This is on a Windows 2003 SERVER machine -- I tried to follow the instructions for running as a service more securely by denying boinc access to everything except the BOINC directory and children. Possibly related. -? (Unrelated, but I really would like a dumbed-down installer for the enterprise that allows schedules including weekend issues.)

What folders and permissions do I need to allow the BOINC service access to so it'll work?
(ugh)

Posted by: 
Date: 10:07 AM 02-09-2007
yep sure enough - I removed the prohibitions from BOINC writing anywhere EXCEPT the BOINC directory and the numbers corrected themselves and stuff started working.

BUG: when you get the error ""you can't read this disk to find out how much is free"" b/c you don't have permissions -- this shouldn't be interpreted as meaning you have a 17206.29TB or 256 TB sized disk with 0 bytes free.

also -- this great CRITICAL instructions: http://boinc.truxoft.com/security.htm
are crap when using the 5.8.8 client since if you follow the instructions then boinc stops working since it can't figure out what disk space is free

PLEASE somebody write an installer for server machines to run in the nighttimes and weekends. There's a lot of unused horsepower out there with machines doing nothing right now--they need a good installer that will install more safely with restricted use.

Posted by: 
Date: 10:09 AM 02-09-2007
This was happening on Windows Server 2003 Enterprise Edition but might happen on any XP box when invoking the restrictions suggested on truXoft.com

Migrated-From: http://boinc.berkeley.edu/trac/ticket/89
","**Commented by Pepo on 28 Dec 41209763 18:13 UTC**
Not fix anymore :-(

I've installed the 6.12.15 on a new Win 7 x64 machine - a compatibility mode installation (to be able to try out GPU) into default location. It has a 500 GB HDD (450.4 GB available on a system partition, 59.2 GB used, 391.1 GB free). Both Manager and client and apps were running under my user account (a plain user).

I believe this all is happening because the disk quota management is switched on, set to 35 GB for the user running BOINC apps (= myself). According to the quota manager and Windows Explorer, this user has currently 31.2 GB free.

global_prefs_override says:

``` xml
<global_preferences>
   <disk_max_used_gb>2.000000</disk_max_used_gb>
   <disk_max_used_pct>2.000000</disk_max_used_pct>
   <disk_min_free_gb>5.000000</disk_min_free_gb>
</global_preferences>
```

The Event log says:

```
Disk: 35.00 GB total, 391.14 GB free
max disk usage: 0.70GB
```

Finally, the BOINC Manager's Disk usage tab diplays:[\* used by BOINC: 54.60 KB (no projects so far)
- free, available to BOINC: 716.75 MB
- free, not available to !BOINC:390.44 GB
- used by other programs: -373444662.60 KB

(can be seen in the attached image)

The client is apparently mixing the quota-limited size with partition free space...

---

There was a fix in changeset:23079 for this, unfortunately it did not made it from trunk into 6.12 area (for testing).

---

I've later installed 6.12.18 as a service and modified the quotas for newly created boinc_master (8 GB) and boinc_project (6 GB). And off course BOINC disk usage settings accordingly. This confirmed that the Manager displays disk usage according to the boinc_master's quotas.

I understand and see that files written by these accounts are counted against their own separate quotas...

I guess that if quotas are noticed being in use, it could be sufficient to:
- take both boinc_master's (directly by the client, as it is done now) and boinc_project's (by launching some tiny test app under boinc_project account) quota free space and use the smaller of them as reference,
- in Manager's Computing Preferences dialog,
  - append ""Use at most [ xx ]([BR]]) GB disk space"" and ""Use at most [ xx ] % of total disk space"" with something like ""might be limited by disk quotas, if in use"",
- in the ""Total disk usage""graph,
  - use the lower quota free reference as an upper cap for the conventionally (i.e. from the whole disk partition) calculated ""free, avaliable to BOINC: xx GB"" value,
  - correct the ""used by other programs"" value.
",30260406
1057,differentiate between number of CPUs to use in idle and busy mode,open,2015-02-03T20:15:43Z,2018-10-29T12:56:29Z,,MEMBER,"**Reported by KSMarksPsych on 12 Oct 37296831 05:46 UTC**
Posted by: 
Date: 3:05 PM 04-07-2007
BOINC detects if the machine is busy and can limit the number of CPUs to use. I'd like to see a feature, that I can define 2 numbers of CPUs to use, one if the machine is idle and one if the machine is busy. Right now I've got a dual processor machine and allow BOINC to use one of them so that my work is not slowed down. But when the machine is idle, BOINC could use both CPUs. Right now, this is not possible. With the advent of multi-processor/core machines spreading out to end users, this would allow more CPU power to be used by BOINC and the necessary preconditions are already implented. 

Migrated-From: http://boinc.berkeley.edu/trac/ticket/41
","@sirzooro  👍 +1 for the ""PrimeGrid challenge"" prefereeeeeeeence! **YEEEEEEEEEAH!** :-)
🥇 ",30260406
1058,Configure proxy settings at install time via command line options,open,2015-02-03T20:06:03Z,2017-04-05T14:29:48Z,,MEMBER,"**Reported by Ageless on 17 Nov 37294705 08:00 UTC**
version 4.27

Posted by: boinc 
Date: 10:14 PM 04-04-2005
would like the ability to set proxy settings during installation from the command line using .msi installation methods

Posted by: rob 
Date: 6:09 AM 04-05-2005
That might be nice... Rom? 

Migrated-From: http://boinc.berkeley.edu/trac/ticket/21
","**Commented by romw on 22 Jun 40859071 06:13 UTC**
Setup bugs are being punted to a future release.  This work item should be completed with the new open source installer.
",30260406
1059,dice metric,open,2020-03-25T14:33:37Z,2020-03-27T05:57:40Z,,NONE,"catalyst/utils/metrics/dice.py
`dice = 2 * (intersection + eps * (union == 0)) / (union + eps)`

It seems like if union = 0 and intersection = 0, we get  dice = 2 * (0 + eps) / (0 + eps) = 2 ?

Should be 
`dice = (2 * intersection + eps * (union == 0)) / (union + eps)`
or I don't understand something?)","Wow, thank you @vsokhatskyi  for you findings!
Would you like to make a PR to handle this issue? It would be great.",145385156
1060,fix text2embedding ,open,2020-03-24T10:52:42Z,2020-03-24T16:42:33Z,,NONE,"## Description

Just small fix. In considering we are using `nn.DataParallel` we should call `.config` through `model.module.config`

## Related Issue

https://github.com/catalyst-team/catalyst/issues/700

## Type of Change

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [ ] Examples / docs / tutorials / contributors update
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] Improvement (non-breaking change which improves an existing feature)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [x] I have read the [Code of Conduct](https://github.com/catalyst-team/catalyst/blob/master/CODE_OF_CONDUCT.md) document.
- [x] I have read the [Contributing](https://github.com/catalyst-team/catalyst/blob/master/CONTRIBUTING.md) guide.
- [ ] I have checked the code-style using `make check-codestyle`.
- [ ] I have written tests for all new methods and classes that I created.
- [ ] I have written the docstring in Google format for all the methods and classes that I used.
- [ ] I have checked the docs using `make check-docs`.
- [ ] I have read I need to click 'Login as guest' to see Teamcity build logs.
","hi, @PUSSYMIPT 
Thanks for contributing, could you please add some small test for this contrib script?
for example, use the data from [bert classification example](https://github.com/catalyst-team/catalyst/blob/master/examples/distilbert_text_classification/input/train.csv)?
something like,
```
catalyst-contrib text2embedding --in-cvs=./examples/distilbert_text_classification/input/train.csv
```
to the end of our nlp testing script?
https://github.com/catalyst-team/catalyst/blob/master/bin/tests/check_dl_nlp.sh",145385156
1061,Warn about missing packages only if environment variable is set,open,2020-03-23T12:48:30Z,2020-03-24T10:49:08Z,,CONTRIBUTOR,"## Description

Why should people get this warning if they don't use `alchemy`? I think it should be under the `if` statement just as `Neptune`.

Fast example screenshot:
![image](https://user-images.githubusercontent.com/9883873/77356815-167c0e80-6d58-11ea-8c30-b7fdea34c055.png)


## Type of Change

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [ ] Examples / docs / tutorials / contributors update
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] Improvement (non-breaking change which improves an existing feature)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [x] I have read the [Code of Conduct](https://github.com/catalyst-team/catalyst/blob/master/CODE_OF_CONDUCT.md) document.
- [x] I have read the [Contributing](https://github.com/catalyst-team/catalyst/blob/master/CONTRIBUTING.md) guide.
- [ ] I have checked the code-style using `make check-codestyle`.
- [ ] I have written tests for all new methods and classes that I created.
- [ ] I have written the docstring in Google format for all the methods and classes that I used.
- [ ] I have checked the docs using `make check-docs`.
- [x] I have read I need to click 'Login as guest' to see Teamcity build logs.
","This is how it looks in Jupyter Notebook
![image](https://user-images.githubusercontent.com/9883873/77417313-3b16cb80-6dd6-11ea-97ff-557e7a38a847.png)
",145385156
1062,"Freezing after training ends, Catalyst 20.03+ version",open,2020-03-19T13:12:22Z,2020-03-24T09:30:21Z,,NONE,"**Describe the bug**
After training end I see next picture forever:
```
Top best models:
logs/4classes/version3_noleak_simpleface_effb0_cosine_ls_e01/112_really_hard_augs/200319.130339.SEybo2/checkpoints/stage1.1.pth  0.6985
```
It just freezes. This happened with my pipeline after upgrading 20.02 -> 20.03.

**To Reproduce**
I have 3 GPU server with NVIDIA 1080Ti and Threadripper, if needed I can give detailed hardware. Training is in DataParallel mode, not DDP. 
My config:
```
model_params:
  model: MixFace
  encoder_name: efficientnet_b0
  encoder_pretrained: True
  encoder_library: timm
  neck_transform: linear
  embeddings_size: 512
  classes: 4
  head_s: 12
  head_m1: 0.2
  head_m2: 0


runner_params:
  input_key: [""x"", ""labels""]


args:
  expdir: ""./experiments/4classes""
  baselogdir: ""./logs/4classes/version3_mixface_s12_effb0_cosine_ls_e01""
  seed: 8
  deterministic: True
  benchmark: True


stages:

  stage1:

    data_params:
      batch_size: 170
      num_workers: 12
      per_gpu_scaling: True
      drop_last: False
      shuffle: True
      loaders_params:
        valid:
          batch_size: 170

      train_dataset_params:
        image_folder_path: ""/usr/local/FileStorage/data_training/glasses_classification/train""
        image_file_path: ""/usr/local/FileStorage/data_training/glasses_classification/version_3/train.txt""

      valid_dataset_params:
        image_folder_path: ""/usr/local/FileStorage/data_training/glasses_classification/valid""
        image_file_path: ""/usr/local/FileStorage/data_training/glasses_classification/version_3/valid.txt""


    state_params:
      num_epochs: 70
      main_metric: &main_metric f1
      minimize_metric: False
      valid_loader: valid


    criterion_params:
      criterion: CrossEntropyLossLabelSmoothing
      reduction: mean
      smooth_eps: 0.1


    scheduler_params:
      scheduler: CosineAnnealingWarmRestarts
      T_0: 900
      T_mult: 2
      eta_min: 0


    optimizer_params:
      optimizer: SGD
      lr: 0.1
      weight_decay: 0.001
      nesterov: True
      momentum: 0.9


    callbacks_params:
      loss:
        callback: CriterionCallback

      optimizer:
        callback: OptimizerCallback

      accuracy:
        callback: AccuracyCallback
        accuracy_args: [1, 2]

      f1_score:
        callback: F1ScoreMetricCallback
        n_classes: 4
        class_mapping: ['face_in_optical_glasses', 'face_in_protective_glasses', 'face_in_sunglasses', 'face_without_glasses']


      scheduler:
        callback: SchedulerCallback
        mode: batch

      logger:
        callback: AlchemyLogger
        token: ""484b162171bce2dc5371f9acc09d9fd2""
        project: ""glasses_classification""
        experiment: ""version3_mixface_s12_effb0_cosine_ls_e01""
        group: ""validation_version_2""
```


**Additional context**
After `Ctrl + C` I see next picture:
```
raceback (most recent call last):
  File ""/home/andrii/anaconda3/envs/face/lib/python3.7/threading.py"", line 1273, in _shutdown
    t.join()
  File ""/home/andrii/anaconda3/envs/face/lib/python3.7/threading.py"", line 1032, in join
    self._wait_for_tstate_lock()
  File ""/home/andrii/anaconda3/envs/face/lib/python3.7/threading.py"", line 1048, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt
```
Obviously it's problem with multithreading. I can suggest, that threads are not daemons and they have something like forever loop or there are deadlocks
","I'm sorry, but I don't know how to give you an working example, because I cannot share my data, only my hardware and configs. But I was able to fix it - may be it will give you a suggestion why it's happening. It was fixed by next actions:
https://github.com/catalyst-team/alchemy/blob/master/alchemy/logger.py#L52 change this line to 
```
self._thread = threading.Thread(target=self._run_worker, daemon=True)
```
and commenting this line https://github.com/catalyst-team/alchemy/blob/master/alchemy/logger.py#L84

I thought my problem was caused my forever loop, so I made an thread daemon and deleted `join` of this thread, when logger is closed, so main thread is not waiting for this thread to join anymore, and this helped.",145385156
1063,Training without storing model states,open,2020-03-14T10:06:51Z,2020-03-14T10:09:39Z,,NONE,"**Description**
In some rare cases, for example, when you need to finetune a large model on a small dataset the majoring part of training loop is waiting for saving model checkpoints to a hard drive.

**Proposal**
Would be logically to add a `CheckpointCallback` with parameter `save_n_best=0` to a configuration and do not store best checkpoints and instead use the latest state of the model.

**Note**
All of the described above is a proposal mostly for **config API** because **config API** during the stages loads the best checkpoint from the previous stage.

So here are a few steps to how it can be achieved:
1. For using the latest state of the model you need to overload a `get_model` property of `ConfigExperiment` to return the latest model (something like in `BaseExperiment`).
2. To prevent saving the model state to a hard drive overload property `get_callbacks` of experiment class and remove `CheckpointCallback ` from default callbacks.
3. Create and use an empty callback instead of `CheckpointCallback`.
",,145385156
1064,text2embedding bug,open,2020-03-11T08:24:19Z,2020-03-24T08:40:21Z,,NONE,"When I tried to get embeddings from my text via `catalyst-data text2embedding` I got this error:

![image](https://user-images.githubusercontent.com/37884009/76395460-676b2a80-6388-11ea-806f-70c328704901.png)

Probably lines 145 and 146 in `catalyst/data/scripts/text2embedding.py` should be

```
                hidden_size=model.module.config.hidden_size,
                output_hidden_states=model.module.config.output_hidden_states,
```

instead of

```
                hidden_size=model.config.hidden_size,
                output_hidden_states=model.config.output_hidden_states,
```

pytorch version: 1.4.0
","Thanks for noting,
Would you like to make a pull request to resolve the issue?",145385156
1065,Reassessing Visdom Support,open,2020-03-06T23:32:27Z,2020-03-24T08:42:27Z,,NONE,"I have a solution for visdom support (https://github.com/facebookresearch/visdom) which was pointed out here in closed issues (https://github.com/catalyst-team/catalyst/issues/344).

I implemented:
-  VisdomLogger based on alchemy.logger (https://github.com/catalyst-team/alchemy/blob/master/alchemy/logger.py)
- VisdomRunner and SupervisedVisdomRunner based on AlchemyRunner (https://github.com/catalyst-team/catalyst/blob/master/catalyst/contrib/dl/runner/alchemy.py)

![image](https://user-images.githubusercontent.com/2220676/76134146-48684380-5fea-11ea-896b-33c51b718850.png)
","Thanks for noting,
would you like to make a pull request on this feature?",145385156
1066,"When using an activation function of ""Softmax2d"" for many callbacks and losses, no argmax is applied",open,2020-03-02T14:33:54Z,2020-03-24T08:41:31Z,,CONTRIBUTOR,"# (I will compile a list and hopefully open a PR if needed)

**Describe the bug**
This behavior is present in a plethora of catalyst's callbacks and losses. It's consistent, but it's definitely confusing for many new users.

**To Reproduce**
Steps to reproduce the behavior:
Use these functions/classes:
Callbacks
* [`MeterMetricsCallback`](https://github.com/catalyst-team/catalyst/blob/master/catalyst/dl/core/callback.py)
  * I'm not sure if this can support meters with softmax? Need to double check

Criterion // Dependents
* [`dice`](https://github.com/catalyst-team/catalyst/blob/master/catalyst/utils/criterion/dice.py)
  * [`DiceCallback`](https://github.com/catalyst-team/catalyst/blob/master/catalyst/dl/callbacks/metrics/dice.py)
  * [`DiceLoss`](https://github.com/catalyst-team/catalyst/blob/master/catalyst/contrib/nn/criterion/dice.py)
* [`f1_score`](https://github.com/catalyst-team/catalyst/blob/master/catalyst/utils/criterion/f1_score.py)
  * [F1ScoreCallback](https://github.com/catalyst-team/catalyst/blob/master/catalyst/dl/callbacks/metrics/f1_score.py)
* [`iou`](https://github.com/catalyst-team/catalyst/blob/a7154b8d37b7368f6984c23df309069d22cd32fd/catalyst/utils/criterion/iou.py#L9)
  * [`IoULoss`](https://github.com/catalyst-team/catalyst/blob/master/catalyst/contrib/nn/criterion/iou.py)
  * [`IouCallback`]()

**Expected behavior**
Should ideally apply argmax when `activation='Softmax2d'`

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Additional context**
Add any other context about the problem here.
","Do you have a minimal example to reproduce the issue?
",145385156
1067,[WIP] Fixed gradient tracking,open,2020-02-25T11:27:52Z,2020-03-24T11:25:38Z,,NONE,"## Description

Fixed storing gradients in OptimizerCallback

## Related Issue

<!-- If your PR refers to a related issue, link it here. -->

## Type of Change

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [ ] Examples / docs / tutorials / contributors update
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] Improvement (non-breaking change which improves an existing feature)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [x] I have read the [Code of Conduct](https://github.com/catalyst-team/catalyst/blob/master/CODE_OF_CONDUCT.md) document.
- [x] I have read the [Contributing](https://github.com/catalyst-team/catalyst/blob/master/CONTRIBUTING.md) guide.
- [x] I have checked the code-style using `make check-codestyle`.
- [x] I have written tests for all new methods and classes that I created.
- [x] I have written the docstring in Google format for all the methods and classes that I used.
- [x] I have checked the docs using `make check-docs`.
- [x] I have read I need to click 'Login as guest' to see Teamcity build logs.
",@pdanilov could you please fix codestyle issues?,145385156
1068,improve wandb to log model grads and use custom time axis [REOPENED],open,2020-02-17T10:37:34Z,2020-03-13T18:26:42Z,,CONTRIBUTOR,"## Description
REOPENED
I've added wandb.watch(model) to _pre_experiment_hook and extend _log_metric interface with
step

## Related Issue

#656 

## Type of Change

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [ ] Examples / docs / tutorials / contributors update
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] Improvement (non-breaking change which improves an existing feature)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [x] I have read the [Code of Conduct](https://github.com/catalyst-team/catalyst/blob/master/CODE_OF_CONDUCT.md) document.
- [x] I have read the [Contributing](https://github.com/catalyst-team/catalyst/blob/master/CONTRIBUTING.md) guide.
- [x] I have read I need to click 'Login as guest' to see Teamcity build logs
- [x] I have checked the code-style using `make check-codestyle`.
- [ ] I have written the docstring in Google format for all the methods and classes that I used.
- [ ] I have checked the docs using `make check-docs`.","@ogvalt Could you please merge with master?
look like we have a conflict for WandbLogger",145385156
1069,Add Vanilla GAN Notebook tutorial,open,2020-02-12T18:41:01Z,2020-03-21T19:14:00Z,,COLLABORATOR,"## Description

Addition of tutorial for GANs in Notebook API.

## Related Issue

This is a clone of PR #653

## Type of Change

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [x] Examples / docs / tutorials / contributors update
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] Improvement (non-breaking change which improves an existing feature)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist

<!-- Mark with an `x` all the checkboxes that apply (like `[x]`) -->

- [x] I have read the [Code of Conduct](https://github.com/catalyst-team/catalyst/blob/master/CODE_OF_CONDUCT.md) document.
- [x] I have read the [Contributing](https://github.com/catalyst-team/catalyst/blob/master/CONTRIBUTING.md) guide.
- [x] I have read I need to click 'Login as guest' to see Teamcity build logs
- [x] I have checked the code-style using `make check-codestyle`.
- [ ] I have written the docstring in Google format for all the methods and classes that I used.
- [x] I have checked the docs using `make check-docs`.",@Arquestro do you have any plan for example update? we have a bunch of framework updates :),145385156
1070,Enhance wandb runner with wandb.watch and log custom step,open,2020-02-12T12:19:06Z,2020-02-12T12:19:06Z,,CONTRIBUTOR,"**Is your feature request related to a problem? Please describe.**
So the ideas here is next:
1. wandb has good solution for tracking gradients of the model via wandb.watch(model). 
2. wandb logs information about the model with respect to steps. Ones increasing each time user call wandb.log(...). Sometimes it's quite confusing when train and valid data don't co-aling in terms of steps.

**Describe the solution you'd like**
1. extend _pre_experiment_hook like:
```
class WandbRunnerV2(WandbRunner):
...
def _pre_experiment_hook(self, experiment: Experiment):
        super(WandbRunnerV2, self)._pre_experiment_hook(experiment)
        model = self.model
        wandb.watch(model)
```
2. I would like to add additional field to log in wandb runner: _epoch for log on each epoch. I've checked that in wandb UI I'm able to set custom axis for visualization.",,145385156
1071,optimize model grads collections,open,2020-02-12T12:07:42Z,2020-02-12T12:07:42Z,,CONTRIBUTOR,"**Is your feature request related to a problem? Please describe.**
I've work with my recent approach to collecting model gradients and find out that is not that optimal (#576 #581 ). 
The main problem is that I need to initialize optimizer in my code with flag save_model_grads an then pass it in callback list. eg:

```
# train.py
optimizer_with_grad_collection = OptimizerCallback(save_model_grads=True)
...
runner.train(...
callbacks = [
optimizer_with_grad_collection,
...
],
)
```
(Also I found that the way that is used to collecting gradients slows down model training and I've already prepared faster solution)

**Describe the solution you'd like**
I want to rollback some changes that I've done and make everything looks like this:
```
# train.py
runner.train(
state_kwargs = {""model_grads"": {}}
...
)
# optimizer.py
class OptimizerCallback(Callback):
...
def on_batch_end(self, state):
...
if hasattr(state, ""model_grads""):
   # collect grads
```",,145385156
1072,Adapt hydra approach to configuration files handling (hydra.cc),open,2020-02-09T20:52:48Z,2020-02-09T20:52:48Z,,CONTRIBUTOR,"**Is your feature request related to a problem? Please describe.**
I found two Issues (#624 and #630) that is trying to address problems that occur when it's
necessary to create new configuration each time when you want to create new experiment.
To be more precise lets consider the case when you want to run experiment with different models to check their performance. You need to create two config files. But if you want to run those experiments with on two datasets you need 4 separate config files. More options you want to have, more numbers of configs you need to create and this number growth exponentially and become hardly to manage.

**Describe the solution you'd like**
Recently FAIR released library called hydra (www.hydra.cc) that address case described above. 
Library support modular approach that is used in convinient programming and use more convinient way to organize input argument parsing.
For more, read: https://hydra.cc/docs/intro
(they describe use cases very clearly)",,145385156
1073,Center Loss,open,2020-02-07T07:35:34Z,2020-02-22T10:06:44Z,,CONTRIBUTOR,"I think it's time to implement also center loss :)

Center Loss: https://ydwen.github.io/papers/WenECCV16.pdf
Implementations: [1](https://github.com/KaiyangZhou/pytorch-center-loss/blob/master/center_loss.py), [2](https://github.com/louis-she/center-loss.pytorch/blob/master/loss.py), [3](https://github.com/SWHL/pytorch-center-loss/blob/master/center_loss.py), [4](https://github.com/jxgu1016/MNIST_center_loss_pytorch/blob/master/CenterLoss.py)",@smivv would you like to make a contribution?,145385156
1074,Config with dependencies,open,2020-02-01T08:44:43Z,2020-02-05T18:48:32Z,,CONTRIBUTOR,"The main idea is one config - one experiment. Some experiments may have common parts. Now this is implemented in the form of passing a list of configs to `catalyst-dl run`. But this is difficult for new people to understand. It is suggested to add the `pre-configs` and `post-configs` fields to the config, which will be converted to the config list when the experiment is started. This should make it easier to understand.","It's like inheriting classes. For example, experiments with different network architectures may have the same data preprocessing. Variations with stages are also possible. Combining several configs via the command line is easy to get confused. Moreover, the presence of one config is easier to explain and it is more intuitive, because a person understands what dependencies this config has and can find all the parameters of the experiment that interest him.",145385156
1075,Pycharm debugger problems with logdir=='',open,2019-11-29T15:00:07Z,2020-03-24T08:36:40Z,,NONE,"**Describe the bug**
If I'll set logdir='' in runner.train method, folder code will be created. 
That folder will become source of an unexpected behavior on next code runs of this script in PyCharm debugger. 

During PyCharm debugger start following code will be evaluated: 
```
try:
    from code import InteractiveConsole
except ImportError:
    from _pydevd_bundle.pydevconsole_code_for_ironpython import InteractiveConsole
```
And will end up with error 
`ImportError: cannot import name 'InteractiveConsole'`
`SyntaxError: Missing parentheses in call to 'exec'`
Because there are folder /code is present in the same location as script, it can be interpreted not as pycharm code.py, but as independent module if there are \_\_init__.py in it (it is possible, because all folder with script will be copied to folder /code). 
I think this is a very unusual behavior, it took me some time to point the problem, so at least some warnings will be perfect.  

**To Reproduce**
Steps to reproduce the behavior:
1. Create script main.py with runner evaluation
2. Create \_\_init__.py file in the same folder
3. Set logdir=''
4. Run script main.py
5. Rerun script with PyCharm debugger

**Expected behavior**
There are several ways to avoid that possible problem
1. Change folder name from /code to something else
2. Do not copy \_\_init__.py file into /code folder
3. Show a warning that logdir='' might cause some problems
","Do you have any troubles with `logdir=""./path/to/logdir""` variant?",145385156
1076,Configurable checkpoints,open,2019-11-18T04:26:18Z,2019-11-18T04:26:18Z,,CONTRIBUTOR,"Right now checkpoint logic in Config API is following: if it's not a first stage, then we'll take `{logdir}/checkpoints/best.pth` as a checkpoint (https://github.com/catalyst-team/catalyst/blob/master/catalyst/dl/experiment/config.py#L117-L121)

I think that would be great and more flexible if we'll add possibility to configure this behavior. What comes in mind:

1. We can add an option on `stages` level to allow to ignore this default behavior and teach each `stage` independently. This way we would be able to teach different folds from one config, different models etc.
2. We can add an option on specific `stage` level to set where to find checkpoint. It will allow us to start not from scratch but from some state even on the first `stage`.
3. Also we can add some param to `infer stage` to allow it to use different checkpoints (and maybe multiple checkpoints and some strategy - how to ansamble those predictions). This one seems as separate interesting feature",,145385156
1077,Output while using utils.plot_metrics,open,2019-11-16T01:08:05Z,2019-12-12T06:52:30Z,,NONE,"**Describe the bug**
I am using 

`
utils.plot_metrics(
    logdir=logdir, 
    # specify which metrics we want to plot
    metrics=[""loss"", ""dice"", 'lr', '_base/lr']
)
`
in google colab and the results are not plotted.

Can you suggest what can be the issue or any other way to get the same.

Thanks.
","You should be able to view your plots by epoch with tensorboard
",145385156
1078,Metrics callbacks should support ignore label,open,2019-11-15T17:14:59Z,2019-11-15T17:14:59Z,,NONE,"**Description**
Sometimes, not all samples in the dataset have annotations for all labels. In this situation, common solution is to use special ""ignore"" value for the not annotated label. However, one-hot encoding, that is required for many metrics callback (e.g. AUCCallback) does not support ignore labels (this is by design of one-hot encoding, for sure), so we cannot calculate metrics for such datasets.

**Proposal**
The idea is to provide two additional parameters for the metrics callbacks:
- labels_filter - any callable, that will take labels list and return indices of labels to preserve
- labels_encoder - any callable, that will take labels and convert them to one-hot like form.
By default, these two params should be None and `one_hot`, correspondingly.

**Describe alternatives you've considered**
As an alternative, I can imagine the user preparing encoded labels together with the list of indices of valida labels, to use in the callback, but it sounds like a pain.",,145385156
1079,Accumulate gradient,open,2019-11-07T21:28:38Z,2020-03-24T08:29:28Z,,NONE,"I was trying to use the accumulate gradient feature but run into an error. The training works without the ```OptimizerCallback(accmulation_steps=2)```.

```
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    callbacks=[DiceCallback(), EarlyStoppingCallback(patience=5, min_delta=0.001), 
                            OptimizerCallback(accumulation_steps=2)],
    logdir=logdir,
    num_epochs=num_epochs,
    verbose=True
)
```
**FYI,  the error message:**

0/60 * Epoch (train):   0% 0/624 [00:00<?, ?it/s]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-32-d63c6b5ac823> in <module>
      9     logdir=logdir,
     10     num_epochs=num_epochs,
---> 11     verbose=True
     12 )

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/runner/supervised.py in train(self, model, criterion, optimizer, loaders, logdir, callbacks, scheduler, resume, num_epochs, valid_loader, main_metric, minimize_metric, verbose, state_kwargs, checkpoint_data, fp16, monitoring_params, check)
    195             monitoring_params=monitoring_params
    196         )
--> 197         self.run_experiment(experiment, check=check)
    198 
    199     def infer(

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/runner.py in run_experiment(self, experiment, check)
    229         except (Exception, KeyboardInterrupt) as ex:
    230             self.state.exception = ex
--> 231             self._run_event(""exception"")
    232 
    233         return self

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/runner.py in _run_event(self, event)
    100 
    101         if self.state is not None and hasattr(self.state, f""on_{event}_post""):
--> 102             getattr(self.state, f""on_{event}_post"")()
    103 
    104     @abstractmethod

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/state.py in on_exception_post(self)
    183     def on_exception_post(self):
    184         for logger in self.loggers.values():
--> 185             logger.on_exception(self)
    186 
    187 

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/callbacks/logging.py in on_exception(self, state)
    194 
    195         if state.need_reraise_exception:
--> 196             raise exception
    197 
    198 

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/runner.py in run_experiment(self, experiment, check)
    226         try:
    227             for stage in self.experiment.stages:
--> 228                 self._run_stage(stage)
    229         except (Exception, KeyboardInterrupt) as ex:
    230             self.state.exception = ex

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/runner.py in _run_stage(self, stage)
    199 
    200             self._run_event(""epoch_start"")
--> 201             self._run_epoch(loaders)
    202             self._run_event(""epoch_end"")
    203 

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/runner.py in _run_epoch(self, loaders)
    186             self._run_event(""loader_start"")
    187             with torch.set_grad_enabled(self.state.need_backward):
--> 188                 self._run_loader(loader)
    189             self._run_event(""loader_end"")
    190 

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/runner.py in _run_loader(self, loader)
    148 
    149         for i, batch in enumerate(loader):
--> 150             self._run_batch(batch)
    151 
    152             self.state.timer.reset()

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/runner.py in _run_batch(self, batch)
    130         self.state.timer.stop(""_timers/model_time"")
    131         self.state.timer.stop(""_timers/batch_time"")
--> 132         self._run_event(""batch_end"")
    133 
    134     def _run_loader(self, loader):

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/runner.py in _run_event(self, event)
     97         if self.callbacks is not None:
     98             for callback in self.callbacks.values():
---> 99                 getattr(callback, f""on_{event}"")(self.state)
    100 
    101         if self.state is not None and hasattr(self.state, f""on_{event}_post""):

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/callbacks/optimizer.py in on_batch_end(self, state)
    117             return
    118 
--> 119         loss = self._get_loss(state)
    120 
    121         self._accumulation_counter += 1

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/callbacks/optimizer.py in _get_loss(self, state)
     91 
     92     def _get_loss(self, state) -> torch.Tensor:
---> 93         loss = state.get_key(key=""loss"", inner_key=self.loss_key)
     94 
     95         if isinstance(loss, list):

~/.conda/envs/mmdet_cloud/lib/python3.6/site-packages/catalyst/dl/core/state.py in get_key(self, key, inner_key)
    114             return getattr(self, key)
    115         else:
--> 116             return getattr(self, key)[inner_key]
    117 
    118     def set_key(self, value, key, inner_key=None):

TypeError: 'NoneType' object is not subscriptable
",could you reproduce the issue with `20.03.3` version?,145385156
1080,"""Lifted Struct Loss"" feature request",open,2019-10-23T17:02:33Z,2019-12-29T10:59:22Z,,CONTRIBUTOR,"""Lifted Struct Loss"" feature request.

Paper: https://arxiv.org/abs/1511.06452
Implementations:
1. https://github.com/bnu-wangxun/Deep_Metric/blob/master/losses/LiftedStructure.py
2. https://gist.github.com/bkj/565c5e145786cfd362cffdbd8c089cf4","Dear @smivv 
Would you like to make a PR with such functionality? Just like our TripletLoss epic :likeacatalyst:",145385156
1081,Distributed training tutorial,open,2019-10-12T09:21:55Z,2019-10-18T21:07:47Z,,COLLABORATOR,"**Is your feature request related to a problem? Please describe.**
We need a tutorial about distributed training in Config API using Catalyst

**Describe the solution you'd like**
A Config API project in `examples` folder (you can create an empty project with `catalyst-dl init` and finish it)

It has to use PyTorch and Catalyst as a train-loop.

Needed features:
- A couple of ec2 instances initialized.
- The model is trained there in distributed mode.
- Weights are uploaded to Weights&Biases.
- Instances are closed.

And the tutorial should explain how to run such training.

```
export MASTER_ADDR=""127.0.0.1""
export MASTER_PORT=29500
export WORLD_SIZE=2 # number of GPUs

RANK=0 LOCAL_RANK=0 catalyst-dl run --config=configs/exp_splits.yml --distributed_params/rank=0:int & # on GPU-0
sleep 5 # sleep for a while, because there's a python source copying and arguing
RANK=1 LOCAL_RANK=1 catalyst-dl run --config=configs/exp_splits.yml --distributed_params/rank=1:int & # on GPU-1
```

**Describe alternatives you've considered**
We have a good [Config API tutorial](https://github.com/catalyst-team/classification) for image classification.
","@ternaus 
This is a bit more complicated for the Notebook API. I suggest we make the first tutorial for Config and then look at Notebook.",145385156
1082,Cover catalyst-data process-images with tests,open,2019-10-02T17:32:27Z,2020-01-13T10:37:37Z,,CONTRIBUTOR,"1. Load jpg/jpeg dataset tests
2. Load png dataset tests (some other formats?)
3. Match the number of images processed
4. Match the file size after processing
5. Test parameters --grayscale, --clear-exif, --expand-dims","As far as I recall, I was not proposing a PR, but found it necessary to cover those scenarios with tests. I guess it can be closed if Catalyst is not planning to implement such tests in the near future.",145385156
1083,Feature request: add ignore_index to Dice loss,open,2019-10-01T21:24:48Z,2019-10-01T21:24:48Z,,CONTRIBUTOR,For pixels that are `target_mask == ignore_index` loss should not be calculated and should be equal to zero.,,145385156
1084,Feature request tiled inference,open,2019-09-27T03:14:55Z,2020-03-27T12:29:31Z,,CONTRIBUTOR,"It would be a killer feature for medical and satellite imagery tasks to run inference on large images.

Example: https://github.com/BloodAxe/pytorch-toolbelt#inference-on-huge-images","@pdanilov deal, thank you for your future contribution :)",145385156
1085,[feature] add pytorch.SWA support,open,2019-05-08T20:13:38Z,2020-01-31T05:54:08Z,,MEMBER,add [Pytorch.SWA](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/) to catalyst optimizers,,145385156
1086,"healing corrupted chunks (""wrong header"")",open,2020-03-27T02:53:27Z,2020-03-27T09:54:12Z,,CONTRIBUTOR,"Something strange happened. A chunkserver started to report large number of errors, all being `chunk_readcrc: ... - wrong header`. There are no other errors whatsoever and all affected chunk files are on all local HDDs (on different file systems e.g. _ext4_, _f2fs_). All damaged chunk files are empty (filled with `00`) but their size varies from smallest being 73728 bytes.

I've identified few thousand corrupted files, all empty, all created within 10 minutes between 16:10 and 16:20 on March 25 (chunkserver was 3.0.111 at a time). I'm investigating hardware but so far could not find anything to blame on the affected machine. I suspect that incident could have been caused by networking hardware (which I'm guessing would be unlikely).

Regardless of how those chunk files became corrupted, the problem seems to be that they are _not being repaired or removed_. Even after some time after reporting yet another corrupted file (which are discovered by background testing regularly) the files remain there, still empty and not even removed.

I suspect that auto-repair of damaged chunks files might be broken. Could you investigate please?","> As far as I'm concerned, not removing corrupted chunk files during testing is a bug. Why delay removing if it is already confirmed that the chunk file is invalid?

Because ""invalid"" may mean that the whole chunk is zeroed or that two bytes are wrong. In the former case of course, the file is useless, in the latter case, if another copy of this chunk is also suddenly found missing or invalid, this copy might yet be repaired manually and some very, very important data might be saved. And, before you ask, no, we won't employ any algorithms to judge ""how big"" is the damage - this is a file system that is supposed to be fast.

Now, how long this may take from discovering to deleting: first, the chunk is marked as invalid and undergoal situation appears. This will be dealt with quickly if your system is not extremely busy otherwise, because undergoal replications have high priority (and endangered even higher, which will be the case if you normally keep stuff in 2 copies). So unless you sit there and reload CGI or you have some script checking via CLI, you may not notice the undergoal before it is replicated. But the invalid copy is still there and will be marked for deletion in the next general check loop in the master. If you have a big installation, this loop may be quite long (CGI, info tab, check loop start time/check loop end time). After it is marked for removal, it will be with respect to deletion limits of course, so if your system deletes a lot of files, it may also take a while. So, on a big installation, 8 hours from discovering to deleting might not be enough.",50505775
1087,"3.0.112: sometimes failing ""mfstest_clocks""",open,2020-03-26T09:02:34Z,2020-03-26T22:30:56Z,,CONTRIBUTOR,"A minor thing: `mfstest_clocks` test fails spuriously sometimes (but not all the time).

```
FAIL: mfstest_clocks
====================

Starting test: monotonic_clocks
used method: clock_gettime
second: 0.013054 ; 13054 ; 13054489
Assertion 'en<0.012' failed:
'en' == 0.0130538610, '0.012' == 0.0120000000
Assertion 'enusec<12000' failed:
'enusec' == 13054, '12000' == 12000
Assertion 'ennsec<12000000' failed:
'ennsec' == 13054489, '12000000' == 12000000
50%: Checks: 6, Failures: 3
FAIL mfstest_clocks (exit status: 1)
```
","Debian GNU+Linux amd64 (x86_64), Linux 5.4.19-1~bpo10+1, physical machine.

Test never failed before but given the narrow timing it might be a one time thing.
Subsequent build finished without errors.

Nevertheless it would be great if test could be made more reliable... Thanks.",50505775
1088,Files can be deleted and removed in AUTO mode when they are opened by a program.,open,2020-03-25T18:49:22Z,2020-03-27T07:28:51Z,,NONE,"In an effort to diagnose the performance issue at https://github.com/moosefs/moosefs/issues/347, I've discovered that open files can be deleted and removed from under the programs that have them open when using mfscachemode=AUTO.

It seems, inodes for opened files are being lost or destroyed.

OS: FreeBSD 12.1p3
MooseFS: 3.0.111 

I've included a script below to test with. Run the script with 0 and then 1 as a sleep parameter. 

> ./test.sh 0
> ./test.sh 1

```
#!/usr/bin/env bash

t=$1                  # Get the `sleep` time in secs from cmd line.

# Create a little test file.
echo ""Line 1 of test file.""  > ./temp.tmp
echo ""Line 2 of test file."" >> ./temp.tmp
echo ""Line 3 of test file."" >> ./temp.tmp

                     # Prove the file now exists.
echo -n ""1st ls: "" ; ls -1 ./temp.tmp

exec 6<./temp.tmp     # open temp.tmp for reading, on fd 6

rm ./temp.tmp         # delete temp.tmp

sleep $t              # delay (`sleep 0` is a NOP)

                     # Prove the file is gone as far as `ls` is concerned.
echo -n ""2nd ls: "" ; ls -1 ./temp.tmp

cat <&6               # Display the contents of the opened file.

                     # Prove, again, that the file is gone.
echo -n ""3rd ls: "" ; ls -1 ./temp.tmp
exit
```

DIRECT mode works as expected. Deleting a file does not  remove it from a program that still has it open as indicated by the output.

```
tmp # ./test.sh 0
1st ls: ./temp.tmp
2nd ls: ls: ./temp.tmp: No such file or directory
Line 1 of test file.
Line 2 of test file.
Line 3 of test file.
3rd ls: ls: ./temp.tmp: No such file or directory

tmp # ./test.sh 1
1st ls: ./temp.tmp
2nd ls: ls: ./temp.tmp: No such file or directory
Line 1 of test file.
Line 2 of test file.
Line 3 of test file.
3rd ls: ls: ./temp.tmp: No such file or directory
```

AUTO mode causes the underlying file to be deleted and removed even though a program still has it open, which obviously causes the program to to malfunction. 

```
 tmp # ./test.sh 0
1st ls: ./temp.tmp
2nd ls: ls: ./temp.tmp: No such file or directory
Line 1 of test file.
Line 2 of test file.
Line 3 of test file.
3rd ls: ls: ./temp.tmp: No such file or directory

tmp # ./test.sh 1
1st ls: ./temp.tmp
2nd ls: ls: ./temp.tmp: No such file or directory
cat: stdin: No such file or directory
3rd ls: ls: ./temp.tmp: No such file or directory
```

This has been tested on 3.0.111_2 as 3.0.112 is not available in FreeBSD yet. ","Hello @chogata 

I removed mfscachemode setting and am testing directly on the mount without nullfs. I can replicate the issue if I'm using a sub folder. I can't replicate it if I'm using the main export. 

```
Here are my exports. 
*                       /       rw,alldirs,admin,maproot=0:0
*                       /web       rw,alldirs,admin,maproot=0:0,password=pass,mingoal=2
*                       /mail      rw,alldirs,admin,maproot=0:0,maxtrashtime=0s,password=pass,mingoal=2
```
The bug shows up when using mfssubfolder=web or mfssubfolder=mail. It does not happen when I mount the / export instead. ",50505775
1089,Feature Request / Question about Tiering -- FR:Additional Class Tier(s) -- Q: Archive Delay?,open,2020-03-23T14:31:19Z,2020-03-25T09:39:16Z,,NONE,"Hey MooseFS Team!

Edit: Typo correction, adding some more detail now that I am not just waking up. Did some testing. Revised.

Quick Feature Request:

As is common, I am using Moosefs as a basis of connecting many drives for the increased storage capability, where my use case may be a little different from some is that 100% of my storage is dedicated to a single learning project working with the CommonCrawl corpus of data. As a result, all of this data is of equal importance during use, and all of it is replaceable (as it comes from a free public dataset, I have less concern about redundancy)

For this reason, functionally 100% of my data is (literally) archive, however the benefits of class tiers are apparent and I hope to use them. 

This is a totally personal project, where in I would not be able to connect this pile of random hard drives I've been gathering without systems like MooseFS, so thank you for your work. I've gathered ~130TB, growing periodically as I have money for more drives, spread across a few old (power hungry) servers from yester-year

Question 1:
Is it possible to set up a class with archive, where archiving immediately moves chunks to the archive chunk servers in the same way that data is immediately moved from ""Creation"" --> ""Keep""?

This requests from me trying to maximize performance across SSD, 10K SAS Drives and normal HDD in my setup.

I.e.
|  SSD    |     10K    |    HDD   |
Create --> Keep --> Archive
TB: (1%  --> 10%  --> ~90% )

Q2: Are there any expected complications with the above setup?
--> Having read the Storage Classes PDF, I suspected that setting the archive (-d) flag to 0 may get me close, but I tested it this evening and it seems that while the create and keep portions work find, data is holding at the KEEP teir, not immediately archiving.

e.g. 
mfsscadmin create -C S -K F -A 2A -d 0 cacheclass

Feature Request: 
Since it appears that the archive class can't be made immediate, or otherwise there are issues with this, I would simply like to Request an additional Tier, or method of achieving the above goal. 








","Thank you @chogata 

Why can -d ARCH DELAY be set to infinite? Is this just to functionally disable the Archive functionality (The implementation used to default to KEEP, if the -A flag is not set)?

If the transition from CREATE --> KEEP is the same ask KEEP --> ARCHIVE, having the ability to set -d <0 such as -d -1 would be a simple to understand flag to indicate moving the chunks immediately, using the same or similar code as the CREATE --> KEEP move. 

However, I suspect that the KEEP --> ARCHIVE tier is a fundamentally different mechanism from the CREATE --> KEEP.

As such, please make this a feature request:
I suspect the path of least resistance would be to have an additional tier modeled after the ""CREATE"" tier. 

Such as FRESH_KEEP, HOLD, or TEMP_HOLD:
""CREATE"" --> ""FRESH_KEEP"" --> ""KEEP"" --> ""ARCHIVE""


Still learning C. Will contribute down the road.


",50505775
1090," Mfsmount slows down over time, or basic usage to the point of being unusable",open,2020-03-21T18:57:38Z,2020-03-27T06:44:22Z,,NONE,"## Have you read through available documentation and open Github issues?

Yes

Note: https://moosefs.com/wp-content/uploads/2018/08/MooseFS-Hardware-Guide-v.0.9.pdf is a 404 error. 

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?

Question and maybe a bug. 

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).

All componets built from the FreeBSD ports tree. 

moosefs3-master-3.0.111_2
moosefs3-cgiserv-3.0.111_2
moosefs3-master-3.0.111_2
moosefs3-chunkserver-3.0.111_2
moosefs3-client-3.0.111_2
fusefs-libs3-3.9.1

#### Operating system (distribution) and kernel version.
<!-- https://moosefs.com/blog/what-are-the-operating-systems-and-networking-requirements/ -->
<!-- If servers and clients run on different platforms include outputs from ""uname -vsrm; mfsmount -V; lsb_release -a"" -->

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.

MooseFS master server: 1x
CPU: Intel(R) Xeon(R) CPU E5-2637 v2 @ 3.50GHz
RAM: 256GB 
HW DISK: 4x 1.2TB in raidz1
OS: Freebsd 12.1p2

Moosefs chunk servers: 5x identical 
CPU: Intel(R) Xeon(R) CPU E5-2407 0 @ 2.20GHz
RAM: 32GB
HW DISK: 4x 8TB in raidz1. 
Moose disk is a single folder using ZFS reservation and quotas mounted at /storage/chunk
DATA: storage/chunk 889G 18.1T 889G  /storage/chunk

MooseFS client servers: 2x identical
CPU: Intel(R) Xeon(R) CPU E5-2667 0 @ 2.90GHz
RAM: 192GB
HW DISK: 4x 1.2TB configured as raidz1



Memory resources: 
The master server shows 240GB of free memory on average. 
All five chunk servers show roughly 10GB of free memory at any time. 
Both client servers average around 170GB of free memory even when everything is operating slowly. 

#### Network
All servers are plugged in twice with 10GB uplinks. The uplinks are configured as LACP across two Cisco switches in a stack. The network cards are Dell branded Intel cards using the 'ix' driver. 

I've run iperf3 tests several times during a time when the mfsmount is slow. The tests are between a client and the master server. Here are the high and low results.  

HIGH
Accepted connection from 192.168.0.32, port 32634
[  5] local 192.168.0.50 port 5201 connected to 192.168.0.32 port 32635
[ ID] Interval           Transfer     Bitrate
[  5]   0.00-1.00   sec  1.00 GBytes  8.62 Gbits/sec                  
[  5]   1.00-2.00   sec  1.15 GBytes  9.89 Gbits/sec                  
[  5]   2.00-3.00   sec  1.15 GBytes  9.86 Gbits/sec                  
[  5]   3.00-4.00   sec  1.15 GBytes  9.89 Gbits/sec                  
[  5]   4.00-5.00   sec  1.15 GBytes  9.89 Gbits/sec                  
[  5]   5.00-6.00   sec  1.15 GBytes  9.89 Gbits/sec                  
[  5]   6.00-7.00   sec  1.15 GBytes  9.89 Gbits/sec                  
[  5]   7.00-8.00   sec  1.15 GBytes  9.89 Gbits/sec                  
[  5]   8.00-9.00   sec  1.15 GBytes  9.89 Gbits/sec                  
[  5]   9.00-10.00  sec  1.15 GBytes  9.88 Gbits/sec                  
[  5]  10.00-10.04  sec  43.3 MBytes  9.88 Gbits/sec                  
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate
[  5]   0.00-10.04  sec  11.4 GBytes  9.76 Gbits/sec                  receiver

LOW
Accepted connection from 192.168.0.32, port 33533
[  5] local 192.168.0.50 port 5201 connected to 192.168.0.32 port 33535
[ ID] Interval           Transfer     Bitrate
[  5]   0.00-1.00   sec   992 MBytes  8.32 Gbits/sec                  
[  5]   1.00-2.00   sec  1.15 GBytes  9.89 Gbits/sec                  
[  5]   2.00-3.00   sec  1.15 GBytes  9.89 Gbits/sec                  
[  5]   3.00-4.00   sec   782 MBytes  6.56 Gbits/sec                  
[  5]   4.00-5.01   sec  1.03 GBytes  8.78 Gbits/sec                  
[  5]   5.01-6.00   sec   906 MBytes  7.66 Gbits/sec                  
[  5]   6.00-7.00   sec  1.15 GBytes  9.88 Gbits/sec                  
[  5]   7.00-8.00   sec  1.15 GBytes  9.88 Gbits/sec                  
[  5]   8.00-9.00   sec   783 MBytes  6.57 Gbits/sec                  
[  5]   9.00-10.00  sec   838 MBytes  7.00 Gbits/sec                  
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate
[  5]   0.00-10.03  sec  9.83 GBytes  8.42 Gbits/sec                  receiver


#### How much data is tracked by moosefs master (order of magnitude)?

 - All fs objects: 10838421
 - Total space: 95 TiB
 - Free space: 91 TiB
 - RAM used: 3.9 GiB
 - last metadata save duration: ~3.5s


## Describe the problem you observed.

The mfsmount slows down over time or basic usage to the point of being unusable. Rebooting the client server restores performance. 

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

Yes. A simple test is creating a tarball of a directory. After a fresh system boot the creation is quick. After several hours, the creation slows down and eventually reaches a point of not being able to complete. 

For example, this command: time tar -czf backup.tar.gz /folder1 /folder2 

On a fresh boot: 
real	3m3.088s
user	0m18.313s
sys	0m4.670s

After the server has been in use:
real	35m25.855s
user	0m21.521s
sys	0m5.037s


For comparison, running the same test on local disk is significantly faster and consistent over time. 
real    0m19.482s
user    0m9.681s
sys     0m4.164s

#### Include any warning/errors/backtraces from the system logs

The mfsmount log only says this at startup. 
```
Mar 21 17:24:06 web00 mfsmount[84308]: monotonic clock function: clock_gettime
Mar 21 17:24:06 web00 mfsmount[84308]: monotonic clock speed: 16967 ops / 10 mili seconds
Mar 21 17:24:08 web00 mfsmount[84308]: my st_dev: 3976265474
```
### Configuration files 

mfsmount.cfg:
```
mfscachemode=AUTO
mfsmaster=IP
mfspassword=password
mfssubfolder=web
mfsmkdircopysgid=1
/storage/chunk
```


mfsexports.cfg:
```

*                       /       rw,alldirs,admin,maproot=0:0
*                       /web       rw,alldirs,admin,maproot=0:0,maxtrashtime=0s,password=PASSWORD,mingoal=2
```

Everything else is as shipped with the exception of IP binding and connection options. 

NOTES:
The client servers are handling traffic and processing for fifty low traffic websites. During normal operation the sites are fine, but the tar stress test causes them to all go offline due to the slow mfsmount. 

I'm not sure if this a bug in MooseFS, FreeBSD, or simply a configuration issue. The network, disk i/o, cpu usage and memory are all essentially idle or very low usage. 

Can anybody help pinpoint why mfsmount becomes so slow over time, or usage?
","The only drawback of multiple mounts is increased RAM usage - each mount needs its own amount of cache. And no, they don't share FUSE cache, because FUSE has now way of knowing that it's the same fs.
Other than that, it works great, we even recommend using more than one mount on clients with loads of operations, for efficiency reasons.

As for your specific usecase - I'm no expert on jails in FreeBSD, only had a passing acquittance with them, but as far as I know you can use symbolic links to pass fragments of filesystem as mountpoints to them. Wouldn't that solve your problem?",50505775
1091,"XFS structure needs cleaning, Moose error opening file.",open,2020-02-29T20:27:10Z,2020-03-05T15:38:29Z,,NONE,"## Have you read through available documentation and open Github issues?
Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
Bug, maybe.

## System information
7x Raspbberry Pi 4(4gb), 1 as metaserver and 6 with 5TB drives, (OS 64GB Partition EXT4, Data 4.5TB Partition XFS). Latest Raspbian OS (Raspbian GNU/Linux 10 (buster)) and all software updates.

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).
3.0.111, following guide from https://moosefs.com/download/#current, using stretch source. I think the initial version was 3.0.109 and upgraded with apt-get upgrade

#### Operating system (distribution) and kernel version.
Distributor ID: Raspbian
Description:    Raspbian GNU/Linux 10 (buster)
Release:        10
Codename:       buster

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.
Hardware above. Network Gb switch.

#### How much data is tracked by moosefs master (order of magnitude)?
 - All fs objects: 906485
 - Total space: 27 TiB
 - Free space: 12 TiB
 - RAM used: 413 MiB
 - last metadata save duration: ~1.9Secs

## Describe the problem you observed.
Error reported on cgi page against chunkserver #4, but no indication what was wrong. After digging around and using the script (slightly modified) from https://github.com/moosefs/moosefs/issues/106 I was able to see 2 chunks with problems:
/mnt/mfschunks1/00/chunk_00000000000BF267_00000001.mfs
/mnt/mfschunks1/07/chunk_0000000000061BB5_00000001.mfs

Running the mfschunktool against these 2 files reported: 'error opening file'.
ls -lh against these 2 files gives: Structure needs cleaning.

So an underlying filesystem problem.

stopped moose-master and all chunkservers. unmount /mnt/mfschunks1 from #4 and ran xfs_repair
against the disk. 'bad CRC for inode 46001118' and it repaired the errors.

It wiped out those 2 chunks when repairing though.

Remount it and I removed the .chunksdb from #4 to force it to scan on start, which it did. Then start all the others and the master. cgi showed 2 chunks undergoal/endangered (not sure which can't remember. goal 2: Valid 1).
Moose then did exactly what it was supposed to and replicate those out.

The problem is: The error had been there for a few days and those 2 chunks were technically undergoal/endangered all those days having only 1 copy that was accessible due to errors on the other copy, which it knew something of.

So I'm not sure if you'd class it as a bug. If not then a suggestion that when there's an error on a chunk it still aims to keep the goal. I think being overgoal is safer the under :)

I'm pretty sure that the fault was caused by a power loss (my fault). I have another problem related to that power issue: 2 chunks (goal 0, valid 0) ready to be removed. Which I assume are the ones showing as locked-unused and different to the above 2 as it's still showing. I'm hoping that just fixes itself after a month (2 weeks so far), I'm sure I've read that rogue chunks from other servers are kept for 30 days. While they're not from another moose instance, I think the power failure may have happened as data was writing and the master just doesn't know what they are. And yes, the whole setup is now on UPS with nut configured to safely shutdown.

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.
No idea how I could force xfs to screw up on purpose.

#### Include any warning/errors/backtraces from the system logs
Can't see anything in the logs:
grep -i BF267 /var/log/moose.log (or the other chunk 61BB5) doesn't return anything on either the master or the affected chunk server.
","@borkd Thanks for the info, dammit now I have a reason to buy even more PI's for testing :) The data isn't exactly disposable, but it is replicate elsewhere so can be recovered but I'm looking into implementing your suggestions this weekend (free time yay).

@chogata That is a reasonable scenario and I've no way of knowing if that's what happened then. 
but sadly #4 has report an error again today :( '2020-03-05 08:12:47 on chunk: 11678' looking at the logs it also found another problem 2 days ago (I don't look at the gui daily).
2 chunks, same problem as before (can't open file), but it's given me the chance to look into it a bit more and using find (find /mnt/mfschunks1/ -name 'chunk_0000000000002D9E_00000001.mfs') on all the other servers I can see #3 and #6 have copies of the one chunk and #1 and #5 on the other chunk. So it looks like you were correct :)

I just have to figure out why #4 seems to be the problem server. Time to go looking for badblocks I guess :(

As I said they are now behind an UPS with NUT configured, so hopefully no more power issues at least.
",50505775
1092,mfsclient,open,2020-02-28T03:07:07Z,2020-03-23T15:28:46Z,,NONE,"Hi:Moosefs 

I have a question, can MFS control the client's ability to add files but not delete files?
 
Thanks.
 ",,50505775
1093,blocked I/O during chunkserver scanning,open,2020-02-20T02:01:21Z,2020-02-27T07:52:43Z,,CONTRIBUTOR,"Observerd the problem on 3.0.111 and previous releases: __I/O on MooseFS mount is blocked while chunkserver is scanning__. We have two (heavy) archival chunkservers with ~30 million chunks each. They (should) have no ""_create_"" or ""_keep_"" chunks but only ""_archived_"" chunks as per Storage Classes definitions so those archival chunkservers are not expected to participate in normal writing operations. Yet during restart of one chunkserver (due to upgrade to 3.0.111) we observed random applications freezed all over the cluster while archival chunkserver was scanning its disks. Since scanning is not particularly fast, operations were blocked for over an hour.

During the incident no applications accessed archived data. There were no missing chunks (since all chunks are fully replicated and second archival chunkserver is fully operational). There was no apparent reason why I/O was freezed...

This problem is not related to duplicates handling (there are none or very few duplicate chunk files on the chunkserver).
","Thank you, Agata.",50505775
1094,mfs.spec extra user groupadd/useradd ,open,2020-02-14T23:32:24Z,2020-02-14T23:32:24Z,,NONE,"## Have you read through available documentation and open Github issues?

YEs
## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?

BUG report


## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).
git version
#### Operating system (distribution) and kernel version.
CentOS-7
#

## Describe the problem you observed.

./linux_build.sh
make dist
rpmbuild -ta moosefs-3.0.110.tar.gz

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.
yes

rpm/mfs.spec:
```
165 %install
166 getent group %{_groupname} >/dev/null || groupadd -r %{_groupname}
167 getent passwd %{_username} >/dev/null || \
168     useradd -r -g %{_groupname} -d %{_localstatedir}/mfs -s /sbin/nologin \
169     -c ""MooseFS"" %{_username}
170 
171 rm -rf $RPM_BUILD_ROOT
```
lines 166-169 are not required to generate the rpm packages and fails it one is building as non-root.

This pre-install stage is already taken care of in the %pre master/metagoller/chunkserver/cgiserv stages.
-->
",,50505775
1095,RFE documentation fix for CentOS-7,open,2020-02-14T23:21:28Z,2020-02-14T23:21:28Z,,NONE,"<!--
Thank you for helping to make MooseFS better!

*IMPORTANT* -  MooseFS documentation is scattered across number of places and PRs to clean that up are welcome.

*Before* creating a new issue please look around:
 - Hardware guide: https://moosefs.com/wp-content/uploads/2018/08/MooseFS-Hardware-Guide-v.0.9.pdf
 - Best practices: 
   - https://moosefs.com/support#best-practices
   - https://moosefs.com/blog/tag/best_practices/
 - MooseFS documentation: https://moosefs.com/support/#documentation
 - FAQ: https://moosefs.com/faq/
 and
 - open issues in Github tracker: https://github.com/moosefs/moosefs/issues
  
If your concern is not properly addressed in any of the above sources, then create a new issue.

Please fill in as much of the template as possible.
-->

## Have you read through available documentation and open Github issues?

yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
<!-- 
Example:
BUG report
FEATURE request
QUESTION for the COMMUNITY
-->

BUG report (documentation)

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).
building from git source

#### Operating system (distribution) and kernel version.
CentOS-7

## Describe the problem you observed.
add required packages listed
(* CentOS/RHEL: sudo yum install gcc gcc-c++ make libpcap-devel zlib-devel fuse-devel pkgconfig)

obviously, one need to add `git`

try to build with:
```
git clone https://github.com/moosefs/moosefs
./linux_build.sh
```

you also need: automake libtool gzip and rpm-build
CentOS-7 comes with automake-1.13.4-3.el7.noarch < 1.16 so the autoreconf step is required.

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.
yes


",,50505775
1096,MFS and Kubernetes – subvolumes problem,open,2020-02-12T04:46:17Z,2020-02-18T14:50:48Z,,NONE,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?

BUG

## System information

Not really important I guess...

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).

MooseFS 3.0.109-1
Ubuntu ppa.

#### Operating system (distribution) and kernel version.

Ubuntu 18.04.3 LTS bionic

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.

XFS JBODs
1 Master server
1 Metalogger
14 Chunk servers w/mix of HDDs and SSDs

#### How much data is tracked by moosefs master (order of magnitude)?

Not important for this bug, really.

## Describe the problem you observed.

When setting up a Kubernetes cluster and a problem was encountered with subvolumes residing on MFS filesystem.

My goal is to create a one docker volume pointing to a directory structure on MFS with subvolumes mounted in the pods under /home/dev/java path. This is my setup [YML]:

```
apiVersion: apps/v1
kind: StatefulSet
[...]
spec:
[...]
  serviceName: zz-transcode-nv
[...]
    spec:
        volumeMounts:
        - mountPath: /home/dev/java/archive-01
          name: archive-01
          subPath: archive-01
[...]
      volumes:
      - hostPath:
          path: /mfs/k8s/zzzz/zz-transcode-nv
          type: """"
        name: archive-01
[...]
```

NOTE: Type: """" is configured by Rancher and it seems to work well with my k8s cluster.

When running this pod k8s reports:

CreateContainerConfigError: stat /mfs/k8s/zzzz/zz-transcode-nv: no such file or directory

Of course the path exists:

```
# stat /mfs/k8s/zzzz/zz-transcode-nv
  File: /mfs/k8s/zzzz/zz-transcode-nv
  Size: 1               Blocks: 1          IO Block: 65536  directory
Device: 7fh/127d        Inode: 4117        Links: 7
Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2020-02-12 04:31:08.000000000 +0000
Modify: 2020-02-11 09:52:23.000000000 +0000
Change: 2020-02-11 09:52:23.000000000 +0000
 Birth: -
```

To get around this, I tried to mount each subdirectory in /mfs/k8s/zzzz/zz-transcode-nv as a separate volume and it does work, here's a snippet from spec file [different directory]:

```
    spec:
        volumeMounts:
        - mountPath: /home/dev/java/ingest-01
          name: ingest-01
[...]
      volumes:
      - hostPath:
          path: /mfs/k8s/zzzz/zz-transocde-nv/ingest-01
          type: """"
        name: ingest-01
```

Now, I've found this vulnerability fix which I believe is related to this issue [CVE-2017-1002101]:

https://kubernetes.io/blog/2018/04/04/fixing-subpath-volume-vulnerability/

This part of the above solution seems important:

Starting with the base volume, open each path segment one by one, using the openat() syscall, and disallow symlinks. With each path segment, validate that the current path is within the base volume.

And the reason I'm reporting this issue is a flawless NFS volumes subpath integration [root accessible, not rootsquash is used] . it's only when I point a volume to MFS-backed filesystem k8s starts to complain about a path being not found.

Other than this, k8s has no problem with MFS-backed volumes, it can create directories when instructed etc. – no permissions problems whatsoever.

Any ideas?

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

This happens every time I create a volume with subpath on MFS.","It seems that this bug is related only to Rancher (RKE) and containerized kubelets and it's a problem on MFS side. I'll keep you posted to get the information to community.

https://github.com/rancher/rancher/issues/14836
",50505775
1097,fstab mount on ubuntu 18.04,open,2020-02-01T21:55:13Z,2020-02-07T10:46:27Z,,NONE,"Not sure if it's worth adding to the documentation but I found I had to add the following to the end of the fstab command otherwise the system would not boot cleanly.

`,x-systemd.mount-timeout=30,_netdev`

The line in fstab now looks like

`mfsmount /mnt/mymnt fuse mfssubfolder=mydir,allow_other,x-systemd.mount-timeout=30,_netdev`

I think it was trying to mount mooseFS before it had finished connecting to the network, which of course is almost always going to end badly.","You can also use `systemd` mount unit file as described [here](https://github.com/moosefs/moosefs/issues/123#issuecomment-397565138).
Piotr / MooseFS Team",50505775
1098,MooseFS on VDO?,open,2020-01-31T14:57:56Z,2020-03-05T14:13:38Z,,NONE,"Has anyone else tried a mooseFS chunkserver on a VDO volume or am I the only one crazy enough?

No idea if VDO will be able to de-deduplicate the data once it's been through mooseFS but i guess we will see, Obviously if it works at all it would only deal with duplicate data on that chunk server.",+1 for VDO from me. I've been running is on that other liz file system for a few months. Recently switched to MooseFS and VDO hasn't been an issue on either.,50505775
1099,"chunkserver: hangs in ""high speed rebalance""",open,2020-01-30T22:02:29Z,2020-01-31T01:03:35Z,,CONTRIBUTOR,"Situation: chunkserver with 30 million chunks and several hard disk is re-started with `HDD_HIGH_SPEED_REBALANCE_LIMIT = 3`:

This is what happens after scanning of local HDDs:

```
Jan 31 09:01:19  mfschunkserver[103002]: long loop detected (13.720038s)
Jan 31 09:01:19  mfschunkserver[103002]: mfschunkserver[103002]: connection was reset by Master
Jan 31 09:01:19  mfschunkserver[103002]: mfschunkserver[103002]: closing connection with master
Jan 31 09:01:19  mfschunkserver[103002]: connection was reset by Master
Jan 31 09:01:19  mfschunkserver[103002]: closing connection with master
Jan 31 09:01:22  mfschunkserver[103002]: mfschunkserver[103002]: connecting ...
Jan 31 09:01:22  mfschunkserver[103002]: connecting ...
Jan 31 09:01:22  mfschunkserver[103002]: mfschunkserver[103002]: connected to Master
Jan 31 09:01:22  mfschunkserver[103002]: connected to Master
Jan 31 09:01:38  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (12.024308s)
Jan 31 09:01:38  mfschunkserver[103002]: long loop detected (12.024308s)
Jan 31 09:01:39  mfschunkserver[103002]: mfschunkserver[103002]: masterconn: connection closed by master
Jan 31 09:01:39  mfschunkserver[103002]: mfschunkserver[103002]: closing connection with master
Jan 31 09:01:39  mfschunkserver[103002]: masterconn: connection closed by master
Jan 31 09:01:39  mfschunkserver[103002]: closing connection with master
Jan 31 09:01:42  mfschunkserver[103002]: mfschunkserver[103002]: connecting ...
Jan 31 09:01:42  mfschunkserver[103002]: connecting ...
Jan 31 09:01:42  mfschunkserver[103002]: mfschunkserver[103002]: connected to Master
Jan 31 09:01:42  mfschunkserver[103002]: connected to Master
Jan 31 09:01:57  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (9.054579s)
Jan 31 09:01:57  mfschunkserver[103002]: long loop detected (9.054579s)
Jan 31 09:02:06  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (6.701045s)
Jan 31 09:02:06  mfschunkserver[103002]: long loop detected (6.701045s)
Jan 31 09:02:14  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (7.280038s)
Jan 31 09:02:14  mfschunkserver[103002]: long loop detected (7.280038s)
Jan 31 09:02:28  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (8.330697s)
Jan 31 09:02:28  mfschunkserver[103002]: long loop detected (8.330697s)
Jan 31 09:02:45  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (7.308572s)
Jan 31 09:02:45  mfschunkserver[103002]: long loop detected (7.308572s)
Jan 31 09:03:00  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (6.759534s)
Jan 31 09:03:00  mfschunkserver[103002]: long loop detected (6.759534s)
Jan 31 09:03:14  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (10.022902s)
Jan 31 09:03:14  mfschunkserver[103002]: long loop detected (10.022902s)
Jan 31 09:03:15  mfschunkserver[103002]: mfschunkserver[103002]: masterconn: connection closed by master
Jan 31 09:03:15  mfschunkserver[103002]: mfschunkserver[103002]: closing connection with master
Jan 31 09:03:15  mfschunkserver[103002]: masterconn: connection closed by master
Jan 31 09:03:15  mfschunkserver[103002]: closing connection with master
Jan 31 09:03:17  mfschunkserver[103002]: mfschunkserver[103002]: connecting ...
Jan 31 09:03:17  mfschunkserver[103002]: mfschunkserver[103002]: connected to Master
Jan 31 09:03:17  mfschunkserver[103002]: connecting ...
Jan 31 09:03:17  mfschunkserver[103002]: connected to Master
Jan 31 09:03:45  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (6.748541s)
Jan 31 09:03:45  mfschunkserver[103002]: long loop detected (6.748541s)
Jan 31 09:04:07  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (13.534299s)
Jan 31 09:04:07  mfschunkserver[103002]: long loop detected (13.534299s)
Jan 31 09:04:07  mfschunkserver[103002]: mfschunkserver[103002]: write to Master error: EPIPE (Broken pipe)
Jan 31 09:04:07  mfschunkserver[103002]: mfschunkserver[103002]: closing connection with master
Jan 31 09:04:07  mfschunkserver[103002]: write to Master error: EPIPE (Broken pipe)
Jan 31 09:04:07  mfschunkserver[103002]: closing connection with master
Jan 31 09:04:12  mfschunkserver[103002]: mfschunkserver[103002]: connecting ...
Jan 31 09:04:12  mfschunkserver[103002]: mfschunkserver[103002]: connected to Master
Jan 31 09:04:12  mfschunkserver[103002]: connecting ...
Jan 31 09:04:12  mfschunkserver[103002]: connected to Master
Jan 31 09:04:29  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (13.842167s)
Jan 31 09:04:29  mfschunkserver[103002]: long loop detected (13.842167s)
Jan 31 09:04:29  mfschunkserver[103002]: write to Master error: EPIPE (Broken pipe)
Jan 31 09:04:29  mfschunkserver[103002]: mfschunkserver[103002]: write to Master error: EPIPE (Broken pipe)
Jan 31 09:04:29  mfschunkserver[103002]: mfschunkserver[103002]: closing connection with master
Jan 31 09:04:29  mfschunkserver[103002]: closing connection with master
Jan 31 09:04:32  mfschunkserver[103002]: mfschunkserver[103002]: connecting ...
Jan 31 09:04:32  mfschunkserver[103002]: connecting ...
Jan 31 09:04:32  mfschunkserver[103002]: mfschunkserver[103002]: connected to Master
Jan 31 09:04:32  mfschunkserver[103002]: connected to Master
Jan 31 09:04:55  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (13.833207s)
Jan 31 09:04:55  mfschunkserver[103002]: long loop detected (13.833207s)
Jan 31 09:04:55  mfschunkserver[103002]: write to Master error: EPIPE (Broken pipe)
Jan 31 09:04:55  mfschunkserver[103002]: mfschunkserver[103002]: write to Master error: EPIPE (Broken pipe)
Jan 31 09:04:55  mfschunkserver[103002]: mfschunkserver[103002]: closing connection with master
Jan 31 09:04:55  mfschunkserver[103002]: closing connection with master
Jan 31 09:04:57  mfschunkserver[103002]: mfschunkserver[103002]: connecting ...
Jan 31 09:04:57  mfschunkserver[103002]: mfschunkserver[103002]: connected to Master
Jan 31 09:04:57  mfschunkserver[103002]: connecting ...
Jan 31 09:04:57  mfschunkserver[103002]: connected to Master
Jan 31 09:05:09  mfschunkserver[103002]: mfschunkserver[103002]: long loop detected (5.245776s)
Jan 31 09:05:09  mfschunkserver[103002]: long loop detected (5.245776s)
```

Basically chunkserver falls off master, reconnects, falls again and so on (making CGI/Disks view practically unusable).","Only disabling high speed rebalance stabilised the chunkserver.

Lowering `HDD_HIGH_SPEED_REBALANCE_LIMIT = 2` by one did not help...

On this particular chunkserver high speed rebalance throws it into continuous ""long loop"" state rendering chunkserver non-operational...",50505775
1100,storage classes: ghost chunks,open,2020-01-30T21:52:37Z,2020-02-17T04:15:23Z,,CONTRIBUTOR,"I use storage classes extensively. All of them are defined with specific Create, Keep and Archive goals pinning data to certain chunkserver labels. Default storage classes 1...9 are not used (and were never used).

At some point I've noticed around two million standard chunks in storage class `2` (which is defined as `*,*`).  
_The problem is that there are no files or directories with that storage class._

Why/how chunks can exist outside of storage class? How to locate (and destroy?) them?

When I restart archival chunkserver, some chunks in storage class `2` shown as undergoal in CGI which means that at least some of them are located on the archival chunkserver...
","Just to confirm that altering storage class `2` (e.g. `mfsscadmin /mnt/mfs modify 2 -K M,P -f`) does not pin data as expected. Yesterday I've removed the last snapshot that had explicit assignment of storage class and data was removed from _all_ chunkservers, un-equalising even utilisation of space as reported in https://github.com/moosefs/moosefs/issues/298#issuecomment-580031942. The conclusion is that double storage class assignment (on the origin and the snapshot) in fact effectively removes all storage class restrictions.",50505775
1101,"< prefix in mfshdd.cfg leaves chunks behind, related to #327 ",open,2020-01-26T17:48:31Z,2020-01-27T19:00:40Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the intended audience?

BUG, related to #327 

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).

3.0.109

#### Operating system (distribution) and kernel version.

Debian

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.

irrelevant

#### How much data is tracked by moosefs master (order of magnitude)?

tens of terabytes

## Describe the problem you observed.

I have noticed that adding a `<` in front of a disk share does not leave the directory structure empty of chunks. Per`man mfshdd.cfg`:
> <      means that all data from this hard drive should be moved to other local hard drives

I checked the directory structure once activity stopped and master reported no undergoal and no overgoal chunks, stating there are 0 chunks in the share prefixed with `<`. Upon examination of the directory there are still thousands of chunks in there, ~16GB worth.

#### Include any warning/errors/backtraces from the system logs

The syslog entries for each disk mentioned in mfshdd.cfg, have different but static NCHUNKS number, and COUNTDOWN increasing, rather than decreasing every hour.

> MFSCS_ID[8331]: on drive '/mount/point/mfs/v3/' NCHUNKS chunk duplicates detected - will remove them in COUNTDOWN hours

##### Suggestion for #327

It appears that fixing the countdown will resolve this issue, but while I am glad that moosefs does not hastily remove duplicates I would even be in favor of artificially slowing down the countdown when errors are detected.

```
MFSCS_1[8331]: read_block_from_chunk: file: /rust/diska/mfs/v3//3D/chunk_00000000002D7C88_00000001.mfs ; block: 287 - crc error (data crc: D2EEC5BA ; check crc: BDE74F62)
MFSCS_1[8331]: move_chunk: file:/rust/diska/mfs/v3//34/chunk_00000000003F239F_00000001.mfs - data read error: EIO (Input/output error)
MFSCS_1[8331]: read_block_from_chunk: file: /rust/diskb/mfs/v3//6E/chunk_000000000040B90B_00000001.mfs ; block: 661 - crc error (data crc: 32EC75DD ; check crc: DB2C2CD1)
MFSCS_1[8331]: test_chunk: file:/rust/diska/mfs/v3//2F/chunk_00000000003C4CDB_00000001.mfs - data read error: EIO (Input/output error)
MFSCS_1[8331]: move_chunk: file:/rust/diska/mfs/v3//2B/chunk_000000000029C82E_00000001.mfs - data read error: EIO (Input/output error)
MFSCS_1[8331]: test_chunk: file:/rust/diskb/mfs/v3//75/chunk_00000000004A819F_00000001.mfs - data read error: EIO (Input/output error)
MFSCS_1[8331]: move_chunk: file:/rust/diska/mfs/v3//35/chunk_00000000004BDB56_00000001.mfs - data read error: EIO (Input/output error)
```

","Yes, timeout should be configurable. Restarting chunkserver resets the counter so one have to wait another week for duplicates to be deleted if (chunk)server is restarted...
",50505775
1102,mfsmetadump swallows UTF-8 characters,open,2020-01-26T17:03:59Z,2020-01-26T17:04:56Z,,NONE,"## Have you read through available documentation and open Github issues?

yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
BUG

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).
latest

#### Operating system (distribution) and kernel version.
Linux, Debian Buster, doesn't matter

## Describe the problem you observed.

mfsmetadump cannot dump non-ASCII filenames

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

Of course. The culprit is this broken piece of code in `mfsmetadump.c` line 61ff;
```
                        if (buff[i]<32 || buff[i]>127) {
                                buff[i]='.';
                        }
```
This causes any file names with non-ASCII characters to be mangled. Personally I don't care about file names with TAB or LF in them (though it'd be nice if tey survived) but destroying UTF-8 is not acceptabe, to say the least..

Also, there is no documentation how to restore the dump to a new master, which I need because I want to migrate away from lizardfs (onto a different architecture) which means that the binary mfsmeta files are incompatible. Help would be appreciated.
",,50505775
1103,slow chunkserver optimisation tips [question],open,2020-01-25T16:00:08Z,2020-01-27T11:38:14Z,,NONE,"Hi,
I have a few chunk servers utilising an elderly AMD Turion II Neo Dual-Core
(they are dedicated only to this task)
these are HP Microservers so can handle up to six HDD.

in the cases of rebalancing of the cluster (e.g few TB deletions, new chunk server), they get seriously stressed 
I am running one chunk server to cover all HDD. when I tried to run multiple chunk servers machine would become unresponsible

Are there any parameters I could tweak to make them operate better?
The ""best practices"" section on the website is pretty limited. Is there something more detailed somewhere? Examples of tweaking?
","https://robo.moosefs.com/mfscalc/ - this can help you with how much RAM you need; as chogata wrote, swapping is a killer, if you have enough RAM even try ``swapoff`` it.

Rebalancing between chunkservers will always generate traffic but in my experience default values are OK if you have a lot CHSRVs (20 in my case). If you have only a few, traffic per CHSRV will be higher and yes there will be problems (been there, done that while having only f.ex. 3-5 CHSRVs in the cluster). And 6 HDDs is also not so much, better to have more of them so you have more IOPS per CHSRV.

If you deleted a lot of data and ended with CHSRVs internal rebalancing that takes lot of time, try ""high speed rebalance"" ( see #39 ) to make HDDs equal in used space fast to get rid of internal rebalancing state.

Best optimalization tip from me... Have as much of chunkservers as you can, than all operations propagates on them (CHSRVs talk to each other if the have to).

Hope this helps you :) ",50505775
1104,mfsmount does not provide always the last file version,open,2020-01-23T14:09:39Z,2020-01-29T18:20:52Z,,NONE,"I am using on chunkservers:

```bash
moosefs-chunkserver-3.0.109-1.rhsystemd.x86_64
```

and mfsmaster:

```bash
moosefs-master-3.0.105-1.rhsystemd.x86_64
```

Both on CentOS 7 with kernel 5.4.11-1.

I mount the moosefs data using mfsmount in some servers. All of my chunkservers are with load under near zero. I do not have too much files to write, but I will do a lot reads (actually is not a lot today, but will be). Today we are only validating this environment.

Sometimes I can see the softwares that use mounted directory from mfs get different versions of the files. Doing manually sha1 on these files in each server I am seeing thet moosefs chunkservers are taking too much time to update the data.

I have two questions here:

1. Why chunkservers take too much time to update replicated data?
2. It is ok to take some time to update the replicas, but moosefs master must deliver in the mfsmount the last file version, is not right? This is a basic procedure of any remote file system. So I think something is wrong here. 

I am using all default options that come with moosefs rpm packages. And the command to mount in fstab is:

```bash
mfsmount /mnt/mfs/ fuse defaults 0 0
```

I did not find a verbose mode on mfs services to try to debug what is happening. So this is my third question:
3. How set debug mode? I not saw anything in the config files documentation. Maybe there is a systemctl paramenter that I have to configure, I do not know.","Hello @jSML4ThWwBID69YC 

Not anymore, but I the problem was due a misconfiguration about mfsmount in one server, so it produced a local data copy and other version on moosefs. I finally found that problem yesterday. 

I think even with different versions between chunk server and master server, everything was ok.

Thank you for all help.",50505775
1105,FreeBSD deployment (Questions),open,2020-01-21T17:21:15Z,2020-01-29T17:54:30Z,,NONE,"Hello, 

I'm working to deploy MooseFS on FreeBSD. Are there any deployment guides, or advice available for this? I've read best-practices, but nothing seems FreeBSD specific. 


1: Does ZFS and MooseFS work well together? I'm concerned about MooseFS vs ZFS memory caching conflicting, or wasting resource. 

2: If using ZFS, should MooseFS point to a folder, or a zvol for it's disk? 

3: What is correct value for vfs.fusefs.data_cache_mode? EX: Off, write-through, or write-back.

4: What should mfscachemode be set to on the clients? I recall it was set to DIRECT on earlier version due to bugs. Is this still the case, or is AUTO working on FreeBSD 12.1+?

The current deployment plan is 5x chunk servers with 4x 12TB drives each as a raidz1 setup. MooseFS chunk server configuration will point to a folder for the disk. Any advice, or questions are welcome. ","@borkd 

Thank you for the advice. A single-vdev pool sounds like a good idea. 

For anybody else thinking of using FreeBSD, note the recommendation on https://github.com/moosefs/moosefs/issues/334#issuecomment-579673283. The mfscachemode should be set to DIRECT, even on FreeBSD 12.1. ",50505775
1106,mfsscripts/favicon.ico: incorrect name; file content is PNG,open,2020-01-19T05:10:15Z,2020-01-21T06:35:22Z,,CONTRIBUTOR,"```
$ file mfsscripts/favicon.ico
mfsscripts/favicon.ico: PNG image data, 16 x 16, 8-bit/color RGBA, non-interlaced
```

Please correct the file name to make it consistent with the content. Thanks.",Why? IE6 legacy is over. All browsers support PNG these days.,50505775
1107,chunkserver: inefficient handling of duplicate chunks,open,2020-01-17T22:37:28Z,2020-03-23T04:19:56Z,,CONTRIBUTOR,"I moved an HDD from one chunkserver to another (same label) so some duplicate chunks ended up on one chunkserver. That exposed several problems:

  * scanning is incredibly slow -- around dozen chunks per second so full scan takes 20+ hours.

  * during scanning all other HDDs on the same chunkserver exhibited numerous `wrong header` errors.

  * scanning blocked I/O everywhere so all the cluster was frozen for the duration of very slow scan -- that includes unrelated storage classes placed to chunkservers with different labels. Under no circumstances scanning should block I/O in unrelated data that is placed elsewhere.

  * in the aftermath some chinks were lost/missing (`INVALID COPIES`).

  * unlike LizardFS that removes duplicate chunks during scanning, MooseFS holds them to be deleted later. Too bad if you need to restart chunkserver before scheduled deletion as slow scanning and all the above problems will happen again.

Interesting to note that LizardFS have none of those problems.
","Duplicates handling is still somewhat flawed. On chunkserver with only SSD disks and about a million chunks, presence of 30_000 duplicates throws frequent ""long loop detected"" and ""replicator: connection lost"" errors under regular load -- a problem never to be seen on the very chunkserver without duplicate chunks. CGI/Disks view is often timeouts when duplicate chunks are present...
",50505775
1108,git clone in moosefs mount is very slow,open,2020-01-14T08:40:26Z,2020-01-16T07:36:08Z,,NONE,"hello,  I am new moosefs user, I have one question as below:
git clone in moosefs mount path is very slow, receiving objects only 1 MiB/s, but git clone use local disk receiving objects have 20+ MiB/s

this is git issue or moosefs fuse mount not support git useage?
my environment have one master server and three chunk server

thanks！","Git works on a large number of small files and performs a lot of operations on those small files. Such type of i/o will always work faster on a local hard drive than on any network file system, MooseFS included.",50505775
1109,create --> keep should work during scanning; side effects of slow scanning chunkserver,open,2020-01-11T09:55:09Z,2020-01-16T00:11:57Z,,CONTRIBUTOR,"Here is another practical problem.

Most of our storage classes are defined like `-C C,C+S -K S,M` where `C` denotes high performance chunkservers equipped with enterprise class high endurance SSDs that we use exclusively to intake newly created chunks. All storage classes use `STRICT` mode.

This configuration was stabilised about a year ago and have been working very well ever since.

Until today when I `fsck`d HDD that belongs to chunkserver labelled `W` that is used to keep archived data. HDD is 6 TB and slow, holding around 15 million chunks so scanning is expected to be slow. Usually this is not a problem thanks to chunks caching. After `fsck` chunkserver decided to do a full scan that took several hours -- also expected.

Now the interesting problem: as soon as chunkserver begin scanning slow HDD, the free space in label `C` started to srhrink as utilisation of those disks grew. As you can imagine there are not much free space there (just around 400GB) but normally those SSDs hardly ever fill up to 100%. There were no unusual activity that could explain that. Yet SSDs filled completely and remained 100% utilised until the very moment when scanning of the slow HDD is finished - at which precise moment utilisation of SSDs in label `C` started to decline. This exposed a problem when (I think) _create_ --> _keep_ process was not working during scanning of HDD that belongs to chunkserver that holds only archived chunks (it would be understandable if _keep_ --> _archive_ would be suspended).

Please make scanning non-blocking for normal cluster operations in unrelated storage classes.
That should be easy to implement since master knows the label of chunkserver that is scanning its disks.

Another concern is that one slow chunkserver can DoS MooseFS cluster. Scanning should be local to chunkservers and to optimise the process I recommend to consider making chunkserver to postpone communication with master until local disks have finished scanning. It might be OK to keep initialising chunkserver in temporary maintenance mode to avoid negative side effects.

As always improvement ideas are welcome.",Empirical observation suggests that _deletetion_ is not happening anywhere in the cluster  as long as one chunkserver is scanning at least one HDD.,50505775
1110,"Client do not wait for unavailable chunks, not respecting `mfsioretries` [serious] [incident]",open,2020-01-09T22:37:05Z,2020-01-29T18:23:47Z,,CONTRIBUTOR,"I had a serious outage on several MooseFS mounts today. Due to emergency power maintenance in a rack (replacement of automatic transfer switch, ATS) I had to gracefully stop two chunkserver nodes at the same time, temporary losing availability of some data.

Chunkservers were down only for some minutes (and they were in the temporary maintenance mode) yet even hours later clients did not recover.

My FUSE3 mounts are configured with `mfsioretries=444` which gives plenty of time to handle such situations. Unfortunately MooseFS just logged several lines like the following:

```
mfsmount[1147]: file: 5759521, index: 0, chunk: 578318316, version: 1 - there are no valid copies
```

and _gave up_(!) so even an hour later applications are still frozen, unresponsive.

It is especially frustrating to me because LizardFS handles such situation gracefully, retrying up to configured `mfsioretries` limit (with adequate logging) with complete recovery after brief unavailability of data.

This is a very serious issue that could have been easily triggered by temporary disruption of connectivity between clients and chunkservers (e.g. reboot of switch).
I believe I had exactly that kind of incident before but did not realised the nature of the problem at a time.

I recommend to investigate this issue with utmost importance. Thanks.",Yes. Thanks a lot - We were able to reproduce this. We are working on fix.,50505775
1111,[question] OS X timemachine backups in moosefs slow - any suggestions?,open,2020-01-09T20:11:44Z,2020-01-09T21:40:07Z,,NONE,"I would like to store my timemachine backups in moosefs 3.0.109
I actually do that already but it is somehow slow 

any guidance? maybe I do something wrong?

I have moosefs mounted on my linux machine 
`mfsmount /mnt/mfs fuse defaults,mfsmaster=mfsmaster,mfsport=9421,mfsdelayedinit,allow_other,nosuid,nodev 0 0`

then I have respective Time machine folder on this mount
the folder is exposed as AFP mount using docker image mbentley/timemachine

I suppose I could mount the folder directly on my mac and create sparsebundle there.
Any experiences? Would it be any faster?
The second option would directly access multiple chunkservers while the first one is limited to accessing a single AFP share
","surprisingly the second option seems to be even slower
but like in dozens of KB/s
mounted with options
-o sparse -o auto_xattr -o extended_security -o allow_other -o daemon_timeout=120
",50505775
1112,Is it possible to make chunk server to utilize multiple NIC just like multipath does?,open,2020-01-02T15:41:08Z,2020-01-02T15:41:08Z,,NONE,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
<!-- 
Example:
BUG report
FEATURE request
QUESTION for the COMMUNITY
-->

QUESTION

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).

3.0.100   personal gentoo ebuild created myself.
",,50505775
1113,moosefs mount folder can’t read and write ,open,2019-12-20T03:57:30Z,2019-12-20T04:03:34Z,,NONE,"
hi friends:
   Hello, I have configured the moosefs file system. After working normally for a period of time, the files on which the directory is suddenly mounted can't be read or written. Check the directory through df-h. / check the directory as follows:
roo:/home/test # df  -h ./
filesystem     size   used   avail  use%   mounted on
mfs#192.168.72.100：112931  0  0  0  -  /mfs/test
","<!--
hi friends:
   Hello, I have configured the moosefs file system. After working normally for a period of time, the files on which the directory is suddenly mounted can't be read or written. Check the directory through df-h. / check the directory as follows:
roo:/home/test # df  -h ./
filesystem     size   used   avail  use%   mounted on
mfs#192.168.72.100：112931  0  0  0  -  /mfs/test
",50505775
1114,3.0.109/client: incorrect exit code when already mounted,open,2019-12-12T07:24:25Z,2019-12-12T07:24:25Z,,CONTRIBUTOR,"### What's expected (no `mfsmount` in `/etc/fstab`):

```
# mount -va; echo ::$?   ## first time
/                        : ignored
/media/cdrom0            : ignored
/media/usb0              : ignored
/tmp                     : already mounted
swap                     : ignored
::0

# mount -va; echo ::$?    ## second time
/                        : ignored
/media/cdrom0            : ignored
/media/usb0              : ignored
/tmp                     : already mounted
swap                     : ignored
::0
```

### The problem (with `mfsmount` in `/etc/fstab`):

```
# mount -va; echo ::$?   ## first time
/                        : ignored
/media/cdrom0            : ignored
/media/usb0              : ignored
/tmp                     : already mounted
swap                     : ignored
mfsmaster accepted connection with parameters: read-write,restricted_ip,admin ; root mapped to root:root
/mnt/mfs                 : successfully mounted
::0

# mount -va; echo ::$?   ## second time, the exit code is invalid!
/                        : ignored
/media/cdrom0            : ignored
/media/usb0              : ignored
/tmp                     : already mounted
swap                     : ignored
mountpoint '/mnt/mfs' is not empty
::32
```

The exit code should be `0` because `mount -va` should be idempotent and everything is already mounted as expected therefore it is ""OK"" (i.e. ""no-op"") condition, not a problem.

This is with FUSE-3 client.
",,50505775
1115,Very poor performance with concurrent random reads,open,2019-12-11T22:55:38Z,2019-12-11T22:55:38Z,,NONE,"mfs version: 3.0.109
os: Debian buster
hw: 16-core AMD Epyc
net: 1Gbit ethernet (3 chunkservers) and 10Gbit ethernet (for one chunkserver)

I have a process that does a lot of random reads over 10,000-ish files, totalling 175TB data. This process opens all the files, and then spawns a bunch of threads and each thread will randomly select a file descriptor and do a pread at a ""random"" location in the file.

_Of course_ random reads will be slower than consecutive reads, but I am frustrated by how if I can improve performance ""for free"" by simply partitioning my reads over multiple mfsmounts.

It seems like mfsmount should fairly treat each reading thread: one thread should not void the cache by reading at the same time as another thread.
",,50505775
1116,[question] ETIMEDOUT while snapshotting,open,2019-11-24T12:31:36Z,2019-11-24T12:31:36Z,,NONE,"Hi,

Sometimes I am getting ETIMEDOUT during mfsmakesnapshot
but I do see the snapshot folder
what is the result? is my snapshot not complete?",,50505775
1117,"3.0.107/FUSE3: FTBFS on i686 :: ""fuse: off_t must be 64bit""",open,2019-11-17T08:38:18Z,2019-11-19T13:34:36Z,,CONTRIBUTOR,"3.0.107 FTBFS with libfuse3-3.7.0 on _i686_ as follows:

```
In file included from /usr/include/fuse3/fuse_lowlevel.h:25,
                 from fusecommon.h:14,
                 from mfs_fuse.h:24,
                 from extrapackets.c:29:
/usr/include/fuse3/fuse_common.h:817:1: error: static assertion failed: ""fuse: off_t must be 64bit""
  817 | _Static_assert(sizeof(off_t) == 8, ""fuse: off_t must be 64bit"");
      | ^~~~~~~~~~~~~~
```",fixed by commit 9299480caabe37ef51e8c349e2ea94ee2d339295,50505775
1118,sequential read speed vs read ahead limits,open,2019-11-15T12:59:32Z,2019-11-30T10:44:28Z,,NONE,"**Case:** we are hosting files on MFS that are downloaded one by one via FTP. After upgrading connection from 100Mbps to 1Gbps we encountered a problem, that single download can perform max @~300Mbps. After some digging we found this: https://sourceforge.net/p/moosefs/mailman/message/33792353/ and changed mfsreadaheadleng from default 1MB to 2MB and speed increased to ~500Mbps. @xandrus suggested at SourceForge that we can rise this to 4MB or more, but... mfsreadaheadleng can be increased max to 2MB, based on the manual and in practice as well:
```
error in fuse_mount
read ahead length too big (4194304 B) - decresed to 2 MiB
```

**Request:** please, make mfsreadaheadleng (and any other options related to it) not limited to 2MB... so we can saturate 1Gbps connection in single thread.

https://github.com/moosefs/moosefs/blob/0aaa5ab938ba18d3c3c88df8e8573a4e2c906645/mfsclient/mfsmount.c#L1800-L1814","Thank you very much for the feedback. It can be that we are wrong and we where making mistake thinking that MFS is the bottleneck (@xandrus post directed us there, sorry). We made some tests like you did and we have strange results while testing from different network locations (from 180MB/s to 50MB/s). We will investigate our network and share later what we will find.",50505775
1119,MFSMASTER physical memory has not been released.,open,2019-11-15T07:09:26Z,2019-12-21T06:58:38Z,,NONE,"Why have you deleted a lot of files and the MFSMASTER physical memory has not been released.


","> why ? help

have you resolve this problem?",50505775
1120,master server metaData size problem,open,2019-10-12T09:03:03Z,2019-10-28T07:10:28Z,,NONE,"mfs has  similar architure with hdfs
so, how to solve the metaData size problem","What do you mean by ""metaData size problem""?",50505775
1121,Problem regarding Memory Consumption during Master Metadata Save,open,2019-09-30T06:54:57Z,2019-09-30T06:54:57Z,,NONE,"Recently we encountered a problem of full mfsmaster memory.

- The timeline of the problem is as follows:
16:59:45  A mfschunkserver performs reload operation because it a disk is added;
16:59:47  The mfschunkserver reload is completed, and the master node log shows that the chunkserver node is reconnected to the master.
17:00:00  On the master server, the memory is occupied twice as much as before, and swap space has been used. Two mfsmasters appear on the server (I guess the master forked out child process during metadata save).
17:22:00  The memory occupation rate is still very high, and all clients are out of response.  MFS cannot work normally. So the master is restarted.
17:35:00  Master restart is completed, and all clients return to normal.

- Our current environment is configured as follows:
mfsmaster： 16C 64G 500G HDD * 1
metalogger： 16C 64G 500G HDD * 2
chunkserver： 16G 64G 9T(3TB hdd * 3) * 5
all chunkserver copies: 339293732
master memory usage: 52G

- According to the operation of the above timeline, I can basically reproduce the same result in the experimental environment. My opinion is as follows:
1. After the chunkserver reload, it can be seen from the log that the chunkserver was disconnected from the master node for 2s before reconnection. At this time, the goal of the chunk on the node changes from 2 to 1. So when the metadata save is triggered, the goal of the chunk has not resumed to 2.
2. In the official document, each metadata save will not consume too much memory, and the child process will only load the updated content in the master memory;
3. Therefore, during my operation last time, the goal of a large number of chunks changed, so the child process took up far more memory than previous expectation, resulting in the huge memory consumption during metadata save.

The above is my point of view, please comment on whether my understanding is correct.
",,50505775
1122,failed hdd = failed chunkserver [question],open,2019-09-26T21:20:27Z,2019-12-11T08:27:56Z,,NONE,"Imagine the following scenario
I have a machine with four HDD
I can run them all as one chunkserver, however, if one HDD fails and after reboot fails to mount, the chunkserver will fail to start. I will effectively lose four HDD from the cluster until I will manually reconfigure the chunkserver.
I don't know if there is an option to start chunkserver regardless?

I figured I can run four chunkservers on one machine, one for each HDD.
This works perfectly well when using storage classes, but chunkservers fight for the resources. As a result, I am getting unnaturally high load and I/O.
Even if I reduced the number of workers per chunkserver to fourth of the default.
Is there something else I am missing I could do to balance multiple chunkservers on one machine?
Or to start chunkserver with failed HDD","Hi,
I would like to add that from version 3.0.109 you can start chunk server with failed hard disks.
Just add this line to your mfschunkserver.cfg file
```
ALLOW_STARTING_WITH_INVALID_DISKS = 1
```
https://github.com/moosefs/moosefs/blob/6e3521346e367ca91cd991b4d78c35cdfc4f8cb4/mfsdata/mfschunkserver.cfg.in#L67",50505775
1123,What's the way that moosefs backup file,open,2019-09-24T06:27:24Z,2019-09-25T06:00:45Z,,NONE,"Recently， I have some problem with the way that how moosefs backup the files? 
 I found when I set mfssetgoa **2**.
It will cost about double  space to store  files. it may occupy too much sapce resource .
Dose moosefs provide other way to backup files to save more space, but also can store files  safely.
**thanks!!** ","> Hi,
> This is how the system works.
> Goal 2 = two copies.
> 
> If you like to have some ""space-saving"" solution you can try to use some filesystem with compression and use MooseFS storage class definition. There are many other available solutions in the open-source world.
## thank's very much !! It has bring so much convenience to me.
",50505775
1124,No storage class integrity,open,2019-09-18T01:22:10Z,2020-03-27T08:19:40Z,,CONTRIBUTOR,"Situation: all storage classes defined as `STRICT`; no `*` is used in storage class definitions.

New chunkserver is added with new label, not defined in storage classes.

As soon as chunkserver is started it receives flow of chunks -- that should have NEVER happened.

This is a _very_ serious issue that compromises integrity of data placement.

Please investigate with utmost attention.

Related to #237, #249.
","Yes, thanks. I'll test and close.",50505775
1125,[question] ec vs replication in SOHO cluster,open,2019-09-10T10:26:20Z,2019-09-16T19:13:52Z,,NONE,"Dear all
I have a question about the viability of pro version features in the home setup

Assuming I have important data I would like to store it with 3 copies.
In case on hdd fails/is replaced, I should still have enough information for moosefs to correct a random error based on the remaining two copies.
So technically I will need three HDD/chunk-servers. If I want to withstand machine failure it means three machines. 

Similar resiliency with moosefs pro ec would mean 8+2 chunkservers
Aiming to withstand machine failure I would need two chunkservers on a machine = minimum 5 machines with two chunkservers each - steep for a home setup.

to cut down on machines I could consider also following setups actually raising resiliency on HDD failure
8+3. In such a case, I could place three chunkservers on a machine = minimum 4 machines with three chunkservers each

or 8+4 = minimum of three machines with four chunkservers each.

This probably exceeds viable combinations for the SOHO environment. 
All above assumes equally sized HDD (which is obviously not always the case)


Could someone suggest what would be other implications of the above ec setups comparing to replicated?
For example, performance assuming each machine runs on single 1Gb NIC?","@onlyjob 
could you please help me understand?

you said ""EC create I/O on many chunkservers at once""
it made me think because I am not sure I understand what I/O means and how this is different from replication and why it is bad

so let's say 3 chunkservers 4 hdd each.
in replica 3 the download operation (big file, multiple files) will contact all three chunkservers and combine throughput - it's pretty fast, I don't deny it.
If I remember chunkserver is also a parallel process, so the operation can possibly read from all 12 hdd at once (I am not sure)

in EC say 8+4, the read operation will contact 8 chunkservers out of 12, but considering multiple files/big files it will (likely) combine throughout from all 12, accessing all 12 hdd
there is undeniably more processes running on each machine, but that's not really I/O (at least I think so)
 
Obviously, there is a processing overhead on EC for sure, but on reading I/O I don't see much difference.

There will probably be a difference on rebalancing and EC will eat more.

I am also not sure about write. EC will write only 1,5x of the data while replication will write 3x
so possibly more I/O on replication

a massive difference will probably be on change of the EC target



",50505775
1126,write error + mfsmaster exit during metadata dump with enough space on disk,open,2019-09-05T08:47:31Z,2019-09-11T09:01:08Z,,NONE,"We've seen some interesting mfsmaster problems with a 3.0.105 on CentOS 7.

The mfsmaster process died several times with a write error plus emergency metadata write whe trying to save the metadata to disk:
```
Sep  2 19:00:19 pfsdh01 mfsmaster[1894]: write error
Sep  2 19:00:19 pfsdh01 mfsmaster[1894]: write error
Sep  2 19:00:19 pfsdh01 mfsmaster[1894]: write error
Sep  2 19:00:19 pfsdh01 mfsmaster[1894]: can't write metadata
Sep  2 19:01:15 pfsdh01 mfsmaster[1894]: metadata file stored in emergency mode, file name: metadata.mfs.emergency
Sep  2 19:01:15 pfsdh01 mfsmaster[32254]: child finished
Sep  2 19:01:15 pfsdh01 mfsmaster[32254]: store process has finished - store time: 75.688
Sep  2 19:01:15 pfsdh01 mfsmaster[32254]: metadata stored in emergency mode (in non-standard location) - exiting
Sep  2 19:01:15 pfsdh01 mfsmaster[32254]: internal terminate request
Sep  2 19:01:18 pfsdh01 mfsmaster[32255]: background data writer - terminating
Sep  2 19:01:18 pfsdh01 mfsmaster[32254]: exited from main loop
Sep  2 19:01:18 pfsdh01 mfsmaster[32254]: exititng ...
```

File size of metadata.mfs.back is ~8GB with ~90GB free space on the /var/lib/mfs partition, there has been more than enough room for the metadata files.

Apparently the process died while trying to write the 8 byte header to the new dump file:
https://github.com/moosefs/moosefs/blob/b49c89890cbeeaf5cdcfde3bdb9087f9c014c2b1/mfsmaster/metadata.c#L763-L773

Looks like the write or the file open some lines before failed for some unknown reason:
https://github.com/moosefs/moosefs/blob/b49c89890cbeeaf5cdcfde3bdb9087f9c014c2b1/mfsmaster/metadata.c#L730

The mfsmaster process was using ~70% of the RAM (of 24GB), vm_overcommit is set to 1. The OOM killer did not start, there are no dmesg or syslog messages about the possible low memory situation (140% overcommit for about 1 minute dump time).

Possible failing operation is the write() in the bio_internal_write(). It returns a 0 when failed, but no check/output/save of the errno to have a reason:
https://github.com/moosefs/moosefs/blob/def1b253ec6044d4cb7ed588fbff4ec22e1e1ae3/mfsmaster/bio.c#L106-L120","Hi, 

we are using ZFS on Linux for the underlying file system at /var/lib/mfs to create fast and lightweight snapshots. This combination works great, with very fast writes as the ZFS caches the metadata dump into RAM before writing it to disk.

But maybe this caching broke the dump as the running mfsmaster, the forked dump process and the ZFS cache needed a lot of RAM for this. This may be some pathological case as it occured 3 times during several weeks, plus this is also hard to reproduce. Knowing the reason why the write() returned an error may help others too.",50505775
1127,FEATURE REQUEST - Make metrics accessible outside the CGI,open,2019-08-28T19:40:44Z,2019-11-13T21:21:52Z,,NONE,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?

FEATURE request

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).

3.0.103

#### Operating system (distribution) and kernel version.

Mix of debian stretch and buster.

I would like to have a way to extract the metrics that are currently displayed in the CGI and write them to `statsd` or `prometheus`.

That would make it much easier to set up proper alerting rules for various cluster conditions that I can currently only see in the CGI interface.","@oszafraniec, all the most important information on one page - nice!
@unixorn, so I will focus on these resources. 

Thanks.",50505775
1128,FEATURE REQUEST: Individual Disk Labeling for storage classes,open,2019-08-28T19:06:37Z,2019-11-07T06:20:23Z,,NONE,"## Have you read through available documentation and open Github issues?

Yes

## System information

A mixture of debian stretch and buster on ARM nodes.

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from 

3.0.103

## Feature Request

I'd like to be able to label individual drives for the purpose of replication storage classes instead of / in addition to labeling an entire chunk server.

For example, let's assume I have 4 chunk servers, 4 SSDs, and 4 spinning disks. I want to use label A for SSD, label P for all the disks, and label Z for spinning disk.

Normally for physical redundancy I'd want to put one SSD and one spinning disk in each chunk server, so that if one server dies, I just lose one SSD and one spinning disk. However, currently, if I put one SSD and one spinning disk on each chunk server, I can't label the SSDs differently than the spinning disks, so I can't define a storage class that say writes to two SSDs for the creation strategy and then has a 1 A (SSD) + 1 Z (spinning disk) + 1 P (anywhere in the pool) for the keep strategy.

I can do all that if I put two SSDs in two chunk servers and two spinning disks in the other chunk servers,  but then I don't get the physical redundancy of putting an SSD and a spinning disk in all the chunk servers.","@struthio - This is very unlikely that we will implement it.

The problem is that it basicaly can't be implemented without redesigning the system. Master has no info about disks. Chunkserver is just basic storage container. In theory it can store chunks wherever it wants - disk, tape, cloud (at least from the master's point of view). Because of that chunkserver can move chunks between disks without askig the master for permission. With labels assigned to disks chunkserver should either ask for permission before every chunk move (not a good idea) or remember together with chunk whole label expression to be able to decide if it is possible to move chunks between disks (also not a good idea - in case of storage class change the master should inform all chunkservers about all changed chunks - nightmare).

As @xandrus explained you can run more than one process of chunkserver, so you can have different labels assigned to different disks on the same machine. Then you can set CHUNKS_UNIQUE_MODE to 1 to tell master that it shouldn't put more than one copy of a chunk on chunkservers with the same IP number. Maybe this one condition should be always obeyed strictly because as for now @borkd is right and in some cases it could lead to data loss, but it will be fixed soon.

@borkd - I'm working just now on extending STRICT placement to all modes.",50505775
1129,When/why chunkserver is OVERLOADED?,open,2019-08-18T01:09:31Z,2019-08-24T21:02:54Z,,CONTRIBUTOR,"What to tweak in `mfschunkserver.cfg` to control when chunkserver is `OVERLOADED`?

Recently I've commissioned a new super-fast NVMe-based chunkserver and ironically it is in `OVERLOADED` state most of the time. I don't quite understand why -- in the _CGI/Servers_ tab load is usually under 30 yet in _CGI/Resources_ tab the chunkserver is marked `OVERLOADED`.

`nmon` show very little activity on NVMe device and it looks like chunkserver is the bottleneck on a very fast device.

Please advise.
","Thanks, @acid-maker. Your explanation make sense and it looks like I might be facing this very issue.
New label `T` has been made specifically for this chunkserver as it is a first chunkserver on NVMe storage...",50505775
1130," How to solve "" can't find metadata.mfs - try using option '-a' """,open,2019-08-16T10:49:20Z,2019-08-27T18:37:02Z,,NONE,"# Can't  start master server rightly
Recently , when I came across the power cut add I cant' rightly shutdown my server and moosefs sysetm.Today when I want to restart my moosefs sysetm, I meet a problem.
![image](https://user-images.githubusercontent.com/17978899/63162740-ee0b9380-c055-11e9-9f90-205b07b0e222.png)
I take a look at the **/var/lib/mfs**, and just found ' metadata.mfs.back', ' metadata.mfs.back
.1', and I don't know  the difference between two files. 
So, how can I finish that problem to protect my data,thanks!","Hello @dragon2611,

> @oxide94 I think you meant @GYUMO, I was just suggesting a feature to automate/semi-automate the recovery from metaloggers in the event of unclean shutdown.

Of course, I meant @GYUMO, I'm sorry.

> Also presumably if a metalogger was available and unaffected by the power loss (I.e it was elsewhere) it's worth copying the changelogs from it before doing the mfsmaster -a as they may be more up2date?

Yes, in such case it is worth copying the changelogs from Metalogger to Master Server. There is no necessity to rename them do do any actions – just copy to Master Server and it will handle them, as @acid-maker mentioned here: https://github.com/moosefs/moosefs/issues/278#issuecomment-515407353.

Thank you,

Best regards,
Piotr / MooseFS Team",50505775
1131,Security Concerns and Questions,open,2019-08-13T20:59:21Z,2019-08-14T10:06:59Z,,NONE,"Hello Team,

I have a few questions related to security and I hope you guys can help me out.

1) I could not find anything in docs on how to enable encryption on rest and while data is in transit between nodes. Is there a way to do it using external tools if Moosefs does not have this feature?

2) Any eta if this feature will be added to Moosefs?

3) Also, is there a way we can add a passphrase to mount points using mfsmount or any other tool because if someone is able to access the server then whole storage is exposed and could be a security breach at the storage level, We want to restrict the access of the mountpoints if that is possible.

Regards
Kiran","1. Encryption in MFS is not implemented (yet)
2. No ETA - this feature is of course on our roadmap but with rather low priority
3. Yes. In 'mfsexports.cfg' you can add passphrase (see man mfsexports.cfg and mfsexports.cfg.sample file)
4. It is also good idea to add passphrase for your chunkservers (define AUTH_CODE in your mfsmaster.cfg and mfschunkserver.cfg on each chunkserver)",50505775
1132,mfsmount on OSX,open,2019-08-07T12:27:01Z,2019-08-07T12:27:11Z,,NONE,"Dear all

I did not find documentation for using OSX as a client.
I can obviously call mfsmount but it does not seem to recover/reconnect properly after a master crash, locking me out o multiple applications (probably something hangs).
I might be doing something wrong because there is very little to go by.

For now, I had to fall back to share moose filesystem via smb, but that's obviously not a good solution.
",,50505775
1133,SOHO moosefs on commodity hardware [question],open,2019-08-04T13:03:31Z,2019-08-13T09:38:43Z,,NONE,"Hi everyone

Share your experience and configuration.

I am currently running GPL 3.0.105
Master on HP ProDesk 400 G2 Mini PC i5-6600T 16GB RAM (sadly not ECC) running Ubuntu 16.04
CPU picked for a compromise between performance and power consumption
additionally two external USB3 HDD as a backend for one chunkserver

In addition ancient HP microserver 54nl with 8GB ECC RAM also ubuntu 16.04, modded to hold six HDD and acting as two chunkservers

I am reasonably pleased with the results I am achieving
At the moment I am only resilient to single HDD failure (maybe three if lucky), but I am planning to add another HP microserver to gain a little bit more of the resilience.

what about you?


","I'm running on a mixture of older hardware (Mostly servers) and a Synology RS816 (It's possible to the chunkserver to crosscompile for it).  Also currently across 2 DC's using Wireguard VPN about 5ms latency between sites.

Personal cluster so it's whatever I can get my hands on for cheap, I did try a chunkserver in Vegas but the RTT was far to high it will work across multiple sites but the latency between them needs to be as low as possible (I.e don't do it with a 130ms RTT)",50505775
1134,connection lost with background data writer - exiting,open,2019-08-02T22:47:11Z,2019-08-31T21:30:14Z,,NONE,"Hi

My master crashes with this weird message

```
connection lost with background data writer - exiting
Aug 02 23:37:13 mercury mfsmaster[5883]: child finished
```

any idea what could be the reason and how to avoid it?

3.0.105 GPL on Ubuntu 16.04
","Thank you @oxide94 
The machine is maxed I believe
I think I know which process ate all the memory
I will try to move my master or this process to a different machine

Thank you!
Closing the issue",50505775
1135,"Chunk files in /lost+found, without proper chunk filename. How to restore the correct filename?",open,2019-07-19T00:03:51Z,2019-08-08T00:46:53Z,,NONE,"<!--
Thank you for helping to make MooseFS better!

*IMPORTANT* -  MooseFS documentation is scattered across number of places and PRs to clean that up are welcome.

*Before* creating a new issue please look around:
 - Hardware guide: https://moosefs.com/wp-content/uploads/2018/08/MooseFS-Hardware-Guide-v.0.9.pdf
 - Best practices: 
   - https://moosefs.com/support#best-practices
   - https://moosefs.com/blog/tag/best_practices/
 - MooseFS documentation: https://moosefs.com/support/#documentation
 - FAQ: https://moosefs.com/faq/
 and
 - open issues in Github tracker: https://github.com/moosefs/moosefs/issues
  
If your concern is not properly addressed in any of the above sources, then create a new issue.

Please fill in as much of the template as possible.
-->

## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
QUESTION for the COMMUNITY/Developers

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).
3.0.105 running on Debian and Centos, from moosefs.com

#### Operating system (distribution) and kernel version.
<!-- https://moosefs.com/blog/what-are-the-operating-systems-and-networking-requirements/ -->
<!-- If servers and clients run on different platforms include outputs from ""uname -vsrm; mfsmount -V; lsb_release -a"" -->
Debian 9 and Centos 7

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.
<!--
One can expect differences in performance between deployments running on RPi, VMs, and on server grade bare metal.
Hardware guide: https://moosefs.com/wp-content/uploads/2018/08/MooseFS-Hardware-Guide-v.0.9.pdf
-->
Varied hardware, XEON Cpus, lots of memory (32 to 64GB ram)... a mix of dell and intel servers.
filesystem is ext4

#### How much data is tracked by moosefs master (order of magnitude)?
<!-- 
https://moosefs.com/blog/master-servers-requirements/
Values reported in the info tab, order of magnitude will suffice
-->
 - All fs objects: 3688192
 - Total space: 16TiB
 - Free space: 7TiB
 - RAM used: 2.3GiB
 - last metadata save duration: ~2.1s


## Describe the problem you observed.
He had a crash on a machine running a chunkserver (an old no-name Xeon box), and at reboot the disk was ""fcheked"", and some chunk files ended up in /lost+found, with those recover weird names. 

I would like to known how I can re-construct the actual chunk name from the file contents, and what folder should I move it into... is there a tool to do that?

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

to reproduce, just rename a chunk to whatever name!
How to reconstruct the original file name back, with info on where to put it?

#### Include any warning/errors/backtraces from the system logs
<!--
If this complaint relates to performance, please include baseline benchmark
results of the underlying infrastructure and any steps you took to troubleshoot.

*IMPORTANT* - Please mark logs and text output from terminal commands 
or else Github will not display them correctly. 
An example is provided below.

Example:
```
this is an example how log text should be marked (wrap it with ```)
```
-->
","Hi. There is no such tool. I just quickly wrote something in python that can help you:

```
#!/usr/bin/env python3

import mmap
import struct
import sys
import os

def chunk_name(fname):
	i = open(fname)
	mm = mmap.mmap(i.fileno(),0,mmap.MAP_PRIVATE,mmap.PROT_READ)
	mms = mm.size()

	if mms < 0x100:
		print(""file %s: wrong size (0x%08X) - can't continue"" % (fname,mms))
		mm.close()
		return

	if mm[:8]!=b'MFSC 1.0' and mm[:8]!=b'MFSC 1.1':
		print(""file %s: wrong header (%s)"" % (fname,mm[:8]))
		mm.close()
		return

	chunkid,version = struct.unpack_from("">QL"",mm,8)
	mm.close()

	propername = (""chunk_%016X_%08X.mfs"" % (chunkid,version))
	fullpath = os.path.join(os.path.dirname(fname),propername)
#	print(""mv %s %s"" % (fname,fullpath))
	os.rename(fname,fullpath)

for fn in sys.argv[1:]:
	chunk_name(fn)
```

If you want to do renames yourself then uncomment ""print"" line and comment ""os.rename"" line.

After renaming your files you can put them to any folder pointed by your mfshdd.cfg (inside any hexadecimal folder - for example into 'FF' or '00'). Remember to stop chunkserver before moving those files. After moving your chunks remove '.chunkdb' file from main folder (the one pointed by your mfshdd.cfg) and then start your chunkserver again.
",50505775
1136,buster repo?,open,2019-07-18T22:38:29Z,2019-08-30T08:55:34Z,,NONE,Can we get a repo added for debian 10 (buster) please?,Thanks a lot for your comment on that. Piotr,50505775
1137,Transport endpoint is not connected?,open,2019-07-17T03:26:44Z,2019-07-23T15:41:06Z,,NONE,"Every day our client will get this error, then you have to unmout, and then mount;
We have three clients mounted on this machine.
like this：
![image](https://user-images.githubusercontent.com/51878240/61344912-8a8b1c00-a885-11e9-8825-6810c22f8ed8.png)

","The error ""Transport endpoint is not connected"" is internal fuse error and it means that mfsmount process has died.

Here we have two options:

1. mfsmount died because of software bug.
2. mfsmount has been killed by the kernel (for example - oom kill).

To distinguish between those two please check your syslog/messages file. There should be some info about that (usually in Unix systems prcesses don't disappear without a trace).",50505775
1138,CGI: incorrect number of chunk replications,open,2019-07-17T02:36:28Z,2019-08-08T00:46:27Z,,CONTRIBUTOR,"""Server Charts"" --> ""number of chunk replications per minute"" incorrectly show number of _attempted_ replications instead of actually _completed_ replications.

On a particular chunkserver free space was exhausted and CGI showed unrealistic number of replications (~8_500_000 replications per minute) for a duration of a problem.

Chunkserver logged the following:
~~~~
mfschunkserver[34514]: replicator: hdd_create status: IO error
mfschunkserver[34514]: create_newchunk: file:/mnt/hdd04/ext4//47/chunk_0000000000AAC76B_00000000.mfs - write error: Success (errno=0)
~~~~

To reproduce the problem, configure `mfshdd.cfg` with free space less than available on the disk, like in the following example:

    /mnt/hdd04/ext4 0.2GiB

",Yes. Thats true. We show only replication/deletion attempts.,50505775
1139,"When I shutdown my chunkserver ,I got some problem",open,2019-07-14T09:48:25Z,2019-07-19T03:09:43Z,,NONE,"Recently, I change my master server, when I restart one of my chunkserver, I got some mistakes:

 **open files limit has been set to: 16384**
![image](https://user-images.githubusercontent.com/17978899/61181958-b036dc00-a65f-11e9-8048-5c1c17ee5e66.png)

and I can't solve this problem","Make sure your moosefs-chunserver is really stopped (e.g. `ps fax | grep chunkserver`) beacuse the start message says the lock file is already taken.
",50505775
1140,[question] one MooseFS + cloud compute + two data centers with bad connection,open,2019-07-11T13:17:21Z,2019-07-19T02:54:27Z,,NONE,"QUESTION

Has anybody here used one MooseFS instance for cloud computing on two or more data centers with a poor connection between data centers?
In cloud computing there are many nodes which write data and many nodes which read data. I've only heard about MooseFS instalations on two data centers but for CDN purposes where there are one writer and many readers. I know about storage classes and all the features, but I need real working examples.

If so, what experience you have with such configuration? ","I'd ask in the mailing list, even search the archives, as you'll probably reach more moosefs users :)

https://sourceforge.net/p/moosefs/mailman/moosefs-users/

BTW, moosefs developers: I couldn't find the mailing list on your website, you might consider displaying it more prominently.
",50505775
1141,backup moosefs master consistent?,open,2019-07-05T11:15:35Z,2019-07-05T12:45:56Z,,NONE,"## Have you read through available documentation and open Github issues?

Yea I think I have read everything about backing up an Chunkserver, Metalogger and Master.

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?

FEATURE request || QUESTION for the COMMUNITY


## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).
I am running an MooseFS 3.0 which is installed by the distro.

#### Operating system (distribution) and kernel version.
CentOS 7

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.
MooseFS is installed on 5 different machines. The first one is the master and the second is the metalogger server. The other three servers are MooseFS chunkservers.

#### How much data is tracked by moosefs master (order of magnitude)?
 - Total space: 1,4TB
 - Free space: 1TB


## Describe the problem you observed.
Our question is, how to save the metadata 100% consistent. I have read the documentation for a long time now and found the part: 
```8.26 I have a Metalogger running – should I make additional backup of the metadata file on the Master Server?``` 
and the description says that the master is flushing his metadata every hour to the ```metadata.mfs.back``` file.

The documentation recommends now to save the file on every 00:30 hours. But I don't want to get into runtime  errors. Sure the file should be backup very fast and I won't get into any trouble. But for the customer it is very important to have an 100% woking restore.   

I tried to trigger this process manually but I didn't found anything. Only that the master process is forking it's files from the RAM into the files, if there are any changes: ```5.1 Metadata save```. 

So does anyone know how to get not into any runtime changes with my backup tool?
",,50505775
1142,How to specify the number of backups for a particular folder in Moosefs,open,2019-07-04T03:10:30Z,2019-07-05T05:29:54Z,,NONE,"## There are three folders a, b, c in my Moosefs system. 
But I don't want to set the backup number of all folders to 2. This will cause my disk to be full.
How to specify **""a""** floder backup number to 1,
 b, c flowder backup number to 2?
I think it is better to set different backup with different data depends on it's priority","OK, I finnaly solve this problem by the command. And we should pay attention to the priviledge, when we be denied, we can execute this command as root (sudo command)",50505775
1143,How to change master server,open,2019-06-30T08:27:31Z,2019-07-04T02:56:16Z,,NONE,"### Recently, I found my server used as  **moofse lead server** has been so slow.
### so I buy a new server machine to replace this master node.
### however ,i can't found a complete turtoial for this condition, there are only some method for adding chunk server.
## So can any body give me some useable way or advice.
# Thanks！","> em cleanly before starting.

Thank you for your answer.You are welcome!",50505775
1144,"What scene will produce heavy load, how is the value of load calculated?",open,2019-06-26T10:07:00Z,2019-07-23T15:38:20Z,,NONE,,"Hello @smallTang123,

Please refer to issue #254, especially [this answer](https://github.com/moosefs/moosefs/issues/254#issuecomment-489503312).

Best regards,
Piotr / MooseFS Team",50505775
1145,modify mfsexports.cfg and mfsmaster need restart?,open,2019-06-14T08:27:14Z,2019-07-23T15:38:52Z,,NONE,,"Hello @zhs077 ,

Changing `mfsexports.cfg` requires running only `mfsmaster reload`, not restart.

Best regards,
Piotr / MooseFS Team",50505775
1146,document mfsioretries,open,2019-06-05T15:45:02Z,2019-08-22T23:55:23Z,,NONE,"The documentation for `mfsmount` says:

>  `-o mfsioretries=N` – specify number of retiries before I/O error is returned (default: 30)

Can we elaborate on what that means? Also, it's misspelled.

Some questions I have is:

* how long before a retry? Are there delays before read attempts?
* If it attempts to read and the chunkserver times out, that counts as an attempt. Where does *that* timeout come from?
* I/O errors in what context? Disk errors from the chunkservers? Failure reaching mfsmaster? Chunks not available?
* Is it going to try multiple chunkservers in a roundrobin fashion for each try?
* How does this correlate to errors such as: `55072862, index: 0, chunk: 185338478, version: 133 - readworker: connection with (REDACTED:9422) was timed out (lastrcvd:294587.688871,now:294589.691715,lrdiff:2.002844 received: 82480/524288, try counter: 7)`

Please make the documentation a lot more comprehensive for this option.

Thanks and keep up the good work.
",,50505775
1147,mfs chunkserver  sync data  slow,open,2019-05-28T02:07:31Z,2019-05-29T09:18:55Z,,NONE,"first question:
 
The chunkserver storage space is insufficient. I purchased the server and added it to the cluster. However, its data equalization time is really slow. The total data is 120TB. After joining the cluster at noon today, only 76GB of data is synchronized until now. At 0.15%, I really hope that there is no way to make it synchronize data faster. I have checked whether the network has a delay and there is no abnormality. Please refer to the following information.
 
Mfsmaster version: 3.0.81
 
Mfschunkserver version: 3.0.81-1

10.212.14.7  add chunkserver

![image](https://user-images.githubusercontent.com/6948476/58445734-553d3a00-8130-11e9-8a74-d1ad7bfb4438.png)

second question
 
Due to server resource issues, I shared Chunkserver (version: 3.0.81-1) with the current cluster, only mfschunkserver -c loads different configuration files, but MFSmaster is standalone (version: 3.0.100), now has a server Resources, I want to join the new chunkserver, I am not sure if its version is using version: 3.0.100 or 3.0.81-1, then I will stop the shared chunkserver, whether it will affect the data, this thing I am not sure.
 
thank you very much! ! ! !","> 你也可以使用主配置玩一点...你会发现下面有这样的东西，但是...我wolud将它保留为默认值:)
> 
> ```
> # Maximum number of chunks to replicate to one chunkserver (default is 2,1,1,4)                                                                                                                                                               
> # one number is equal to four same numbers separated by colons                                                                                                                                                                                
> #  first limit is for endangered chunks (chunks with only one copy)                                                                                                                                                                           
> #  second limit is for undergoal chunks (chunks with number of copies lower than specified goal)                                                                                                                                              
> #  third limit is for rebalance between servers with space usage around arithmetic mean                                                                                                                                                       
> #  fourth limit is for rebalance between other servers (very low or very high space usage)                                                                                                                                                    
> # usually first number should be grater than or equal to second, second greater than or equal to third, and fourth greater than or equal to third ( 1st >= 2nd >= 3rd <= 4th )                                                                
> CHUNKS_WRITE_REP_LIMIT = 2,1,1,4                                                                                                                                                                                                              
>                                                                                                                                                                                                                                               
> # Maximum number of chunks to replicate from one chunkserver (default is 10,5,2,5)                                                                                                                                                            
> # one number is equal to four same numbers separated by colons                                                                                                                                                                                
> # limit groups are the same as in write limit, also relations between numbers should be the same as in write limits ( 1st >= 2nd >= 3rd <= 4th )                                                                                              
> CHUNKS_READ_REP_LIMIT = 10,5,2,5   
> ```

I want to try to discard the old chunkserver and mark the hard disk as * (delete) and reload the chunkserver. Since the copy of the file is 2, is it available for existing affecting clients?
The reason is that the  add new chunkserver equalization data is slower. Will the new chunkserver provide services for new read and write IO?
 ",50505775
1148,master exits / stops unexpectedly,open,2019-05-25T10:04:42Z,2019-05-27T21:56:47Z,,NONE,"Hi,

moosefs 3.0.104 on ubuntu 16.04
My master keeps crashing with the following last entries in the log
Any idea what could be the cause and how to prevent it from happening?

```
mfsmaster[2034]: chunk 0000000000437260_00000001: there are no copies
mfsmaster[2034]: chunk 00000000003DBE96_00000001: there are no copies
mfsmaster[2034]: chunk 00000000002378D8_00000001: there are no copies
mfsmaster[2034]: chunk 0000000000422738_00000001: there are no copies
mfsmaster[2034]: chunk 00000000003E8A5A_00000001: there are no copies
mfsmaster[2034]: chunk 0000000000386A9F_00000001: there are no copies
mfsmaster[2034]: chunk 0000000000437AAE_00000001: there are no copies
mfsmaster[2034]: csdb: found cs using ip:port and csid (192.168.1.80:8421,3)
mfsmaster[2034]: chunkserver register begin (packet version: 6) - ip: 192.168.1.80 / port: 8421, usedspace: 11684969693184 (10882.48 GiB), totalspace: 11999962742784 (11175.84 GiB)
mfsmaster[2034]: chunkserver register end (packet version: 6) - ip: 192.168.1.80 / port: 8421
mfsmaster[2034]: connection with client(ip:192.168.1.71) has been closed by peer
mfsmaster[2034]: (192.168.1.21:8422 -> 192.168.1.80:8421) chunk: 00000000002A3291 replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:8422 -> 192.168.1.80:8421) chunk: 00000000001C731F replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:8422 -> 192.168.1.80:8421) chunk: 00000000001C9F9D replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:8422 -> 192.168.1.80:8421) chunk: 00000000004202CC replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:9422 -> 192.168.1.39:9422) chunk: 0000000000187668 replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:8422 -> 192.168.1.80:8421) chunk: 000000000023E339 replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:8422 -> 192.168.1.80:8421) chunk: 00000000001F0F8B replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:8422 -> 192.168.1.80:8421) chunk: 00000000001975FA replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:8422 -> 192.168.1.80:8421) chunk: 00000000001C6EC2 replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:8422 -> 192.168.1.80:8421) chunk: 000000000029FA6B replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:9422 -> 192.168.1.80:8421) chunk: 00000000002AD053 replication status: Disconnected
mfsmaster[2034]: (192.168.1.21:9422 -> 192.168.1.80:8421) chunk: 00000000002A35D2 replication status: Disconnected
mfsmaster[2034]: child finished
mfsmaster[2034]: store process has finished - store time: 22.410
mfsmaster[2034]: metadata not stored !!! (child was signaled) - exiting
mfsmaster[2034]: internal terminate request
mfsmaster[2034]: exited from main loop
exited from main loop
mfsmaster[2034]: exititng ...
exititng ...
mfsmaster[2034]: main master server module: closing *:9421
mfsmaster[2034]: master <-> chunkservers module: closing *:9420
mfsmaster[2034]: master control module: closing *:9419
mfsmaster[2034]: cleaning metadata ...
cleaning metadata ...
cleaning objects ... done
cleaning names ... done
cleaning deletion timestamps ... done
cleaning quota definitions ... done
cleaning chunks data ...done
cleaning xattr data ...done
cleaning posix_acl data ...done
cleaning flock locks data ...done
cleaning posix locks data ...done
cleaning chunkservers data ...done
cleaning open files data ...done
cleaning sessions data ...done
cleaning storage classes data ...done
cleaning dictionary data ...done
mfsmaster[2034]: metadata have been cleaned
metadata have been cleaned
mfsmaster[2034]: process exited successfully (status:0)
process exited successfully (status:0)
kisiel@mercury:~$ sudo service moosefs-master start
```","@oxide94 thank you
I missed the best practices when was moving the master to a different machine

could the exit be caused by filling the chunk servers to the brim?
For a moment I had a very little space and I noticed that one of chunkservers went to sleep a few times triggering replication of chunks
",50505775
1149,Feature request: More granular control of replication.,open,2019-05-06T15:05:59Z,2019-07-23T15:39:49Z,,NONE,"Is there anyway to exclude my SSD based chunk-server(s) from being used as a target in the event that a chunk-server goes offline and MooseFS is otherwise unable to meet the SC Goal.

Edit: realised it does put a chunkserver in temp maintenance if it vanishes.

Lets assume I have Site A and Site B they are connected via a VPN.

Site A Has a Small amount of SSD based storage and a larger amount of HDD Based storage.

Site B only has HDD based storage and mostly an offsite replica of Site A (it could also house some non critical files)

The SC states that a the new files should be created on Site A's SSD storage then moved to Site A + B Hdd storage, this works fine.

However if the Link between Site A and Site B drops what ends up happening is MFS will then fill the SSD's trying to hit the replication target only to delete the data from the SSD once the VPN reconnects.

Obviously Ideally this would be an MPLS or some other Dedicated circuit but as this is a personal cluster that's not within the realms of my budget.  Usually the VPN is stable unless I break something.","Bump, I'd really like a way to exclude specific chunk-servers from being replication targets maybe a change to storage classes that can include an Not definition for a chunk server

e.g -K  B,N,!A  

So use B,N but never use A even if it's because a servers gone offline and you can't meet the replication target.",50505775
1150,Docs: Block devices,open,2019-05-01T20:18:55Z,2019-06-18T11:47:42Z,,NONE,"Is it possible to get better documentation around block devices, does it require the master to allow / to be mounted as I can't see a way of allowing for example /blockd to be used for block devices.

If i only allow a mount that is a subfolder mfsbdev start seems to fail.","Hm, I do not understand about mfsbdev.

If I use standard mfsmount with fuse, then I simply mount moosefs , do work, change replica goals, etc...

But I look manual about mfsbdev, and dont understand.

Standard example:
mfsmaster:9421  3.4T  121G  3.3T   4% /var/cluster - this is mounted moosefs

Mfsbdev can create block device from entire MooseFS and how to work with this?

I look only option about file -f (where this file need stay? in /var/cluster/ mounted MooseFS)

Or mfsbdev its a feature for mount different native file from other native file system from separate hdds (eg ext4) inside a subfolder in /var/cluster/block , where block = file on /dev/sde4 (ext4) ?

Thanks.",50505775
1151,CGI: not showing replications to be done?,open,2019-04-29T00:05:12Z,2019-04-29T00:05:12Z,,CONTRIBUTOR,"Where in CGI can I see how many chunks are to be moved in order to comply with Storage Class placement? Looks like currently such information is not exposed and can not be seen in CGI...

When I change storage class using `mfssetsclass` utility, data starts flowing to chunkservers with corresponding labels yet CGI have no indication of how many chunks should be relocated, etc.

I can only track replication activity in Charts...
",,50505775
1152,STRICT policy is not strictly obeyed; SC boundaries not respected,open,2019-04-28T23:58:50Z,2020-03-26T09:33:53Z,,CONTRIBUTOR,"All my Storage Classes (SC) are in `STRICT` mode to enforce boundaries (see #237) and to avoid this very issue.

Yet one chunkserver defined with exclusive SC and label received 6 chunks despite having no files in the corresponding SC. In CGI, _Servers_ and _Disks_ show 6 chunks placed to unallocated chunkserver (confirmed by checking chunkserver's data directory) whilst _Resources_ tab show `0` (zero) chunks -- clearly a spill from other Storage Class in violation of _STRICT_ policy.

Please investigate.
","Thank you very much for addressing this very important issue in 3.0.112.

I'm yet to provide a feedback on this and so far my cluster is massively ""`replicate to correct label (all modes)`"", already for over 24 hours. I'll evaluate (and report) data placement here when cluster finish moving data.

What would be very useful to add is some indication of how many chunks are out of place to ""CGI/Resources"" page.",50505775
1153,is moosefs compatible with hadoop filesystem? ,open,2019-04-26T02:23:46Z,2019-09-05T12:43:52Z,,NONE,"
more precisely,  can work with Spark intead of HDFS ?","I plan to test this in the next 3 months, I will let you know.",50505775
1154,3.0.105: temporary chunkserver maintenance is not working as expected,open,2019-04-25T22:46:25Z,2019-08-07T15:56:22Z,,CONTRIBUTOR,"Replication starts immediately as soon as one chunkserver is stopped without waiting for `CS_TEMP_MAINTENANCE_MODE_TIMEOUT`.

Ideally there should be a way to delay replication of ""undergoal"" or ""endangered"" chunks as result of temporary chunkserver shutdown.

Also it might be useful to stop some chunkservers overnight and in such case it is necessary to delay replication too.","> We can add to the mfsmaster.cfg new option ""DAYS_TO_REMOVE_UNUSED_CHUNKSERVER"" (or something like that) with default value set to 30 or something like that.

Sounds like a good idea. Let's do that please. :)

Thanks.
",50505775
1155,[question] Chunkserver's metaid,open,2019-04-22T09:21:11Z,2019-04-28T13:47:50Z,,NONE,"mfschunkserver report wrong meta id, and the cgi can not find the mfschunkserver, when i delete the mfschunkserver meta.id and restart it, it restored. what can i do when i find the wrong meta id error, thank you !","> Wrong meta id is MooseFS security to prevent connecting chunkservers from one cluster to another.
> 
> Question is why you have different meta id on this chunkserver?
> 
> Basically, if you are 100% sure that this chunkserver was connected to this specific MooseFS master and all chunks previously belonged to this Cluster you can remove the .metaid file from the root directory on all disks defined in mfshdd.cfg.
> 
> for example:
> /mnt/chunks1/.metaid
> /mnt/chunks2/.metaid
> 
> But please remember if these chunks belong to different MooseFS cluster they will be removed!
hi @xandrus , thank you for your replay! we designed and implemented the master HA based on mfsmaster and mfsmetalogger. It appeared that chunkserver report wrong meta id error when master node switched not long ago. We think chunkserver metaid needs to be confirmed by master, if not, the master will not receive the chunkserver's connection, the master switch process may cause the meta id consistent between master and chunkserver, but we do not understand how the master determine whether the chunkserver's metaid is right or not, can you explain it? and if that happens, how can we make the chunkserver connect to master successful, delete the .metaid file will cause the data loss, but we do not want that happen, this problem is very important to us, thank you very much!
",50505775
1156,"chuinkserver reported missing chunk file, no apparent reason",open,2019-04-16T22:21:17Z,2019-12-27T21:11:18Z,,CONTRIBUTOR,"I'm investigating an error reported on one of the disks in the CGI and found the following in logs:

~~~~
03:51:02 node01 mfschunkserver[225550]: hdd_io_begin: file:/var/lib/mfs/SSD400/ext4//86/chunk_000000000034E2A0_00000001.mfs - open error: ENOENT (No such file or directory)
~~~~

There were no other error reported around this time.
This may be due to software bug as chunkserver has been running for a long time (without restart) and the hardware is healthy...
","We encountered the same problem with version 3.0.105
Health check passed
mfschunktool did not help anything
unmounted the filesystem and ran e2fsck looked good
neither physical nor logical error messages (e.g. filesystem) reported
Restarting moosefs-chunkserver could clear the alert message

It seems like a dummy error message... not sure how to stop it?

===========================
mfschunkserver[24633]: hdd_io_begin: file:/var/mfs/disk5/mfs//55/chunk_00000000005FF855_00000001.mfs - open error: ENOENT (No such file or directory)

~# smartctl -a /dev/sdo1
smartctl 6.2 2013-07-26 r3841 [x86_64-linux-4.4.0-133-generic] (local build)

=== START OF INFORMATION SECTION ===
Model Family:     Seagate Barracuda 7200.12
Device Model:     ST31000524AS
Serial Number:    6VPEX4F8
LU WWN Device Id: 5 000c50 03cecdc38
Firmware Version: JC4B
User Capacity:    1,000,204,886,016 bytes [1.00 TB]
Sector Size:      512 bytes logical/physical
Rotation Rate:    7200 rpm
Device is:        In smartctl database [for details use: -P show]
ATA Version is:   ATA8-ACS T13/1699-D revision 4
SATA Version is:  SATA 3.0, 6.0 Gb/s (current: 6.0 Gb/s)
Local Time is:    Fri Dec 27 13:03:49 2019 PST
SMART support is: Available - device has SMART capability.
SMART support is: Enabled

=== START OF READ SMART DATA SECTION ===
SMART overall-health self-assessment test result: PASSED

~# mfschunktool -r /var/mfs/disk5/mfs//4E/chunk_0000000003A94556_00000001.mfs
/var/mfs/disk5/mfs//4E/chunk_0000000003A94556_00000001.mfs: error opening file !!!

~# e2fsck /dev/sdo1
e2fsck 1.42.9 (4-Feb-2014)
disk5: clean, 1984806/61054976 files, 154010716/244190208 blocks",50505775
1157,mfscgiserv configure lockfile path [feature],open,2019-04-11T21:35:24Z,2019-04-11T23:13:24Z,,NONE,"moosefs version: 3.0.104
on MacOS 10.13.6

when I try to run mfscgiserv the system complains about the inability to create a lockfile
OSError: [Errno 13] Permission denied: '/usr/local/var/mfs/.mfscgiserv.lock'

Since I am running moosefs components as user-agents I am forced to ensure that /usr/local/var/mfs/ exists and that my user has access to it

It would be really nice to have the ability to configure this path
","`.mfscgiserv.lock` is actually a PID file, redundant under Systemd when daemon is running in foreground.

There was an old loosely related discussion about removing PID management code from LizardFS's cgiserv:  https://github.com/lizardfs/lizardfs/pull/219

IMHO it is not the daemon's job to manage its PID file. This can be offloaded to init system.",50505775
1158,"""mfsmaster -a"" should have been a utility",open,2019-04-05T06:42:51Z,2019-06-27T18:07:41Z,,CONTRIBUTOR,"Promoting metalogger's data to master is needlessly difficult because master should be started with `-a` option exactly once to convert `_ml` files.  
Metalogger data conversion should have been implemented as a command line utility because master is typically started by init system (e.g. systemd) and administrator must insert and then remember to remove `-a` parameter from init script or `.service` file.
","Interesting, but in case of use PRO version of MooseFS. Where we have for example 1 Leader and 2,3,4 Follower servers, without using metaloggers. If all servers have power failure at one moment(including dump time), -a option can restore metadata from changelogs , not _ml changelogs? Or in this case we can only use for repair metadata.back file?",50505775
1159,mfscli: incorrect exit code on error,open,2019-04-05T03:33:16Z,2019-04-19T07:09:06Z,,CONTRIBUTOR,"~~~~
# mfscli -H broken-host  -SIG; echo $?
Can't find masters (resolve 'broken-host') !!!
0
~~~~

~~~~
# mfscli -H localhost  -SIG; echo $?
Working master servers not found !!! - maybe you are using wrong port number or wrong dns name
0
~~~~

When there is an error, `mfscli` utility should exit with non-zero status code.
","In HA of course you can check each master individually (use in '-H' one IP or DNS name that resolves only to one master). 
So my proposition '1'-'3' as 'success' and '4'-'5' as 'failure' seems to be ok (we have working master(s), but maybe without a LEADER). And option to change it. With such option only case '1' (LEADER or single master available) reported as 'success' and all others as 'failure'.
What do you think?",50505775
1160,preventing cross-label spills; respecting storage class boundaries,open,2019-04-02T23:27:14Z,2019-04-02T23:36:11Z,,CONTRIBUTOR,"Creation modes have one severe limitation which is lack of semantics to exclude labels/chunkservers from fallback in non-strict mode.

When there is no free space on chunkservers of a certain class, in `STD` and `LOOSE` modes chunks can be created on another storage labels/chunkservers.

I need to protect some of ""other"" storage classes from spills of chunks from labels running out of space. In order to avoid that _all_ my storage classes should be `STRICT` because currently one can only define which storage classes can send chunks to other labels but not which classes allowed to receive such chunks.

It may be important to prevent cross-labels spills. A chunkserver can be unreliable, slow, have little space or it can be allocated to VIP tasks so overflow by chunks from other labels is undesirable.

It would be handy to be able to mark certain chunkservers to never ever, under any circumstances, take overflow chunks from other storage classes. 

Cross label replication is a dangerous thing, especially by default. Violating storage class definitions - even temporary, invalidates data placement which can endanger data, degrade performance and cause cascading out of space issues across all labels.
","Another option to solve this problem could be extending storage classes to define fallback location(s) when no free space is available.
When there is no place for a chunk it should not be made anywhere else but on designated chunkservers as per goal definition.

Here I've made some relevant comments to a similar problem:

  * https://github.com/lizardfs/lizardfs/issues/612#issuecomment-342015460
  * https://github.com/lizardfs/lizardfs/issues/612#issuecomment-342025433
",50505775
1161,crosscompile for RS816,open,2019-04-02T22:27:57Z,2019-04-06T19:44:45Z,,NONE,"I can get moosefs to compile for an RS816 (and probably others using the same arch) using  https://github.com/SynoCommunity/spksrc if I use the following make file

```
PKG_NAME = moosefs
PKG_VERS = 3.0.104
PKG_EXT = tar.gz
PKG_DIST_NAME = v$(PKG_VERS).$(PKG_EXT)
PKG_DIST_SITE = https://github.com/moosefs/moosefs/archive
PKG_DIR = $(PKG_NAME)-$(PKG_VERS)

DEPENDS= cross/fuse cross/boost
BOOST_LIBRARIES +=filesystem iostreams program_options system
ENV += BOOST_LIBRARIES=""$(BOOST_LIBRARIES)""

HOMEPAGE = http://moosefs.com
COMMENT  = Distrubuted Filesystem
LICENSE  = GPLv3

GNU_CONFIGURE = 1
CONFIGURE_ARGS = --host=armv7

include ../../mk/spksrc.cross-cc.mk
```

I'd then run make arch-armada38x-6.1 -j2

However I have to hack out the tests for bswap in the configure file as it errors on cannot run them if cross compiling.

Weirdly if I run ./configure --host=armv7 from the moosefs folder it doesn't try to run those tests so I presume it's something to do with the way the spksrc scripts try to compile it.  However using the spksrc scripts is beneficial as it's compiles other pre-reqs 

I've never bothered to go as far as packaging it into an SPK but I did get the chunkserver and metalogger from 3.0.103 running on the Syno and whilst I don't try to push it very hard it appeared to be stable for all the time i've been running it.","Just managed 60MB/s sustained write speed over a wireguard tunnel ( https://github.com/runfalk/synology-wireguard ) - Given the RS816 is the entry level Rackmount and has a fairly weak CPU that's really not bad :-)

About 5ms network latency between the master and the Syno (They're on different sites)

Previously using a Zerotier tunnel it would manage about 8-12MB/s on a good day.",50505775
1162,Register MooseFS ports with IANA,open,2019-04-01T02:23:46Z,2019-04-01T02:23:46Z,,CONTRIBUTOR,"http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.txt

",,50505775
1163,Number of archived chunks is reported incorrectly,open,2019-03-31T03:56:17Z,2019-04-10T14:55:39Z,,CONTRIBUTOR,"There are 20+ million chunks in Storage Class `-K M+P,M+P -A M+P,W` -- all stable, modified a while ago and properly distributed between `M` and `P` chunkservers.

Yesterday I've added new/first chunkserver labelled `W` and all chunks in that storage class jumped from ""# of standard chunks"" column into ""# of archived chunks"". Numbers in ""exact"" and ""over"" are changing while chunkserver `W` is being filled with chunks but this is clearly wrong because data is not in place yet, as defined by ""archive"" section of Storage Class (`-A M+P,W`).

There are no other storage classes that use `W` chunkservers and all storage classes in use are defined with explicit chunkserver labels.
","The trouble with current representation of Resources in CGI is that ""# of archived chunks"" and ""exact"" numbers suggest that data is already in place as required by storage class while in fact it is a desirable state of how things should be but not necessary how things are.
""Exact"" chunks might need to be moved or replicated but there is no indication that data is not where it should have been. See also related #223 demonstrating how ""exact"" chunks can be actually undergoal and in wrong place at the same time...",50505775
1164,Unsafe goal change can lead to data loss and availability problems,open,2019-03-29T11:18:04Z,2019-04-10T12:39:13Z,,CONTRIBUTOR,"I have experienced data availability issue due to unsafe Storage Class change.

At first, I created some files in Storage Class ""2ssd"" defined as `-C C,C+S -K S,C+S`.  
(`C` and `S` are just different types of SSDs.) Basically ""2ssd"" have goal=2 and both replicas are on SSDs.

Then (some time later) I have changed Storage Class of those files to ""archive"" class defined as `-C C,C -K L`. As you can see new chunks in ""archive"" class are created with goal=2 where two replicas are on SSDs, then goal reduces to 1 when one replica is moved to `L` (chunkserver with somewhat redundant HDD configuration).

Before all chunks were created in `L`, I have temporarily stopped chunkserver labelled `C` and immediately got 2200+ missing chunks (during chunkserver `C` offline time) in files which Storage Class has been recently changed ""2ssd"" --> ""archive"".

Such problem could happen only if one replica has been removed from `C` before chunks were created in `L` i.e. goal has been reduced before chunks landed in their final destination.

Implication of such incorrect behaviour is quite serious as premature changing goal before moving data as per Storage Class can lead to data loss.  
One chunk on `C` is not as safe as one chunk in `L` -- that's why goal and Storage Class were designed to have at least two chunks on `C` and reduce goal to one when chunks are safely landed in `L`.

Please make sure that goal (number of replicas) is never reduced until data placement is compliant with Storage Class. Changing goal is safe only after data is placed where is belongs as per Storage Class definition.","It is definitely an archival bug. In order to relieve this problem and make chunks archival safer I've modified Storage Class to `-C C,C -K C,L -A L -d 1` to make sure that two replicas are available until chunks are archived. Nevertheless some of the new chunks created in that Storage Class became ""missing"" when I stopped just one chunkserver 'C'. That should have never happened...",50505775
1165,offload part of mfsmaster database to hdd,open,2019-03-23T20:19:51Z,2019-03-25T00:27:32Z,,NONE,"Is there a way to offload part of mfsmaster database to hdd to decrease memory usage?
The machine I would like to run mfsmaster of has only 4GB of ram.
the reported ram usage of mfsmaster is about 2GB so it should work, but it boosts to 3.5GB in case of trouble.

I am conscious that such operation would decrease performance but still less than constant memory swapping","Hello @eleaner,
Unfortunately there is no option to move part of Master's in-memory database to HDD. It would cause a *huge* slow down on metadata operations (even if HDD would be SSD/NVMe disk).

Best,
Piotr",50505775
1166,file repair/replace [feature],open,2019-03-19T22:40:15Z,2019-03-22T01:14:23Z,,NONE,"Imagine a situation that for whatever reason a chunk is missing. Either cannot be read or does not exist) and there is no other copy.
Imagine the chunk belongs to one file but there are multiple snapshots including this file.
Imagine that you have the same file in a different location than moose.

the repair tool would allow overwriting the contents of the busted file with the fresh copy, technically reinstating missing chunk. In the process, it would also heal all the snapshots.

I actually managed to pull something like this manually by uploading another file into moose
identifying missing chunks and replacing it with the new ones.
It also required fix using mfschunktool to align chunk number in the header of the chunk file (apparently available in version: 3.0.103-1)",,50505775
1167,missing chunk correction [question],open,2019-03-18T10:45:22Z,2019-03-22T01:13:36Z,,NONE,"Hi,

Today morning I found that moose identified one missing chunk.
One of my files is busted and so are it's 18 snapshots

How do I fix it?

I do have this file in another filesystem.

I believe that if I delete the file and replace with a correct copy my snapshots will still report the error right?","@marcomilano 
I am aware of what you say
I am just testing nasty scenarios.

By ""gone"" do you mean that I simply should delete it from all the snapshots and forget?

The file is not gone. I have another copy, just not in moose.
If I subsequently copy it to all the snapshots they won't be snapshots anymore, right?

Since moose does not compare different chunks it won't realise it is the same file.

I was able to fix the issue by
- copy the file to moose
- identify chunk(s)
- overwrite missing chunk with the new one
- running fix on the new chunk to align chunk numbers

so there is a way to fix it, but it is very manual and for more than one file (or in fact chunk) you need a bottle of wine and a lot of time on your hands to do it.
In retrospect, wine is probably better after you fixed it.

",50505775
1168,single machine 6 HDD - chunkservers [question],open,2019-03-17T16:11:53Z,2019-03-27T13:48:36Z,,NONE,"Hi

I have an elderly AMD Turion II Neo N54L Dual-Core with 6HDD.
I have set it up with one chunkserver per each HDD but as you can imagine my load is often exceeding 16.
Would it be safe/correct to use one chunk server for all HDDs?
Technically making it moosefs with single chunkserver","Thank you @nickb937 
I used to run ZFS but it is expensive to grow especially with 6 hdd.
resilience wise I was running raid 10 anyway so it's neither loss nor gain space wise to MooseFS replication.
I have two machines at home (although the migration will take time) and longer term I think of setting two chunkservers to allow one machine failure.
as a benefit, I should also have a read boosts when accessing the data",50505775
1169,metadata operations without mounting [feature],open,2019-03-16T21:44:36Z,2019-03-22T01:14:10Z,,NONE,"Hi,

I was trying to design a docker swarm service that would do periodic snapshots for me.
Swarm should keep it fairly HA.

unfortunately, mfsmakesnapshot (like all mfs tools) require fuse mounted volume and that's something I cannot do in the swarm.

But technically all I need is to connect to master and request metadata operation.
It should be perfectly doable without fuse mount.",,50505775
1170,there is a question of the type of inode,open,2019-03-15T05:40:07Z,2019-03-15T08:10:36Z,,NONE,"In libfuse,the type of file's inode is unsigned long. However ,in mfsmaster is uint32_t. Why did not define inode with unsigned long in mfsmaster ?",,50505775
1171,deduplication - bandwidth savings [feature],open,2019-03-07T00:21:08Z,2019-04-01T23:05:13Z,,NONE,"Hi,

I found an old discussion about implementing deduplication.
I understand that this feature is not a priority and the disk space is cheap these days.

but what about bandwidth?

Say I have a few TB in my cluster with one of the replicas offsite. The link is asynchronous and in technical terms just crap.
I also maintain a local copy of data on my workstation for various reasons.
This data is synchronised with the cluster.

Now if I happen to move a big folder with few TB of data - the cluster will see the deletion of the old folder and creation of a new one.
It will happily move the old chunks to trash and assuming I have enough space will start creating another set of identical chunks.
It will also start transferring them to my office copy through my crappy link.

Deduplication would let moose recognised that the chunks already exist and do not have to be created let alone replicated again.

","you might misunderstand what I wrote
I have another copy of my data outside of moose, the copy is synced to moose.
I am talking about moving the folder outside of moose and then syncing",50505775
1172,bandwidth/latency to master (lan/wan) [question],open,2019-03-01T19:35:21Z,2019-04-02T22:57:50Z,,NONE,"I am considering home setup of GPL version.
It will be a bunch of chunk servers at home plus extra storage server in an offsite datacentre.

What are the pros and cons setting up master in my home LAN vs setting it up on the remote machine?
Majority of my accesses to the data are from LAN, so I assume the first setup is preferred but could someone explain the amount of data that actually is transferred to and from the master and how important here will be bandwidth/latency?",It works only problem is if you lose connection master will try to rebalance to meet goals (if there other chunkservers available),50505775
1173,[feature] MATOCS_MASTER_ACK for metaloggers,open,2019-03-01T11:04:53Z,2019-03-01T18:45:05Z,,NONE,"I noticed that in case something goes wrong with the master (e.g. restarted cleanly with no metadata), the chunkservers refuse to connect to it to protect the stored data.

MATOCS_MASTER_ACK - wrong meta data id (file chunkserverid.mfs:5C72EC6C20591380 ; received from master:5C75DD2A8444D444)

There is no such protection for meta servers and they are happy to accept connection from the new master.
Of course, they keep a few backups of metadata, but the more recent changes stored in changelogs are gone soon.
",,50505775
1174,"storage classes, optional labels [feature]",open,2019-03-01T10:57:52Z,2019-03-02T19:12:21Z,,NONE,"The idea is to bring to the pool machines that are not necessary 100% available.

I would like to be able to define storage class A,B,{C}
where {} indicates that C is optional and two copies on A and B are good enough for me.

When C is online it would get replicated with the overall pool.
But when C is offline, moose would accept that and do not create another replica of chunks.

","hi @oxide94 

I thought it through and I think that the definition of optional chunkservers/lables is more secure/flexible

e.g. o in the example above A,B,{C}

if C goes down - all is fine, no further replication required
but if either A or B goes down then there plication should start regardless of C being online.

this is not a goal of 3 with minimum 2.
That's a goal of 3 with minimum 2, but only with one specific label optional  

edit:
the scenario you described could be defined as 2*,{*}
which mean any two are mandatory + one is optional.
any three chunks would not do over goal but one chunk would trigger replication to minimum two.

",50505775
1175,CGI: Missing files :: show_less,open,2019-02-27T06:40:01Z,2019-03-08T17:10:09Z,,CONTRIBUTOR,"After clicking ""show_all"" in _Missing Files_ there is no going back because there is no ""show_less"" link.

The only way to reset to default view is to remove `&MFlimit=` from URL.

Please consider adding ""show_less"" link for usability.
",,50505775
1176,V4: is it real?,open,2019-02-23T09:28:09Z,2019-07-01T08:57:21Z,,NONE,"So, where is the new fully-opensource most awaited v4 ?

Is it real or just a joke to create hype and rumors about MooseFS?","@marcomilano any news on this one?

Will there be an opensource release of V4?",50505775
1177,labels for partitions [feature],open,2019-02-22T00:19:31Z,2019-03-01T10:57:01Z,,CONTRIBUTOR,"It would be great to be able to assign labels to partitions in `mfshdd.cfg`.

For example, on a server with several HDDs (mix of SSDs and Rotational ones), a chunkserver labelled `S+R` could have the following `mfshdd.cfg`:

~~~~
/var/lib/mfs/SSD01/ext4   [S]
/var/lib/mfs/HDD01/ext4   [R]
~~~~

Currently to emulate this it is necessary to have two chunkservers with different sets of disks/labels.
","@onlyjob 
a workaround would be to set up one chunkserver for one partition and label it this way.
But you probably know that",50505775
1178,CGI: equalize charts on Y scale [feature],open,2019-02-14T08:59:23Z,2020-03-13T00:51:48Z,,CONTRIBUTOR,"Server Charts are not equalised on Y-axis and that makes it difficult to compare charts.

If charts were equalised it would be easier to find slowest (or most active) server by looking on charts.

It would be nice to be able to pin all graphs to certain scale -- user-defined or auto-detected maximum.
","The flaw in the current charts design is obvious on ""cpu usage"" graph where none of the graphs have 100% at the highest point. Because of different scale of each graph, a graph of a server with 1.5% CPU usage can look higher than a graph of a server with 15% CPU usage.

Although individual graphs are somewhat useful, unaligned graphs are practically worthless for comparison...",50505775
1179,CGI: show Labels in Server Charts [feature],open,2019-02-14T08:56:15Z,2019-02-22T03:13:57Z,,CONTRIBUTOR,It could be useful to have chunkserver's Labels on Server Charts.,,50505775
1180,CGI: adding time stamp to a page? [feature],open,2019-02-14T08:37:02Z,2019-03-10T06:56:28Z,,CONTRIBUTOR,"Could it be useful to add a timestamp to CGI pages?

Similar to this: https://github.com/lizardfs/lizardfs/pull/687
","Perfect, thanks. Though IMHO it would be better to use ISO date notation: `YYYY-MM-DD HH:MM:SS TZ`.",50505775
1181,Re-formatted comments in config files.,open,2019-02-12T13:42:33Z,2019-03-14T17:15:59Z,,CONTRIBUTOR,,"Let's merge this please, shall we? It is trivial. Any blockers?",50505775
1182,"[question] ""Heavy load server detected""",open,2019-02-12T10:48:15Z,2019-02-22T03:15:11Z,,NONE,"<!--
can  you  help me to make the error ？：
Feb 12 14:11:59 lf2-aidevops-spark-71-70-1 mfsmaster[39826]: Heavy load server detected (10.71.67.16:9422); load: 217 ; threshold: 150 ; loadavg (without this server): 25.00 ; ratio_threshold: 3.00
Feb 12 14:12:59 lf2-aidevops-spark-71-70-1 mfsmaster[39826]: Heavy load server detected (10.71.67.16:9422); load: 231 ; threshold: 150 ; loadavg (without this server): 22.00 ; ratio_threshold: 3.00

how to solve ?

Mfs version is 3.0.86","load File fail by mfs，MFS log：
Feb 12 13:29:00 lf2-aidevops-spark-71-70-1 mfsmaster[39826]: Heavy load server detected (10.71.67.16:9422); load: 170 ; threshold: 150 ; loadavg (without this server): 27.00 ; ratio_threshold: 3.00
Feb 12 14:11:56 lf2-aidevops-spark-71-70-1 mfsmaster[39826]: Heavy load server detected (10.71.67.16:9422); load: 188 ; threshold: 150 ; loadavg (without this server): 25.00 ; ratio_threshold: 3.00
Feb 12 14:11:59 lf2-aidevops-spark-71-70-1 mfsmaster[39826]: Heavy load server detected (10.71.67.16:9422); load: 217 ; threshold: 150 ; loadavg (without this server): 25.00 ; ratio_threshold: 3.00
Feb 12 14:12:59 lf2-aidevops-spark-71-70-1 mfsmaster[39826]: Heavy load server detected (10.71.67.16:9422); load: 231 ; threshold: 150 ; loadavg (without this server): 22.00 ; ratio_threshold: 3.00

how to  solve ？
",50505775
1183,[Debian packaging] deprecated ignoregid parameter,open,2019-02-11T11:35:50Z,2019-02-19T09:42:52Z,,MEMBER,"Hi Dmitry @onlyjob,

Thank you for all your contributions done so far and especially thank you for contributing your precious time in making official Debian packaging for MooseFS! We appreciate it a lot! :)

I have found one thing which could be improved: [this patch](https://salsa.debian.org/debian/moosefs/blob/master/debian/patches/conf-default-ignoregid.patch) is not applicable for MooseFS 2.x+.

This issue was known in MooseFS 1.6.x (and LizardFS) – in this old MooseFS version there was no proper implementation of auxiliary groups.

In case that user had a permission to do something but only because he/she belonged to a particular auxiliary group then kernel accepted this action, but MooseFS didn't. Without `ignoregid` MooseFS could reject such action even though the action was perfectly correct. Option `ignoregid` informed MooseFS that it can just test only `uid` and `other` rights and ignore `gid` checking because we used 'default permissions' on the client/fuse side, so kernel did all permission checking for us.

Since MooseFS 2.x we have implemented auxiliary groups and we no longer use FUSE option 'default permissions', so all security tests are done on the Master Server level. In such case defining `ignoregid` without 'default permissions' can lead to a security breach.

We are thinking on deprecating this parameter and removing it soon.

@onlyjob, could you please take it into a consideration in official Debian packaging?

Thank you in advance!

Best,
Piotr","Thanks. Write access to repo is not a problem, I just won't commit, merge or touch the repository at all. LizardFS team trusts me with similar arrangements and I only work on wiki and bug reports -- after all, more than _quarter_ of all their issues are mine. ;)
",50505775
1184,libvirtd support (netfs),open,2019-02-10T04:48:16Z,2020-01-24T02:23:12Z,,CONTRIBUTOR,"I can not add MooseFS as a _netfs_ type storage pool in _libvirtd_ (using `virsh` or `virt-manager`) hence it is not possible to use VM migration. (local _dir_ type pool works perfectly but that's not the storage type that allows migration of VMs between hosts).
NFS and GlusterFS are already supported so it should be trivial to introduce MooseFS support.

See also:
- https://libvirt.org/storage.html#StorageBackendNetFS
",Probably not in the near future... Definitely not soon...,50505775
1185,"Recursive accounting (nested files, directories, bytes) [feature]",open,2019-02-10T03:58:38Z,2019-02-22T03:31:29Z,,CONTRIBUTOR,"`linux-doc/Documentation/filesystems/ceph.txt` describes a nice feature of _CephFS_:

> Ceph also provides some recursive accounting on directories for nested
files and bytes.  That is, a 'getfattr -d foo' on any directory in the
system will reveal the total number of nested regular files and
subdirectories, and a summation of all nested file sizes.  This makes
the identification of large disk space consumers relatively quick, as
no 'du' or similar recursive scan of the file system is required.

This feature very conveniently allows to see directory size in file managers like _mc_ instantly and without relying on `du` utility.

This is like `mfsdirinfo` exposed through file system.

Please consider implementing this feature. 

Also it would close the gap with rival competitor a little bit. :)
","You could also look for inspiration to how this is done in ZFS:

properties:
```
     userused@user         The amount of space consumed by the specified user in this dataset.  Space is charged to the owner of each file, as displayed by ls -l.  The amount of space charged is dis‐
                           played by du and ls -s.  See the zfs userspace subcommand for more information.

                           Unprivileged users can access only their own space usage.  The root user, or a user who has been granted the userused privilege with zfs allow, can access everyone's usage.

                           The userused@... properties are not displayed by zfs get all.  The user's name must be appended after the @ symbol, using one of the following forms:

                           · POSIX name (for example, joe)

                           · POSIX numeric ID (for example, 789)

                           · SID name (for example, joe.smith@mydomain)

                           · SID numeric ID (for example, S-1-123-456-789)

                           Files created on Linux always have POSIX owners.

     userobjused@user      The userobjused property is similar to userused but instead it counts the number of objects consumed by a user. This property counts all objects allocated on behalf of the
                           user, it may differ from the results of system tools such as df -i.

                           When the property xattr=on is set on a file system additional objects will be created per-file to store extended attributes. These additional objects are reflected in the
                           userobjused value and are counted against the user's userobjquota.  When a file system is configured to use xattr=sa no additional internal objects are normally required.
```

and commands:
```
     zfs userspace [-Hinp] [-o field[,field]...] [-s field]... [-S field]... [-t type[,type]...] filesystem|snapshot
       Displays space consumed by, and quotas on, each user in the specified filesystem or snapshot.  This corresponds to the
       userused@user, userobjused@user, userquota@user, and userobjquota@user properties.

       -H  Do not print headers, use tab-delimited output.

       -S field
           Sort by this field in reverse order.  See -s.

       -i  Translate SID to POSIX ID.  The POSIX ID may be ephemeral if no mapping exists.  Normal POSIX interfaces (for example, stat(2),
           ls -l) perform this translation, so the -i option allows the output from zfs userspace to be compared directly with those utili‐
           ties.  However, -i may lead to confusion if some files were created by an SMB user before a SMB-to-POSIX name mapping was estab‐
           lished.  In such a case, some files will be owned by the SMB entity and some by the POSIX entity.  However, the -i option will
           report that the POSIX entity has the total usage and quota for both.

       -n  Print numeric ID instead of user/group name.

       -o field[,field]...
           Display only the specified fields from the following set: type, name, used, quota.  The default is to display all fields.

       -p  Use exact (parsable) numeric output.

       -s field
           Sort output by this field.  The -s and -S flags may be specified multiple times to sort first by one field, then by another.
           The default is -s type -s name.

       -t type[,type]...
           Print only the specified types from the following set: all, posixuser, smbuser, posixgroup, smbgroup.  The default is -t
           posixuser,smbuser.  The default can be changed to include group types.

     zfs groupspace [-Hinp] [-o field[,field]...] [-s field]... [-S field]... [-t type[,type]...] filesystem|snapshot
       Displays space consumed by, and quotas on, each group in the specified filesystem or snapshot.  This subcommand is identical to zfs
       userspace, except that the default types to display are -t posixgroup,smbgroup.
```
",50505775
1186,Please update client for FUSE-3,open,2019-02-07T07:58:11Z,2019-02-12T15:51:21Z,,CONTRIBUTOR,"According to https://github.com/libfuse/libfuse ""Users are encouraged to transition to the actively developed libfuse 3.x.""

Please consider updating client for _libfuse3_. Thanks.",,50505775
1187,mfshdd.cfg is needlessly sensifive to leading whitespaces,open,2019-02-07T01:28:51Z,2019-12-04T10:30:05Z,,CONTRIBUTOR,"It would be nice if `mfshdd.cfg` ignore leading spaces in front of mount points.

Thanks.
","3.0.109 did not improve on this, right?",50505775
1188,master: systemd notification [feature],open,2019-02-06T03:06:20Z,2019-02-11T16:52:33Z,,CONTRIBUTOR,"Master needs a long time to initialise so it would be great if it could notify Systemd for readiness for the following to work:

```
[Service]
Type=notify
```

See [sd_notify(3)](http://www.freedesktop.org/software/systemd/man/sd_notify.html) and [systemd.service/Type](http://www.freedesktop.org/software/systemd/man/systemd.service.html#Type=) for more information.

---

This issue is similar to https://github.com/lizardfs/lizardfs/issues/340
",,50505775
1189,CGI - datain shows average write bw per disk rather than aggregate?,open,2019-01-30T15:21:27Z,2019-01-30T15:28:44Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
BUG report

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).

3.0.104-wip built from source

#### Operating system (distribution) and kernel version.

Debian Stretch

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.

CS disk to disk chunk replication can easily saturate gigabit ethernet. XFS on the chunkserver in this particular case.

#### How much data is tracked by moosefs master (order of magnitude)?

Irrelevant

## Describe the problem you observed.

I recognize that depending on the OS and MooseFS config and chunk sizes replication I/O is subject to sparsification and then potentially merging, but even with this accounted for the CGI `datain` plot cannot possibly reflect aggregate writes to the chunkserver in this case. Considering the chunkserver in question added 8 disks, an average per disk is likely being plotted.

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

After new capacity is added to the cluster and tested for reliability, I switch replication limits to accelerate space rebalancing. IO monitoring on the chunkservers confirms that works as intended, but CGI server charts seem to plot single disk average rather than aggregate bandwidth hitting the chunkserver. This is a cosmetic issue, but nonetheless a bug, and could give new/less experienced MooseFS users a wrong idea of what is happening with their systems.

Output from dstat showing resource utilization during rebalancing onto a newly added chunkserver:
<img width=""709"" alt=""moosefs-177-dstat"" src=""https://user-images.githubusercontent.com/12971584/51990731-aafe2000-2477-11e9-8f7f-c1a56c14fe91.png"">

Corresponding plot from CGI interface:
<img width=""1671"" alt=""moosefs-177-cgi"" src=""https://user-images.githubusercontent.com/12971584/51990657-843fe980-2477-11e9-8e0b-2b8c23abe656.png"">
",,50505775
1190,client does not push file closed notifications to master fast enough,open,2019-01-19T18:42:48Z,2019-01-20T15:59:32Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes
## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
BUG report

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).

Versions up to 3.0.104. Also present in 4+.

## Describe the problem you observed.

This issue is related to #174, master is not informed about closed file descriptors fast enough. Pushing this info out lazily may be fine for a number of use cases, but it cannot keep up with rapidly changing namespace.

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

Best way to reproduce is to start high throughout file move operations, Both CLI and CGI will report files that were moved away from the filesystem as still open by the client.",Possibly related: #104 ,50505775
1191,operational metrics at scale: CGI > Resources and locks,open,2019-01-18T03:20:44Z,2019-01-18T14:10:22Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
BUG report

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).
3.0.104-wip

#### Operating system (distribution) and kernel version.
Debian Stretch, 4.9 kernel series

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.
Storage class pointing at 12 chunkservers. Server class machines, ECC RAM, Gigabit ethernet. 

## Describe the problem you observed.

In preliminary tests leading to #173, a small subset of clients would process workloads utilizing short-lived files and relying on file locking. Tests themselves have kept mfsmaster quite busy, making all CGI responses slow in general and returning errors every now and then. CGI was not helpful in the attempts to troubleshoot performance issues.

Secondary issue and contributor to the overall performance impact - vast majority of reported files were unlinked while open (reported as SUSTAINED). At any moment up to tens of thousands of chunks would be scheduled for removal. Lazy chunk removal #10, until underlying filesystem goes above a % full threshold could help.

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

With just ~8K open files per client, the`Show open files for:` selector in Resources tab was of limited value when it worked, but the real UX fail was trying to navigate to the inode in the `Show acquired locks for: ` selector. 

","
<img width=""1103"" alt=""moosefs issue 174 - cgi"" src=""https://user-images.githubusercontent.com/12971584/51391721-d5a7ba80-1b00-11e9-900d-3bcd13eaea64.png"">
",50505775
1192,"error reporting issues at scale: mfsmaster, CGI",open,2019-01-18T02:29:35Z,2019-01-19T18:45:57Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
BUG report

## System information

mfsmaster, all platforms and versions up to latest 3.0.104-wip.

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.

Irrelevant.

#### How much data is tracked by moosefs master (order of magnitude)?
Irrelevant, but current interface becomes severely limited as the namespace grows.

## Describe the problem you observed.

When enough chunkservers go missing chunk availability may be affected for portions of the namespace. This is fully expected. What could be improved, is the usability of the CGI and CLI for deployments with large number of files in the namespace.

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.

Take down enough chunkservers to make chunks for 10-50M files unavailable for a couple of hours to a couple of days. In real world this would likely be the least replicated and least important subset of files within the namespace, if hosted on server-class machines, or would represent subset of chunks on temporarily unreachable, but otherwise healthy workstations.

- While CGI shows the first 100 missing files`Missing files (gathered by previous file-loop) - 100/15238253 entries - show more - show all`, both `show more` and `show all` are rather useless.
- The only usable option to explore affected files is parse the output from `mfscli -SMF`.
- Observe continuous log trashing coming from mfsmaster. This in itself could cause other issues.

#### Include any warning/errors/backtraces from the system logs

For the duration of the outage, logs are flooded with repetitive information about missing chunks:
```
[...]
mfsmaster[9880]: chunk 00000000005E25E5_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E48E6_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E471A_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E7E65_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005EAD98_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F2116_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E8527_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F36DE_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E8A68_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E9D3F_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F18E9_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F90B6_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005ECEE8_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F1F16_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F63C0_00000001: there are no copies
mfsmaster[9880]: chunk 00000000004E25D0_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F22A5_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E7E0A_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E8D8D_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F1EBE_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E2451_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F83D1_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E36F0_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E5207_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E535E_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E833C_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005EAF07_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005E2ED7_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F494F_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F814D_00000001: there are no copies
mfsmaster[9880]: chunk 00000000005F3BA9_00000001: there are no copies
[...]
```

I'd suggest the following improvements:
- Add an option to make error reporting in master's check loop terse, maybe to the point of  emitting a one-line summary with a number of missing chunks and unreadable files at certain intervals.
- In CGI leave numerical summary for missing files on the info tab, where it lives today, but move details, possibly with a query box allowing to search for path patterns to a separate tab.",Historical plot with number of missing chunks could also be added under `Master Charts` tab.,50505775
1193,[feature] trigger restart of filesystem check loop,open,2019-01-14T19:59:47Z,2019-01-14T21:57:49Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
FEATURE request

## What and why?

Filesystem (namespace) check loop is cyclical, and may take many hours to complete (12hrs+ for a single loop is not an exaggeration). The default behavior seems OK for typical deployments, but presented stats and damage report are shown for up to two loops since problems are cleared, not making cluster admins' job any easier.

This proposal is for the master to respond to a signal which would abort current fs check loop and start it from the beginning. Example use: a number of chunkservers are brought back after a prolonged network partition or outage, or to schedule an on-demand scan.","This is separate from, but would augment the functionality proposed in #165.",50505775
1194,[documentation] spell out FUSE-related limitations and issues in FAQ / documentation,open,2019-01-11T13:42:45Z,2019-01-11T13:42:45Z,,COLLABORATOR,,,50505775
1195,[feature] file/directory/pattern pinning,open,2019-01-10T20:08:25Z,2019-01-10T21:54:45Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
FEATURE request

## What and why?

Storage classes offer tremendous flexibility, but there are some corner cases when admin's work could be made easier by allowing them to administratively pin a file/directory/pattern to a particular set of chunkserver labels, while letting the end user or operator to recursively change classes for remaining data within the limits set in `mfsexports.cfg`. Basically having an admin set storage class for a directory/file/pattern that overrides whatever user/operator would normally be allowed to accomplish.

Use cases:
- Hotspot mitigation. Directories full of small and frequently changing files where access patterns do create hot spots, slowing down response times for everyone and everything using that chunkserver.
- Wear-out mitigation (premature failure of flash-based chunkservers) and network load management. Very large, mostly static datasets, where replication created by erroneous class changes would result in unnecessary wear and tear on the rest of the infrastructure. Think namespaces where (create==keep==archive) is intermixed with (create=>keep=>archive) and (create=>(keep==archive)).

Whether your cluster runs on a bunch of RPis or on racks worth of of server grade hardware, benefits would be immediate. 

While this feature request is separate from #164, it would greatly benefit from more flexible labeling scheme.","Yes, I know, and that's why ""pattern"" made it to issue's title. Folks who evaluate storage solution based on available open-source codebase may have no idea they need to ask for a demo of 4.x, and if they don't need HA may simply walk away because hotspots and/or wear out will manifest themselves on small commodity-based deployments much faster than on true scale-out installations. If public release of 4.x is months or longer away, it may be a good idea to backport that feature.",50505775
1196,mfsbdev feature parity and potential bug,open,2019-01-09T22:45:47Z,2019-01-09T22:46:08Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
BUG report

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).

3.0.104-wip

#### Operating system (distribution) and kernel version.

Debian 9

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.

Servers with ECC RAM, irrelevant in this context.


## Describe the problem you observed.

Two problems:
1. `mfsbdev` does not support configuration via file (`-f`)
1. `mfsbdev` only accepts password, not md5pass
2. `mfsbdev start` fails when fed a correct password to operate on a corresponding `mfsexports.cfg` entry, `mfsmount` against that share works just fine

#### Can you reproduce it? If so, describe how. If not, describe troubleshooting steps you took before opening the issue.


```shell
mfsmaster: /etc/mfs#  grep block mfsexports.cfg
# blockdev
*		/block	rw,mapall=0:0,mintrashtime=4H,maxtrashtime=1W,password='REDACTED'
```

```shell
client: /etc/mfs# install -m 0640 -o root -g root /dev/null /etc/mfs/blkpass
client: /etc/mfs# echo REDACTED > /etc/mfs/blkpass
client: /etc/mfs# mfsbdev start -H mfsmaster -P 9421 -x /etc/mfs/blkpass; echo EC=$?
mfsmaster register error: Incorrect password
can't connect to master
EC=1
client: /etc/mfs# mfsmount -o mfspassword='REDACTED' -H mfsmaster -o mfssubfolder=/block /mnt/block/; echo EC=$?
mfsmaster accepted connection with parameters: read-write,restricted_ip,map_all ; root mapped to root:root ; users mapped to root:root ; settrashtime limited to (4h:7d)
EC=0
client: /etc/mfs# mount | grep /mnt/block
mfsmaster:9421 on /mnt/block type fuse.mfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
```

When the share is configured w/o password (never a good idea outside of testing), the error message changes to:
> mfsmaster register error: Password is needed
can't connect to master


What did I miss?",,50505775
1197,Block device setup,open,2019-01-07T05:40:00Z,2019-02-11T17:07:22Z,,NONE,"Hello,

Sorry first I think here is not correct place to put my question.  However, the support (I thru submit form) still did not any reply for long time.

I got a problem when trying setup Block device.  The daemon is able to start 
```
#> mfsbdev start -H moosefsmaster
mfsmaster accepted connection with parameters: read-write,restricted_ip,admin ; root mapped to root:root ; settrashtime limited to (0s:0s)
```

However, it got error as below.
```
# > mfsbdev map -f ./mybdev1.bin -s 10Gi -n mytestvm
```
can't find free NBD device

Could you please advise or assistant pass this question to your support team?

Also, where I can get correct channel to contact your support team?

Thanks!","I had to install a newer kernel from elrepo.org (http://elrepo.org/tiki/tiki-index.php) to make the nbd module work on centos.
",50505775
1198,[feature] trust model for .chunkdb,open,2019-01-04T15:57:30Z,2019-01-07T06:05:50Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
FEATURE request


## What and why?

`.chunkdb` is written and used during chunkserver startup up to 3.0.104-wip. While this seems to be a prudent way to speed up restarts of chunkserver with lots of chunks, current approach is too optimistic and could lead to data safety issues in certain scenarios, see discussion in #146.

Possible improvements:

- The trust placed in this file should rapidly decrease as a function of time since last **full chunk scan** and number of chunks on said disk. Please allow the administrator to decide what level of paranoia works for them, up to not using `.chunkdb` at all.

- Assume the data fed from .chunkdb is tainted, and have master hold delay removal of any extra replicas until full chunkserver scan completes.

- Being able to schedule a periodic chunkserver-side (namespace-independent) scrub of chunks across all disks in the cluster would go a long way to gain even more trust from current and future users. See discussions on #106.

",I like that idea. I'll try to find time to implement it in close future.,50505775
1199,[feature] alternatives for chunkserver labeling,open,2018-12-19T21:17:23Z,2019-02-16T15:37:06Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
FEATURE request

## What and why?

Chunkserver labeling and cluster topology while well tested, are rather simplistic. Beyond trivially simple configurations manually mapping network and datacenter topology into single letter codes doesn't add any value for the admins of a MooseFS cluster. Label combinations in the CGI look like giberish to any human operator, and over time that could lead to data loss when cluster members are inadvertently plugged into a failure domain they were supposed to provide replicas for.

Being able to express individual label components as continuous human-readable strings (no spaces) instead of single characters, would make it possible to tap into existing information sources and use them directly in the configuration. It would also open up the possibility for safe migration of data from solutions where data placement policies allow for this level of flexibility.",Related to commit 7264d6428d818661682098b51040ce675f3c0eb9,50505775
1200,[feature] use a different shade or color to mark maintenance mode periods in master and server tabs,open,2018-12-14T21:50:55Z,2018-12-14T21:51:04Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
FEATURE request

## What and why?

It would be helpful to annotate the data presented via graphs in CGI with markers showing any periods when any of the cluster members was in maintenance mode. Different shade/color or even vertical lines. If nothing else, this would make it much easier to spot and explain performance or behavior issues.",,50505775
1201,[feature] mfsmakereplica (backend side copy/clone),open,2018-12-07T18:14:35Z,2019-01-08T15:35:40Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?

Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
FEATURE request

## Describe the problem you observed.

`mfsmakesnapshot` is time tested and works well for many use cases. Problems start when snapshotted data sticks around until either the end user or storage admin needs to modify storage class definitions, affecting some of the data shared between snapshots and the ""trunk"" of the filesystem. See #158 for one example.

## Describe the suggested fix

To the end user `mfsmakereplica` command would seem almos like an alias for `mfsmakesnapshot`, with a possible addition of specifying a target storage class. Behind the scenes:
- a temporary snapshot of the source would be taken
- chunk replication would be triggered from the chunks used in a temporary snapshot to new ones, matching target storage class
- upon success the command would return 0 and temporary snapshot would be removed
- upon failure, non-zero exit code would be returned

This approach would be much faster, easier and less error prone than going through above the steps on the client side.


",,50505775
1202,"no safeguards against conflicting goals/storage class settings during chunk removal, potential for data loss up to 3.0.104-wip",open,2018-12-05T16:40:46Z,2019-01-30T03:01:52Z,,COLLABORATOR,"## Have you read through available documentation and open Github issues?
Yes

## Is this a BUG report, FEATURE request, or a QUESTION? Who is the indended audience?
BUG report

## System information

#### Your moosefs version and its origin (moosefs.com, packaged by distro, built from source, ...).
3.0.104-wip, built from source


#### Operating system (distribution) and kernel version.

Debian stretch

#### Hardware / network configuration, and underlying filesystems on master, chunkservers, and clients.

Server grade x86_64

#### How much data is tracked by moosefs master (order of magnitude)?

Only ~10M files


## Describe the problem you observed.

Background prep:
- defined a new storage class, named ""purge""
- made a new subfolder and assigned ""purge"" to it
- moved files from keep3
- recursively changed class on the folder from keep3 to purge
- Oddly the resources tab shows right amount of files and dirs, but not a single chunk is accounted for in purge.

48hrs+ passed since then.

```
#  mfsxchgsclass -r keep3 purge .prune
.prune:
 inodes with storage class changed:              0
 inodes with storage class not changed:         17
 inodes with permission denied:                  0

# du -sxc .prune
12045820	.prune
12045820	total

# mfsgetgoal -r .prune
.prune:
 files with storage class       'purge' :         16
 directories with storage class 'purge' :          1

# mfsgetsclass -r .prune
.prune:
 files with storage class       'purge' :         16
 directories with storage class 'purge' :          1

# mfsscadmin list -l
[...]
keep3 : 3 ; admin_only: NO ; create_mode: STRICT ; create_labels: [V] , [P] , [P] ; keep_labels: [P] , [P] , [P] ; arch_labels: [P] , [P] , [P] ; arch_delay: 1d
[...]
purge : 1 ; admin_only: NO ; create_mode: LOOSE ; create_labels: [P] ; keep_labels: [P] ; arch_labels: [P] ; arch_delay: 1d
```

Mandatory screenshot:
<img width=""1295"" alt=""gh146_class_change_issue"" src=""https://user-images.githubusercontent.com/12971584/49515488-fffd2b80-f864-11e8-8c1a-9cf0573d5514.png"">

Why are the chunk numbers 0 for both KEEP and ARCHIVE sets in class `purge`, when this is clearly not the case? In certain automation scenarios, this could lead to unexpected behavior at best.","To make it easier to troubleshoot similar problems and monitor progress during storage class 
 or even simple RL changes please add a parameter to `mfsfileinfo` and `mfsdirinfo` that reports the `%coverage` between replication level as set in object's storage policy and actual on-disk state for all chunk copies belonging to said files at the time of the inquiry. Alternatively `mfscoverage` could be a separate utility,  moving us towards on-demand scrub of namespace and objects #117.",50505775
1203,build mfsmount in msys2 fuse_lowlevel.h not found,open,2018-11-01T16:41:04Z,2019-02-22T00:44:49Z,,NONE,"I build moosefs especially client in windows msys2. But I did not found fuse_lowlevel.h in dokany. And the build was failed. So, is there an official tutorial or guide or release on windows?","I found fuse_lowlevel.h in libfuse, but it may not be availible for windows",50505775
1204,[feature] mfsfind - native utility to find files with moosefs-specific attributes,open,2018-10-29T13:34:44Z,2019-01-08T15:33:57Z,,NONE,"Hi,

it would be great to have native utility like `mfsfind`, which actually could be some kind of `find` fork.
`mfsfind` should allow to iterate through filesystem and report files with attributes which are specific for MFS.
Options examples:
`-cslabel LABEL_EXPR` - find all files where at least one chunk is on chunkserver  matching LABEL_EXPR
`-cslabelmin num LABEL_EXPR` - find files where where at least num of chunks are on chunkservers matching LABEL_EXPR
`-sclass SCLASS` - find files/directories with SCLASS assigned
`-csid CSID <IP:port>` - find files stored on CSID chunkserver
`-mfsattr ATTR` - find files with MFS attributes like archive, snapshot etc.
`-mfsstat STATUS` - find files which contain at least one chunk in specified state (MISSING, ENDANGERED,UNDERGOAL,OVERGOAL)
`-mfsdirsize` - can take any of arguments like chunk count, realsize, length, size (based on mfsdirsize command)

There are many many other possible mfsfind features to add. Those above are just first which comes to mind. 

Operations should be performed on metadata where possible and mfsmeta optimized.

Of course all above features should extend existing `find` features and be possible to combine generic find and mfsfind features in one line.

Best regards.

",,50505775
1205,[feature] Ability to expose only parts of moosefs filesytem,open,2018-10-21T22:35:34Z,2019-01-08T15:35:54Z,,NONE,"I would like to see ability to expose only specific parts of moosefs tree master side. I know you can currently use subfolder but it's real pain to use and eats a lot of resources client side (separate mfsmount for each exposed mount point).
The way I see it, we could declare something like this in mfsexports.cfg:
```
<IP_of_client>  / rw,alldirs,maproot=0:0,dirs:""/home,/projects/a,projects/b/space_shared_with_a""
```
Currently, above example would require running 3 instances of mfsmount, 3 separate entries in mfsexports.cfg  and manual creation of all intermediate paths on the client side (and adding 3 fstab entries).
This feature should also improve performance. No copying between mfsmount instances, better caching etc.

I know this might be hard to implement and have impact on other options (`alldirs` comes to mind)...",,50505775
1206,[feature] mfsdiff - to complement mfsmakesnapshot,open,2018-10-21T17:51:58Z,2019-01-30T03:16:10Z,,COLLABORATOR,"Snapshots are an essential feature of MooseFS, but managing their lifecycle after they were created is a bit rough, and could be improved quite a bit.

Admins and end-users could greatly benefit from a *native* tool that shows what has changed between one snapshot and another, or a snapshot and current filesystem. Master server could answer that question in a fraction of time it takes to run a diff against actual filesystem contents. 

Rather than reinventing the wheel reporting format could be borrowed from ZFS, making existing tooling immediately usable with moosefs. Excerpt from the man page shown below for reference.

```
       zfs diff [-FHt] snapshot snapshot|filesystem

           Display  the  difference  between  a snapshot of a given filesystem and another snapshot of that filesystem from a later time or the current contents of the filesystem.  The first column is a
           character indicating the type of change, the other columns indicate pathname, new pathname (in case of rename), change in link count, and optionally file type and/or change time.

           The types of change are:
             -       The path has been removed
             +       The path has been created
             M       The path has been modified
             R       The path has been renamed

           -F

               Display an indication of the type of file, in a manner similar to the -F option of ls(1).
                 B       Block device
                 C       Character device
                 /       Directory
                 >       Door
                 |       Named pipe
                 @       Symbolic link
                 P       Event port
                 =       Socket
                 F       Regular file

           -H

               Give more parsable tab-separated output, without header lines and without arrows.

           -t

               Display the path's inode change time as the first column of output.
```","Since metadata information stored in master already contains checksums, they could be assembled into a merkle tree to enable rapid (file or chunk-level) diffs between any sections of the namespace. Taken waaay into the future, this approach could allow for lightweight mirroring of the entire moosefs instances between different locations, similar to `zfs send | zfs receive`.",50505775
1207,[feature] more flexibility with storage class label specification,open,2018-10-19T17:00:06Z,2019-01-08T15:34:12Z,,COLLABORATOR,"While it is possible to create a set of labels to be used in create-only storage class, there is no way to tell the system that any preexisting data stored using unlabeled ""best-effort"" replication goals 1,2,... should never be rebalanced and replicated over to chunkservers bearing ""create-only"" labels. This introduces unnecessary data migrations, adding extra wear, and accelerate time to data loss.

Possible workarounds / improvements:
- ~enable a way to set/alter site-specific Keep and Archive policies for ""legacy"" classes 1,2,..~ 
> ""man mfsscadmin"": -f force the changes on a predefined storage class (see PREDEFINED STORAGE CLASSES section), use with caution
- enable a way to exclude sets of labels from Create, Keep and Archive storage class specification. I.e. `-C 2[A+B+C] -K 3![A+B+C]`
- enable a way to mark a label as ""create-only""
",,50505775
1208,[feature] events and configurable event handlers,open,2018-10-04T15:41:40Z,2019-01-08T15:35:00Z,,COLLABORATOR,"Opening for discussion:

In its simplest form the implementation of this feature would allow the cluster admin to define a 
`MFSEVENT_ID = /path/to/HANDLER $args` pairs in the configuration file for master, chunkserver, metalogger, and possibly even the client. By default, those would be undefined, and thus operationally equivalent to legacy codebase.

Example usage scenarios (moosefs component: event type => example action performed by the handler script)
- master: about to elect a new leader => flush metadata to disk and snapshot the underlying filesystem
- master: sees chunkserver disconnect/reconnect => check against planned operations schedule and fire off an alarm when unplanned, automatically provision more capacity
- master: sees chunkserver join => validate the labels provided by the chunkserver make sense, update topology
- master: sees client join => check with resource manager and reconfigure labels to add workload-specific data replicas closer to the client, if warranted
- master: sees client leave => check with resource manager and reconfigure labels to shrink the number of workload-specific data replicas near the client, if warranted
- master: sees disk errors or failure => send alert, mark node for planned maintenance

- metalogger/follower: desync => check against planned operations schedule and fire off an alarm when unplanned

- chunkserver: starts/stops => flush metadata to disk and snapshot the underlying filesystem

- client: encounters problems accessing CS or master => analytics for best user experience, save a snapshot of state information which will help with troubleshooting and reduce finger-pointing. `mfs-site-support` script could produce a tarball aggregating such snapshots.

@acid-maker, @oxide94 - did I miss anything?",,50505775
1209,"[feature] track and expose number of times a server lost contact with the master, or peers",open,2018-10-02T17:00:15Z,2019-01-08T15:34:44Z,,COLLABORATOR,"It would be useful to track and expose flapping services, i.e. number of times a chunkserver or a metalogger could not reach a master, but connectivity was restored before configured timeout.","I agree I have one of my chunkserver which loses the connection with the master every time the load reach ""30"" (on the dashboard) or I clean my trash manually which has a lot of data.

",50505775
1210,[feature] Is it possible to get which file is endangered?,open,2018-09-29T04:13:47Z,2019-01-08T15:37:06Z,,NONE,"![image](https://user-images.githubusercontent.com/5325686/46240838-22e00f00-c3e0-11e8-9b73-01291dc716c1.png)

some times, I will get some ""endangered files"", I also have some failed file operation. I think these they are related. Can I get which file are ""endangered"" to verify  my theory?",,50505775
1211,[feature] Chunk replication should use posix_fadvise,open,2018-09-12T22:34:28Z,2019-01-08T15:36:54Z,,NONE,"From what I see chunk replication doesn't currently use posix_fadvise. You could use mix of POSIX_FADV_SEQUENTIAL and POSIX_FADV_DONTNEED and maybe new mode for it in hdd_io_begin(). This will give nice speed boost (bigger read-ahead cache with POSIX_FADV_SEQUENTIAL) and lower memory pressure on chunk servers. The same goes for internal rebalancing.
",,50505775
1212,[feature] Binary packages for arm64,open,2018-08-30T12:28:55Z,2019-01-08T15:36:32Z,,NONE,I would really like to see (and if you want also test) binary packages for arm64 architecture in official repo.,,50505775
1213,Network partitioning results in an inconsistent state of the file system for the client,open,2018-07-14T04:56:28Z,2018-10-01T23:10:10Z,,NONE,"Version number: v3.0
I deployed a cluster of 3 chunk servers (S1, S2, and S3) , one Master server and one Client node. In a case of network partition that splits (S1 and S2) from (S3 and Client) while all nodes can reach the Master server, the Client will be able to list all files in the system including files stored on S1, S2, but when he tries to access files stored on S1 and S2, he will receive an error. Note that the client may receive this  error even though there is a copy of the same file stored on S3.

This behavior is confusing as the files are exists but I cannot access them.  ",,50505775
1214,multihomed systems,open,2018-06-14T06:48:19Z,2018-12-20T21:30:16Z,,NONE,"Hello,

I have question about setup MATOCL_LISTEN_HOST under mfsmaster.cfg for the clients connection operations.

Question is if I have 3 network interfaces at Master server.  But only two of them (2 different subnets) is allow clients to connect.   How MATOCL_LISTEN_HOST to config?

I tried split into 2 lines but it is wrong
Also tried add conman or space in between 2 IPs but it looks not work.

Additional for the chunkserver  parameter CSSERV_LISTEN_HOST under mfschunkserver.cfg I guess also require to confirm same as well?

Thanks! 
",,50505775
1215,Partial goal,open,2018-05-20T13:36:01Z,2018-12-19T21:24:09Z,,NONE,"Let's assume a goal of 4
Is possibile to set how many chuncks should be wrote before returning an ACK to the client?

In example, to improve performance, we can set a redundancy level of 2, in this case, after 2 succsefully wrote chunks, the ACK is returned to the client while the remaining 2 chunks are still being written in a sort of writeback

(Lizard has this and i think something similiar is also supported by ceph)
",Didn't know about that :) Thank you AGAIN @oxide94  ;) ,50505775
1216,Force FSYNC,open,2018-05-20T13:32:55Z,2019-01-30T20:35:20Z,,NONE,"Is possible to force MooseFS to issue fsync on every operation before returning an ACK to the client?

What if i want to be 100% sure that data is properly written to disk even if the client is not asking for fsync?","@acid-maker: to follow up on our conversation - one idea was to keep current fsync behavior as-is, but use an extended attribute to mark files or directory trees where FSYNC or DIRECT compliance is required, and honoring that flag all the way to the chunk writes.",50505775
1217,"Replace disk, same server",open,2018-05-20T13:30:58Z,2018-12-19T21:24:38Z,,NONE,"Is possibile to mark a disk for removal and then start migrating data to another disk on the same chunkserver?

It would be much better and faster to sync 2 disks locally without involving any network operation","@4Dolio: you can also read a bit more about Master forking and `overcommit_memory` in Best Practices [here](https://moosefs.com/support/#best-practices) - it is Best Practice no. 7 ""`overcommit_memory` on Master Servers (Linux only)"".

Best,
Peter / MooseFS Team",50505775
1218,Qemu driver,open,2018-05-20T13:29:05Z,2019-01-25T08:03:17Z,,NONE,"Any plans ti to add a qemu native driver to skip the whole FUSE stack?

Performance would be greately improved and this will bring MooseFS to the cloud world, qemu is supported by almost any open cloud orchestrator/hypervisors (proxmox, OpenNebula, openstack, ...)","Block device is a good feature, however it will be great together with qemu support so that it readily integrates with KVM cloud orchestration software available.",50505775
1219,Global umask export option,open,2018-05-20T09:09:23Z,2018-12-20T21:06:50Z,,NONE,"Hi,

[MooseFS 3.0.72-1](/moosefs/moosefs/commit/cc899b49b872b64a1ef72c87371cafc15374fc45) introduced a new ``umask`` export option which implements _global umask_. Would you mind documenting it in the mfsexports.cfg(5) man page to let us know what it does?

Thanks!

Marin.",,50505775
1220,trashtime TIMEDURATION is multiple of 3600,open,2018-05-10T07:30:51Z,2018-06-01T06:30:56Z,,NONE,"hi,

i was very confused with settrashtime for about 8 hours. setting the trash time, nothing was changed and i dont know why. 
```
/mnt/mfs# mfsgettrashtime .
.: 3600
/mnt/mfs# mfssettrashtime -r 10 .
.:
 inodes with trashtime changed:              0
 inodes with trashtime not changed:         11
 inodes with permission denied:              0
```
following went ok:
```
/mnt/mfs# mfssettrashtime -r 0 .
.:
 inodes with trashtime changed:             11
 inodes with trashtime not changed:          0
 inodes with permission denied:              0
/mnt/mfs# mfsgettrashtime .
.: 0
```

next test is very annoying. i want to set 60 seconds, but something else appears.
```
/mnt/mfs# mfssettrashtime -r 60 .
.:
 inodes with trashtime changed:             11
 inodes with trashtime not changed:          0
 inodes with permission denied:              0
/mnt/mfs# mfsgettrashtime .
.: 3600
```

and finally, this is how it really works. ceil of multiples of 3600.
```
/mnt/mfs# mfssettrashtime -r 3601 .
.:
 inodes with trashtime changed:             11
 inodes with trashtime not changed:          0
 inodes with permission denied:              0
/mnt/mfs# mfsgettrashtime .
.: 7200
```

please document this ""feature"". short mention in mfsexports.cfg and pdf documentation will be enough. this must drive people crazy.

thanks
jan","mfssettrashtime parameter is in seconds ceiled to hours. you should run
`mfssettrashtime -r $((24*60*60)) *`

confusing, right? :) thats why i recommend to amend the documentation and config files comments.",50505775
1221,Tool to get/locate/correct invalid chunk,open,2018-04-20T12:08:15Z,2018-12-20T21:24:58Z,,NONE,"Let's say something went wrong (HW problems) and we have file (small one < 64MB) with ""INVALID COPIES"". What we can do is...
```
mfsfileinfo /path/to/file_with_invalid_copies
/path/to/file_with_invalid_copies:
        chunk 0: 00000000189C9EB1_00000001 / (id:412917425 ver:1)
                copy 1: 192.168.10.111:9422 (status:INVALID)
                copy 2: 192.168.10.117:9422 (status:INVALID)
                no valid copies !!!
```
Scary, but not so much. First thing that you want to do is to use `mfsfilerepair` but... **DON'T**! It will (don't know why) delete invalid chunks and make the file `empty`. 

Next thing comes to mind is... mayby the copies are invalid, but they still are present on chunk servers. Mayby only CRC/header/someting little is wrong. So let's take a look at them... So let's find them.
```
find /moose/*/ -name chunk_00000000189C9EB1_00000001.mfs -print
```
...and you go for a walk if you have big chunkserver with a lots of chunks :)

After a walk you will have a path to a chunk with data. So you can copy it, get rid of first 8k of MFS headers, look at it and so on... Mayby the data is OK.
You can also use `mfschunktool` to check what's wrong and find out that only header/crc is not OK. Then you can make `mfschunktool -r /path/to/chunk_file.mfs` to correct this and end up with VALID chunk. 

That's a lot of time, mainly for finding a chunk file. So I have a feature request :)

`mfsgetchunkdata` tool to download raw data of given chunk id from a given chunkserver, valid or not from Moose point of view, without headers etc. Having such data we can play with it, mayby correct it, overwrite invalid file with corrected data... many things can be done to get data back instead of just looking at ""missing"" status and ""invalid copies"" only because of wrong crc
or 
`mfslocatechunk` tool to get /path/to/chunk on a chunkserver so we don't have to find it and walk too much which can be very time consuming (30min per chunk_id if you have CHSRV with 68 4TB drives and 100mln chunks on it ;) )

or mayby even extend `mfsfilerepair` with some new options to let the human decide what to do in case of INVALID COPIES... because now I have no idea what `mfsfilerepair` does (@oxide94 : more info needed) and again, **DON'T USE IT!** if you don't have chunk copies on a side, which gets you back to the beginning of this story.

All above is based on real case from today with small files = 1 chunk per file. **And the end files/data have been restored in 100%!** 😁 ","MooseFS doesn't know exact file location. From master point of view there is only association between chunkid and chunkserver. Chunkserver keeps chunk localization in RAM (of course to speed up starting procedure all file localizations are stored in '.chunkdb' file). When chunkserver starts it tries to use previously stored '.chunkdb' and it removes this file after using it. When chunkserver stops then it stores new '.chunkdb' file. To locate file we should need either working chunkserver (with fully scanned disk) or correct '.chunkdb' file. When chunkserver detects hard drive problems usually it removes all data from memory - in such case we don't have file locations in chunkserver data structures, also in such case file '.chunkdb' doesn't exist. There are no other ways to locate file on broken disk than use ordinary 'find' tool. There ain't no magic formula that will **decode chunk path/folder from it's ID**

What can be improved? When chunkserver starts it can rename '.chunkdb' file to '.chunkdb.back' insted of removing it. Maybe even such file can be updated hourly. Then  I can add simple tool that can use '.chunkdb.back' file to locate chunk - but likely information in this file will be outdated - chance that your file is still in location pointed by such file should be quite high, but not 100%.

BTW. For finding 94 chunks you don't need to perform such find 94 times - when you have all your chunk ID's then you can use something like this:

`$ find /Volumes/Extra/mfsdata/ | egrep 'chunk_(00000000714E2FD7|00000000714E2FBA|00000000714E2FD8|00000000714E2FF3)_'
/Volumes/Extra/mfsdata//mfsdata_11a/00/chunk_00000000714E2FF3_00000001.mfs
/Volumes/Extra/mfsdata//mfsdata_5a/00/chunk_00000000714E2FD7_00000001.mfs
/Volumes/Extra/mfsdata//mfsdata_7a/00/chunk_00000000714E2FBA_00000001.mfs
/Volumes/Extra/mfsdata//mfsdata_9a/00/chunk_00000000714E2FD8_00000001.mfs
`",50505775
1222,When is the size of the file updated?,open,2018-04-16T05:20:56Z,2019-01-20T15:58:49Z,,NONE,"
When I use the moosefs client to write a file, when I write a large file, when will I submit an update to the master to update the current file size? I looked at the code. Only when the flush, fclose, and the current file exceeds 64M, the client will notify the master to update the file size. I understand right?","In the master file size is updated after ""truncate"" command and after **each** write transaction. Of course we cannot update file size in the master after each ""write"" command performed by the user due to efficiency reasons. Mount tries to group subsequent writes and make bigger transaction. Of course ""flush"", ""fsync"" and ""close"" always finishes all pending writes and therefore update file size, but standard writes also update the file size - but sometimes this process is little delayed (transaction is prolonged as long as new data are comming, but cannot exceed 20 seconds).

There is no special '64M' case for updating file size - but of course each chunk has to be written in separate transaction and this is why you may observe that updates on ""chunk borders"" are maybe more frequent than the others.",50505775
1223,[feature] tools to access trash files,open,2018-04-12T16:11:07Z,2019-01-10T02:54:18Z,,NONE,"Is it possible to implement a better way to browse and restore files from trash?

The initial hash list with the files from the same directory separated in it is not very practical, is it possible to add a option to browse the original directory tree? 

[root@mst1 trash]# ls -l /mnt/mfsTrash/trash/
drwx------.    3 root root 0 Apr 12 15:52 000
drwx------.    3 root root 0 Apr 12 15:52 001
[...]
drwx------.    3 root root 0 Apr 12 15:52 FFD
drwx------.    3 root root 0 Apr 12 15:52 FFE
drwx------.    3 root root 0 Apr 12 15:52 FFF
d-w-------. 4098 root root 0 Apr 12 15:52 undel

Thank you","Files in the trash are recognized by their inode numbers, not names, so there are no name collisions there. You can recreate and redelete the same file couple of times and then ""undel"" each of them - the only problem is that if you want to ""undel"" them all then you need to change the name of recently undeleted file before you recover the next one - because in standard filesystem name collisions are forbidden.

Usually flat trash is what you want, but sometimnes (case of our company) you may have millions of files in the trash (currently we have 8mln files there) and just listing them all in one directory causes network issues - this is why we decided to split trash into 4k ""subtrashes"".

Currently we are working on tools to access trash files - it should be much more practical than current special ""meta"" mount.",50505775
1224,Device or resource busy ,open,2018-03-13T08:08:04Z,2020-03-23T15:28:29Z,,NONE,"sometimes I checkout  a svn dir on mfs mount point  ,   shows the error ""Device or resource busy "".  anyone knows why ?

version: 3.0.88","I'm afraid fuse3 won't help you. The problem lies within the kernel itself. There is no way to ""ask"" the kernel, which inodes it still keeps in its cache. So if there are bugs in the kernel that cause it to keep inodes it absolutely shouldn't keep (inodes it was instructed to ""forget""), we don't know about it. And version of fuse doesn't matter, because it's not fuse that decides, which inodes to keep.

We have one more idea how to prevent the kernel from keeping inodes cached for too long, we are currently preparing to test it, but we cannot yet predict if it will help at all. I will keep you posted.",50505775
1225,Moosefs Replication between two DC,open,2018-02-23T16:59:44Z,2018-12-20T21:04:06Z,,NONE,"Is it possible to replicated data between two datacenter using moosefs, For example one DC is in New York and another is in Chicago then how could we sync the data between these DC and keep all metadata and chunks in sync between them.

Does Moosefs have feature of asynchronous replication between sites?

Thanks

","Short answer: in many cases, yes.

You could configure clients in each DC to write to redundant storage classes served by a set of local chunkservers, and switch to storage class replicated across multiple DCs after a period of time. 

Network between DCs should be fast enough with respect of the volume of new/changed data to allow the replication to catch up, even in the face of maintenance or failures which could result inter-DC traffic. This requirement is not specific to MooseFS though.

Start with replication levels which let you do maintenance or tolerate failures within a DC without needing to pull data from other DCs.

Then do some homework w.r.t current and planned workloads:
- Size distribution of modified files. Replication of bazillions of small files will be latency bound and has to be tuned for that edge case.
- Frequency of changes and volume of changed data per unit of time. When latency is accounted for how long will it take to replicate changes? If on paper your are barely able to catch up, there is a good chance something important will be missing when you need it the most (sudden and extended outage in one of the DCs causing abnormal traffic patterns).

Hope this helps",50505775
1226,mfsmount showing wrong version of files,open,2018-01-16T22:10:41Z,2019-02-16T04:01:44Z,,NONE,"Hello, 

Files do not always update through the mfsmount point in certain cases. This especially happens when a file is updated, but the name does not change. For example, an index.html file. 

I can duplicate the issue by running this test. All systems are running 3.0.99 on FreeBSD 11.1. Installation was done by source through the ports tree. I'm running the following components. 

1x master
3x chunk servers
3x clients
All systems are dedicated hardware. The mfsmount options from /usr/local/etc/mfs/mfsmount.cfg are

mfscachemode=DIRECT
mfsmaster=IP
/storage/sites

I am uploading the testing file through SSH using rsync. The file is uploaded directly to a location under the mfsmount point. 

First test
1: Create a small text file with a single line of text in it on your local computer. Lets call it 'testing.txt'.
2: Use 'rsync' to transfer the file to server 1.
3: Run 'mfsfileinfo testing.txt' on server 1 and 2. Here are the results.

Server 1:
testing.txt:
        chunk 0: 00000000014990AD_00000001 / (id:21598381 ver:1)
                copy 1: 192.168.0.26:9422 (status:VALID)
                copy 2: 192.168.0.27:9422 (status:VALID)

Server 2: 
testing.txt:
        chunk 0: 00000000014990AD_00000001 / (id:21598381 ver:1)
                copy 1: 192.168.0.26:9422 (status:VALID)
                copy 2: 192.168.0.27:9422 (status:VALID)

So far so good. The file shows correctly on all sides. However, if I change the local file and rsync it again, it does not show as updated on the second (or third) server. 

Second test. 
1: Edit the local testing.txt file and change the text. 
2: Rsync the changed file up to the same server over top of the previous file.
3: Run 'mfsfileinfo testing.txt' to test it again. 

Server 1:
testing.txt:
        chunk 0: 000000000149918C_00000001 / (id:21598604 ver:1)
                copy 1: 192.168.0.26:9422 (status:VALID)
                copy 2: 192.168.0.27:9422 (status:VALID)

Server 2:
testing.txt:
        chunk 0: 00000000014990AD_00000001 / (id:21598381 ver:1)
                copy 1: 192.168.0.26:9422 (status:VALID)
                copy 2: 192.168.0.27:9422 (status:VALID)

The file on the second mfsmount point does not change. If I unmount the mfsmount point on the second server and then mount it again, I see the correct file. Again, this only appears to happen when the file name has not changed. ","Hi everyone, sorry about the delay on this.  I'm hoping Piotr et al can test these changes and verify that things are (somewhat) improved — we know there is other work to do, but some of the most egregious issues should be resolved by these changes.

The other thing to keep in mind is that these fixes are only in FreeBSD head and not yet in stable/12 or stable/11.  I don't work on Freebsd-stable myself, but in the past, MarkJ@FreeBSD has backported some FUSE fixes to stable branches and may be willing to facilitate that if you ask.",50505775
1227,Securing MooseFS public installation,open,2017-12-30T14:10:02Z,2018-12-19T21:36:55Z,,CONTRIBUTOR,"I couldn't find in the documentation all details about securing MooseFS installation which would be installed on the public IP addresses, to be accessible from anywhere.

So, to be able to mount the mfs share, port 9421 on master and port 9422 on chunkservers need to be open to everybody, right? Then we secure master - cs communication by setting AUTH_CODE in master and chunkservers. Then we secure mounts by requiring password for all of them in mfsexports.cfg. Ports 9419/9420/9425 can be firewalled, as they're not used for user access.

Can the above described MooseFS cluster setup be declared secure enough to be open in public? Meaning, no unauthorized access will be possible? At this point I'm not concerned with unencrypted traffic, just that data can't be accessed, unless you have provided proper password.

This is also a great opportunity to thank you for probably the best distributed file system out there, and wish you all the best in 2018!",,50505775
1228,moosefs metalogger ,open,2017-12-16T11:05:23Z,2018-12-20T21:24:07Z,,NONE,"If I use the moosefs metalogger on the master and standby nodes of mfsmaster, if you set the metadata download every 24 hours, does the 24 hour data loss if the primary node fails before downloading the metadata? I know you can use changelog + metadata recovery , but this will not lose data? metalogger slave Modification log is loaded when starting mfsmaster is a log file Or is it based on the metadata time to load multiple log files?
","> CRC in TCP etc. prevents from flooding slave (follower) with garbage. In case of any network issue follower will desync.
> Why should I add my own CRC to each changelog? This will duplicate OS checksums. We use CRC in I/O packets but only because we already have them in chunks, so we can send them.

https://tech.vijayp.ca/linux-kernel-bug-delivers-corrupt-tcp-ip-data-to-mesos-kubernetes-docker-containers-4986f88f7a19 this is why i won't rely only on TCP/IP checksumming but also on checksumming on application level.

TCP/IP checksumming will protect you on corruption during the network communication, but if the network card or something else is corrupting data sent by the master, the follower will receive garbage

Is the same like using ECC ram with ZFS. ZFS is able to apply a checksum on every data, but it can't know if data is coming already corrupted from the RAM (that's explain why ECC RAM is highly suggested)

IMHO, if I have to be sure that what I've sent is exactly what I've received on the other side, i'll apply a checksum on the sender and on the received i would check it. This protect the whole stack: software, hardware, transmission and so on, on both sides.",50505775
1229,Archive based on last access time,open,2017-11-02T06:54:04Z,2019-01-08T03:07:11Z,,NONE,It'd be good if you could archive based on last access time as opposed to modification time,Nice! I think I could use that feature... ;),50505775
1230,Server select menu for Disks tab,open,2017-03-21T13:18:18Z,2017-03-22T15:40:49Z,,NONE,"It would be very nice to by able to narrow the Disks tab to 1 chunkserver via the select dropdown, same as on Servers Charts to display all charts for a particular chkserver. As the cluster grows (f.ex. we have 10 CHKSRV with 200 drives total), Disks tab starts to be difficult to read if you want to focus on a particular chunkserver.",,50505775
1231,Drive's SMART data at Disks tab,open,2017-03-21T13:13:27Z,2017-03-22T15:40:38Z,,NONE,"It would be really nice to see SMART data of a drive via Disks tab. [smartmontools](https://www.smartmontools.org) can be used by chunkserver for this if you don't want to make internal own code for this feature. It should be avaible per drive per request, some small popup with SMART table in it.","@marcin-github just from curiosity, why you need write access to retrieve SMART data? why not to use smartctl to get job done?

@zcalusic I'm aware of SMART and the way every vendor uses it's own ways to collect and present data differently even between drive models of the same vendor (Kingston SSDs f.ex.). It's a pain and I know it :) 
I don't want to present SMART info on the Disks tab directly. I was thinking only about some icon/link next to IP path which opens modal window with simple table of SMART data with values, typical what you can see in HD Tune or smartmontools output. Just a shortcut to get smartmontools output with 1 click rather then via cli on chunkserver.

I don't want to exaggerate the problem, it's a feature, a cosmetic one... but it's nice to see some discussion about it that can motivate MFS team to develop something and community to throw new ideas for them ;) ",50505775
1232,Archive delay in hours,open,2017-02-02T10:37:17Z,2019-02-11T17:28:58Z,,NONE,"It would be nice to have option to set delay time for archive in hours, not only in days ;) This will allow f.ex. to create/keep data during the day and archive it at night.","Hi @oszafraniec,

I am happy to let you know, that this feature is already implemented and will be released in upcoming MooseFS 4.x release! Also, in MooseFS 4.x it will be possible to set delay basing on `mtime`, `ctime` and `atime` (see #77).

Thanks for the idea.

Best regards,
Peter / MooseFS Team",50505775
1233,Please make master multithreaded,open,2016-06-05T16:26:34Z,2019-01-08T02:55:57Z,,NONE,"IMHO the biggest bottleneck in MooseFS is the single-threaded nature of the master server. We have about 320 roaming windows profiles on Samba+Moose and loading them is about 20-50% slower than on a single XFS hosted samba machine. During times when lots of users are logging in the master server CPU becomes almost saturated on the single master process.

If an organisation with 1000+ users was doing the same I should imagine it would be proportionally worse.

I appreciate this if probably not an easy task, so I won't hold my breath.

Regards

Alex
","Agreed with @acid-maker, multithread may not the good solution.

@alexcrow Could you show some stats from webui when the master is busy?

How many clients and chunkservers do you have? When there are hundreds of clients or chunkservers, the CPU cycles spent in network (and related bookkeeping) are not trivial, epoll should be used to serve more than 1000+ clients or chunkservers. I had done that before (also posted in the maillist), together with other optimization, it did decrease the CPU usage to 10-%.",50505775
1234,Lazy chunk removal,open,2016-04-25T08:41:13Z,2019-01-18T14:07:21Z,,MEMBER,"Original idea from #8:

> I think an option to make removing extra chunks more lazy would be good.
> 
> Scenario is that a disk goes offline and gets replicated then comes online and gets removed, then it happens again. All this churn would be reduced by more lazy removal of extra chunks.
","> Original idea from #8:
> 
> > I think an option to make removing extra chunks more lazy would be good.
> > Scenario is that a disk goes offline and gets replicated then comes online and gets removed, then it happens again. All this churn would be reduced by more lazy removal of extra chunks.

Another scenario: entire datacenter/rack worth of chunkservers disappears due to unplanned outage, remaining locations bring replicas to defined levels and everything continues as normal. When datacenter reappears master rapidly deletes known good copies, not knowing that the outage resulted from a cooling failure and that dozens of disks in the originally offlined location will start returning I/O errors next time filesystem activity or scan hits some or any chunks stored on them. This tightly couples with trust model for `.chunkdb` proposed in #165. This is not a hypothetical scenario, btw.",50505775
1235,[feature] erasure-coding,open,2016-03-19T05:41:24Z,2020-01-27T10:26:16Z,,CONTRIBUTOR,"Erasuse coding is a space-efficient yet reliable way to store data in distributed manner.
It is already implemented in Ceph, LizardFS and some other storage systems.

See also https://github.com/lizardfs/lizardfs/issues/207
",All this talk of Moosefs v4. Where can I download v4 to try it? Thank you. ,50505775
1236,chunkserver: please implement data compression [feature],open,2016-03-10T12:49:12Z,2019-02-10T04:53:22Z,,CONTRIBUTOR,"Chunk size is not optimal for small files (e.g. sources). Because chunk files are not sparse HDD space utilisation is not optimal and a lot of space is wasted. To certain extent it can be compensated by using file system with compression (e.g. Btrfs mounted with compress=lzo) on chunkservers but it would be great if chunkservers could optionally compress their chunks natively with LZ4 (preferable) or similar compression algorithm.

Related to #6 and https://github.com/lizardfs/lizardfs/issues/366
","LZ4 overhead is negligible and its speed is close to RAM-to-RAM copy:

> LZ4 is a very fast lossless compression algorithm, providing compression speed > 500 MB/s per core, scalable with multi-cores CPU. It also features an extremely fast decoder, with speed in **multiple GB/s per core**, typically reaching RAM speed limits on multi-core systems.

Also LZ4 was implemented natively in the Linux kernel 3.11.",50505775
1237,add gitignore for working directories,open,2020-03-26T20:50:37Z,2020-03-26T20:50:37Z,,NONE,"Running `git status` or `git ls-files` on a deployed instance of
hashtopolis is noisy, because working files show up as untracked.

This commit gitignores everything but .htaccess and .gitignore in

src/files/
src/import/
src/log/
src/tmp/",,73367610
1238,Issue with Electrum Wallet (Salt-Type 4),open,2020-03-18T19:47:43Z,2020-03-23T20:42:54Z,,NONE,"Cracking freezes on 98.55. Hashcat 5.1 beta
![image](https://user-images.githubusercontent.com/8035828/77001048-5dd65980-696a-11ea-8ff7-fc7c0a0ebebf.png)
![image](https://user-images.githubusercontent.com/8035828/77001082-6595fe00-696a-11ea-9ce2-5b1bd60487c1.png)
![image](https://user-images.githubusercontent.com/8035828/77001093-6b8bdf00-696a-11ea-853a-a2c6086cddd1.png)


hashcat64.exe  --machine-readable --quiet --status --restore-disable --session=hashtopolis --status-timer 5 --outfile-check-timer=5 --outfile-check-dir=..\..\hashlist_19 -o ..\..\hashlists\19.out --outfile-format=1,2,3,4 -p ""	"" -s 0 -l 11881376 --potfile-disable --remove --remove-timer=5 ..\..\hashlists\19 -a 3 ?l?l?l?l?l?l  --hash-type=21700","I found consistent pattern!
When i try solve simple task - all works
```
hashcat64.exe  --machine-readable --quiet --status --restore-disable --session=hashtopolis --status-timer 5 --outfile-check-timer=5 --outfile-check-dir=..\..\hashlist_19 
-o ..\..\hashlists\19.out --outfile-format=1,2,3,4 -p ""	"" -s 0 -l 456976 --potfile-disable --remove --remove-timer=5 ..\..\hashlists\19 -a 3 ?l?l?l?l?l  --hash-type=21700 
```

But when i send harder - its freeze on **98.55** percentage
```hashcat64.exe  --machine-readable --quiet --status --restore-disable --session=hashtopolis --status-timer 5 --outfile-check-timer=5 --outfile-check-dir=..\..\hashlist_19 -o ..\..\hashlists\19.out --outfile-format=1,2,3,4 -p ""	"" -s 0 -l 11881376 --potfile-disable --remove --remove-timer=5 ..\..\hashlists\19 -a 3 ?l?l?l?l?l?l  --hash-type=21700 ```",73367610
1239,Benchmark not works in latest release of hashcat,open,2020-03-18T10:00:43Z,2020-03-18T12:43:35Z,,NONE,"I downloaded latest version hashcat - **v5.1.0-1745-g434ad763**

in 5.0 this line works correct, but in latest release show incorrect result

`hashcat64.exe --machine-readable --quiet --progress-only --restore-disable --potfile-disable --session=hashtopolis -p ""\t"" ..\\..\\hashlists\\15 -a 3 -2 ?l -1 ?a -3 !LlPp?d ?3?3?3?3?3?3?3?3  --hash-type=11300  -o ..\\..\\hashlists\\15.out 
`
5.0
![image](https://user-images.githubusercontent.com/8035828/76948664-70c03e00-6918-11ea-88cc-cc9111279bb9.png)
5.1
![image](https://user-images.githubusercontent.com/8035828/76948680-76b61f00-6918-11ea-8ef1-80ab78ecdd5c.png)

","Which last release files do you mean? The one from hashcat?
I only build hashcat for linux, the typical hashcat beta builds are not far behind the master of the repository: https://hashcat.net/beta/",73367610
1240,Cant search for hashes,open,2020-02-29T17:57:53Z,2020-03-06T04:17:42Z,,NONE,"Hi

When searching for a hash this happens:

`Column 'hashlistId' in where clause is ambiguous in /var/www/hashtopolis/src/dba/AbstractModelFactory.class.php:607\nStack trace:\n
#0 /var/www/hashtopolis/src/dba/AbstractModelFactory.class.php(607): PDOStatement->execute(Array)\n
#1 /var/www/hashtopolis/src/dba/AbstractModelFactory.class.php(638): DBA\\AbstractModelFactory->filterWithJoin(Array)\n
#2 /var/www/hashtopolis/src/dba/models/HashFactory.class.php(56): DBA\\AbstractModelFactory->filter(Array, false)\n
#3 /var/www/hashtopolis/src/inc/handlers/SearchHandler.class.php(72): DBA\\HashFactory->filter(Array)\n
#4 /var/www/hashtopolis/src/inc/handlers/SearchHandler.class.php(21): SearchHandler->search()\n
#5 /var/www/hashtopolis/src/search.php(21): SearchHandler->handle('search')\n
#6 {main}\n  thrown in /var/www/hashtopolis/src/dba/AbstractModelFactory.class.php on line 607, referer: https://domain.com/search.php`",,73367610
1241,Running an instance of hashcat as preprocessor,open,2020-02-28T12:00:12Z,2020-02-28T12:00:12Z,,NONE,"Server: 0.12.0 latest master

A question:

I would like to run this for example:
`hashcat --stdout -a 1 wordlist1 wordlist2 | hashcat -m 0 -a 0 -r dive.rule hashfile`

Today I have to generate al the combinations to a textfile which takes time and space and then I have to run hashcat against that file.

If I could run hashcat as a preprocessor to the main hashcat instance I wouldn't have to do that. Is it possible? I can't try it right now otherwise I would give you the answer =)",,73367610
1242,Task Creation Logging of User ID,open,2020-02-27T21:42:16Z,2020-03-02T19:45:15Z,,NONE,"- Server 0.11.0

Currently when a new hashlist is created the corresponding user performing this action is logged. However no logging appears to be performed of a users actions for any task type (New Task, New Preconfigured Task, New Supertask, etc.).

I'm requesting for logging of all user actions against tasks (creation, modification, deletion). ",,73367610
1243,Combination attacks with prefix,open,2020-02-27T19:53:10Z,2020-02-27T19:53:10Z,,NONE,"Im having troubles using -j '$-' to make words like:
hello-world

I belive is the quotes that may be escaping a string and causes this.

Any workaround?

```
Error during keyspace measure: Command 'hashcat.exe --keyspace --quiet -a1 ..\..\files\Wordlist1.txt ..\..\files\Wordlist2.txt -j '$-'  --hash-type=16800 -w4 -O --backend-ignore-opencl' returned non-zero exit status 4294967295.

..\..\files\Wordlist1.txt: empty file.
```
",,73367610
1244,"Feature Request: Add a Completed checkmark ""✔"" for super tasks",open,2020-02-26T09:13:41Z,2020-02-27T10:17:48Z,,NONE,"Hashtopolis 0.12.0 latest master

It would be nice to have the checkmark for a completed task for supertasks as well, when all tasks in the supertask have finished.",,73367610
1245,Abort all agents chunks without deleting the task,open,2020-02-26T09:03:48Z,2020-02-27T10:17:37Z,,NONE,"Hashtopolis 0.12.0 latest master git

Hi,

Is there any option to abort a running task without deleting it? If I am running a PRINCE task for example and I add another task with higher priority I would like to abort all running agents in the PRINCE-task to be able to assign them to the new task and not have to wait for a new chunk to be dispatched in the PRINCE-task. Now you have to abort all chunks for each agent one by one which can be quite time consuming

","Like staxmood says it would be more of a pause button, keeping the task active and at the same priority but forcing the agents to abort their chunks to get assigned to the new task with a higher priority.

If you don’t want the agents to continue with the task they were aborted from you could always set priority zero. Or maybe it should be implemented as a permanent pause, meaning abort all agents AND set priority to zero. Could lead to confusion and problem keeping track of tasks if you have many of them though. ",73367610
1246,Feature Request: Auto-Updating Graphs,open,2020-02-26T05:58:59Z,2020-02-27T10:17:03Z,,NONE,"- Your current Server version located at the bottom of any Hashtopolis webpage. v12
- Current Client version v6
- Your current Hashcat version 5.1

Hey there, 
Just was working within the user interface and was looking for an auto-updating graph feature and didn't see it. I think this feature would do well for better monitoring ability of progress.

Feature Request:
Request feature of setting to allow auto-updating in intervals for:
1. Chunk Activity Entries
2. Agent Status Page
3. Agent Details Device Temperature Graph
4. Agent Details Device Utilization Graph

",,73367610
1247,Option under UI to set Night mode Permanent,open,2020-02-19T14:05:23Z,2020-02-21T15:56:02Z,,CONTRIBUTOR,"How about an option to set the dark mode permanent , cant see the white anymore :-(","damn users admin rules :-)
global is def fine.
i dont want to look at the grey login ever again haha",73367610
1248,dictionary.txt file not uploading,open,2020-01-14T20:30:54Z,2020-02-06T15:32:44Z,,NONE,"**Use github issues only for bugs/feature requests - for discussions, general help and questions use the forum: https://hashtopolis.org**
**Before you submit an issue please include the following information if you do not your issue will be closed.**

- Your current Server version located at the bottom of any Hashtopolis webpage. - 0.11.0
- Current Client version - N/a
- Your current Hashcat version - N/A
- The exact task command you are trying to run. - Uploading dictionary.txt file from usb stick
- Debug output from the client by running ""hashtopolis.exe -d"" or with debug flag set on the python client. -N/A

**Describe your problem in as much detail as possible "" It's broke "" is not a description.**

Evening, 
I am trying to upload a dictionary  file that is in .txt format from a usb stick to be able to run a dictionary attack on a captured 4 way handshake converted to hashcat format from a wireless network.  When i try to upload the file it does not upload the file and when the page is refreshed nothing is available. 

I have looked through the previous issues finding others that have had the same problems on lower versions and have tried the following fixes. 

1. Changed the upload_max_filesize to 200M (file is 8M) within /etc/php/8.2/apache2/php.ini  as per the server requisites documentation
. 
2. Making sure that the owner is www-data for the import/files folders in /var/www/hashtopolis/import or files and annotated permissions so that any it is 777 across the board to make sure anyone can write or read. 

Does anyone else have any suggestions as currently I still cant upload a dictionary to run as a wordlist.txt. Brute force via .a  mask  works fine along with uploading capture files. 
One question I have is when you change the config in php.ini do you have to restart apache2 or reboot machine or will it dynamically change the config?

Any help on this would be great as my ultimate goal is to run a hybrid attack with a dict and mask, 

Thanks in advance. 
","As far as I know, when php ini values are changed, apache2 needs to be restarted to take these also in effect for the webserver.
Do you still have the issue afterwards?",73367610
1249,Agent may download a malicious executable if server is compromised,open,2020-01-12T10:18:37Z,2020-02-22T09:01:43Z,,NONE,"Currently the agent will download and run any executable that server instructs it in the response. In case the server gets compromised all the agents will get infected too very easily:

1. register a malicious cracker
2. create a task with the new cracker
3. assign the task to all the agents
4. agents will download and run the malicious executable 

I would suggest the agent to accept only whitelisted hashes of 7z archives. So user can choose during agent installation:

1. secure mode - provide trusted SHA512 hashes of 7z archives
2. old insecure mode - disable any checks and run whatever server returns

In this case user is explicitly confirms that the risk is known to him and he is responsible for any damage in case server gets hacked","> By installing hashtopolis you are basically making the hashtopolis master the admin of all your systems

I disagree with this statement. Personally, by installing hashtopolis agent I only give permission to GPU, and that's it. Think about it as a client option similar to `Trust agent with secret data` which is not on by default on the server side

So it would be nice to allow user explicitly select during agent install: does he want to grant full admin rights to the server or only allow access GPU? This would be very important feature",73367610
1250,Attempting to start new hashcat process before last is finished,open,2019-10-31T14:22:28Z,2019-12-10T14:24:21Z,,NONE,"System specs are 10 GTX 1080ti's for a single agent. While trying to crack more difficult hashes such as 13100 (kerberos tickets) or 5600 (netntlmv2) hashtopolis instructs the agent to start a new job before the previous has finished.

I somewhat found a work around by setting the status timer to 60 seconds to allow the job to complete.","These values can be tweaked client side.

allow-piping | boolean | true | Allows hashcat to read password candidates from stdin
piping-threshold | integer | 95 | Restarts chunk in piping mode when GPU UTIL is below this value

This should help the issue you are having.

More client-side setting can be found here https://github.com/s3inlc/hashtopolis-agent-python

",73367610
1251,Error: File is empty on Pre-Crack import,open,2019-08-09T14:24:11Z,2019-08-09T14:24:30Z,,NONE,"Current Server Version: Hashtopolis: 0.11.0+repository
Current Client Version: s3-python-0.5.0
Current Hashcat Version: 5.1.0
Command: API Request through BurpSuite/Python and Web interface
API-Request to import Pre-Cracked hashes
```
{
	""section"":""hashlist"",
	""request"":""importCracked"",
	""hashlistId""9,
	""separator"":"":"",
	""data"":""ZmY0ODc2MWYzOWIwZThjMmE0YWJjYmRhZjAwNjM1MmY6UGFzc3dvcmQ4ODg="",
	""accessKey"":""Nope!""
}
```
Response: 
```
{
  ""section"": ""hashlist"",
  ""request"": ""importCracked"",
  ""response"": ""ERROR"",
  ""message"": ""File is empty!""
}
```

This is a requests that has been done through Burpsuite, python lib Requests, but also when trying to import the file (its way bigger ~5k lines), the response is always the same

![image](https://user-images.githubusercontent.com/14073684/62785933-e2522700-bac1-11e9-877f-20661992abfb.png)

This is the same for all the different hashlists! the file is always empty. 
Have also tried importing and downloading from the web interface, with no luck.
",,73367610
1252,Change export behavior,open,2019-05-22T20:24:56Z,2019-06-27T16:05:39Z,,NONE,"When exporting wordlists and precracked can you have them go into the import folder rather than direct to the files folder. This way they are available for import directly into other hashlists or are ready to be imported for use in other crack jobs.

When they go direct to files they don't go into the database and the webui isn't ready to use them for a direct import into another job
","There are certain cases where selecting the output location of any export function (export of cracks, left lists, wordlists etc.) would be handy, ideally being able to select files, the import dir or [files+insert into the files db].

Please modify all areas where exports are possible to select the output location (import, files dir, or files dir + insert into the files db] and enable via the user-api",73367610
1253,API function to create/list/delete API Keys,open,2019-05-13T21:29:11Z,2019-06-27T12:28:07Z,,NONE,"I'm going to be creating a temporary API Key generator for my users to tie into our other access granting pipelines. Is it possible to get new API functions to manage API keys?

At a minimum I would like to:
1) List api keys, and their associated user just like the webui
2) Create new api keys, attached to a specific user and have associated start and end dates just like the webui
3) remove an api key

Bonus feature: edit an existing API key
",,73367610
1254,agents not processing Rules,open,2019-04-09T17:20:38Z,2019-06-27T12:24:11Z,,NONE,"
Hashtopolis Server version: 0.10.1 ()
{'action': 'checkClientVersion', 'token': 'bvcyqTGkRN', 'version': '0.4.0', 'type': 'python'}
http://localhost:80 ""POST /api/server.php HTTP/1.1"" 200 67
b'{""action"":""checkClientVersion"",""response"":""SUCCESS"",""version"":""OK""}'
Client is up-to-date!

[debug.docx](https://github.com/s3inlc/hashtopolis/files/3060192/debug.docx)
 Tried on Hashcat Version 5.0 and 5.1

Trying to run -a0 #HL# 20k.txt -r d3ad0ne.rule from new task

it runs through the 20k words then stops.  Never processes the rules.

Any ideas? 
","It is using rules, the task is just very small and that's why it's done pretty fast.",73367610
1255,Adaptive chunk sizes,open,2019-03-12T18:25:20Z,2020-02-26T10:05:06Z,,CONTRIBUTOR,"This is a feature suggestion posted based on a discussion between s3inlc and Thor.

Fact: Hashcat benchmarking is bad

When running large hash lists, different hash types or having an issue on the client side, different results will come in on the benchmark. In this test case we had a 250k vbulletin list (mode 2611) from hashes.org that functioned at subpar speeds.

9MH/s on 4x 1080 Ti
12MH/s on 1x 1080 Ti

The chunk size is about 3k out of the total wordlist attack keyspace of 1,464,244,267 and was completed in approx 50 seconds to 1 minute 20. This is far off from the 600 second target.

The goal of this feature is to adjust the benchmark based on the target and have it adapt the size, resulting in greater speeds & less chunks without reduced functionality or performance.

The proposed formula for this (by S3inlc) is `<new chunk size> = 600s / <time needed> * <old chunk size>`

The goal of the formula is to adjust the chunk size UP while the time needed for the last benchmark is less than the ideal chunk size (600). This allows for more utilization in case the benchmark turned out too low and could also be used to reduce utilization / chunk time if too high.","The proposed formula for this (by S3inlc) is perfect
This would be a good function 
Doning it manually at the moment",73367610
1256,Benchmark off by 10x,open,2019-02-07T20:01:08Z,2019-03-12T18:26:48Z,,NONE,"- Your current Server version located at the bottom of any Hashtopolis webpage: 0.10.1
- Current Client version: Latest
- Your current Hashcat version: 5.0.0
- The exact task command you are trying to run: mask attack on SHA1 wordlist

Currently testing on three different setups before rolling out to 72 GPU mining rig and I've run into an issue where one of the CPU client's benchmark is off by 10x. The other dual GPU and single CPU clients works as expected and typically show ~1-2% progress each update and take roughly the 600 seconds to finish a chunk. The bad machine does about 10-20% progress each update and finishes a chunk in about 10-15 seconds.

The improperly performing CPU instance will behave as expected if I manually assign the GPU's benchmark to it (around 40Mh/s give or take).

I have tried both benchmark options and all clients perform the same (2 good, 1 bad).

I'm worried that when it gets rolled out to the bigger rig, I'll end up having clients spend more time reading and re-reading the hashlist than actually doing real work, especially if it ends up doing something like being 100x off the mark.",#551 might be relevant to this.,73367610
1257,Feature Request: Edit Supertasks,open,2019-02-05T08:07:42Z,2019-06-27T12:25:41Z,,NONE,"- Your current Server version located at the bottom of any Hashtopolis webpage.: 0.10.0
- Current Client version: 0.4.0
- Your current Hashcat version: 5.1.0
- The exact task command you are trying to run.: N/A - Feature Request
- Debug output from the client by running ""hashtopolis.exe -d"" or with debug flag set on the python client.: N/A - Feature Request

Currently it is not possible to edit Supertasks.  If I create a Supertask, then add another preconfigured task at a later date, the only way it can be ""added"" to an existing Supertask is to delete the Supertask and recreate it.  This is the same if you accidentally create a Supertask incorrectly.

It'd be good if Supertasks could be edited, to remove/add preconfigured tasks.
",,73367610
1258,Suggestions,open,2019-01-18T01:45:35Z,2019-06-27T12:26:21Z,,NONE,"
Server Publishing task, if there is an unforeseen error, the client will be incorrectly exited, whether the error can be returned to the server to write to the log, skip this wrong task to continue to perform the following tasks. Because in the case of 8 clients or more, it is depressing to need to adjust the task level or delete the task, and then restart all clients, if an unforeseen error is encountered.","I think watchdog'ing clients is one part of this request. I think he is also looking for a way for the server to identify and mark tasks that users create that are not the correct syntax or have clear errors. Once they are marked bad consider them a priority of ""0"" and move on to the next task.",73367610
1259,Nginx Config Creation,open,2019-01-09T21:07:13Z,2019-01-09T21:07:13Z,,COLLABORATOR,Create Nginx config Documentation ,,73367610
1260,Redo Documentation for installation of server and client,open,2019-01-03T01:12:34Z,2019-01-03T01:13:09Z,,COLLABORATOR,"Winxp, need to redo Server and Client Documentation 

Wiki should be updated to inform users the C# client is technically Depreciated at this time and  current wiki should be converted to using the python client as its main focus ( Or make the link to the Client repo much more apparent ((probably the best option)))

Videos should be rather generic on setup Pointing users to use commands found directly on the Wiki pages (Explain clearly to follow wiki documentation more than the video itself for specifics )

And for the love of god make sure you have some videos explaining Common ""Gotcha's""

How to use the Platform and common uses of the software. (Low priority IMO)",,73367610
1261,Checksum for downloaded files,open,2019-01-01T21:56:17Z,2019-01-02T23:50:45Z,,NONE,"- Server 0.10.1
- Python client 0.4.0
- Hashcat 5.1.0
- Command independant
- Debug not needed
**Problem description:**
Each time an agent downloads a task, it could calculate an MD5sum (or something similar) of the dictionary and check with the server if the download went correctly. The same could be applied for hashlists. The server would have to calculate the sum itself or the user would have to manually calculate it and provide it to the server. Also if implemented, there should be an option to disable the sum checking (for example when dealing with very large files).","Ok, makes sense.

I cannot follow the last sentence, what should be enabled on the client startup?",73367610
1262,Option to change benchmark type in tasks after creation.,open,2018-12-31T15:09:13Z,2018-12-31T16:04:00Z,,NONE,"- Server 0.10.1.
- Python 0.4.0
- Hashcat 5.1.0
- Command-independant
- Debug not needed
**Problem description:**
When having lots of preconfigured tasks, it is a real pain to recreate (or copy) all of them in order to change one setting. There should be an option to change the benchmark type in preconfigured tasks (and maybe normal tasks if possible) **afrer** they were created.",,73367610
1263,more extensive task sheduling options,open,2018-09-03T15:27:47Z,2019-06-27T12:27:10Z,,NONE,"* cyclic execution on running tasks with the same priority when no higher priority tasks exists (notes: disable benchmarking again when an agent returns to a task that it already has processed a chunk for)
* round robin on multiple same high priority tasks (available after each task has been benchmarked) to spread equal time slices on the tasks.",,73367610
1264,Set Per Job --limit so that long running jobs can abort when a per task limit is hit,open,2018-08-13T19:20:55Z,2018-08-14T11:57:08Z,,NONE,"We would like the ability to specify a hard --limit for a specific task, so per client --skip and --limit would be automatically assigned but once a specific --limit is hit for the entire task the job would stop. This allows flexibility in scheduling long running tasks.

**Before you submit an issue please include the following information if you do not your issue will be closed.**

- Your current Server version located at the bottom of any Hashtopolis webpage.
latest master 0.7

- Current Client version
latest master as of 8/13/2017 0.1.7 I think

- Your current Hashcat version
4.2.1

- The exact task command you are trying to run.
any tasks

- Debug output from the client by running ""hashtopolis.exe -d"" or with debug flag set on the python client.
N/A

**Describe your problem in as much detail as possible "" It's broke "" is not a description.**

We would like the ability to specify a hard --limit for a specific task, so per client --skip and --limit would be automatically assigned but once a specific --limit is hit for the entire task the job would stop. This allows flexibility in scheduling long running tasks.",,73367610
1265,Improve CPU and GPU task handling with single agent,open,2018-08-13T16:19:29Z,2018-08-14T11:57:15Z,,NONE,"**Before you submit an issue please include the following information if you do not your issue will be closed.**

- Your current Server version located at the bottom of any Hashtopolis webpage.
Current Master 0.7

- Current Client version
Current Master 0.1.7 I think or whatever as of 8/13/2018

- Your current Hashcat version
4.2.1

- The exact task command you are trying to run.
CPU and GPU tasks concurrently

- Debug output from the client by running ""hashtopolis.exe -d"" or with debug flag set on the python client.
N/A


**Describe your problem in as much detail as possible "" It's broke "" is not a description.**
Currently the agent deploys a task to both GPU and CPU, it would be nice to be able to use a single agent and deploy a CPU only task and a GPU only task and use --device-type to run them on the same node. The only way to do this right now is deploy 2 agents to the same machine and designate one as CPU only and use --device-type however that is not ideal.  It would be nice to be able to run a GPU task on a fast hash and a CPU task on a bcrypt hash for example on the same host.",,73367610
1266,One too many subtasks,open,2018-08-12T23:00:03Z,2018-08-16T07:11:44Z,,CONTRIBUTOR,"I see some subtasks are created when rules are involved. I'm not entirely sure what the deal is here, but this is obviously going over the top. It makes everything unorganized.

![image](https://user-images.githubusercontent.com/32413926/44007360-299f141a-9e94-11e8-9f07-2e46aaacef95.png)
","There are some points which could be improved with supertasks. It's possible to see how many hashes were cracked by a supertask, but not to view them directly.
And also it's possible to see the progress in total of the supertask with the normal task progress image which is rendered when you hover over the ID of the supertask.
I might introduce a supertask detail page like for tasks.",73367610
1267,LDAP Authentication,open,2018-01-24T14:51:00Z,2019-08-20T15:41:46Z,,COLLABORATOR,Implement LDAP Authentication for single sign on in enterprise environments ,"LDAP is good, but what about SAML or RADIUS?",73367610
1268,Analytics and Customers,open,2017-11-12T16:27:07Z,2018-08-29T23:15:59Z,,CONTRIBUTOR,"Something nice I noticed with a different managing system called https://github.com/hashview/hashview.
I know analytics was on the agenda already but would be nice to see added soon :)","I also would love to see some analytics, but more like cracked hashes/overall",73367610
1269,"[dev]: Listing of ideas/tasks, things which need to be considered",open,2017-08-17T13:35:14Z,2018-08-16T08:58:27Z,,OWNER,"Based on this, issues will be created.

Initial:
* [x] Database scheme #269 

Core:
* [x] Protocol Version 2
* [x] Task management #272 
  * Create Task Wrapper 
  * Move pretasks to separate table 
  * Extend tasks
  * Task deletion change to keep task history
* [x] Groups #270 
* [x] Binary Types #271 
* [x] Binary Version handling #271 

Features:
* [x] Generic Cracker support example
* [x] Access via API for management via command line console
* [ ] Client side scripting
* [ ] Client side specific parameters
* [x] Temperature, Fan information handling
* [x] 3rd File Group
* [x] Add client support for platform specific stuff
* [x] Large list support (import with progress, download, etc.) #273

Finalization:
* [x] Create upgrade script","Hey @s3inlc I don't want to open another issue as I think this one will do just fine. My ideas are:

- [x] add pagination to: Tasks, Lists, Files
- [x] add search field to: Tasks, Lists, Files
- [ ] when creating supertasks or managing lists, make it possible to search interesting items and 'select all' to create/delete list ",73367610
1270,"Feature Request: Export ""Search Hashes"" result to a text file.",open,2017-07-19T08:08:25Z,2018-03-19T09:56:16Z,,NONE,"So having used hashtopussy the last few weeks our database is now growing nicely, it's more useful to search for hashes ahead of time rather than re-cracking easy ones so it is possible we can have some way of handling mass searching of hashlists?

Upload a hashlist, hit a button to ""Search for Pre-Cracked hashes"" and link them to the hashlist or allow you to export them as a text file?

Or... paste in a hashlist and search, allow me to download all results as a plain text file. hash:plain format would do.

Then I can sort -u, remove all the previously cracked ones from my submitted hashlist and submit only the ones that need further cracking.",,73367610
1271,Feature Request - Ability to control agent tasking more granularly,open,2017-06-28T11:00:02Z,2017-06-28T12:02:23Z,,NONE,"I'm currently implementing htpsy as a centralised cracking solution for our group but while we have a cracking farm that I've no problem downloading multi-GB sized dictionaries to as they're all local lan, we also have 10-20 laptops with discrete GPU's that users want to throw into the mix.

Unfortunately we're having to juggle the ""secret"" flag and be quick with the unassign button right now to prevent agents from reporting in and being tasked with say a 15GB crackstation dict download over a 512k hotel wifi connection.

It would be nice to be able to have more control over tasking such as...

* Agents marked as ""manual assignment only""
* Agents able to set a ""maximum acceptable download size"" and the auto-tasking be smart enough to allocate other tasks than allocate ones that exceed that file size.

Yes I understand that this doesn't take account of the edge case of the file already existing but maybe this could be a check done on the client side too.

""You need xyz files""
""okay I have x and y but need to download 20GB of Z""
""Nope... that exceeds your download limits, task rejected, new task found"".

All in all not quite sure how to go about it but having more tasking control would be a boon.
",,73367610
1272,Feature request around multiple users,open,2017-06-16T16:21:22Z,2017-06-19T11:48:50Z,,NONE,"Can we look to implement the following.
- A default colour for each user that can be configured in the users area. This colour then gets selected by default for all new tasks.
- Display which user created a new hash list
- Display which user create a new task

Many thanks
",s3 should be back actively coding sometime next month.,73367610
1273,Priority assignment correcting,open,2017-05-28T18:09:47Z,2017-06-19T11:32:31Z,,NONE,"When pre-conf tasks are set with a priority and there are already tasks with an arbitrary priority in the queue, creating a new task from the pre-confs with a set priority puts this one on top instead of finishing the current one up to finish. This way of assigning priority seems a bit backwards for me, unless there is some other kind of priority assignment system I'm not aware of (sorry for using ""priority 1000 times).","Or perhaps there could be a whole other option such as ""add to bottom of queue""? Then you could still assign a higher priority if needed, but also the current running task would just be pushed up by 1 and the new task could be added in the bottom.",73367610
1274,MDXfind implementation,open,2017-05-14T12:40:36Z,2019-03-15T19:03:55Z,,CONTRIBUTOR,"Would it be a consideration to implement MDXfind as an attack alternative?
Not sure how possible it is to distribute tasks with it.","Hmm, this would be interesting to look at. I know mdxfind has a way to do offsets now (not sure if that was implemented at the time)

What evilmog said is a fair point as well.",73367610
1275,Collect more information from the agent,open,2017-04-09T18:29:04Z,2017-04-09T18:29:12Z,,OWNER,"The client should also send some more values about the agent to the server that they can be viewed there:

- Temp for GPUs
- number of rejected combinations",,73367610
1276,[feature] ACL for connecting clients,open,2017-03-13T18:23:43Z,2017-03-13T20:02:30Z,,NONE,"Hey Guys, would it be possible to implement some sort of ACL for connecting nodes? I'm thinking about limiting access to control panel and API itself to selected IPs only. So that login page and all other features are available only for whitelisted clients. What do you think?",Ultimately its s3inlc's decision so i will defer to him.,73367610
