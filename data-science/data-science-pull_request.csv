,Created At,Updated At,Closed At,Merged At,Details,Id
0,2019-11-26T04:58:26Z,2020-02-10T03:15:18Z,,,"Check the issue #311 
some error cases when installed by pip in latest ubuntu(or CentOS). because of c compiler version issue, the `init_log()`'s parameter is empty. maybe specify the parameter is needed.",93925242
1,2019-11-25T13:26:15Z,2019-12-09T09:44:11Z,,,"In current version support of sparse matrixes was unexpectedly depricated and data conversion was switched to `DMatrix` function only, which returns this message if we try to add sparse matrix as input data:

[<https://github.com/aksnzhy/xlearn/blob/499a1836532e6efd9643bd1d17d76f5d874f8903/python-package/xlearn/data.py#L42>]

As previous interface included support and conversion of sparse matrixes, it would be a good point to keep it working.
Also it might be that `DMatrix` function itself could be performed to be able to solve this issue.


 ",93925242
2,2019-05-02T15:43:53Z,2019-05-07T14:41:24Z,,,"xLearn::InmemReader suffers from the problem of memory leak, which will affect the performance of training/prediction on large dataset.",93925242
3,2018-05-21T00:00:19Z,2018-05-21T00:00:19Z,,,"Imitate Solver::init_predict to add online learning feature in Solver::init_train, and add SetScaleParameter API in the Model class to support this feature.",93925242
4,2018-05-04T09:44:42Z,2018-05-07T04:29:30Z,,,"- previously build.sh will try to install xlearn to all versions of python in a machine, which is quite unreasonable. This commit fix it and now build.sh only install xlearn to the current command line env's default python",93925242
5,2018-01-09T08:17:47Z,2018-01-09T08:17:47Z,,,"Added several implementations of Unix header files, and fixed inconsistent behaviors in Windows. Build pass in VS2017 & Ubuntu.",93925242
6,2017-12-15T15:22:50Z,2018-01-25T14:16:09Z,,,,93925242
7,2017-12-13T06:38:35Z,2017-12-31T07:42:25Z,,,,93925242
8,2020-02-21T12:37:14Z,2020-02-21T12:37:14Z,,,fixes issue #107 ,80898688
9,2018-12-04T08:32:25Z,2019-07-08T10:00:35Z,,,#95 Change cumulative gain curve for adding different classes to plot and also with 'micro' and 'macro',80898688
10,2020-02-25T16:03:12Z,2020-02-25T16:03:12Z,,,seems like it requires at validation the property `experiment_timeout_minutes` and not `experiment_timeout_hours`.,145148726
11,2020-02-21T02:36:21Z,2020-02-22T01:43:41Z,,,"The issues fixed here are:

1.	Notebook run failing. Error:
```
v2 driver import failed with exception: cannot import name 'setup_wrapper'. Falling back to v1 driver.
Traceback (most recent call last):
  File ""setup_37a4b86d-1315-410d-92f5-d3ef02b27868.py"", line 11, in <module>
    from azureml.train.automl._remote_script import setup_wrapper
ImportError: cannot import name 'setup_wrapper'
ImportError: cannot import name 'automl'
```

I think we don't need the RunConfiguration with the explicit Docker IMAGE and pip command. And this is what I think is causing the error above in versions missmatch related to the Docker image+pip. I simplified the notebook not to use the RunConfiguration and it works good for me now. 

2.	The AutoMLConfig is using outdated data approach by providing X and y instead of an AML Dataset and the name of the label column. That's why the user gets this warning:
`WARNING - The AutoMLConfig inputs you have specified will soon be deprecated. Please use the AutoMLConfig shown in our documentation: https://aka.ms/AutoMLConfig`
I also fixed that in my notebook by using an AML Dataset and a label name in the AutoMLConfig class.

3.	Warning/Exception from the widget if the dataset doesn’t have a name. 
•	If `‘dataset = Dataset.Tabular.from_delimited_files(example_data)’` doesn’t require a name/Id, why are we raising that exception below?
•	This is also a bug in the widget, so I created a bug in VSO about it:  615924
 
However, I created a WORKAROUND in the notebook by saving/registering the Dataset in the Workspace, so after that the dataset has a name and the widget doesn’t raise that exception/warning.

4.	We should mention the ""Compute Instance"" instead of the ""Notebook VM"" that it mentions the notebook in its original version.
",145148726
12,2020-02-17T22:16:55Z,2020-02-17T22:16:55Z,,,,145148726
13,2020-02-08T00:38:58Z,2020-02-22T01:47:58Z,,,"AutoML README.md simplified.
AML 'Compute Instance' should be the by default way to go.
Local Conda installation is now in a segregated page so it doesn't make the home README.md so complex.
",145148726
14,2020-01-10T22:36:17Z,2020-01-28T22:10:59Z,,,fitted_model.forecast  function requires 2nd parameter. added by copying y_test and filling with nans.,145148726
15,2019-12-11T00:45:29Z,2019-12-11T18:53:25Z,,,,145148726
16,2019-11-19T21:00:15Z,2019-11-19T21:01:29Z,,,This cells `test_data` overwrite changes the type from `TabularDataset` to `DataFrame` this breaks cells from re-executing and can make the examples hard to follow and retool.,145148726
17,2019-11-14T20:38:34Z,2019-11-14T20:38:34Z,,,"Running the code in local, there is a key error when using 'from sklearn.externals import joblib'

It is because the difference version of joblib.
As there is a newer version of joblib, I recommend using newer joblib by correcting 'from sklearn.externals import joblib' to 'import joblib'
Otherwise, there is possibility to get key error in local. THX",145148726
18,2019-11-14T17:38:19Z,2019-11-14T17:38:19Z,,,"Changed
1.
1.

to
1.
2.",145148726
19,2019-10-21T17:05:09Z,2019-10-21T17:05:09Z,,,`accees` => `access`,145148726
20,2019-10-14T12:37:19Z,2019-10-15T04:59:24Z,,,Add reference to missing ContainerImage,145148726
21,2019-10-02T07:20:09Z,2019-10-08T07:43:41Z,,,,145148726
22,2019-09-27T13:48:09Z,2019-09-27T13:48:09Z,,,"Adding references to the following repos
(https://github.com/microsoft/nlp)
(https://github.com/Microsoft/Recommenders)
(https://github.com/Microsoft/ComputerVision)",145148726
23,2019-09-17T16:54:38Z,2019-09-19T23:56:21Z,,,Update notebook to adapt dataset diff dependency switch from azureml.contrib.datadrift to azureml.core which won't ignore column not existing warning.,145148726
24,2019-08-21T20:32:10Z,2019-08-21T20:32:10Z,,,,145148726
25,2019-08-19T16:17:17Z,2019-08-19T16:18:15Z,,,,145148726
26,2019-08-14T22:34:17Z,2019-08-15T17:51:09Z,,,"This is something I did a few weeks ago as I was debugging my own pipeline. For me, this is a a clearer and more robust way to do train test split w/ `AutoMLStep` and `PipelineData` objects. The changes I made:
1) using `os.path.join()` instead of `+` to concat strings
2) `testTrainSplitStep` has 1 `PipelineData object instead of 4
@sanpil @yanrez @j-martens let me know what y'all think",145148726
27,2019-08-08T21:11:24Z,2019-08-08T21:11:24Z,,,"- Removed mention of outdated Python SDK client code, kept the C# example since I couldn't find the API reference.
- Updated hardcoded `'images'` that was used as a tensor input name to `in_images.name`. Since the name is configurable early on, it shouldn't be hardcoded later on.",145148726
28,2019-08-06T10:55:58Z,2019-08-06T10:55:58Z,,,"It's called ""cpu-cluster"" but in fact creates a cluster based on NC_6 VMs. This change fixes that and uses DS3_V2 VMs instead.",145148726
29,2019-07-31T23:24:33Z,2019-07-31T23:24:33Z,,,,145148726
30,2019-07-30T21:41:01Z,2019-07-31T23:22:40Z,,,"There are rendering problems with html tags, trying to fix it.",145148726
31,2019-07-29T18:05:09Z,2019-07-29T18:35:33Z,,,@rastala ,145148726
32,2019-07-25T00:33:16Z,2019-07-25T00:33:16Z,,,,145148726
33,2019-07-12T14:33:28Z,2019-07-12T14:33:28Z,,,,145148726
34,2019-07-11T16:25:13Z,2019-07-11T16:25:13Z,,,,145148726
35,2019-07-09T12:39:26Z,2019-07-09T12:39:26Z,,,Replacing hyperdriveRunConfig with HyperDriveConfig,145148726
36,2019-07-08T20:11:04Z,2019-07-08T20:11:04Z,,,,145148726
37,2019-07-03T17:58:55Z,2019-07-03T18:00:39Z,,,#sign-off,145148726
38,2019-07-02T19:48:47Z,2019-07-02T19:48:47Z,,,Updated information on quota request. The resulting error otherwise was difficult to parse.,145148726
39,2019-06-27T22:28:24Z,2019-06-27T22:28:24Z,,,"PredictionClient is in azureml.accel. Azureml.accel.client was throwing ""module not found"" error.",145148726
40,2019-06-25T02:39:49Z,2019-06-25T18:53:56Z,,,Automated notebook push,145148726
41,2019-06-24T18:42:09Z,2019-06-24T18:44:46Z,,,print exception instead of raising it,145148726
42,2019-06-20T19:25:25Z,2019-08-12T17:34:03Z,,,,145148726
43,2019-06-14T04:04:37Z,2019-06-14T04:04:37Z,,,fixed typo,145148726
44,2019-06-08T11:03:27Z,2020-02-10T18:14:56Z,,,"The PredcitionClient class is no longer available in: azureml.accel.client .
According to the official [docs](https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.predictionclient?view=azure-ml-py) it is in : azureml.accel .",145148726
45,2019-06-05T00:41:10Z,2020-03-18T21:08:50Z,,,,145148726
46,2019-05-31T18:50:45Z,2019-05-31T18:55:25Z,,,Two tutorials that go form scratch to dpeloyment with appropriate starter code. Tutorial 1 is a happy path and tutorial 2 trains with remote data (iris dataset uploaded to a blob store).,145148726
47,2019-05-17T16:59:17Z,2019-05-17T16:59:17Z,,,Apply commit 3b41faa87154793cec10d5714a690d494cec9055 from the https://github.com/horovod/horovod/commits/master/examples/pytorch_mnist.py,145148726
48,2019-05-16T03:56:21Z,2019-05-20T14:39:19Z,,,,145148726
49,2019-05-09T14:31:01Z,2019-05-09T14:31:01Z,,,"update list of metric and object to be logged in the description, add Logging images that was missing",145148726
50,2019-04-30T04:29:14Z,2019-04-30T04:29:14Z,,,"This is the sample notebook using the BYOC path to deploy an ONNX model on GPU for acceleration with TensorRT.

NOTE: This notebook uses the v1.0.23 SDK and related APIs.",145148726
51,2019-04-22T16:39:08Z,2019-04-22T16:39:08Z,,,Spelling correction,145148726
52,2019-04-22T03:44:03Z,2019-04-22T03:44:03Z,,,The data reference `ds_data` is not referenced in the tutorial or training script.,145148726
53,2019-04-12T22:29:21Z,2019-04-12T22:29:21Z,,,,145148726
54,2019-04-12T21:31:01Z,2019-04-12T21:31:01Z,,,,145148726
55,2019-04-12T14:50:08Z,2019-04-12T14:50:08Z,,,A recent hack involving AzureML provided feedback on areas for improvement.,145148726
56,2019-04-08T23:12:33Z,2019-05-13T20:39:37Z,,,,145148726
57,2019-04-08T20:51:08Z,2019-04-08T20:51:08Z,,,,145148726
58,2019-04-07T19:47:10Z,2019-04-08T22:07:29Z,,,,145148726
59,2019-04-04T17:30:58Z,2019-04-04T17:31:04Z,,,@rastala @vizhur ,145148726
60,2019-03-31T15:02:10Z,2019-03-31T15:02:10Z,,,,145148726
61,2019-03-23T13:08:26Z,2019-03-23T13:08:26Z,,,,145148726
62,2019-03-09T07:46:55Z,2019-03-09T07:46:55Z,,,For onnxruntime v0.3.0 we're adding a dependency on openmp. This PR is to ensure that the image has that when it is created.,145148726
63,2019-01-29T16:01:47Z,2019-04-17T14:30:37Z,,,Otherwise it ='s returning an object who's not == 0 for me.,145148726
64,2019-01-29T15:38:55Z,2019-03-20T20:30:04Z,,,It's not working or it's missing the documentation related to it.,145148726
65,2019-01-29T07:45:16Z,2019-01-29T07:45:16Z,,,"If user skips the first cell as they already completed the training, then they need to import os here again.",145148726
66,2019-01-14T21:43:26Z,2019-03-12T17:29:55Z,,,,145148726
67,2019-01-09T22:35:54Z,2019-01-09T22:35:54Z,,,,145148726
68,2019-01-01T02:15:26Z,2019-01-07T19:44:45Z,,,,145148726
69,2018-12-19T08:28:17Z,2018-12-19T08:28:17Z,,,"Compute name is invalid, it should start with a letter, be between 2 and 16 character, and only include letters (a-zA-Z), numbers (0-9) and \'-\'""",145148726
70,2018-12-18T15:38:47Z,2018-12-18T15:38:47Z,,,Typo,145148726
71,2018-11-30T03:59:00Z,2019-04-23T17:35:32Z,,,"@savitamittal1 
@rastala 
@hning86 ",145148726
72,2018-11-27T23:40:56Z,2018-11-27T23:40:56Z,,,typo fix,145148726
73,2018-11-21T06:34:43Z,2018-11-21T06:34:43Z,,,,145148726
74,2018-11-14T20:32:02Z,2018-11-14T20:32:02Z,,,"removed y_hat, doesn't seem to be used. maybe i'm missing something. ",145148726
75,2018-11-04T14:31:20Z,2018-11-04T14:31:20Z,,,"Current code doesn't run if I try to attach existing DSVM using the given code. It's because the naming rule for the RemoteVM doesn't allow _ (underscore). I've removed the underscore to make it run successfully, then changed the variable name from `attached_dsvm_compute` to the original one (`dsvm_compute`) to avoid additional changes as `dsvm_compute` is referenced in the following codes.",145148726
76,2020-02-14T16:42:34Z,2020-03-11T20:44:42Z,,,"Fix deprecated Werkzeug imports removed in version 1.0.0:
- `werkzeug.url_encode` -> `werkzeug.urls.url_encode`
- `werkzeug.secure_filename` -> `werkzeug.utils.secure_filename`

This PR solves issue https://github.com/airbnb/knowledge-repo/issues/531.",65949398
77,2019-08-28T20:55:37Z,2019-08-28T21:01:56Z,,,"Tried to use this method to enforce consistency in usernames but it has a bug -- seems like in general this functionality is somewhat deprecated but as long as it is still around, might as well raise the correct error! 

",65949398
78,2019-06-05T14:12:30Z,2019-06-13T07:40:36Z,,,"Fixes #516

### Description of the changeset

We have modified the plain and rich mails to include more information. For the rich email, we have included a new template which shows a preview of the post, along with its thumbnail and related data, with a link to check the full article in the Knowledge Repo.

#### Preview of the rich email

![Screenshot 2019-06-05 at 15 38 03](https://user-images.githubusercontent.com/483532/58960671-ef902400-87a7-11e9-869f-2ba5fc8899bd.png)

### Test Plan:

Current tests have been modified.

#### Test results

![Screenshot 2019-06-05 at 08 37 23](https://user-images.githubusercontent.com/483532/58936726-ba1c1400-8770-11e9-84fc-68835575305a.png)",65949398
79,2019-03-25T15:03:07Z,2019-03-25T15:11:19Z,,,"Description of changeset: 
Potential fix for #493. 

This change creates a new policy for viewing /tag_pages anonymously, and changes the permissions required to view /tag_pages from post_comment to tags_view.

Test Plan: 
I didn't see any existing tests around policies/roles/permissions, so I just confirmed that the existing test suite still passes with these changes.

Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
80,2019-03-25T14:58:24Z,2019-03-25T15:04:46Z,,,"Description of changeset: Configuration option to collapse all code in notebook as default display option for a knowledge post.

Test Plan: 


Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
81,2019-03-23T17:39:50Z,2019-03-23T17:44:48Z,,,"Description of changeset:
This is a quick patch for #489, which was an issue for me as well. I just updated the regex in the `KnowledgePost` `read`/`write` methods to match `\r` in addition to `\n` to avoid having to manually deal with CRLF/LF conversions.

Test Plan: 
1. Build a knowledge repo where the `knowledge.md` file has CRLF line endings.
2. Run `knowledge_repo --repo <repo-path> preview <post-path>` (this was the command that failed for me and in the original issue) to make sure YAML headers are now correctly extracted.

Full disclosure, I haven't tested the `write` method with the regex change, but I assumed it was best to keep the three header regexes consistent.

Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
82,2019-03-19T20:14:47Z,2019-03-20T01:01:21Z,,,"Description of changeset: 

When adding an ipython notebook, a postprocessor scans for images in the JSON by searching with a regex for markdown-style images like `[tag](url)`. The regex is as you see below. 

I had a cell with some inline Javascript like: `![123]...lots of stuff...](...stuff...)`. This matched as an image because the reluctant quantifier still forces it to expand the match of the tag until it sees ""]("". It shouldn't match.

The current regex looks for ""space, or not space"" in between the square braces. That didn't sound right -- why not "".""? But in any event my guess is the intent is to match ""anything except a closing square brace"".

This at least made my case work. Submitted for your consideration.


Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
83,2019-01-17T18:55:07Z,2019-03-07T15:19:32Z,,,"Description of changeset: 
Make it easy to setup knowledge repo on a local machine.  

Test Plan: 
1. download docker
1. build an image by running `docker build -t knowledge -f docker/Dockerfile.dev .`
1. find a folder where your notebooks are and executing the following command with the folder
1. run the docker image by `docker run --rm -it -p 7000:7000 -v <notebook-folder-path>:/data -e KNOWLEDGE_REPO=test_repo knowledge`  


Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
84,2019-01-08T18:46:52Z,2019-01-08T18:58:22Z,,,"Description of changeset: 
Potential fix for #483 

I ran into the same behavior today as I did in #483 with different unicode character in the body. I'm not sure if other people are running into the same errors as me but this seems to fix the problem. 

I copied the logic in the `KnowledgePost.read` function into the `KnowledgePost.write` function in order to extract just the header which is passed into `KnowledgePost._get_headers_from_yaml` (which was where the code was breaking). This allowed me to have unicode characters in the rest of the notebook and still have the notebook processed correctly.

Let me know if this makes sense, happy to rework this as needed

Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
85,2018-05-15T15:18:33Z,2018-12-17T21:50:43Z,,,"**Description of changeset:**

Implements a simple auth blueprint for LDAP user verification. Currently uses username entered into the HTML from as the uid/cn for verification. I'm only familiar with how my organization has implemented this, but others can implement a username parsing function if they want.

Also updated the login page to look a little better, and require a password.

**Test Plan:**

I tested this using [this LDAP docker image](https://github.com/rroemhild/docker-test-openldap), but it would take some work to add this to the travis/appveyor tests. Any ideas on this?

Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
86,2018-04-01T23:15:11Z,2018-12-17T21:50:42Z,,,"Description of changeset:

In order to handle plotly plots, we use the rmarkdown::render function, which is a bit more capable than knitr. We render the Rmd to html format, with the option to keep the intermediate .md file. This is almost the same as the knitr output, but contains divs and a json description of the plots.

Additionally some javascript is necessary to have the actual plots rendered. I have a somewhat lazy check for whether we need to add the dependency. This could be improved on by check each code block for either library(plotly) or plotly::.

I also introduced a new dependency (python-frontmatter) to add the javascript tags after the frontmatter. There might be another way of doing this where this additional package isn't needed.

Test Plan:
A new post is included with three different plotly plots.

Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
87,2017-07-07T06:30:35Z,2018-12-17T21:50:42Z,,,"This PR addresses #298 by adding support for exporting knowledge posts to PDFs using `weasyprint`, as well as exposing support for this format in the web app user interface. At this point, the code works, but there are no stylesheets are applied, and so it is very much a work in progress.

Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
88,2017-06-13T14:21:25Z,2018-12-25T04:46:50Z,,,"This PR:
- Dockerizes the knowledge_repo application, using an image based on the current Travis build environment
- Updates the Travis build/test script to use the dockerized application
- Adds a system test container to check that the application is being served correctly (this consists of a stub for now with just one test to check that GET /feed returns a page)
- Deploys a Docker image to Docker Hub, providing that the environment variables DOCKER_USERNAME, DOCKER_PASSWORD and DOCKER_REPO are configured in Travis
- Adds a requirements.txt so that prerequisites can be installed in a Docker layer without running setup.py (improves caching of Docker layers during development)
- Adds environment variables PYPI_USER, PYPI_PASS_SECURE and PYPI_REPO to make the deploy process to PyPI configurable in Travis (and prevent deploy being triggered from an outside fork like this one!)

Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
89,2017-05-16T21:30:09Z,2018-12-17T21:50:41Z,,,"Description of changeset: 

Adds support for [orgmode](http://orgmode.org/) documents. 

We decided to keep the orgmode [export settings](http://orgmode.org/manual/Export-settings.html) syntax instead of requiring a YAML header, so that `#+TITLE: the title` gets translated to `- title: the title`. 

In the cases where there is no convention for an orgmode document (e.g. the `updated_at` field of the metadata YAML), we defined as prefixed by KR_, for knowledge repo (so `KR_UPDATED_AT`). 

I would especially like your thoughts on that.

Test Plan: 


Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
90,2016-12-19T15:26:54Z,2018-12-17T21:50:41Z,,,"This automatically generates the current timestamp in the template and resolves #103 

Auto-reviewers: @NiharikaRay @matthewwardrop @earthmancash @danfrankj
",65949398
91,2016-11-04T07:28:17Z,2018-12-17T21:50:41Z,,,"This PR is the beginnings of some miscellaneous cleanups thoughout the app, particularly as pertaining to data model usage. This is not urgent, but clears the way for some upcoming PRs regarding url and reindexing cleanups. Also no where near finished.",65949398
92,2020-03-18T22:29:27Z,2020-03-18T22:29:27Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
93,2020-03-18T17:35:11Z,2020-03-18T17:37:58Z,,,"Improve readability to help learners sort out where the values come from.

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
94,2020-03-18T15:46:28Z,2020-03-18T17:37:38Z,,,"This allows a straightforward comparison for beginners.

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
95,2020-03-18T02:41:19Z,2020-03-20T02:49:45Z,,,"# Summary 
This readme links to tensorboard_embeddings_mnist.py, which currently does not exist:
https://github.com/keras-team/keras/blob/master/examples/README.md

### Related Issues
Solves #13719 .
### PR Overview
Added the pertinent code (namely,  Tensorboard embeddings mnist) from keras.io

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
96,2020-03-12T18:46:53Z,2020-03-12T18:46:53Z,,,"### Summary
TensorFlow 2.0 has changed the behavior of Batch Normalization to be in inference mode when the `trainable` parameter is set to False. Reference from the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization#output_shape_2):

> ...in the case of the BatchNormalization layer, setting trainable = False on the layer means that the layer will be subsequently run in inference mode... This behavior has been introduced in TensorFlow 2.0, in order to enable layer.trainable = False to produce the most commonly expected behavior in the convnet fine-tuning use case.

2 years ago, I have submitted a PR (#9965) that changed Keras' BN behaviour to what TF uses tofday but it was not merged. I submit it for reconsideration, hoping we can align the two behaviors.

I'm happy to make changes to the PR to see it getting merged to master.

### Related Issues

#9214 
#10014
#10045
#11803
",33015583
97,2020-03-09T19:08:27Z,2020-03-10T18:46:14Z,,,"the code example does not reflect the description so it has been updated

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
98,2020-03-08T18:55:29Z,2020-03-08T18:56:16Z,,,"Updating TensorBoard Class to make it compatible to tensorflow 2.0. This is done to solve the issue #13870. Earlier, we were unable to call TensorBoard Class, because it was importing from tensorflow.contrib while new version of tensorflow import with tensorflow.compat.v1 . This problem was preventing many users to call several functions inside TensorBoard Class such as _write_logs.

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [x] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
99,2020-03-06T06:49:28Z,2020-03-06T07:07:18Z,,,"I think it is significant for 1d sequence processing.

and there is also a bug in the original implement while: 1. data_format != 'channels_last'; 2. seq_length is uncertain.

",33015583
100,2020-03-04T09:12:44Z,2020-03-04T09:12:44Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary
Get error as:
'''
Using TensorFlow backend.
Traceback (most recent call last):
  File ""cnn_seq2seq.py"", line 68, in <module>
    input_text, target_text = line.split('\t')
ValueError: too many values to unpack (expected 2)
'''
### Related Issues

### PR Overview

- [N] This PR requires new unit tests [y/n] (make sure tests are included)
- [N] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [Y] This PR is backwards compatible [y/n]
- [N] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
101,2020-03-03T04:44:36Z,2020-03-03T04:45:28Z,,,"shortened the declaration code for  x & y

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [x] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
102,2020-03-01T10:22:21Z,2020-03-01T10:22:21Z,,,"
### Summary
`num_predictions` variable is not used in this example

### Related Issues
None

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
103,2020-02-24T08:25:06Z,2020-02-24T08:25:06Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
104,2020-02-20T03:26:23Z,2020-02-21T06:51:37Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

Change the loop index so that the embedding matrix is correctly built.

### Related Issues

This embedding_matrix[i] = embedding_vector creates an incorrect embedding vector, and it should be embedding_matrix[i - 1] = embedding_vector.

Here's why the embedding matrix is wrong and what it can causes. The loop starts from index 1 in word_index.items(), so this ""i"" starts as 1. Therefore, it always ignores the first row of the embedding matrix (leaving it all zeros) and shifts every word embedding vector backward, so that each word has the vector of its previous word. Having an ""i - 1"" resolves this issue because once we loop from the 0 index of the embedding matrix, every word will have its correct corresponding vector.

The Keras blog post can be found at [here](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)",33015583
105,2020-02-19T09:15:37Z,2020-02-19T09:15:37Z,,,"using sudo gives much more privileges to pip avoid using sudo by using --user with pip command

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
106,2020-02-17T20:32:13Z,2020-02-17T20:32:13Z,,,"When visiting a site that returns a 404 error, for example ""keras.io/atreatthth"", using the searchbar functionality results in the browser navigating to ""https://search.html/?q=SEARCH_TEXT"". This was documented in DOCS BUG #13736.

This is due to the searchbar form ""action"" attribute being set to ""//search.html"" on the 404-returning sites. 

By changing the searchbox to always be ""action=""./search.html"""", instead of {{base_url}} /search.html, I believe this issue can be avoided.

<!--
![searchbox_action_example](https://user-images.githubusercontent.com/55195974/74684749-4735ba80-5192-11ea-9228-0bf28436a343.png)

Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
107,2020-02-14T16:01:40Z,2020-02-17T17:46:55Z,,,"Wrong backend call on output shape computation in Conv2DTranspose

### Related Issues
Shape inference (implicit or explicit) was not working (undefined shapes). Now it works.

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [X] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
108,2020-02-14T09:51:10Z,2020-02-14T18:07:09Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
109,2020-02-14T03:34:52Z,2020-02-14T03:36:43Z,,,"it should be gradient which loss wrt the dream rather than dream wrt the loss.

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
110,2020-02-09T09:19:11Z,2020-03-02T09:56:25Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary
Removes unnecessary variable `clip_norm` in the example https://github.com/keras-team/keras/blob/master/examples/mnist_irnn.py

### Related Issues
Fixes #13734 (support issue) 

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
111,2020-02-08T16:03:00Z,2020-02-08T19:31:49Z,,,"type: feature
files added: none
description: adding classifier and regressor mixins

This pr closes: https://github.com/keras-team/keras/issues/13754

Not sure if any tests need to be updated since this just conforms to scikit-learn api.  But happy to add a test or two with some guidance!",33015583
112,2020-02-05T09:17:00Z,2020-02-05T09:17:00Z,,,"defined  it

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
113,2020-02-04T11:06:15Z,2020-02-04T11:06:15Z,,,"sudo with pip make available global administrative access and should not be used with pip otherwise it conflicts with pip commands in future instead sudo use --user.

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
114,2020-01-29T09:02:51Z,2020-01-29T09:03:30Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
115,2020-01-28T02:40:45Z,2020-01-28T02:43:38Z,,,"### Summary

A fix for this issue is given in two separate commits. 

The first changes a bug fix for issue #12171. That bug had caused newline and tab control characters to be interpreted as actual whitespace instead of as \n and \t. However, the previous fix had been to escape these characters in the code_snippet method, which made it impossible to add any intentional newlines before this point in the the method headers' rendering. The escaping of the control characters was moved to get_function_signature and applied only to string values of keyword arguments (since other parts of the headers won't contain control characters).

The second commit updates autogen.py to spread long class and method headers over multiple lines (#13701). The arguments are all printed on the initial line and then each keyword argument is printed on its own indented line.

### Related Issues
keras docs: long \<code\> lines need to be rearranged to short lines to be easier to read #13701
Fix documentation rendering issue #12171 

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
116,2020-01-27T00:25:59Z,2020-01-27T00:25:59Z,,,"### Summary
Some code in scikit-learn checks the `_estimator_type` attribute to distinguish between Regressors and Classifiers. For example, [this one](https://github.com/scikit-learn/scikit-learn/blob/b194674c42d54b26137a456c510c5fdba1ba23e0/sklearn/model_selection/_split.py#L2003):
```
    if isinstance(cv, numbers.Integral):
        if (classifier and (y is not None) and
                (type_of_target(y) in ('binary', 'multiclass'))):
            return StratifiedKFold(cv)
        else:
            return KFold(cv)
```
where `classifier` parameter is calculated using [`is_classifier()`](https://github.com/scikit-learn/scikit-learn/blob/b194674c42d54b26137a456c510c5fdba1ba23e0/sklearn/base.py#L639) function.

As a consequence, if we pass `KerasClassifier` to `GridSearchCV()` with numeric `cv` argument
```
clf = KerasClassifier(...)
...
grid = GridSearchCV(estimator = clf, param_grid = params, n_jobs = 1, cv = 10)
```
the `KFold` splitter would be created instead of expected `StratifiedKFold` because `KerasClassifier` contains no `_estimator_type` and, therefore, treated as regressor.

Adding such an attribute to `KerasClassifier` and `KerasRegressor` seems be a reasonable improvement solving this problem.

An alternative solution would be using [`ClassifierMixin`](https://github.com/scikit-learn/scikit-learn/blob/b194674c42d54b26137a456c510c5fdba1ba23e0/sklearn/base.py#L339) and [`RegressorMixin`](https://github.com/scikit-learn/scikit-learn/blob/b194674c42d54b26137a456c510c5fdba1ba23e0/sklearn/base.py#L372) as base classes for their corresponding wrappers but this would introduce a dependency from `scikit-learn` code.



### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)

Not sure about backward compatibility. The proposed change doesn't break the existing code base but it may change the produced results.",33015583
117,2020-01-21T00:31:01Z,2020-01-21T00:31:01Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

The history dictionary for the model seems to be wrong for accuracy, when I tried the visualisation example, the correct key was `accuracy` and `val_accuracy` not `acc` and `val_acc`

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
118,2020-01-20T17:42:54Z,2020-02-12T23:47:43Z,,,"[experimental_list_devices is removed on Tensorflow 2.1](https://github.com/tensorflow/tensorflow/releases). tf.config.list_logical_devices can be used to retrieve the same information. 

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary
experimental_list_devices is removed on Tensorflow 2. tf.config.list_logical_devices can be used to retrieve the same information.
### Related Issues
https://github.com/keras-team/keras/issues/13684
### PR Overview

- [ n] This PR requires new unit tests [y/n] (make sure tests are included)
- [ n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ y] This PR is backwards compatible [y/n]
- [ n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
119,2020-01-19T20:57:17Z,2020-01-19T21:07:52Z,,,"- Fix tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead
- Fix tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
120,2020-01-15T08:31:22Z,2020-01-15T08:50:45Z,,,"### Summary

Hi!


[Optuna]( (optuna.org) ) is a new hyperparameter optimization library, and we’ve written an integration module for Keras to make it easy to use Optuna to search for good hyperparameter settings and prune unpromising trials. We’re looking for ways to let Keras users know this is available and thought a badge to show Optuna integration is available would be helpful.

Here’s our example of using the pruning integration with Keras: https://github.com/optuna/optuna/blob/master/examples/pruning/keras_integration.py

If there are other ways you would recommend reaching out to Keras users to let them know about Optuna, please let us know.

Thanks!

### Related Issues

None

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [n] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
121,2020-01-13T18:21:31Z,2020-01-14T16:49:28Z,,,"Added utility to stop iteration early by setting appropriate values of ""patience"" and ""min_delta"".
And also added utility to visualize train and validation loss as well as accuracy.",33015583
122,2020-01-13T10:03:55Z,2020-01-13T10:03:55Z,,,"### Summary
Looks like the documentation for ReduceLROnPlateau was mistakenly overwritten with the documentation from EarlyStopping. This patch fixes that.

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
123,2020-01-10T07:27:45Z,2020-03-06T06:39:36Z,,,"Even we do not mask zero, we may has other mask from previous layer. Return None directly is not very rigorous.",33015583
124,2020-01-09T07:23:55Z,2020-01-09T07:24:30Z,,,"…orflow_backend.py

Using tensorflow backend, any(x) and all(x) return a tensor of dtype with bool, not uint8.

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
125,2020-01-03T16:38:59Z,2020-01-03T16:38:59Z,,,"At line 281 the initialization of the labels should be multiplied by -1 as in the build_word_list method at line 227, otherwise, when filling with the blank label at line 293 the rest of the label in the blank inputs have the value 1 which is a valid character (i.e b).

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
126,2020-01-01T09:14:39Z,2020-01-18T20:00:07Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary
In the method 'fit' of class 'Model'  in keras/engine/training.py,
When performing step wise training('steps_per_epoch'), if validation_data is provided, validation_steps need to be specified.
### Related Issues
issue  #13652 
### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
127,2019-12-24T13:11:23Z,2019-12-26T05:10:21Z,,,"
I get a warning like this:
```
DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
    from collections import Iterable
```

Here is the picture:

[https://i.imgur.com/SAot5Zd.png](https://i.imgur.com/SAot5Zd.png)

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [N] This PR requires new unit tests [y/n] (make sure tests are included)
- [N] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [Y] This PR is backwards compatible [y/n]
- [N] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
128,2019-12-14T10:36:54Z,2020-03-02T10:02:06Z,,,"Closes #13634

",33015583
129,2019-12-11T10:41:24Z,2019-12-11T10:41:24Z,,,"Currently, if we run lstm_seq2seq.py, and then run lstm_seq2seq_restore.py, we'll get different results, because we only pad the input sequence in lstm_seq2seq.py.
This PR pad the input sequence in lstm_seq2seq_restore.py. After this PR, if we run lstm_seq2seq.py, and then run lstm_seq2seq_restore.py, we'll get the same result.",33015583
130,2019-12-11T06:47:37Z,2019-12-11T06:47:37Z,,,"Adding a new optimization algorithm named- COntinuous COin Betting(https://arxiv.org/pdf/1705.07795.pdf)
This algorithm gives a faster convergence rate compared to some of the other algorithms.
",33015583
131,2019-12-10T10:50:41Z,2020-03-20T13:44:27Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary
Modified past version of the ""cifar10_cnn_capsule.py"" so that it works with the latest version. TF/Keras changed the batch_dot implementation, so this version skips batch_dot in favor for an included ""caps_batch_dot"" function.

### Related Issues
Relies on tf.matmul, because K.dot modifies the dimensions, due to the Numpy implementation.

### PR Overview

- [ n ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ y ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ y ] This PR is backwards compatible [y/n]
- [ n ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
132,2019-12-10T00:16:05Z,2019-12-10T00:16:15Z,,,"this commit was generated automatically on repl.it

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
133,2019-12-08T14:30:35Z,2019-12-08T14:30:35Z,,,This particular example in the docs is confusing because the variable name for the validation generator is `test_datagen`. Obviously a test set is not the same as a validation set and using appropriate variable names avoids confusing the two.,33015583
134,2019-12-08T14:29:35Z,2019-12-08T14:29:35Z,,,This particular example in the docs is confusing because the variable name for the validation generator is `test_datagen`. Obviously a test set is not the same as a validation set and using appropriate variable names avoids confusing the two.,33015583
135,2019-12-06T19:08:00Z,2019-12-06T19:18:46Z,,,"Please consider this update to the text generation example.

The example, as it is, uses one-hot encoding and can quickly start
consuming large amounts of memory, for example, when running against
training data that has a broader set of chars like code.
Instead, one can leverage sparse_categorical_crossentropy and achieve
the same results.

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
136,2019-12-06T17:50:51Z,2019-12-08T15:11:08Z,,,"### Summary
Fixes `self._dynamic_display` not being set to true for printing out the verbose training updates. Previously in PyCharm, it would print a new line every update to the progress bar and with this change it works as expected, clearing each previous update to the line and removing the annoying bug of many, many fast printing lines to the console.",33015583
137,2019-12-02T17:48:43Z,2020-01-27T18:38:29Z,,,"

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

For compatibility with the scikit-learn `clone` function `get_params` has to return the same parameters as given in the __init__ function. Fix issue #13586

### Related Issues

#13586

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)",33015583
138,2019-11-30T06:02:02Z,2019-11-30T07:38:32Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary
I have added an example in vision section of street signs classification in 40 different classes.
### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
139,2019-11-29T15:34:45Z,2019-11-29T15:34:45Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

This PR fixes the issue https://github.com/keras-team/keras/issues/13584
The following equation does not match the one mentioned in the paper (equation 4 in http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)
```
return K.mean(y_true * square_pred + (1 - y_true) * margin_square)
```
The actual loss equation is

![equation](https://user-images.githubusercontent.com/3022518/69878771-1e560580-12c6-11ea-976b-d58609c2e9f8.png)


### Related Issues

None


### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
140,2019-11-24T16:50:37Z,2019-11-24T16:50:37Z,,,"Layer attribute tracking was introduced in 479fc3a978221bb835f7e2781dcef0db66c96e1c by @fchollet and it included what I suspect to be a debug print statement that wasn't meant to be merged.

It causes unwanted noisy outputs when loading models in [keras-retinanet](https://github.com/fizyr/keras-retinanet) for instance.",33015583
141,2019-11-24T14:45:58Z,2019-11-24T14:45:58Z,,,we find that sometimes bias_correction in Adam will increase the risk of overfitting. Actually some varieties of Adam (such as LAMB and AdamW) has removed bias_correction. So add an option of bias_correction for Adam may be a better choice.,33015583
142,2019-11-19T14:32:28Z,2019-12-11T04:36:45Z,,,"### Summary

`extract_named_arg` changed it's given parameter `kwargs`, if it contains the extracted `arg`.
This lead to an error when calling `load_wrapper` with `filename=foo` (when not using gcs), as it expects to continue working with the original variable `kwargs`.
Therefore now a copy of `kwargs` is edited and returned.

### Issue

```python
model.load_weights(filepath=foo)
```

gave:

```
TypeError: load_weights() missing 1 required positional argument: 'filepath'
```

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
143,2019-11-15T15:13:07Z,2019-11-15T15:15:35Z,,,"The history.history dictionary seems to have changed in the meantime

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
144,2019-11-11T08:16:34Z,2019-11-17T11:57:12Z,,,"Fixed a bug in model_load function.

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
145,2019-11-10T18:30:53Z,2019-11-13T17:08:51Z,,,"### Summary
Since version 2.2.5 the load_weights function raises a TypeError while passing the filepath value with explicitly accessing the filepath parameter.

`model.load_weights(filepath=my_path)`

This does not work because the filepath is popped out of the kwargs dictionary inside the `extract_named_arg` function [at line 469](https://github.com/keras-team/keras/blob/master/keras/engine/saving.py#L469). 
When returning the load_function at [line 492](https://github.com/keras-team/keras/blob/master/keras/engine/saving.py#L492) the kwargs dictionary does not contain filepath anymore.

### Related Issues

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
146,2019-11-07T18:46:56Z,2019-11-07T18:47:08Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
147,2019-11-05T00:06:56Z,2019-11-07T20:29:12Z,,,"Error description: see https://stackoverflow.com/questions/56054312/program-shows-nonetype-object-has-no-attribute-strip

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
148,2019-10-31T17:21:34Z,2019-10-31T17:21:34Z,,,"hi
<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
149,2019-10-29T07:52:33Z,2020-03-01T17:27:10Z,,,"# Summary

In templates/optimizers.md, The parameters of SGD have been changed correctly.

+ lr -> learning_rate
+ delete decay parameter

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [x] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
150,2019-10-22T07:08:13Z,2019-10-22T07:08:13Z,,,"This Pull request is in regard of the issue #4875 which is a long thread of comments where people are experiencing issues with saving the model, which ranges from convolutional net to any basic layer network.
Summary

We provided a few updates on the saving.py python script and expected outcome was achieved as far as we tested. We hope this PR could resolve the issues which are mentioned by the people in the comments.
PR Overview

    [N] This PR requires new unit tests [y/n] (make sure tests are included)
    [Y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
    [Y] This PR is backwards compatible [y/n]
    [N] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
151,2019-10-21T12:07:22Z,2020-02-07T14:25:30Z,,,"### Summary
Due to a difficult migration to tf2, I plan to disable tf2 behavior for now in our project (using `tf.compat.v1.disable_v2_behavior`). However, with tf2 behavior disabled, I run into the following runtime error:

```shell
Traceback (most recent call last):
  File ""keras_retinanet/bin/train.py"", line 528, in <module>
    main()
  File ""keras_retinanet/bin/train.py"", line 523, in main
    validation_data=validation_generator
  File ""/usr/lib/python3.7/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""/usr/lib/python3.7/site-packages/keras/engine/training.py"", line 1732, in fit_generator
    initial_epoch=initial_epoch)
  File ""/usr/lib/python3.7/site-packages/keras/engine/training_generator.py"", line 260, in fit_generator
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/usr/lib/python3.7/site-packages/keras/callbacks/callbacks.py"", line 152, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/usr/lib/python3.7/site-packages/keras/callbacks/callbacks.py"", line 1036, in on_epoch_end
    logs['lr'] = K.get_value(self.model.optimizer.lr)
  File ""/usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py"", line 2927, in get_value
    return x.numpy()
  File ""/usr/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py"", line 579, in numpy
    ""numpy() is only available when eager execution is enabled."")
NotImplementedError: numpy() is only available when eager execution is enabled.
```

The reason for this is that `tensorflow_backend` uses the version of tensorflow to decide if it will execute eagerly or if it will use a `tf.Session` to evaluate a variable. A better check would be to check if tensorflow is executing eagerly or not, regardless of its version. This PR makes the necessary changes for that.

### Related Issues

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
152,2019-10-21T07:46:10Z,2019-10-21T08:01:46Z,,,"The phrase ""axis that should be normalized"" is ambiguous and unclear.  

See: https://stackoverflow.com/a/47549590/6296435

Many people understand it to be taking mean and std **along** that axis, i.e. collapse that dimension and preserve all other dimensions. This is also the behavior of np.mean, and other operations.  

But this is obviously the opposite of what is going on here, and so the documentation should be corrected to reflect this.

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
153,2019-10-20T17:09:53Z,2019-10-20T17:09:53Z,,,"This Pull request is in regard of the issue #4875 which is a long thread of comments where people are experiencing issues with saving the model, which ranges from convolutional net to any basic layer network.

### Summary
We provided a few updates on the saving.py python script and expected outcome was achieved as far as we tested. We hope this PR could resolve the issues which are mentioned by the people in the comments.

### PR Overview

- [N] This PR requires new unit tests [y/n] (make sure tests are included)
- [Y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [Y] This PR is backwards compatible [y/n]
- [N] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
154,2019-10-19T17:18:44Z,2019-10-21T08:12:52Z,,,"Fix spelling in README.md
I found a better expression of language to understand.
line 42)  User friendliness -> User-friendliness
line 48) No separate models configuration files in a declarative format. -> No separate models' configuration files in a declarative format.

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
155,2019-10-19T12:15:43Z,2019-10-20T03:58:11Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary
changed conv1d to conv1D
changed `channels_first`, `channels_last` to `""channels_first""`, `""channels_last""`.
### Related Issues

### PR Overview

- [n ] This PR requires new unit tests [y/n] (make sure tests are included)
- [y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
156,2019-10-19T07:21:29Z,2019-10-19T07:21:29Z,,,"Updated the Embeddings layer code block documentation to reflect the correct type of input

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
157,2019-10-18T09:32:18Z,2019-10-18T09:32:18Z,,,"

### Summary
Give the option to override save method of ModelCheckpoint 

### Related Issues

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
158,2019-10-17T13:52:03Z,2019-10-19T16:17:15Z,,,"explicit explanation on `__monitor__` argument in ModelCheckpoint

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [n] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
159,2019-10-16T19:05:19Z,2019-10-18T08:08:06Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
160,2019-10-15T01:09:36Z,2019-10-18T08:08:55Z,,,"correct a numpy to a NumPy in dot string

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ ] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
161,2019-10-14T18:10:50Z,2019-10-29T07:48:33Z,,,"### Summary
Changed more intuitively while maintaining the unity of sentence as follows:
+ CIFAR10 small image classification
+ CIFAR100 small image classification
+ MNIST database of handwritten digits
+ Fashion-MNIST database of fashion articles
+ Boston housing price regression dataset

Update arguments of dataset
+ MNIST --> 'path'
+ Boston --> ""test_split"" and ""seed""

fixed Numpy -> NumPy in Boston housing price regression dataset

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [n] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
162,2019-10-14T16:02:04Z,2019-10-14T16:23:50Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

There were two separate documentations for `validation_steps`.
Merged them into one.

### Related Issues

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
163,2019-10-14T11:11:18Z,2019-10-17T08:34:05Z,,,"### Summary
This PR adds a new PrecisionAtRecall metric to Keras. It also fixes a couple of minor typos in the code around it.

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [X] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
164,2019-10-14T06:00:21Z,2020-01-10T04:05:02Z,,,"### Summary

This PR adds EfficientNet variants. EfficientNet was published in ICML 2019 and is the current SOTA. @Callidior helped this PR.

### Related Issues

keras-team/keras-applications#113

### PR Overview

- [y] This PR requires new unit tests [y/n] (make sure tests are included)
- [y] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [y] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
165,2019-10-01T16:56:59Z,2019-10-28T14:35:46Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

I'm trying to centralize the code fetching the docstring and making the documentation across keras repositories. I made a repository called keras_autodoc with all the logic there. This repo should be moved to github.com/keras-team eventually.

Linked to https://github.com/keras-team/autokeras/pull/797

### Related Issues

### PR Overview

- [ ] This PR requires new unit tests [y/n] (make sure tests are included)
- [ ] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [x] This PR is backwards compatible [y/n]
- [ ] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
166,2019-09-18T00:41:09Z,2019-09-30T22:03:49Z,,,"If the model does not has any targets or sample_weights, the corresponding lists hold `None` values.
While `None` values still contribute to the length of the `tensors` variable, `assert len(val_data) == len(tensors)` asserts.
This faulty behavior is circumvented by refining the `tensors` variable so that it holds no more `None` values.

    Signed-off-by: Timo Korthals <tkorthals@cit-ec.uni-bielefeld.de>

<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md

Note:
We are no longer adding new features to multi-backend Keras (we only fix bugs), as we are refocusing development efforts on tf.keras. If you are still interested in submitting a feature pull request, please direct it to tf.keras in the TensorFlow repository instead.
-->

### Summary

see above

### Related Issues

None

### PR Overview

- [n] This PR requires new unit tests [y/n] (make sure tests are included)
- [n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
167,2019-07-10T09:32:25Z,2019-09-13T10:59:34Z,,,"<!--
Please make sure you've read and understood our contributing guidelines;
https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md
-->

### Summary
- After visiting https://keras.io/preprocessing/text/ , I cannot find explaination for fit_on_texts (and other methods) under Tokenizer class and subsequently for several other classes
- On the other hand if I visit Japanese docs https://keras.io/ja/preprocessing/text/ , I can find the class and the methods listed as expected.
- The `keras/docs/structure.py` has an option to enable display of method docstrings. This commit covers that.

### Related Issues
#12980 

### PR Overview

- [ n] This PR requires new unit tests [y/n] (make sure tests are included)
- [ n] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [ y] This PR is backwards compatible [y/n]
- [ n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)
",33015583
168,2018-10-17T15:20:38Z,2020-02-19T17:11:38Z,,,"### Summary
Standalone example of recurrent attention as per @farizrahman4u [suggestion](https://github.com/keras-team/keras/issues/11172#issuecomment-429958323). There is thorough documentation in the script itself.

The script contains a base class for recurrent attention mechanisms. The purpose of this is to make it simple to write custom attention mechanisms. This is the main logic needed to implement the specific mechanism (by extending the base class):
```
def attention_call(self,
                   inputs,
                   cell_states,
                   attended,
                   attention_states,
                   attended_mask,
                   training=None):
    # only one attended sequence (verified in build)
    assert len(attended) == 1
    attended = attended[0]
    attended_mask = attended_mask[0]
    h_cell_tm1 = cell_states[0]

    # compute attention weights
    w = K.repeat(K.dot(h_cell_tm1, self.W_a), K.shape(attended)[1])
    u = K.dot(attended, self.U_a)
    e = K.exp(K.dot(K.tanh(w + u), self.v_a))

    if attended_mask is not None:
        e = e * K.cast(K.expand_dims(attended_mask, -1), K.dtype(e))

    # weighted average of attended
    a = e / K.sum(e, axis=1, keepdims=True)
    c = K.sum(a * attended, axis=1, keepdims=False)

    return c, [c]
``` 
The lines below summarizes how the attention mechanism is used, in summary: an RNNCell is wrapped by the attention mechanism and the attended constans are provided to the RNN:
```
decoder = RNN(
    cell=DenseAnnotationAttention(
        cell=GRUCell(RECURRENT_UNITS),
        units=DENSE_ATTENTION_UNITS),
    return_sequences=True)
h1 = decoder(y_emb, constants=x_enc)
```

### Related Issues
https://github.com/keras-team/keras/issues/11172 (+multiple previous issues and PRs linked from there)

### PR Overview
The PR contains a single example script. It is under review/discussion what parts might make it into the core api.

- [y] This PR requires new unit tests [y/n] (make sure tests are included)
TODO, definitely needed if `RNNAttentionCell` is added to core api. Tests should be added/done also to validate implementation in this example.
- [?] This PR requires to update the documentation [y/n] (make sure the docs are up-to-date)
- [y] This PR is backwards compatible [y/n]
- [n] This PR changes the current API [y/n] (all API changes need to be approved by fchollet)",33015583
169,2020-03-26T12:49:44Z,2020-03-27T09:51:43Z,,,"<!--- Provide a general summary of your changes in the title. -->

## Description
See #5207.

### Types of change
meta

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
170,2020-03-26T12:07:32Z,2020-03-26T12:07:45Z,,,"## Description
The parser/ner was broken on GPU. This could be tested by adding `require_gpu` to the `overfitting_IO` test in `test_ner`.

This PR includes several fixes to make sure the parser again runs correctly on GPU. To ensure also IO works, it needs the additional fix from https://github.com/explosion/thinc/pull/332.

### Types of change
bug fix

## Checklist
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
171,2020-03-24T14:29:05Z,2020-03-26T15:22:35Z,,,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

Do not use cupy-cuda v8, as it contains breaking changes. See #5193 for a full description.

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

Bug fix in the install process.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed. _(not done, as no code changes)_
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.

Closes #5193 
",21467110
172,2020-03-21T15:42:13Z,2020-03-25T13:46:26Z,,,"A bot powered by Clarifai Predict API and spaCy. Can be found in Telegram messenger at @pic2phrase_bot

<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x ] I have submitted the spaCy Contributor Agreement.
- [x ] I ran the tests, and all new and existing tests passed.
- [x ] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
173,2020-03-18T13:14:38Z,2020-03-25T10:57:28Z,,,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

When creating docs to pair with gold parses, modify test to check whether a doc is unset rather than whether it contains tokens.

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
174,2020-03-18T13:13:20Z,2020-03-18T13:13:29Z,,,"

<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

Pass the `exclusive_classes` setting to the bow model within the ensemble textcat model.

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

Bugfix.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
175,2020-03-17T15:10:38Z,2020-03-17T15:18:16Z,,,"## Description
This PR add example sentences for the Kannada language which were missing as per issue #1107 

### Types of change
This is an enhancement.

## Checklist
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
176,2020-03-16T19:51:12Z,2020-03-24T15:37:25Z,,,"Small fix in NEL script, fixes #5131 

## Description
Avoid enumerate and read training file line by line. This should avoid long waiting at 0% in the beginning of processing a huge training file.

### Types of change
enhancement

## Checklist
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
177,2020-03-14T22:20:12Z,2020-03-14T22:25:06Z,,,"## Description
There was a bug when calculating the similarity of a `Span` of X tokens (""`self`""), to a `Token` with X characters (""`other`""), because of this line:
``` elif hasattr(other, ""__len__"") and len(self) == len(other)```
which would be `True` in that case, and then `other[i]` would crash because a `Token` can't be indexed.

I rewrote the check to actually look at the class of the instance instead (like it is in the implementation in `doc.pyx`, I found out afterwards).

Also, I thought there was a bit of a weird construct with an `else` statement with an unmatched `if`. I rewrote it for clarity.

Fixes #5152

### Types of change
bug fix

## Checklist
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information. 
",21467110
178,2020-03-13T00:00:45Z,2020-03-25T10:52:47Z,,,"[Update 18 March: requires [Thinc 8.0.0a3](https://github.com/explosion/thinc/pull/327)]

## Description
Adopted `textcat` for the new 3.0 design:
* Found & fixed a few bugs in the new implementation. Requires `thinc` > 8.0.0a2 with the new `softmax_activation`
* Implemented the old `build_text_classifier` and use that as current default
* added `textcat_bow_defaults.cfg` and `textcat_cnn_defaults.cfg` to reflect the default implementations on `master` for those specific architectures
* added textcat unit tests for a variety of possible configs (which proved to be extremely useful during development)

Changes to `train_textcat.py:`
* build the NLP model from a config file + provide a default to make the transition to config-based as easy as possible
* support for multi-class and multi-label datasets
* enable specifying a different dataset that gets loaded with `ml_datasets.loaders()` by using `-d`
* enable specifying a threshold for multi-label classification. Labels with an occurrence count below that threshold are discarded from training.

With respect to Tok2Vec:
* revert shenanigans with `_set_dims` for Tok2Vec as initialization can now be done with `tok2vec.initialize(X=docs)`

## Evaluation:
To ensure there are no `textcat` regressions moving from Thinc 7 / spaCy 2 to Thinc 8 / spaCy 3, I performed several tests. Showing in the table the best F-score across 50 epochs, using 2000 training articles, e.g. `train_textcat.py train_textcat_config.cfg -n 50 -d dbpedia`.

Datasets:
 * IMDB: **binary classification**
 * DBpedia: **multi-class, single-label classification**

| Dataset | Architecture | spaCy 2 (current `master`) | spaCy 3 (current PR) |
| ------- | ------------ | -------------------------- | -------------------- |
| IMDB    | BOW          | 78.5 %                       | 81.4 %                 |
| IMDB    | CNN          | 79.1 %                       | 79.4 %                 |
| IMDB    | DEFAULT      | 76.4 %                       | 77.1 %                 |
| DBpedia | BOW          | 90.5 %                       | 90.7 %                 |
| DBpedia | CNN          | 84.8 %                       | 86.4 %                 |
| DBpedia | DEFAULT      | 89.7 %                       | 90.0 %                 |   

This clearly shows there are no regressions - on the contrary, the new implementations seem to work slightly better, even though we attempted to make a 1-1 mapping. I assume Thinc 8 has some issues fixed from the previous version.

I've also done some preliminary experiments on a new **multi-class, multi-label** dataset, CMU (cf https://github.com/explosion/ml-datasets/pull/1). It has >300 labels but a threshold of 20 was used to keep only the 66 most frequent labels.

| Dataset | Architecture  | spaCy 3 (current PR) |
| ------- | ------------ | -------------------- | 
| CMU | BOW                       | 21.7 %                 |
| CMU | CNN                       | 30.5 %                 |
| CMU | DEFAULT                 | 14.4 %                |   

The CNN result of 30.5% seems to be in line with the first result (31.5%) reported in [this blog](https://www.analyticsvidhya.com/blog/2019/04/predicting-movie-genres-nlp-multi-label-classification/), where they further boosted performance to 43.7% by playing with the threshold for determing a ""true"" prediction. So it looks encouraging, but definitely some more work could be done on this.

### Types of change
enhancement

## Checklist
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [ ] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
179,2020-03-09T11:17:32Z,2020-03-09T13:18:00Z,,,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

* Revert changes to priority of `token_match` so that it has priority over all other tokenizer patterns

* Add lookahead and potentially slow lookbehind back to the default URL pattern

* Expand character classes in URL pattern to improve matching around lookaheads and lookbehinds related to #4882

* Revert changes to Hungarian tokenizer

* Revert (xfail) several URL tests to their status before #4374

* Update `tokenizer.explain()` and docs accordingly

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

Bug fix, I guess.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
180,2020-03-06T08:40:04Z,2020-03-06T10:17:20Z,,,"<!--- Provide a general summary of your changes in the title. -->

## Description
Matcher support for searching on a Span, as well as a Doc. #5056
Missing of an appropriate error message at line 228 in `matcher.pxy`

### Types of change
Enhancement/feature for the Matcher.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [ ] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
181,2020-03-05T13:25:01Z,2020-03-26T08:29:33Z,,,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

Update morphologizer:

* switch to tagger-based morphologizer
* add `Doc.is_morphed` flag
* add `pos` and `morph` scoring to `Scorer`
* modify/extend evaluate CLI to show tag / pos / morph instead of tag as ""pos""
* add `morphologizer` to train CLI
* add basic initialization and overfitting morphologizer pipeline tests

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

Enhancement

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
182,2020-02-18T21:01:37Z,2020-03-04T13:02:27Z,,,"Offering a working solution to the problem of merging multiple Doc objects.

## Description
- Addresses the issue #2229
- Implements from_docs as a new static method in Doc
- Adds error description for incompatible docs
- Adds corresponding test for new method in test_doc_api.py

### Types of change
enhancement, new feature, doc, error, test

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
183,2020-02-12T14:48:03Z,2020-03-02T22:56:29Z,,,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

Add warning for misaligned character offset spans

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

Enhancement.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
184,2020-02-10T02:46:15Z,2020-03-02T22:58:50Z,,,"<!--- Provide a general summary of your changes in the title. -->

WIP idea of allowing custom `get_candidates` function for KnowledgeBase based on this issue https://github.com/explosion/spaCy/issues/4981

@svlandeg I don't really like the idea of adding a full new registry for the kb itself.
And I'm not sold on the implementation here but this is just an idea of how to provide the custom functionality for methods of arbitrary spaCy objects. 

It would be interesting to expand the idea to a more generic solution but I assume that's a lot of what you're thinking about for spaCy v3.

Would love to discuss further

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

Enhancement

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [ ] I ran the tests, and all new and existing tests passed.
- [ ] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
185,2020-02-01T06:38:26Z,2020-02-11T01:32:50Z,,,"## Description
Currently, the color of an entity in displaCy is determined by its `label`. These colors can be overridden (at the entity label level) by setting the `colors` key in the `options` parameter for `displacy.serve`.

I would like the color of an entity to be determined by an arbitrary attribute of the `entity`. Consider negation with [negspaCy](https://spacy.io/universe/project/negspacy) -- a term like ""hypertension"" could be red (indicating a negated entity) in the phrase ""no history of hypertension"" but green (no negation) in ""patient does have hypertension"". Another use case would be to return a different intensity of a color, based on a confidence score.

I made a one-line change to the `parse_ents` function in `displacy/__init__.py` to add a `params` key to each entity dictionary sent to the renderer. The renderer is already looking for a params key (although I was never able to determine where it is set).

This change allows me to write the following in my code to change the color based on an entity's span._.negex value:
`spacy.tokens.Span.set_extension(""params"", getter=lambda span: { ""bg"": span._.negex and ""#ff8197"" or ""#bfeeb7"" })`

I'm totally open to feedback. This may be the wrong approach, but I thought it was an interesting use case.

### Types of change
enhancement
",21467110
186,2019-12-12T09:00:28Z,2020-03-09T14:22:18Z,,,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

Fix alignment of character entity spans in cases where spacy's tokenization is different from the provided gold words. Create a temporary doc from the text and words in order to be able to use `biluo_tags_from_offsets()` to generate entity tags for those words. In case anything goes wrong, warn the user and discard all entity annotation for the instance.

In order to create a temporary doc, add an option to initialize a `Doc` from a text and list of words where the words may or may not include all whitespace tokens. If the text and words are mismatched, raise an error.

Fixes #4791.

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

Bugfix.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
187,2019-12-08T21:55:41Z,2020-02-03T12:00:59Z,,,"*tag_map.py*: I added a tag set corresponding to the Prague Arabic Treebank. The tag set might look dreadfully huge but that's unfortunately the cost of Arabic being a highly inflectional language.

*tokenizer_exceptions.py*: I changed the *lemma* designation to *norm* since none of the expansions for the given orthographic units are lemmas. The ""lemmas"" were either inflected forms,  or expand like so:
```python
{NORM: ""قبل الميلاد"", ORTH: "".ق.م""}
```
In this example, the orthographic unit is ""b.c."" and the expansion is (obviously) ""before christ"". So it makes much more sense for them to be considered the norms. I also removed the following two exceptions:
https://github.com/explosion/spaCy/blob/c208eb6e4d26ae874a84dd4485bd809599748552/spacy/lang/ar/tokenizer_exceptions.py#L29-L30
These were really strange to find, because the first orthographic unit ""ثنى"" means ""he bent [something]"" but the lemma paired with it strangely means ""tell me"", and the second orthographic unit ""أنا"" means the singular pronoun *I*, but the lemma is ""tell us [a prediction]"". It looks like based on the following comment
https://github.com/explosion/spaCy/blob/c208eb6e4d26ae874a84dd4485bd809599748552/spacy/lang/ar/tokenizer_exceptions.py#L20
that the pairings make sense in the context of Arabic academic literature. However, even if the tokenizer isn't required to be as *domain agnostic* as possible, the fact remains that the above tokenizer exceptions overload very common words with incorrect lemmas. So I removed them.

### Types of change
Enhancement

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I have submitted the spaCy Contributor Agreement.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
188,2019-11-22T09:53:40Z,2020-03-02T22:52:15Z,,,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->
Added #3637.
I added some basic tests in the tokenizer and the senten 

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->
Added a new feature.
Added test for Tokenizer and Sentencizer to test the new attribute

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [X] I have submitted the spaCy Contributor Agreement.
- [X] I ran the tests, and all new and existing tests passed.
- [X] My changes don't require a change to the documentation, or if they do, I've added all required information.
",21467110
189,2020-03-26T22:43:27Z,2020-03-26T23:47:38Z,,,"Albert uses a different way of storing the vocab, so we need a different way of getting it out and filling our own `Vocab` object.",91356408
190,2020-03-26T19:20:54Z,2020-03-26T19:43:41Z,,,"# Overview

This is a small modification to `GradientDescentTrainer` that persists the `amp.state_dict` when checkpointing the model and loads it when recovering training state from a checkpoint.

WIP.

## TODO

- [ ] Add tests (help?)
- [x] `Model._load()` should probably call `amp.initialize`, otherwise, a model trained with `amp` might be loaded without it (say, during prediction) which will hurt performance (I think? there would be information loss afaik).
- [ ] Related to the point above, it is possible that a user will try to load a model that was trained with `amp`, but `amp` is not installed (i.e. `amp = None`). Should there be a warning or even error in this case?

## Closes

Closes #3875.",91356408
191,2020-03-25T13:21:24Z,2020-03-25T13:21:25Z,,,"Updates the requirements on [transformers](https://github.com/huggingface/transformers) to permit the latest version.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/huggingface/transformers/releases"">transformers's releases</a>.</em></p>
<blockquote>
<h2>BART, organizations, community notebooks, lightning examples, dropping Python 3.5</h2>
<h1>New Model: BART (added by <a href=""https://github.com/sshleifer"">@sshleifer</a>)</h1>
<p>Bart is one of the first Seq2Seq models in the library, and achieves state of the art results on text generation tasks, like abstractive summarization.
Three sets of pretrained weights are released:</p>
<ul>
<li><code>bart-large</code>: the pretrained base model</li>
<li><code>bart-large-cnn</code>: the base model finetuned on the CNN/Daily Mail Abstractive Summarization Task</li>
<li><code>bart-large-mnli</code>: the base model finetuned on the MNLI classification task.</li>
</ul>
<p>Related:</p>
<ul>
<li><a href=""https://arxiv.org/abs/1910.13461"">paper</a></li>
<li>model pages are at <a href=""https://huggingface.co/facebook"">https://huggingface.co/facebook</a></li>
<li><a href=""https://huggingface.co/transformers/model_doc/bart.html"">docs</a></li>
<li><a href=""https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html"">blogpost</a></li>
</ul>
<p>Big thanks to the original authors, especially Mike Lewis, Yinhan Liu, Naman Goyal who helped answer our questions.</p>
<h1>Model sharing CLI: support for organizations</h1>
<p>The huggingface API for model upload now supports <a href=""https://huggingface.co/organizations"">organisations</a>.</p>
<h1>Notebooks (<a href=""https://github.com/mfuntowicz"">@mfuntowicz</a>)</h1>
<p>A few beginner-oriented notebooks were added to the library, aiming at demystifying the two libraries huggingface/transformers and huggingface/tokenizers. Contributors are welcome to contribute links to their notebooks as well.</p>
<h1><a href=""https://github.com/PyTorchLightning/pytorch-lightning"">pytorch-lightning</a> examples (<a href=""https://github.com/srush"">@srush</a>)</h1>
<p>Examples leveraging pytorch-lightning were added, led by <a href=""https://github.com/srush"">@srush</a>.
The first example that was added is the <a href=""https://github.com/huggingface/transformers/tree/master/examples/ner"">NER example</a>.
The second example is a lightning GLUE example, added by <a href=""https://github.com/nateraw"">@nateraw</a>.</p>
<h1>New model architectures: CamembertForQuestionAnswering,</h1>
<ul>
<li><code>CamembertForQuestionAnswering</code> was added to the library and to the SQuAD script <a href=""https://github.com/maximeilluin"">@maximeilluin</a></li>
<li><code>AlbertForTokenClassification</code> was added to the library and to the NER example <a href=""https://github.com/marma"">@marma</a></li>
</ul>
<h1>Multiple fixes were done on the fast tokenizers to make them entirely compatible with the python tokenizers (<a href=""https://github.com/mfuntowicz"">@mfuntowicz</a>)</h1>
<p>Most of these fixes were done in the patch 2.5.1. Fast tokenizers should now have the exact same API as the python ones, with some additional functionalities.</p>
<h1>Docker images (<a href=""https://github.com/mfuntowicz"">@mfuntowicz</a>)</h1>
<p>Docker images for transformers were added.</p>
<h1>Generation overhaul (<a href=""https://github.com/patrickvonplaten"">@patrickvonplaten</a>)</h1>
<ul>
<li>Special token IDs logic were improved in run_generation and in corresponding tests.</li>
<li>Slow tests for generation were added for pre-trained LM models</li>
<li>Greedy generation when doing beam search</li>
<li>Sampling when doing beam search</li>
</ul>
</tr></table> ... (truncated)
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/huggingface/transformers/commit/fbc5bf10cfe4d4ca81f8daacc148b0abd51dda5a""><code>fbc5bf1</code></a> v2.6.0 release: isort un-pinned</li>
<li><a href=""https://github.com/huggingface/transformers/commit/b88bda6af36cf3a22dfe69abb81fd3590062ac11""><code>b88bda6</code></a> Add right model and tokenizer path in example</li>
<li><a href=""https://github.com/huggingface/transformers/commit/b31ef225cfdcbcf23b7f0d0d8b2e22ab3f066396""><code>b31ef22</code></a> [model_cards] 🇹🇷 Add new (uncased, 128k) BERTurk model</li>
<li><a href=""https://github.com/huggingface/transformers/commit/b4009cb00136fefb0a7fc1b899b44f8ef5043339""><code>b4009cb</code></a> [model_cards] 🇹🇷 Add new (cased, 128k) BERTurk model</li>
<li><a href=""https://github.com/huggingface/transformers/commit/d3283490ef89ac28f1a422ea05fff7658c00f3b0""><code>d328349</code></a> [model_cards] 🇹🇷 Add new (uncased) BERTurk model</li>
<li><a href=""https://github.com/huggingface/transformers/commit/e279a312d6857bf8cd94b354c7156d6d8c158d7d""><code>e279a31</code></a> Model cards for CS224n SQuAD2.0 models (<a href=""https://github-redirect.dependabot.com/huggingface/transformers/issues/3406"">#3406</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/7372e62b2c3b03745cf46a4d46b3dd6d00585271""><code>7372e62</code></a> Added precisions in SciBERT-NLI model card (<a href=""https://github-redirect.dependabot.com/huggingface/transformers/issues/3410"">#3410</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/471cce24b38c75226d1428eae899f03c9521a995""><code>471cce2</code></a> Release: v2.6.0</li>
<li><a href=""https://github.com/huggingface/transformers/commit/e392ba6938f50655a195ea7ec8a260b1e9fc6058""><code>e392ba6</code></a> Add camembert integration tests (<a href=""https://github-redirect.dependabot.com/huggingface/transformers/issues/3375"">#3375</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/a8e3336a850e856188350a93e67d77c07c85b8af""><code>a8e3336</code></a> [examples] Use AutoModels in more examples</li>
<li>Additional commits viewable in <a href=""https://github.com/huggingface/transformers/compare/v2.4.0...v2.6.0"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",91356408
192,2020-03-23T18:02:26Z,2020-03-23T19:32:55Z,,,`find . -name '*.py' -exec gsed -i 's/:[a-z][a-z]*:~*//g' \{\} \;`,91356408
193,2020-03-17T20:31:18Z,2020-03-24T00:41:12Z,,,"Related to #2827 and #3100.

Can you give me feedback? It's ready to review in abstraction and code working fine (not for documentation and testing).

I'm gonna leave some comments as a review.",91356408
194,2020-03-04T04:56:32Z,2020-03-27T04:53:53Z,,,"Fixes #3888, which explains how Predictors don't freeze / set a model to evaluation mode by default when building a Predictor directly from the constructor (instead of `from_archive` or `from_path`). This can lead to non-deterministic predictions due to things like dropout. 

@matt-gardner I added the parameter you requested to the constructor of Predictor, and updated `from_archive` and `from_path` to use it rather than manually calling `model.eval()`. I set it to `True` by default because it seemed like the intended behavior (given that both other methods of loading a Predictor function this way as is). Also added a couple notes in the docstrings. Let me know if there's anything I should change.",91356408
195,2020-02-28T00:11:46Z,2020-03-04T01:04:56Z,,,,91356408
196,2020-02-27T16:14:38Z,2020-03-04T18:13:01Z,,,"I'm still noodling around, but in about 20 minutes I was able to get a simple test passing.  This needs a lot of work to actually be usable, but the proof of concept is here, it doesn't look like it's very much work, or very much code that we'd have to maintain.

I only used `Params` here because that's what the test I was copying did.  Still to do:
- [ ] Completely remove the `Params` bit out of the code that's added here, and just make a `TrainWithLightning` object that takes concrete objects as arguments (that we can construct `FromParams` if anyone wants to use that pipeline).  This probably also requires a `Registrable` `AllenNlpLightningModule` that can be subclassed for different configuration.
- [ ] Make sure this actually trains a real model, and serializes stuff how we want (we almost certainly want to create our typical model archive, for example, though I'm not sure what pytorch lightning does by default)
- [ ] Add whatever other bells and whistles people might want (not really sure what's necessary here; if anyone reading this uses or wants to use pytorch lightning, let me know what features should definitely be supported)

Bottom line: this is definitely possible, and should be very easy to support.  I don't know exactly what that support should look like, as I have no experience with this other trainer.  If you have specific requests, comment below.  I also will be out of town for a week and likely won't be working on this for a little bit, so if anyone is super eager and wants to try fleshing this out, let me know.",91356408
197,2020-02-24T10:48:54Z,2020-03-07T11:36:17Z,,,"This PR attempts to expand the documentation the ConditionalRandomField code. Specifically I found the details around the mask and start-end tokens to be confusing on first reading. Hopefully this makes it bit more explicit. 

This is a simple docs-only change. In future might come back and try clean up the code a bit / improve performance (The CRF layer seems to add a pretty non-trivial overhead to training time. Given the sequential loops and lots of pointwise-ops, it seems like a prime use case of TorchScript, but currently it seems the code would need some changes to be compatible.)

Feedback welcome.",91356408
198,2020-01-17T18:25:12Z,2020-02-14T23:21:02Z,,,fix for [#2292](https://github.com/allenai/allennlp/issues/2292),91356408
199,2020-03-06T02:20:46Z,2020-03-06T02:39:28Z,,,"Hello,
I am a security engineer at r2c.dev. We are working to write code
checks for security in open source code.

In python, the default values of function parameters are instantiated
at function definition time. All calls to that function that use the
default value all point to the same global object. e.g.:

```
def func(x=[]):
   x.append(1)
   print(x)

func() # [1]
func() # [1 , 1]
```

Because of this ChainCallback class potentially all share the same
list of callbacks.

Fix:
The recommended solution is to either set default to None and assign
a new empty object when the variable is None.

We have a tool called Bento you can use for your project that continuously detects problems like this one. The check that identified this will be available in the very near future.
Thanks, and I hope this helps! Let me know if you have any questions.",55147386
200,2020-01-10T00:36:00Z,2020-01-10T00:36:00Z,,,"This pull request configures this repository to be run on Repl.it.      It adds a `.replit` configuration file and a Repl.it badge to the `README`.
     You can read more about running repos on Repl.it [here](https://docs.repl.it/repls/dot-replit), or view the Repl [here](https://repl.it/@JosephChatfield/tflearn).",55147386
201,2019-11-11T22:06:09Z,2019-11-11T22:06:09Z,,,"Mish is a new novel activation function proposed in - [Paper](https://arxiv.org/ftp/arxiv/papers/1908/1908.08681.pdf)
It has shown promising results so far and has been adopted in several packages including: 
1. TensorFlow Addons
2. FastAI-dev
3. SpaCy
4. Thinc
5. DarkNet

All benchmarks, analysis and links to official package implementations can be found in this [repository](https://github.com/digantamisra98/Mish) 

Seems like a good addition considering it has been evaluated against GELU which was recently added to TFLearn and the results show it performs slightly better. 

Post the upcoming release of TFA, Mish can be directly imported from the same.",55147386
202,2019-04-12T10:05:38Z,2019-04-12T10:05:38Z,,,,55147386
203,2019-02-27T19:26:48Z,2019-02-27T19:26:48Z,,,"Since the tutorials was empty, so I've added some standard codes in Computer Vision and Natural Language Processing.",55147386
204,2018-01-26T06:25:06Z,2018-02-23T02:41:25Z,,,It would be nice if the `sequence_length` argument can be passed when the function is called. Addresses feature request https://github.com/tflearn/tflearn/issues/95,55147386
205,2018-01-26T05:57:19Z,2018-01-26T05:57:19Z,,,Replaced function call with set literal construction for speed improvement according to https://stackoverflow.com/questions/36674083/why-is-it-possible-to-replace-set-with,55147386
206,2017-11-04T10:56:56Z,2019-01-10T14:15:08Z,,,"I think it has a mistake in this function: 
When I test the codes below from the Quickstart from tflearn :
//---------------------split line --------------------------------
import numpy as np
import tflearn
# Download the Titanic dataset
from tflearn.datasets import titanic
titanic.download_dataset('titanic_dataset.csv')
# Load CSV file, indicate that the first column represents labels
from tflearn.data_utils import load_csv
data, labels = load_csv('titanic_dataset.csv', target_column=0,
                        categorical_labels=True, n_classes=2)
//---------------------split line --------------------------------
the python IDE shows me the error message:
    return (y[:, None] == np.unique(y)).astype(np.float32)
    TypeError: list indices must be integers or slices, not tuple
I checked the source codes of the file ""data_utils.py"", the error was found in the function ""to_categorical"",
it should use ""y = np.array(y)"" before ""return (y[:, None] == np.unique(y)).astype(np.float32) "" because y should be a array rathen than a list. 
![tflean_data_utils](https://user-images.githubusercontent.com/15982113/32404761-e2410626-c191-11e7-8994-e4bec58b4bd5.png)





",55147386
207,2017-08-30T03:26:07Z,2017-08-30T03:27:53Z,,,"I think there is a small mistake in the last several steps in the source code of residual_bottleneck.
I think the last two lines codes should not be included in the condition of  in_channels!= out_channels",55147386
208,2017-05-01T19:56:47Z,2017-05-01T19:56:47Z,,,,55147386
209,2017-04-23T13:53:52Z,2018-07-25T05:49:19Z,,,"### Proposal
In the previous tensorflow code, the RMSProp Optimizer is often used with a **constant** Learning Rate, such as:
`rmsp = tflearn.RMSProp(learning_rate=0.01, decay=0.999)`
`net = tflearn.regression(net, optimizer=rmsp, loss='categorical_crossentropy')`
In my proposal, I try to use **exponential decay** before using RMSProp OPtimizer, i.e., append the following codes:
`self.learning_rate = tf.train.exponential_decay(self.learning_rate, self.decay, self.momentum, self.epsilon, self.use_locking, self.name)`
`tf.add_to_collection(tf.GraphKeys.LR_VARIABLES, self.learning_rate)`

### Effects
Based on the experiment results, I find that the modifications could promote the accuracy of classifications to some extent. For instance, I use the new optimizer in the work of image classification on **cifar-10** data set with **ResNet 20**. The training performs 10 rounds, each taking 50 epochs with the same parameter settings. According to the results, the accuracy of the previous RMSProp Optimizer on average is 80.05% while the new one is 82.46%.",55147386
210,2017-02-19T10:22:43Z,2017-02-19T10:22:43Z,,,"* tf.mul -> tf.multiply
* tf.sub -> tf.subtract

See issue #586",55147386
211,2017-01-13T05:01:29Z,2017-01-22T06:29:28Z,,,"Three modifications:

    1. Return tensor of rnn operations while the original implementation tflearn 
        return list of tensors.
    2. Add support for multiple network input in model.DNN, which is useful in 
        multi-task learning. The original implementation of DNN expects input 
        network to be a tensor which is hard to feed in multi-task learning.
    3. Return tensor (single task/network) or list of tensors (multi task/network) in Evaluator.predict.",55147386
212,2016-12-30T02:10:52Z,2017-07-16T22:44:40Z,,,"At the current version of tflearn, using some modules could end up with:

> TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.

So I replaced some judgements like ""if b:"" to ""if b is not None"" to avoid this.",55147386
213,2016-12-14T11:17:33Z,2017-01-11T20:31:42Z,,,See the details here https://github.com/tensorflow/tensorflow/issues/5742,55147386
214,2016-12-06T20:34:45Z,2016-12-15T01:40:31Z,,,,55147386
215,2016-11-22T13:12:50Z,2018-01-25T19:56:27Z,,,"This PR fixes featurewise zero centring and normalization in order to work also with datasets that are not `float32` eg. `uint8` images.

It also replaces redundant for loop by numpy broadcasting.",55147386
216,2016-11-01T22:01:50Z,2016-11-02T01:11:51Z,,,Right now DNN only allow for a single output.  This change allows the user to predict/evaluate multiple outputs.,55147386
217,2016-09-18T09:50:35Z,2016-09-22T19:15:11Z,,,"As tf 0.7 is a very old release, I think these workarounds should be removed.
",55147386
218,2016-08-11T15:26:29Z,2016-08-11T23:08:14Z,,,"This PR allows to use tf.train.exponential_decay together with Trainer. It adds an optional argument to pass a global_counter to Trainer, and to hence initialise the global_counter before initializing Trainer. This is necessary if the parameter to be decayed is part of the loss function.
",55147386
219,2020-03-16T16:47:33Z,2020-03-16T16:47:33Z,,,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines <https://github.com/spotify/chartify/blob/master/CONTRIBUTING.rst>
2. If the PR is unfinished, please prefix the subject line with [WIP], [DRAFT], or [RFC].
-->

**What this PR does / why we need it**:
This PR intents to point users from readme to the docs as described at #103 
**Which issue(s) this PR fixes**
<!-- optional; in `fixes #<issue number>, fixes #<issue_number>, ...` format, will close the issue(s) when PR gets merged: -->
Fixes #103

**Special notes for your reviewer**:
This is my first contrib to spotify code bases, if there's some additional action required for fixing this issue please let me know.
",149135719
220,2019-12-17T13:41:32Z,2019-12-17T13:41:32Z,,,"<!--  Thanks for sending a pull request!  Here are some tips for you:
1. If this is your first time, read our contributor guidelines <https://github.com/spotify/chartify/blob/master/CONTRIBUTING.rst>
2. If the PR is unfinished, please prefix the subject line with [WIP], [DRAFT], or [RFC].
-->

**What this PR does / why we need it**:

**Which issue(s) this PR fixes**
<!-- optional; in `fixes #<issue number>, fixes #<issue_number>, ...` format, will close the issue(s) when PR gets merged: -->
Fixes #

**Special notes for your reviewer**:

**Release note**:
<!--  Write your release note:
1. Enter your extended release note in the below block. If the PR requires additional action from users switching to the new release, start the release note with the string ""action required: "".
2. If no release note is required, just write ""NONE"".
-->
```release-note

```
",149135719
221,2019-11-27T17:14:59Z,2019-11-27T17:15:00Z,,,"Bumps [pillow](https://github.com/python-pillow/Pillow) from 6.2.0 to 6.2.1.
<details>
<summary>Release notes</summary>

*Sourced from [pillow's releases](https://github.com/python-pillow/Pillow/releases).*

> ## 6.2.1
> https://pillow.readthedocs.io/en/stable/releasenotes/6.2.1.html
</details>
<details>
<summary>Changelog</summary>

*Sourced from [pillow's changelog](https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst).*

> 6.2.1 (2019-10-21)
> ------------------
> 
> - This is the last Pillow release to support Python 2.7 [#3642](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/3642)
> 
> - Add support for Python 3.8 [#4141](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4141)
>   [hugovk]
</details>
<details>
<summary>Commits</summary>

- [`6e0f07b`](https://github.com/python-pillow/Pillow/commit/6e0f07bbe38def22d36ee176b2efd9ea74b453a6) Pillow 6.2.1 is the last to support Python 2.7
- [`39d26d3`](https://github.com/python-pillow/Pillow/commit/39d26d3f903e151423a711f2f5eeac6168a97692) 6.2.1 version bump
- [`ee9e21a`](https://github.com/python-pillow/Pillow/commit/ee9e21aff1cf0ec6c07dc68d6f266eee4c84f948) Add release notes for Pillow 6.2.1
- [`efcfb91`](https://github.com/python-pillow/Pillow/commit/efcfb91b71955dff1d1fdd17da1147580f872859) Update CHANGES.rst [CI skip]
- [`f97c4dd`](https://github.com/python-pillow/Pillow/commit/f97c4ddb0afde378a40a5f94ac88f25327387055) 6.2.x: Add support for Python 3.8 ([#4151](https://github-redirect.dependabot.com/python-pillow/Pillow/issues/4151))
- [`b78edcc`](https://github.com/python-pillow/Pillow/commit/b78edcc9e4bae68fb206e71895d6a9fd23d49f40) Add support for Python 3.8
- See full diff in [compare view](https://github.com/python-pillow/Pillow/compare/6.2.0...6.2.1)
</details>
<br />

[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=6.2.0&new-version=6.2.1)](https://help.github.com/articles/configuring-automated-security-fixes)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/spotify/chartify/network/alerts).

</details>",149135719
222,2019-02-16T23:16:17Z,2019-02-16T23:27:17Z,,,,149135719
223,2018-12-03T00:44:04Z,2019-02-16T22:04:40Z,,,"#20 

Todo:
- [ ] Verify that this works by installing from a test release",149135719
224,2020-02-23T14:25:54Z,2020-03-01T18:09:36Z,,,The goal of this PR is to add additional documentation around mixture cure models.,12420595
225,2019-08-25T17:50:07Z,2019-08-25T21:58:29Z,,,"I've recently learned how to do PRs.
So, here's a tiny change I'm proposing. ",12420595
226,2019-06-11T18:31:00Z,2019-07-16T14:44:35Z,,,"Fixes #553

Though these improvements in fact may interfere with the subclass use case (I have not mind it a lot and I still think that dynamically generated classes for each fitter class are undesireable, and so is messing with globals)",12420595
227,2019-06-11T18:25:23Z,2019-07-16T14:44:34Z,,,"Version is now fetched from a git tag.
Lifted restriction on scipy version because `worksforme` (tests pass) at the latest one.",12420595
228,2019-03-25T00:30:22Z,2019-08-27T14:22:12Z,,,,12420595
229,2018-12-17T17:04:40Z,2019-07-16T14:44:35Z,,,"Added a potentially more powerful XGBoost-powered fitter. Can deal with missing variables and other cases where lifelines Cox fitter stucks.
Though there is a problem: https://github.com/dmlc/xgboost/issues/4005
Also this fitter is built around of my libraries automating some preprocessing and making easier models storing and loading, but feel free to remove them and use raw design matrices instead.",12420595
230,2018-06-14T05:17:27Z,2018-06-16T05:28:15Z,,,Chaoqi Yang contribution: some demos and Twitter dataset,12420595
231,2018-02-20T14:15:02Z,2018-03-04T08:53:52Z,,,Ok I've tried again :) ,12420595
232,2020-03-13T13:25:34Z,2020-03-13T13:36:39Z,,,I want to use random undersampling when my X data has more than 2 dims. ,23011147
233,2020-03-07T22:02:15Z,2020-03-07T23:15:47Z,,,"- [x] Travis
- [ ] Azure Pipelines",23011147
234,2020-03-03T02:43:56Z,2020-03-03T02:43:56Z,,,"
#### Reference Issue
Fixes #686


#### What does this implement/fix? Explain your changes.
Add joblib into show_versions function outputs

",23011147
235,2020-02-29T21:31:15Z,2020-03-01T00:16:18Z,,,Implement new scikit-learn estimator requirements.,23011147
236,2020-02-28T08:38:18Z,2020-02-29T19:47:15Z,,,,23011147
237,2020-02-20T10:29:46Z,2020-02-28T09:28:06Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn-contrib/imbalanced-learn/blob/master/CONTRIBUTING.md#contributing-pull-requests
-->
#### Reference Issue
Fixes #685
",23011147
238,2020-01-14T20:55:28Z,2020-01-31T17:23:44Z,,,"Fixes #662

#### What does this implement/fix? Explain your changes.
If the median standard deviation is 0, the SMOTENC class will now store the categorical features before multiplying the 1's by the median standard deviation. This way, information about the most common categorical labels can still be used in _get_samples.

Checklist:
- [ ] Write tests

#### Example:
```
import numpy as np
from imblearn.over_sampling import SMOTENC
from sklearn.datasets import make_classification

np.random.seed(2)

# Original data
X = np.array([[1, 2, 4, 2], #minority class
              [1, 2, 5, 2], #minority class
              [1, 2, 1, 0], 
              [2, 1, 2, 0], 
              [1, 2, 3, 1]])
y = np.array(['A', 'A', 'B', 'B', 'B'])

# Construct SMOTENC with masks
smotenc = SMOTENC(
    [False, False, False, True], 
    sampling_strategy = ""not majority"",
    k_neighbors=1
)

# Resample
X_resampled, y_resampled = smotenc.fit_resample(X, y)
print(X_resampled)
print(y_resampled)
```
*Output on master*:
```
[[1.         2.         4.         2.        ]
 [1.         2.         5.         2.        ]
 [1.         2.         1.         0.        ]
 [2.         1.         2.         0.        ]
 [1.         2.         3.         1.        ]
 [1.         2.         4.18508208 1.        ]]
['A' 'A' 'B' 'B' 'B' 'A']
```
Only the last row is new. It has the category 1 in the fourth column, even though all rows from the minority class have the category 2 in the fourth column. This is incorrect.

*Output on this fork*:
```
[[1.         2.         4.         2.        ]
 [1.         2.         5.         2.        ]
 [1.         2.         1.         0.        ]
 [2.         1.         2.         0.        ]
 [1.         2.         3.         1.        ]
 [1.         2.         4.18508208 2.        ]]
['A' 'A' 'B' 'B' 'B' 'A']
```
Here, the resampled row correctly has the category 2 in the fourth column.",23011147
239,2020-01-08T13:11:56Z,2020-01-08T13:11:56Z,,,,23011147
240,2019-11-04T03:21:31Z,2020-03-16T03:02:18Z,,,"This is an implementation of the safe-level SMOTE proposed in the following paper:

C. Bunkhumpornpat, K. Sinapiromsaran, C. Lursinsap, ""Safe-level-SMOTE: Safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem,"" In: Theeramunkong T.,
Kijsirikul B., Cercone N., Ho TB. (eds) Advances in Knowledge Discovery and Data Mining. PAKDD 2009. Lecture Notes in Computer Science, vol 5476. Springer, Berlin, Heidelberg, 475-482, 2009.

Todo list:

 - [x] add unit tests







",23011147
241,2019-09-19T00:52:53Z,2019-12-06T15:56:32Z,,,"As per requested by the pinned [New Methods](https://github.com/scikit-learn-contrib/imbalanced-learn/issues/105), I have implemented the Selective Pre-processing of Imbalanced Data (SPIDER) sampling algorithm. 

I have developed unit tests based on drawing out a sample dataset and working out by hand what the expected results would be as it is deterministic. See the following notebook for diagrams [here](https://github.com/MattEding/SPIDER-Algorithm/blob/master/SPIDER%20Unit%20Test%20%26%20Diagrams.ipynb).

~~Currently, the implementation for dense and sparse return the same data points but in different orders (`np.lexsort` will show they are indeed the same). Consequently this fails the existing unit test that compares dense vs sparse outputs. I would rather not have to require sorting results to ensure that test passes due to the overhead for sorting large datasets. Maybe I can just have a parameter that defaults `sort=True` to give the option to bypass this issue.~~

~~The only other unit test I saw fail with PyTest is a `with raises(MESSAGE)`, but I will fix that later since I am not sure if other developers will want to keep my new `sample_type = 'preprocess-strategy'`.~~ I had chosen do this because SPIDER does both cleaning and oversampling and I did not feel that inheriting from either was appropriate--oversamplers allow `sampling_strategy` as a `float` which does not make sense for this algorithm, and cleaners only really allow for under-sampling sampling strategies which is also inappropriate.

Benchmark results using cross-validation with the mean of 5 folds comparing None, NCR, SMOTE, and the 3 SPIDER variants are [here](https://github.com/MattEding/SPIDER-Algorithm) in the PKL, CSV, and PNG folders. I used the Zenodo datasets (excluding set 26 due to it being a large dataset for my local computer to work with in a time effective manner). I set all `n_jobs=-1` on my 4 core macbook if you want to infer the `time` values. Additionally when a scorer was undefined, I left the value as 0 rather than changing it to NaN.

TODO:
- ~~Resolve the two failing tests I addressed above~~
- ~~Online reference explaining the algorithm~~
- Possibly re-categorize algorithm
- Maybe change self._{xyz} to be variables to be explicitly passed into methods to avoid possible multiprocessing mutation complications",23011147
242,2019-05-05T17:38:24Z,2019-12-06T15:56:32Z,,,"#### Reference Issue
<!-- Example: Fixes #1234 -->


#### What does this implement/fix? Explain your changes.
I've seen some issues regarding the need for SMOTE with just categorical data.

Therefore I've added a class SMOTEN which operates on purely categorical data.

#### Any other comments?

This is not the final version but I'd love some feedback on the code - I understand there might be more elegant ways to implement said class as there is some code duplication going on between SMOTENC and SMOTEN - I can look into this issue.

",23011147
243,2018-08-27T23:03:06Z,2019-12-06T15:56:32Z,,,"Implement the last point of #462 and should be merged after it.
Partially addressing #460",23011147
244,2018-07-11T19:37:53Z,2020-02-20T06:37:39Z,,,"#### Reference Issue
[sklearn] Feature Request: Hellinger split criterion for classificaiton trees [#9947](https://github.com/scikit-learn/scikit-learn/issues/9947)
#### What does this implement/fix? Explain your changes.
Hellinger Distance as tree split criterion, cython implementation compatible with sklean tree based classification models 

#### Any other comments?
This is my first submission, sorry in advance for the many possible things I've missed.
Looking forward for your feedback. 
",23011147
245,2018-03-27T20:50:27Z,2019-12-06T15:56:33Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn-contrib/imbalanced-learn/blob/master/CONTRIBUTING.md#contributing-pull-requests
-->
#### What does this implement/fix? Explain your changes.
This implements a new technique called ""class senstive scaling"" that removes borderline noise by scaling samples to their corresponding class center. This computes insanely fast and eases the identification of a decision boundary and is a alternative to Tomek Link based concepts like CNN or OSS that supposedly reveal the decision boundary by removing noisy borderline samples. For details please refer:

B. Schlegel, and B. Sick. ""Dealing with class imbalance the scalable way: Evaluation of various techniques based on classification grade and computational complexity."" 2017 IEEE International Conference on Data Mining Workshops, 2017.

#### Any other comments?
This is my first pull request! 

If you want to have a simple visualization you can use the following snippet

```
import numpy as np
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

# create sample dataset
rng = np.random.RandomState(42)
n_samples_1 = 50
n_samples_2 = 5
X_syn = np.r_[1.5 * rng.randn(n_samples_1, 2),
              0.5 * rng.randn(n_samples_2, 2) + [2, 2]]
y_syn = np.array([0] * (n_samples_1) + [1] * (n_samples_2))
X_syn, y_syn = shuffle(X_syn, y_syn)
X_syn_train, X_syn_test, y_syn_train, y_syn_test = train_test_split(X_syn, y_syn)
idx_class_0_orig = y_syn == 0

# apply CSS
from imblearn.scaling import CSS
css = CSS(sampling_strategy=""both"", mode=""linear"", c=0.1, shuffle=True)
X_train_res, y_train_res = css.fit_sample(X_syn, y_syn)

# plot original dataset
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(14, 6))
ax = fig.add_subplot(1, 2, 1)
import matplotlib.pyplot as plt
plt.scatter(X_syn[idx_class_0_orig, 0], X_syn[idx_class_0_orig, 1],
            alpha=.8, label='Class #0')
plt.scatter(X_syn[~idx_class_0_orig, 0], X_syn[~idx_class_0_orig, 1],
            alpha=.8, label='Class #1')
ax.set_xlim([-5, 5])
ax.set_ylim([-5, 5])
plt.yticks(range(-5, 6))
plt.xticks(range(-5, 6))

plt.title('Original dataset')
plt.legend()

# plot CSS dataset
idx_class_0 = y_train_res == 0
ax = fig.add_subplot(1, 2, 2)
plt.scatter(X_train_res[idx_class_0, 0], X_train_res[idx_class_0, 1],
            alpha=.8, label='Class #0')
plt.scatter(X_train_res[~idx_class_0, 0], X_train_res[~idx_class_0, 1],
            alpha=.8, label='Class #1')
ax.set_xlim([-5, 5])
ax.set_ylim([-5, 5])
plt.yticks(range(-5, 6))
plt.xticks(range(-5, 6))
plt.title('Scaling using CSS')
plt.legend()
plt.show()
```

Thanks for the great library by the way !

edit: updated sample code to match code changes.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.

Thanks for contributing!
-->
",23011147
246,2020-03-26T19:38:47Z,2020-03-26T19:38:47Z,,,"##### Issue
<!-- E.g. Fixes #1, Closes #2, Resolves #3, etc. -->
<!-- Or a short description, if the issue does not exist. -->
Partialy fixes #4382

##### Includes
- [X] Code changes
- [X] Tests",8357227
247,2020-03-26T16:06:35Z,2020-03-26T16:07:54Z,,,"##### Issue

Could be a fix for #4515.

Based on #4535. The relevant commit is aca0d30d879684a08c0be3b54cbc5d1b2fafe708.

I don't like it because if a variable has 100 values, which are merged into 15, they will not use a better palette for 15 colors, but instead have a HSL-based palette with 15 hues out of 100. This rather defeats the purpose of merging.

I further don't like the way it's implemented.

##### Description of changes

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
248,2020-03-26T15:25:38Z,2020-03-27T08:08:44Z,,,"##### Issue
<!-- E.g. Fixes #1, Closes #2, Resolves #3, etc. -->
<!-- Or a short description, if the issue does not exist. -->

Ref gh-4264

##### Description of changes

* Read configured paths from a .cfg file. The cfg file can use and define install prefix relative paths
* Use the defined paths to override the default paths for storing widget and canvas settings.


##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
249,2020-03-26T13:44:34Z,2020-03-27T08:08:00Z,,,"##### Issue

Fixes #4555.

##### Description of changes

Print two more decimals when the format is not `%g` and the difference from the rounded values is outside numeric error.

##### Includes
- [X] Code changes
- [X] Tests
",8357227
250,2020-03-26T08:11:03Z,2020-03-27T08:39:01Z,,,"##### Issue
<!-- E.g. Fixes #1, Closes #2, Resolves #3, etc. -->
<!-- Or a short description, if the issue does not exist. -->

conda added support for a new' .conda' package format. The current installer only handles .tar.bz2 packages.

##### Description of changes

Add support for .conda packages

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
251,2020-03-24T14:54:39Z,2020-03-27T08:05:40Z,,,"##### Issue
<!-- E.g. Fixes #1, Closes #2, Resolves #3, etc. -->
<!-- Or a short description, if the issue does not exist. -->
Documentation for Test and Score was out of date - old images, info about some new features missing.

##### Description of changes
Updated documentation for Test and Score

##### Includes
- [X] Documentation
",8357227
252,2020-03-23T15:17:06Z,2020-03-27T08:05:06Z,,,"##### Issue
<!-- E.g. Fixes #1, Closes #2, Resolves #3, etc. -->
<!-- Or a short description, if the issue does not exist. -->
`maximumContentsLength` parameter on ComboBox is deprecated and does not have an effect.

##### Description of changes
Removing all appearances of `maximumContentsLength` in Orange.

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
253,2020-03-23T14:30:37Z,2020-03-23T14:52:03Z,,,"##### Description of changes
Now when we have ComboBoxSearch available, it can be used where suitable. With this PR I am adding it to all widgets from the unsupervised section where a lot of options can be in the combo box - mostly where variables appear in combo boxes:

- Distance Map
- Self-organizing map

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
254,2020-03-23T14:06:25Z,2020-03-23T14:40:30Z,,,"##### Description of changes

Now when we have ComboBoxSearch available, it can be used where suitable. With this PR I am adding it to all widgets from the evaluate section where a lot of options can be in the combo box - mostly where variables appear in combo boxes:

- Test and Score
- Roc analysis
- Lift curve
- Calibration plot

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
255,2020-03-23T11:42:55Z,2020-03-23T11:56:57Z,,,"##### Description of changes
Now when we have ComboBoxSearch available, it can be used where suitable. With this PR I am adding it to all widgets from the visualize section where a lot of options can be in the combo box - mostly where variables (and values) appear in combo boxes:

- Tree viewer
- Distributions
- Mosaic
- Sieve diagram
- Heat Map
- Pythagorean Forest
- Pythagorean Tree
- Silhouette Plot
- Nomogram

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
256,2020-03-23T10:18:22Z,2020-03-27T08:04:48Z,,,"##### Description of changes
Now when we have ComboBoxSearch available, it can be used where suitable. With this PR I am adding it to all widgets from the data section where a lot of options can be in the combo box - mostly where variables appear in combo boxes:

- Pivot table
- Select rows
- Merge data
- Transpose
- Create class
- Feature constructor
- Feature statistics

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
257,2020-03-17T01:12:57Z,2020-03-20T08:32:48Z,,,"##### Issue
After Orange 3.21 we introduced quite a few performance-regressions to K-means, which became very slow for large data set. 

##### Description of changes

1. Preprocess data once. In master, data set is preprocessed separately for each number of clusters and then also when computing silhouettes. If used with from-to this caused too many in-memory copies of data. Which means crashes on big data due to memory usage. Note that sklearn's k-means makes another copy of the data.

2. Only compute approximate (sampled) silhouettes for big data. Fixes bug introduced in Orange 3.22. Compute them in worker threads.

3. Fix O(|features|^2) when creating centroids.

There is a further obvious improvement that I did not tackle in this PR: preprocessing in a worker thread. Now (=this branch, master, current release) the preprocessing for big data blocks UI for about a minute for a data set with 98304 rows and 806 columns. It is mainly normalization. If it is disabled, it only blocks for a few seconds. 

Benchmarks
=========

Orange  3.21

```
[from_to_100_100] with 3 loops, best of 3:
	min 372 msec per loop
	avg 374 msec per loop
[from_to_100_100_no_normalize] with 3 loops, best of 3:
	min 379 msec per loop
	avg 414 msec per loop
[from_to_sampled_silhouette] with 3 loops, best of 3:
	min 2.1 sec per loop
	avg 2.18 sec per loop
[wide] with 3 loops, best of 3:
	min 306 msec per loop
	avg 315 msec per loop
```

Master

```
[from_to_100_100] with 3 loops, best of 3:
	min 1.98 sec per loop
	avg 1.99 sec per loop
[from_to_100_100_no_normalize] with 3 loops, best of 3:
	min 824 msec per loop
	avg 828 msec per loop
[from_to_sampled_silhouette] with 3 loops, best of 3:
	min 10.6 sec per loop
	avg 10.7 sec per loop
[wide] with 3 loops, best of 3:
	min 2.43 sec per loop
	avg 2.44 sec per loop
```

This branch

```
[from_to_100_100] with 3 loops, best of 3:
	min 636 msec per loop
	avg 663 msec per loop
[from_to_100_100_no_normalize] with 3 loops, best of 3:
	min 462 msec per loop
	avg 486 msec per loop
[from_to_sampled_silhouette] with 3 loops, best of 3:
	min 2.62 sec per loop
	avg 2.99 sec per loop
[wide] with 3 loops, best of 3:
	min 640 msec per loop
	avg 649 msec per loop
```

Still not quite Orange 3.21 performance, but will do. The different in [wide] is due to normalization.


##### Includes
- [X] Code changes
- [X] Benchmarks
- [ ] Documentation
",8357227
258,2020-03-15T20:04:25Z,2020-03-20T14:54:19Z,,,"##### Issue

Fixes #4522.

##### Description of changes

- Instead of `%.2f`, it now uses `str_val`. `str_val` will show a larger number of decimals in the future (in another pull request).
- Model's `data` method was split into functions for readability and lower cyclomatic complexity.

##### Includes
- [X] Code changes
- [ ] Tests
",8357227
259,2020-03-04T14:20:00Z,2020-03-04T14:20:00Z,,,"##### Issue
<!-- E.g. Fixes #1, Closes #2, Resolves #3, etc. -->
<!-- Or a short description, if the issue does not exist. -->


##### Description of changes


##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
260,2020-02-28T15:31:10Z,2020-03-27T09:54:41Z,,,"##### Issue

`PyTableModel.__setitem__` didn't properly signal the change: it emitted `dataChanged` for indices in the existing slice, which was empty for empty models.

0. In Time Series, pull https://github.com/biolab/orange3-timeseries/pull/80.
1. Take Datasets, load Philadelphia. 
2. Connect to Aggregate, change Aggregation function for one of the attributes.
3. Disconnect Aggregate
4. Connect again. Previous changes are gone (because they were not saved because the model didn't emit the signal properly).

##### Description of changes

Add proper signals.

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
261,2020-02-07T22:09:03Z,2020-03-19T17:32:26Z,,,"##### Description of changes
The code editor (a QPlainTextEdit) is replaced with Qutepart (a QPlainTextEdit subclass).
The console is replaced with a jupyter qtconsole, which runs a customized IPython kernel. 

The main process and kernel process communicate in JSON, so variables are pickled, base64 encoded, sent, decoded, unpickled. This is very space inefficient. Perhaps something like https://jsonpickle.github.io/ could alleviate the issue?

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
262,2020-02-03T00:11:42Z,2020-02-04T21:07:41Z,,,"Another crossover PR with canvas core!
This PR depends on https://github.com/biolab/orange-canvas-core/pull/79, hence raising the requirement, but is not necessary for merging the orange-canvas-core PR. Please check that the raised requirement is correct before merging.

I'll remove the NOMERGE tag when the canvas-core PR is released.

##### Description of changes
This applies to opening Orange without a file request (not specifying a file in args or double-clicking an `.ows` file).

Orange will check whether there were any crashed scratch (unsaved) workflows, asking the user if they wish to restore them.

I couldn't find any nice way to implement behavior on startup solely in canvas-core. Should we make a standard way for doing this? I could then also refactor some of the notifications code out of orange3.

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
263,2020-01-21T16:21:32Z,2020-03-16T16:57:59Z,,,"#### Issue
<!-- E.g. Fixes #1, Closes #2, Resolves #3, etc. -->
<!-- Or a short description, if the issue does not exist. -->

Ref gh-4284

Fix an runtime error at process exit

Will require https://github.com/biolab/orange-widget-base/pull/37 to be merged and released

##### Description of changes

* Give QGraphicsScenes in VennDiagram and BoxPlot a parent the (OWWidget instance). Otherwise the scenes survive until process exit on PyQt 5.14.* 
* Enable testing on PyQt5 5.14

##### Includes
- [X] Code changes
- [ ] Tests
- [ ] Documentation
",8357227
264,2020-03-20T13:17:07Z,2020-03-20T13:17:07Z,,,,33702544
265,2020-02-27T23:49:39Z,2020-03-17T20:45:52Z,,,"This is an attempt to reduce warnings printed during tests. Currently whenever a `wait_cond` fails, an exception stacktrace is printed. Since failures are expected fairly often, the test output of even a successful run of the Dash Enterprise automated integration tests are filled with them.

The basic idea here is to leave the functionality of `until()` and `until_not()` unchanged by default, but allow `wait_cond` classes to specify exceptions to be considered ""false"".  If exceptions are specified, `until()` will catch them and will print the output of the most recent one only if the test times out. This results in much less noise than the current design, where `wait_cond` needs to print every exception it catches to allow debugging in the event of a timeout.

## Contributor Checklist

- [x] I have broken down my PR scope into the following TODO tasks
   -  [x] Test with the Dash Enterprise automated integrated tests
   -  [x] Make a similar change to `until_not()`
   -  [x] Make similar changes to the other `wait_cond()` classes in `wait.py`
- [x] I have run the tests locally and they passed. (refer to testing section in [contributing](https://github.com/plotly/dash/blob/master/CONTRIBUTING.md))
- [x] I have added tests, or extended existing tests, to cover any new features or bugs fixed in this PR",33702544
266,2020-02-19T18:06:41Z,2020-03-26T20:00:11Z,,,"## About 
Related to https://github.com/plotly/dash-core/issues/152

## Contributor Checklist

- [ ] I have broken down my PR scope into the following TODO tasks
   -  [ ] task 1
   -  [ ] task 2
- [ ] I have run the tests locally and they passed. (refer to testing section in [contributing](https://github.com/plotly/dash/blob/master/CONTRIBUTING.md))
- [ ] I have added tests, or extended existing tests, to cover any new features or bugs fixed in this PR

### optionals

- [ ] I have added entry in the `CHANGELOG.md`
- [ ] If this PR needs a follow-up in **dash docs**, **community thread**, I have mentioned the relevant URLS as follow
    -  [ ] this github [#PR number]() updates the dash docs
    -  [ ] here is the show and tell thread in plotly dash community 
",33702544
267,2020-02-18T23:45:33Z,2020-03-10T19:15:47Z,,,"Hello,

I'd like to add support for dashR to dash-bootstrap-components. I hit a bit of a stumbling block though because we put the generated components in `dash_bootstrap_components/_components`, whereas the build pipeline in `_r_components_generation.py` appears to use the project shortname as the assumed location of the generated components / package.json etc. 

Passing `dash_bootstrap_components/_components` as the project shortname doesn't resolve the issue as this is passed to importlib which then of course can't find a module by that name.

This tentative PR contains what I believe to be a minimal change that allows me to build R components for dash-bootstrap-components. It splits the package name off from the front of the path to the generated components.

I have tested that my changes don't break the build processes for dash-core-components, dash-html-components and dash-table.

One point for discussion is that I think the variable names such as `project_shortname` are now misleading in some places, but I left them as is for now in the interest of making minimal changes.

Anyway, very happy to discuss whether this is the right way forward or if there is a better solution.

## Contributor Checklist

- [ x ] I have run the tests locally and they passed. EDIT - looks like percy is failing because of a visual change that is somewhat surprising to me. Might need a bit of assistance figuring out where that is coming from and whether my proposed changes are responsible.

@rpkyle - I think this mostly falls under your remit? All of the changes are contained to `_r_components_generation.py`.
",33702544
268,2020-02-14T18:45:50Z,2020-03-06T03:22:06Z,,,"Via a new property in the `app.callback`, `prevent_initial_call=True`


```
@app.callback(Output('another-output', 'children'), [Input('another', 'value')], prevent_initial_call=True)
```

This will prevent it for both initial renders in `app.layout` as well as new component renders from a callback.

WIP / POC for now. Would need lots of tests.",33702544
269,2020-01-30T01:11:45Z,2020-03-27T02:18:51Z,,,"Wildcards!

Closes #475 - using the API discussed there (I'll post some examples soon, but for now here's the description):
- String IDs still work the same way in both layout and callbacks
- Now you can also make dict IDs. Keys should be strings, values can be strings, numbers, booleans, like:
  - `id={""type"": ""cat"", ""age"": 4, ""purrs"": True}`
  - IDs must still be unique
- Callbacks can use dict IDs with wildcard values for any of the keys. The supported wildcards are `MATCH`, `ALL`, and `ALLSMALLER` - and can be imported from `dash.dependencies`. There are various rules about how these can combine, driven by the need to resolve these to a unique callback invocation for each set of outputs and identify uniquely the inputs required for this invocation...

To do before this is ready:
- [x] Move most callback validation logic to the front end (so it doesn't need to be replicated in R)
- [ ] Do something with the callback graph when there are wildcards
- [x] Test that this PR closes callback failure bugs: #1053, #1071, #1084, #1105
- [x] See if it also closes redundant callback bugs #635, #832

## Contributor Checklist
- [x] I have run the tests locally and they passed. (refer to testing section in [contributing](https://github.com/plotly/dash/blob/master/CONTRIBUTING.md))
- [x] I have added tests, or extended existing tests, to cover any new features or bugs fixed in this PR
- [ ] I have added entry in the `CHANGELOG.md`
- [ ] If this PR needs a follow-up in **dash docs**, **community thread**, I have mentioned the relevant URLS as follow
    -  [ ] this github [#PR number]() updates the dash docs
    -  [ ] here is the show and tell thread in plotly dash community 

Closes #832 (among other callback bugs - the others all have commits that close them with matching tests, but #832 is a bit heavy for a test of its own)
Closes #1146",33702544
270,2020-01-15T22:30:04Z,2020-01-15T22:30:04Z,,,"I've been using Dash for a while and would like to dip my toes into Dash development -- would this PR be welcome?

- [x] Previously, all callbacks were emerald (green), now clientside callbacks will be sienna (red).
- [ ] I would like to color code components that are not present in the layout differently -- this would be very useful for debugging if `suppress_callback_exceptions` is enabled. However, it's not clear to me what the cleanest way is to do this, would anyone have any suggestions here?

- [ ] I have run the tests locally and they passed. (refer to testing section in [contributing](https://github.com/plotly/dash/blob/master/CONTRIBUTING.md))
- [ ] I have added tests, or extended existing tests, to cover any new features or bugs fixed in this PR

### optionals

- [ ] I have added entry in the `CHANGELOG.md`
- [ ] If this PR needs a follow-up in **dash docs**, **community thread**, I have mentioned the relevant URLS as follow
    -  [ ] this github [#PR number]() updates the dash docs
    -  [ ] here is the show and tell thread in plotly dash community 
",33702544
271,2020-01-11T13:46:55Z,2020-01-28T08:32:42Z,,,"Install aspell on CircleCI. Add a custom dictionary file.

Spell check the Markdown files with a shell script.

Add section on spell checking to the contributing guide.

Update the CHANGELOG.

http://aspell.net/

https://en.wikipedia.org/wiki/GNU_Aspell",33702544
272,2020-01-10T17:22:36Z,2020-03-26T18:29:47Z,,,"Closes #481 

Add arbitrary file support for component suite.

This is being done with the idea of packaging fonts with the components that require them without the need for adding assets or loading everything upfront as base64 inside css files. Images would be handled similarly, also with the `file-loader`.

Once added, component repos using webpack can use the following loader configuration:

```
{
    test: /\.(woff|woff2|eot|svg|ttf)$/,
    use: [{
        loader: 'file-loader',
        options: {
            limit: 0, // no resource gets inlined in the js bundle
            esModule: false, // this option is required for file-loader >= 5.0.0
            name: '[name].[ext]'
        }
    }]
}
```

And include source resources in css files like so:
```
@font-face {
    font-family: 'My-Font';
    src: url(./My-Font.ttf) format('truetype');
}
```

`url(./My-Font.ttf)` will be resolved to the correct path by the same plugin used for async (https://github.com/plotly/dash/tree/dev/%40plotly/webpack-dash-dynamic-import)

And include the resource in the component's `__init__.py` (and MANIFEST):
```
{
    'relative_package_path': 'My-Font.ttf',
    'external_url': (
       'https://unpkg.com/my-dash-components@{}'
        '/my_dash_component/My-Font.ttf'
    ).format(__version__),
    'namespace': 'my_dash_component',
    'dynamic': True
},
```

Adding `dynamic:True` will ensure that the file is only loaded when requested by the css (e.g. async scenarios).

Do note that as implemented, these resources will not be fingerprinted and will default to using `eTag` for caching purposes.",33702544
273,2019-12-23T15:15:38Z,2020-01-31T09:34:34Z,,,"Add a basic .yamllint config file.  Add yamllint to requires-dev.txt.  Lint some YAML.

Update CHANGELOG.md

List yamllint in the coding style section of the contributing guide.

Add links to flake8, pylint and yamllint documentation.

Update CONTRIBUTING.md to include new information on tmux, Powerline, Powerlevel9k and Oh-My-Zsh.

https://pypi.org/project/yamllint/

https://yamllint.readthedocs.io/en/stable/index.html
",33702544
274,2019-11-22T00:36:40Z,2019-11-22T19:02:40Z,,,"1. use pip-compile to pin the deps
2. use makefile to simplify the CI

*Start with a description of this PR. Then edit the list below to the items that make sense for your PR scope, and check off the boxes as you go!*

This PR is trying to solve the #1005 with pip-tools, I found that `poetry` is still not mature enough to be ready for the whole workflow management. the `poetry install` is hard to control and sometimes it misses the dependencies of a dependency, like `flake8` => `entrypoints`. though it's still a solid tool to consider in the future for `dash`, because it can put all the constraints for different python versions in a single place. 

The pip-tools has different approach, it contains `pip-compile` and `pip-sync`. I use the first here to do one simple job, it will upgrade and freeze the version from a simple dependency list file. we can use this tool to lock the dependency in a consistent way.   

## Contributor Checklist

- [x] I have broken down my PR scope into the following TODO tasks
   -  [x] use pip-compile to lock
   -  [x] use makefile to simplify the config.yml
- [x] I have run the tests locally and they passed. (refer to testing section in [contributing](https://github.com/plotly/dash/blob/master/CONTRIBUTING.md))


### optionals

- [ ] I have added entry in the `CHANGELOG.md`
",33702544
275,2019-10-25T10:43:56Z,2019-10-25T10:49:27Z,,,"This PR migrates Dash from Flask to Quart. This change should not break existing Dash apps.

This is an effort to move the ship a few degrees closer to supporting some form of web socket update or callback. However I would expect support for this to be added in another PR.

TODO:
- [] Replace minor dependencies on flask such as 'flask.helpers.get_root_path'
- [] Swap references from flask to quart in code and docs
- [] Update docs to discuss async callbacks including pitfalls such as calling sync code
",33702544
276,2019-10-06T13:30:32Z,2019-10-17T02:42:51Z,,,"Hello,

I've made some tweaks to the component generator in an attempt to make it more lenient with the paths that Dash boilerplate projects can be set up at. Specifically, paths with whitespace should now be acceptable.

Some remarks concerning the changes:

* Quoting the path to the `extract-meta.js` script helps avoid it being interpreted as multiple arguments (i.e. delimited by whitespace) when passed to Node
* Non-POSIX mode for `shlex.split()` appears to undesirably interfere with the script path, producing something incorrect on my Windows box, which I thought was odd.
* Also got rid of the `shell=True` because according to the [`subprocess.Popen` documentation][1], _""The only time you need to specify shell=True on Windows is when the command you wish to execute is built into the shell (e.g. dir or copy).""_

I also tested these changes on an Ubuntu box to see whether paths with whitespace could work, and they did.

Relevant issue reported previously: plotly/dash-component-boilerplate#71

[1]: https://docs.python.org/3.7/library/subprocess.html#subprocess.Popen",33702544
277,2019-09-17T19:39:30Z,2019-09-24T11:04:27Z,,,"This is a tiny PR inspired from my reviewing https://github.com/plotly/dash-bio/pull/411. Why the distance (expressed by ""those"")? I believe the meaning we want is that of ""these.""

## Contributor Checklist

- [ ] I have broken down my PR scope into the following TODO tasks
   -  [ ] task 1
   -  [ ] task 2
- [ ] I have run the tests locally and they passed. (refer to testing section in [contributing](https://github.com/plotly/dash/blob/master/CONTRIBUTING.md))
- [ ] I have added tests, or extended existing tests, to cover any new features or bugs fixed in this PR

### optionals

- [ ] I have added entry in the `CHANGELOG.md`
- [ ] If this PR needs a follow-up in **dash docs**, **community thread**, I have mentioned the relevant URLS as follow
    -  [ ] this github [#PR number]() updates the dash docs
    -  [ ] here is the show and tell thread in plotly dash community 
",33702544
278,2019-06-22T15:36:31Z,2019-08-14T18:52:51Z,,,"Hello! this is the idea referenced in #787 I thought I might as well show you what I'm on about. This gives me the chance to solve issue 2 there.

There is a lot here, but in the end, there is no significant change to currently running and tested code. On the other hand the new functionality can only be tested locally yet. ( circleci/python:#.#-stretch-node-browsers does not have a redis server set up, so i expect the ci tests to break. sorry for that, if this direction is approved, that obviously needs to be fixed)

I am merely suggesting, that this is a possible way towards the the feature of being able to dynamically add interactive elements in dash, which I find to be quite powerful. I'm working on a few projects involving dash, and all of them would greatly benefit from something like this, so if you have suggestions or some ideas how this can evolve into something that can be merged, I'm willing to put in the work.


## Contributor Checklist

- [x] I have broken down my PR scope into the following TODO tasks
   -  [x] ```validate_callback_output``` function needed to be moved from ```@staticmethod``` to top level function to allow dill pickling. it is not used anywhere else
   -  [x] callback_map assignment needed to be changed, this way it's elements can be immutable
   -  [x] a ```RedisDict``` class added and unit tested to optionally replace the dict type of callback_map
   -  [x] tests for the redis callback_map and multiple output callbacks (this was also a necessity, as multiprocessing.Value does not work with dill pickled functions)
- [x] I have run the tests locally and they passed. (refer to testing section in [contributing](https://github.com/plotly/dash/blob/master/CONTRIBUTING.md)) - since the linked file does not work as dev-requirements.txt does not exist, I needed to improvise
- [x] I have added tests, or extended existing tests, to cover any new features or bugs fixed in this PR
  - [x] unit test for the redis dict
  - [x] simple callback tests for redis callback map",33702544
279,2019-05-24T23:34:05Z,2019-08-14T18:53:07Z,,,"Very much a work in progress. Not attempting to rewrite all the generation, just to provide some level of abstraction between the generation and the file structure through parametrization + isolating package building from the nitty-gritty of the generation of the various parts. So a good portion of what is happening here is moving `with open(...) as f:` higher up the stack.

- support building R and Python artefacts to arbitrary folder (dist/<flavor>) folders  (`--dist` flag)
- untie R and Python build implementations
- use `root` folder otherwise if `--dist` is not present

- [ ] meta file for js artefacts (will untie R from Python)
- [ ] meta file for name/description/etc. -- can default to package.json -- untie ourselves from the pacakge.json entirely at some point",33702544
280,2019-03-13T23:05:10Z,2019-05-08T01:21:41Z,,,,33702544
281,2019-03-11T07:57:18Z,2019-05-08T01:20:14Z,,,"This is a proposed approach for handling delayed callback validation. As discussed in #519, immediate callback validation requires the Dash layout to be already set, imposing rigid constraints on where callbacks can be defined within a Dash app. In particular, this making arrangement of pages within multi-page apps that contain both callbacks and layout fragments impossible. 

This change was more extensive than expected, so at the moment, this is perhaps more of way to get the conversation going.

* this approach involves adding to a list of registered callbacks in `Dash._callback_list` (distinct from the `Dash.callback_map`) when each callback is defined. The list is then used  to perform validation when the app starts
* I think it's critical that this callback validation be applied for WSGI apps as well as apps run through `run_server`, so I did not put this in the enable_dev_tools_folder, where I think there is a risk of users not running it. So instead I've created a `_setup()` method that should both be run by `run_server` and also `get_wsgi_application`, which would become the required way to get the WSGI callable for when using a WSGI server. This is modelled off [Django's approach](https://github.com/django/django/blob/master/django/core/wsgi.py). So this is also a proposed solution to the general problem of keeping the `run_server` and WSGI app entry points synchronised (which perhaps should go in a different PR though)
* I had to split the current callback validation tests into two, as the current code assumes that buggy callbacks can be caught through exceptions and continue on to use the already defined Dash app, which doesn't work that way now
* A slightly unsatisfying property of this solution is that for an app with invalid callbacks, before validation is applied (and an error thrown), the `Dash.callback_map` will be in a somewhat dirty state, with eg duplicate callbacks clobbering  existing ones. ",33702544
282,2020-03-18T23:42:06Z,2020-03-21T22:20:21Z,,,"This PR contains the following updates:

| Package | Type | Update | Change |
|---|---|---|---|
| [flow-bin](https://togithub.com/flowtype/flow-bin) ([changelog](https://togithub.com/facebook/flow/blob/master/Changelog.md)) | devDependencies | minor | [`^0.120.0` -> `^0.121.0`](https://renovatebot.com/diffs/npm/flow-bin/0.120.1/0.121.0) |

---

### Release Notes

<details>
<summary>flowtype/flow-bin</summary>

### [`v0.121.0`](https://togithub.com/flowtype/flow-bin/compare/v0.120.1...v0.121.0)

[Compare Source](https://togithub.com/flowtype/flow-bin/compare/v0.120.1...v0.121.0)

</details>

---

### Renovate configuration

:date: **Schedule**: At any time (no schedule defined).

:vertical_traffic_light: **Automerge**: Enabled.

:recycle: **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.

:no_bell: **Ignore**: Close this PR and you won't be reminded about this update again.

---

 - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box

---

This PR has been generated by [WhiteSource Renovate](https://renovate.whitesourcesoftware.com). View repository job log [here](https://app.renovatebot.com/dashboard#nteract/hydrogen).",35395056
283,2020-03-10T19:31:03Z,2020-03-21T22:20:32Z,,,"This PR contains the following updates:

| Package | Type | Update | Change |
|---|---|---|---|
| [react-table](https://togithub.com/tannerlinsley/react-table) | dependencies | major | [`^6.10.0` -> `^7.0.0`](https://renovatebot.com/diffs/npm/react-table/6.11.5/7.0.0) |

---

### Release Notes

<details>
<summary>tannerlinsley/react-table</summary>

### [`v7.0.0`](https://togithub.com/tannerlinsley/react-table/blob/master/CHANGELOG.md#&#8203;700-)

-   Fixed an issue where page options array could be empty
-   Fixed an issue where duplicate columns would be silently deduped. There is now a warning when duplicate columns are found based on their IDs
-   Moved some functions around so they will get treeshaked with their respective plugins that use them.
-   Fixed an issue where filters, sorting, or grouping changes would not reset pagination
-   Added table and column level options for disabling global filters
-   Fixed an issue where row selection was not deselecting rows
-   Fixed an issue where flex table rendering was not giving the table a minimum width with necessary
-   Fixed an issue where row selection would not work when using other row-transformative plugins like filters or grouping
-   Fixed an issue where header groups were not memoized correctly

</details>

---

### Renovate configuration

:date: **Schedule**: At any time (no schedule defined).

:vertical_traffic_light: **Automerge**: Disabled by config. Please merge this manually once you are satisfied.

:recycle: **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.

:no_bell: **Ignore**: Close this PR and you won't be reminded about this update again.

---

 - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box

---

This PR has been generated by [WhiteSource Renovate](https://renovate.whitesourcesoftware.com). View repository job log [here](https://app.renovatebot.com/dashboard#nteract/hydrogen).",35395056
284,2020-02-28T17:55:22Z,2020-03-21T22:20:27Z,,,"This PR contains the following updates:

| Package | Type | Update | Change |
|---|---|---|---|
| [@jupyterlab/services](https://togithub.com/jupyterlab/jupyterlab) | dependencies | major | [`^0.52.0` -> `^5.0.0`](https://renovatebot.com/diffs/npm/@jupyterlab%2fservices/0.52.0/5.0.2) |

---

### Release Notes

<details>
<summary>jupyterlab/jupyterlab</summary>

### [`v5.0.2`](https://togithub.com/jupyterlab/jupyterlab/compare/f6b15daa707364b5e59384920a3849650972e473...621e371dc73091986d0b28a8d31b6288f54e561e)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/f6b15daa707364b5e59384920a3849650972e473...621e371dc73091986d0b28a8d31b6288f54e561e)

### [`v5.0.1`](https://togithub.com/jupyterlab/jupyterlab/compare/757e5cb736ed60716fb9e8de5ad07ddae6fd6126...f6b15daa707364b5e59384920a3849650972e473)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/757e5cb736ed60716fb9e8de5ad07ddae6fd6126...f6b15daa707364b5e59384920a3849650972e473)

### [`v5.0.0`](https://togithub.com/jupyterlab/jupyterlab/compare/477084d700c421af8feb53f4a1d8542ef672c2a8...757e5cb736ed60716fb9e8de5ad07ddae6fd6126)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/477084d700c421af8feb53f4a1d8542ef672c2a8...757e5cb736ed60716fb9e8de5ad07ddae6fd6126)

### [`v4.2.0`](https://togithub.com/jupyterlab/jupyterlab/compare/d726f5dd9a5d4bd0c92306b0fc0cff576ba27fba...477084d700c421af8feb53f4a1d8542ef672c2a8)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/d726f5dd9a5d4bd0c92306b0fc0cff576ba27fba...477084d700c421af8feb53f4a1d8542ef672c2a8)

### [`v4.1.1`](https://togithub.com/jupyterlab/jupyterlab/compare/65af312f8977f050b259b0d1c30901c555c261ad...d726f5dd9a5d4bd0c92306b0fc0cff576ba27fba)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/65af312f8977f050b259b0d1c30901c555c261ad...d726f5dd9a5d4bd0c92306b0fc0cff576ba27fba)

### [`v4.1.0`](https://togithub.com/jupyterlab/jupyterlab/compare/dab1810ec8a9c44452cbc57ce8194bc7b1726720...65af312f8977f050b259b0d1c30901c555c261ad)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/dab1810ec8a9c44452cbc57ce8194bc7b1726720...65af312f8977f050b259b0d1c30901c555c261ad)

### [`v4.0.5`](https://togithub.com/jupyterlab/jupyterlab/compare/879d00168755fdab1c067d078221073458899c53...dab1810ec8a9c44452cbc57ce8194bc7b1726720)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/879d00168755fdab1c067d078221073458899c53...dab1810ec8a9c44452cbc57ce8194bc7b1726720)

### [`v4.0.4`](https://togithub.com/jupyterlab/jupyterlab/compare/b5f2c50eda10332be812658b23ad649eca22bf80...879d00168755fdab1c067d078221073458899c53)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/b5f2c50eda10332be812658b23ad649eca22bf80...879d00168755fdab1c067d078221073458899c53)

### [`v4.0.3`](https://togithub.com/jupyterlab/jupyterlab/compare/325e6244934e6e3b29c630e326953101f7ec172d...b5f2c50eda10332be812658b23ad649eca22bf80)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/325e6244934e6e3b29c630e326953101f7ec172d...b5f2c50eda10332be812658b23ad649eca22bf80)

### [`v4.0.2`](https://togithub.com/jupyterlab/jupyterlab/compare/7907e52fc52e4155945c8e55064467464f4f9c5c...325e6244934e6e3b29c630e326953101f7ec172d)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/7907e52fc52e4155945c8e55064467464f4f9c5c...325e6244934e6e3b29c630e326953101f7ec172d)

### [`v4.0.1`](https://togithub.com/jupyterlab/jupyterlab/compare/382da7c3efb2696a0ae26a4fea27aeeead99fbd9...7907e52fc52e4155945c8e55064467464f4f9c5c)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/382da7c3efb2696a0ae26a4fea27aeeead99fbd9...7907e52fc52e4155945c8e55064467464f4f9c5c)

### [`v4.0.0`](https://togithub.com/jupyterlab/jupyterlab/compare/7fc900168981e58051253ecc66a18015a052cd2f...382da7c3efb2696a0ae26a4fea27aeeead99fbd9)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/7fc900168981e58051253ecc66a18015a052cd2f...382da7c3efb2696a0ae26a4fea27aeeead99fbd9)

### [`v2.0.1`](https://togithub.com/jupyterlab/jupyterlab/compare/v2.0.0...v2.0.1)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/v2.0.0...v2.0.1)

### [`v2.0.0`](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.4...v2.0.0)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.4...v2.0.0)

### [`v1.1.4`](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.3...v1.1.4)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.3...v1.1.4)

### [`v1.1.3`](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.2...v1.1.3)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.2...v1.1.3)

### [`v1.1.2`](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.1...v1.1.2)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.1...v1.1.2)

### [`v1.1.1`](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.0...v1.1.1)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/v1.1.0...v1.1.1)

### [`v1.1.0`](https://togithub.com/jupyterlab/jupyterlab/compare/v1.0.1...v1.1.0)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/v1.0.1...v1.1.0)

### [`v1.0.1`](https://togithub.com/jupyterlab/jupyterlab/compare/v1.0.0...v1.0.1)

[Compare Source](https://togithub.com/jupyterlab/jupyterlab/compare/v1.0.0...v1.0.1)

</details>

---

### Renovate configuration

:date: **Schedule**: At any time (no schedule defined).

:vertical_traffic_light: **Automerge**: Disabled by config. Please merge this manually once you are satisfied.

:recycle: **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.

:no_bell: **Ignore**: Close this PR and you won't be reminded about this update again.

---

 - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box

---

This PR has been generated by [WhiteSource Renovate](https://renovate.whitesourcesoftware.com). View repository job log [here](https://app.renovatebot.com/dashboard#nteract/hydrogen).",35395056
285,2019-07-14T17:34:37Z,2019-08-25T00:57:16Z,,,"## :tada: New feature - Inspection datatip

Use datatip service from Atom IDE, we can show inspection result of the code on hover.

## Overview

![Screenshot 2019-07-15 02 26 51](https://user-images.githubusercontent.com/40514306/61187065-4989e080-a6a8-11e9-891a-9adea90db360.png)

![Screenshot 2019-07-15 02 30 10](https://user-images.githubusercontent.com/40514306/61187086-9a013e00-a6a8-11e9-95cf-8b75f3ef2586.png)


## Tasks

- [x] Datatip for non-active editors
- [ ] Make inspected code range valid:
    * We need the start of inspected code range as well as the end of that
    *  I could implement the end of the range correctly referring to our `getCodeToInspect` function, but as for the _start_ still not
- May merge #1706 first, and I should refactor this PR according to that
- Configs
    * [ ] Config to turn off this feature
    * [ ] Config to set the priority of the Hydrogen datatip
- [ ] Update documentation

## :question: Questions and Discussions

- How does it work ?
    * ""Atom IDE packages"" (I will elaborate them below) offers the datatip service and we can consume it just like Status bar.
    * The datatip service creates the base react component and we can pass an inspection component using `@nteract` packages to it.
- Atom IDE is already dead and this can be a source of problem ?
    * `nuclide` is certainly already dead, but Atom IDE is not IMO.
    * There is a community maintained package for the datatip service at least: https://atom.io/packages/atom-ide-datatip
- What packages do offer the service ?
    * Old [Atom-IDE-UI](https://github.com/facebookarchive/atom-ide-ui):
        + Bundles the other frontend packages for Atom LSPs as well, but they are not necessary for this feature to work
        + As you may know, dead
        + [x] Works
    * [Atom-IDE-Datatip](https://github.com/atom-ide-community/atom-ide-datatip)
        + Community maintained (and hopefully alive)
        + [x] Confirmed to work as well
    * **An user should install one of the above to enjoy this feature**
        + :question: Shall we add them to our dependencies, or just recommend in our docs ?
- If an user doesn't install a package providing datatip service ?
    * Just nothing happens


## Refs

- https://github.com/atom-ide-community/atom-ide-datatip
- Closes #1415


## Misc

I also tweaked a style in our inspector pane:
- Use editor font in inspection result
- Use our custom `Markdown` component instead of `@nteract/outputs.Media.Markdown`",35395056
286,2019-04-30T16:15:11Z,2020-03-21T22:20:37Z,,,"This PR contains the following updates:

| Package | Type | Update | Change |
|---|---|---|---|
| [ws](https://togithub.com/websockets/ws) | dependencies | major | [`^3.3.1` -> `^7.0.0`](https://renovatebot.com/diffs/npm/ws/3.3.3/7.2.3) |

---

### Release Notes

<details>
<summary>websockets/ws</summary>

### [`v7.2.3`](https://togithub.com/websockets/ws/releases/7.2.3)

[Compare Source](https://togithub.com/websockets/ws/compare/7.2.2...7.2.3)

### Bug fixes

-   `WebSocket#{p{i,o}ng,close}()` now thow an error if the data to send is too
    large to fit in a control frame ([`e54f08d`](https://togithub.com/websockets/ws/commit/e54f08da)).

### [`v7.2.2`](https://togithub.com/websockets/ws/releases/7.2.2)

[Compare Source](https://togithub.com/websockets/ws/compare/7.2.1...7.2.2)

### Bug fixes

-   Fixed an issue where calling `webSocketStream.end()` could cause the process
    to crash ([`9535702`](https://togithub.com/websockets/ws/commit/9535702e)).
-   The connection is now closed if a non-masked frame is received on the server
    or a masked frame is received on the client ([#&#8203;1681](https://togithub.com/websockets/ws/issues/1681)).
-   The status code 1014 is now allowed to be used ([#&#8203;1682](https://togithub.com/websockets/ws/issues/1682)).

### [`v7.2.1`](https://togithub.com/websockets/ws/releases/7.2.1)

[Compare Source](https://togithub.com/websockets/ws/compare/7.2.0...7.2.1)

### Bug fixes

-   Added `bufferutil` and `utf-8-validate` as peer dependencies ([#&#8203;1626](https://togithub.com/websockets/ws/issues/1626)).

### [`v7.2.0`](https://togithub.com/websockets/ws/releases/7.2.0)

[Compare Source](https://togithub.com/websockets/ws/compare/7.1.2...7.2.0)

### Features

-   Added ability to specify the `readableObjectMode` option when using
    `WebSocket.createWebSocketStream()` ([#&#8203;1647](https://togithub.com/websockets/ws/issues/1647)).

### [`v7.1.2`](https://togithub.com/websockets/ws/releases/7.1.2)

[Compare Source](https://togithub.com/websockets/ws/compare/7.1.1...7.1.2)

### Bug fixes

-   Fixed a bug that caused compress jobs to never be marked as complete
    ([#&#8203;1618](https://togithub.com/websockets/ws/issues/1618)).

### [`v7.1.1`](https://togithub.com/websockets/ws/releases/7.1.1)

[Compare Source](https://togithub.com/websockets/ws/compare/7.1.0...7.1.1)

### Bug fixes

-   An error is now thrown if the `WebSocket` server constructor is used
    incorrectly ([`3641266`](https://togithub.com/websockets/ws/commit/36412662)).

### [`v7.1.0`](https://togithub.com/websockets/ws/releases/7.1.0)

[Compare Source](https://togithub.com/websockets/ws/compare/7.0.1...7.1.0)

### Features

-   Added utility to wrap a `WebSocket` in a `Duplex` stream ([#&#8203;1589](https://togithub.com/websockets/ws/issues/1589)).

### Bug fixes

-   Reverted ""[minor] Remove unneeded `if` statement"" ([`dbacf58`](https://togithub.com/websockets/ws/commit/dbacf582), [#&#8203;1591](https://togithub.com/websockets/ws/issues/1591)).

### [`v7.0.1`](https://togithub.com/websockets/ws/releases/7.0.1)

[Compare Source](https://togithub.com/websockets/ws/compare/7.0.0...7.0.1)

### Bug fixes

-   Added ability to disable sending the SNI extension ([#&#8203;1587](https://togithub.com/websockets/ws/issues/1587)).

### [`v7.0.0`](https://togithub.com/websockets/ws/releases/7.0.0)

[Compare Source](https://togithub.com/websockets/ws/compare/6.2.1...7.0.0)

### Breaking changes

-   Dropped support for Node.js 6 ([`1e6999b`](https://togithub.com/websockets/ws/commit/1e6999bb)).
-   Dropped support for `url.Url` instances in the `WebSocket` constructor
    ([`692d7b4`](https://togithub.com/websockets/ws/commit/692d7b47)).
-   The behavior of `WebSocket#{p{i,o}ng,send}()` has changed when the
    `readyState` attribute is not `OPEN` ([#&#8203;1532](https://togithub.com/websockets/ws/issues/1532))
    -   If the readyState attribute is `CONNECTING`, an exception is thrown.
    -   If the readyState attribute is `CLOSING` or `CLOSED`
        -   The `bufferedAmount` attribute is increased by the length of the `data`
            argument in bytes.
        -   If provided, the `callback` function is called with an error.
        -   No exception is thrown even if the `callback` function is not provided.

### [`v6.2.1`](https://togithub.com/websockets/ws/releases/6.2.1)

[Compare Source](https://togithub.com/websockets/ws/compare/6.2.0...6.2.1)

### Bug fixes

-   Fixed a bug that, under certain circumstances, prevented the close timer from
    being set ([`aa1dcd5`](https://togithub.com/websockets/ws/commit/aa1dcd5)).

### [`v6.2.0`](https://togithub.com/websockets/ws/releases/6.2.0)

[Compare Source](https://togithub.com/websockets/ws/compare/6.1.4...6.2.0)

### Features

-   Added ability to follow redirects ([#&#8203;1490](https://togithub.com/websockets/ws/issues/1490)).

### Bug fixes

-   The opening handshake is now aborted if the `Sec-WebSocket-Key` header field
    value is invalid ([`160af45`](https://togithub.com/websockets/ws/commit/160af45b)).

### [`v6.1.4`](https://togithub.com/websockets/ws/releases/6.1.4)

[Compare Source](https://togithub.com/websockets/ws/compare/6.1.3...6.1.4)

### Bug fixes

-   Fixed an issue that caused the `Host` header to always include a port ([#&#8203;1510](https://togithub.com/websockets/ws/issues/1510)).

### [`v6.1.3`](https://togithub.com/websockets/ws/releases/6.1.3)

[Compare Source](https://togithub.com/websockets/ws/compare/6.1.2...6.1.3)

### Bug fixes

-   Fixed a bug that, under certain circumstances, prevented the close frame from
    being parsed ([#&#8203;1494](https://togithub.com/websockets/ws/issues/1494)).

### [`v6.1.2`](https://togithub.com/websockets/ws/releases/6.1.2)

[Compare Source](https://togithub.com/websockets/ws/compare/6.1.1...6.1.2)

### Bug fixes

-   Restored compatibility with Node.js &lt; 6.13.0 ([`26436e0`](https://togithub.com/websockets/ws/commit/26436e0)).

### [`v6.1.1`](https://togithub.com/websockets/ws/releases/6.1.1)

[Compare Source](https://togithub.com/websockets/ws/compare/6.1.0...6.1.1)

### Bug fixes

-   Queued messages to send are now discarded if the permessage-deflate is enabled
    and the socket closes prematurely ([#&#8203;1464](https://togithub.com/websockets/ws/issues/1464), [#&#8203;1471](https://togithub.com/websockets/ws/issues/1471)).

### [`v6.1.0`](https://togithub.com/websockets/ws/releases/6.1.0)

[Compare Source](https://togithub.com/websockets/ws/compare/6.0.0...6.1.0)

### Features

-   The WebSocket server now emits a `'close'` event when the server
    closes ([#&#8203;1453](https://togithub.com/websockets/ws/issues/1453)).

### [`v6.0.0`](https://togithub.com/websockets/ws/releases/6.0.0)

[Compare Source](https://togithub.com/websockets/ws/compare/5.2.2...6.0.0)

### Breaking changes

-   Dropped support for Node.js 4 ([`d73885c`](https://togithub.com/websockets/ws/commit/d73885c)).
-   Added a shim that throws an error when used if the package is bundled for the
    browser ([#&#8203;1345](https://togithub.com/websockets/ws/issues/1345)).
-   Added a `maxPayload` option on the client. Defaults to 100 MiB ([#&#8203;1402](https://togithub.com/websockets/ws/issues/1402)).
-   Dropped support for the `memLevel` and `level` options. Use
    `zlibDeflateOptions` instead. ([`80e2002`](https://togithub.com/websockets/ws/commit/80e2002)).

### [`v5.2.2`](https://togithub.com/websockets/ws/releases/5.2.2)

[Compare Source](https://togithub.com/websockets/ws/compare/5.2.1...5.2.2)

### Bug fixes

-   Fixed a use after invalidation bug introduced in [`6046a28`](https://togithub.com/websockets/ws/commit/6046a28) ([`8aba871`](https://togithub.com/websockets/ws/commit/8aba871)).

### [`v5.2.1`](https://togithub.com/websockets/ws/releases/5.2.1)

[Compare Source](https://togithub.com/websockets/ws/compare/5.2.0...5.2.1)

### Bug fixes

-   Fixed a bug that could prevent buffered data from being processed under
    certain circumstances ([`6046a28`](https://togithub.com/websockets/ws/commit/6046a28)).

### [`v5.2.0`](https://togithub.com/websockets/ws/releases/5.2.0)

[Compare Source](https://togithub.com/websockets/ws/compare/5.1.1...5.2.0)

### Features

-   Added ability to specify custom headers when rejecting the handshake ([#&#8203;1379](https://togithub.com/websockets/ws/issues/1379)).

### [`v5.1.1`](https://togithub.com/websockets/ws/releases/5.1.1)

[Compare Source](https://togithub.com/websockets/ws/compare/5.1.0...5.1.1)

### Bug fixes

-   Fixed a regression introduced in [`9e152f9`](https://togithub.com/websockets/ws/commit/9e152f9) ([#&#8203;1347](https://togithub.com/websockets/ws/issues/1347)).

### [`v5.1.0`](https://togithub.com/websockets/ws/releases/5.1.0)

[Compare Source](https://togithub.com/websockets/ws/compare/5.0.0...5.1.0)

### Features

-   The `address` argument of the `WebSocket` constructor can now be a [`URL`][]
    instance ([#&#8203;1329](https://togithub.com/websockets/ws/issues/1329)).
-   The `options` argument of the `WebSocket` constructor now accepts any TLS
    option that is also accepted by [`https.request()`][] ([#&#8203;1332](https://togithub.com/websockets/ws/issues/1332)).

[`https.request()`]: https://nodejs.org/api/https.html#https_https_request_options_callback

[`url`]: https://nodejs.org/api/url.html#url_class_url

### [`v5.0.0`](https://togithub.com/websockets/ws/releases/5.0.0)

[Compare Source](https://togithub.com/websockets/ws/compare/4.1.0...5.0.0)

### Breaking changes

-   Dropped support for Node.js &lt; 4.5.0 ([#&#8203;1313](https://togithub.com/websockets/ws/issues/1313)).
-   The connection is no longer closed if the server does not agree to any of
    the client's requested subprotocols ([#&#8203;1312](https://togithub.com/websockets/ws/issues/1312)).
-   `net.Socket` errors are no longer re-emitted ([`a4050db`](https://togithub.com/websockets/ws/commit/a4050db)).

### Features

-   Read backpressure is now properly handled when permessage-deflate is enabled
    ([#&#8203;1302](https://togithub.com/websockets/ws/issues/1302)).

### [`v4.1.0`](https://togithub.com/websockets/ws/releases/4.1.0)

[Compare Source](https://togithub.com/websockets/ws/compare/4.0.0...4.1.0)

### Features

-   Added `WebSocketServer.prototype.address()` ([#&#8203;1294](https://togithub.com/websockets/ws/issues/1294)).
-   Added `zlib{Deflate,Inflate}Options` options ([#&#8203;1306](https://togithub.com/websockets/ws/issues/1306)).

### [`v4.0.0`](https://togithub.com/websockets/ws/releases/4.0.0)

[Compare Source](https://togithub.com/websockets/ws/compare/3.3.3...4.0.0)

### Breaking changes

-   The close status code is now set to 1005 if the received close frame contains
    no status code ([`a31b1f6`](https://togithub.com/websockets/ws/commit/a31b1f6)).
-   Error messages and types have been updated ([`695c5ea`](https://togithub.com/websockets/ws/commit/695c5ea)).
-   The `onerror` event handler now receives an `ErrorEvent` instead of JavaScript
    error ([`63e275e`](https://togithub.com/websockets/ws/commit/63e275e)).
-   The third argument of `WebSocket.prototype.ping()` and
    `WebSocket.prototype.pong()` is no longer a boolean but an optional callback
    ([`30c9f71`](https://togithub.com/websockets/ws/commit/30c9f71)).
-   The non-standard `protocolVersion` and `bytesReceived` attributes have been
    removed ([`30c9f71`](https://togithub.com/websockets/ws/commit/30c9f71)...[`ee9b5f3`](https://togithub.com/websockets/ws/commit/ee9b5f3)).
-   The `extensions` attribute is no longer an object but a string representing
    the extensions selected by the server ([`fdec524`](https://togithub.com/websockets/ws/commit/fdec524)).
-   The `'headers'` event on the client has been renamed to `'upgrade'`. Listeners
    of this event now receive only the `response` argument ([`1c783c2`](https://togithub.com/websockets/ws/commit/1c783c2)).
-   The `WebSocket.prototype.pause()` and `WebSocket.prototype.resume()` methods
    have been removed to prevent the user from interfering with the state of the
    underlying `net.Socket` stream ([`a206e98`](https://togithub.com/websockets/ws/commit/a206e98)).

</details>

---

### Renovate configuration

:date: **Schedule**: At any time (no schedule defined).

:vertical_traffic_light: **Automerge**: Disabled by config. Please merge this manually once you are satisfied.

:recycle: **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.

:no_bell: **Ignore**: Close this PR and you won't be reminded about this update again.

---

 - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box

---

This PR has been generated by [WhiteSource Renovate](https://renovate.whitesourcesoftware.com). View repository job log [here](https://app.renovatebot.com/dashboard#nteract/hydrogen).",35395056
287,2020-03-26T22:47:25Z,2020-03-27T07:01:11Z,,,This fix #1022,55262614
288,2020-03-21T03:20:35Z,2020-03-24T16:57:02Z,,,"Cosine embedding loss could be useful in Flux.jl for
- Semi-supervised learning
- Learning nonlinear embeddings
- For finding similarity or dissimilarity of 2 inputs.

**cos-embedding_loss( x1, x2, y, margin, pad = false)** 
It takes, 
- x1 and x2 as inputs
- y = 1 or -1 as mode
- margin = 0 which can range from -1 to 1 and has 0 as its default value
- pad has default value 'false'. If pad = true is passed zeros are padded and the cos_embedding_loss() is calculated. It requires PaddedViews.jl to be added. 

This PR resolves issue #1094 ",55262614
289,2020-03-20T11:49:20Z,2020-03-27T07:33:16Z,,,"Added Docstrings for expected argument types for pad, stride and dilation in conv layers",55262614
290,2020-03-14T18:11:01Z,2020-03-15T07:45:47Z,,,"People keep doing this and it is bad.
Having it in the performance tips should at at least help.

But throughout the docs we violate this rule nearly constantly.
My bad example was basically copied straight from https://fluxml.ai/Flux.jl/stable/training/training/#Loss-Functions-1

But this PR is just going to add it to the performance tips.
Its beyond scope of this PR to rewrite every example that violates this",55262614
291,2020-03-13T14:33:17Z,2020-03-16T19:15:21Z,,,the docstrings in the module for functions onhot and onecold  proposed a function with some examples that returns 0 or 1 instead of boolean .,55262614
292,2020-03-06T00:52:55Z,2020-03-17T19:14:19Z,,,"So I tried to implement what was proposed in #637 
I guess this is what @MikeInnes wanted? But I didn't see why need `isimmutable` trait tho. With this PR, the following would work at least and a bit faster since it's type stable now.

```julia
using Flux
using Flux.Optimise: update!, ADAM

m = Chain(Dense(100, 100, tanh), Dense(100, 100, tanh))
opt = ADAM()
x = rand(Float32, 100)
loss(m, x) = sum(m(x))
∇m, _ = gradient(loss, m, x)
update!(opt, m, ∇m)
```
",55262614
293,2020-03-03T08:39:54Z,2020-03-04T17:02:42Z,,,,55262614
294,2020-03-02T18:10:25Z,2020-03-10T14:40:58Z,,,"I have added the following callback functions:
1. terminateOnNaN
2. HistoryCallback
3. lrdecay
4. ModelCheckpoint

As there aren't any built-in callback functions existing in Flux, I hope these may be helpful in improving the Flux-user experience. Reference taken from keras.",55262614
295,2020-03-02T00:30:39Z,2020-03-02T18:31:26Z,,,"Added  the following function optimizers :
- Rprop
- SparseAdam
- HeavyBall",55262614
296,2020-02-15T14:30:23Z,2020-02-27T08:13:26Z,,,,55262614
297,2020-02-08T19:43:47Z,2020-03-26T10:17:13Z,,,"This adds a few warning messages about conversions to Float64 or worse, which seem to be a common problem, today #1026, last week #815.

These are behind `@debug`, which I see is also used by NNlib for some such warnings. Another option would be to make these errors, controlled by some `Flux.allowconvert(false)` along the lines of what CuArrays does for scalar access, see earlier commit. 

I'm not sure they will catch all the activation-function bugs mentioned in `performance.md`. Perhaps it would be better to attach the warning to every layer, not to `Chain`. ",55262614
298,2020-02-07T12:18:59Z,2020-03-03T01:50:00Z,,,"crossentropy and binarycrossentropy can have label smoothing. 

Addressing as per issue #1016.
",55262614
299,2020-02-03T09:16:12Z,2020-03-17T14:35:35Z,,,"This is in response to #666 where we can visualise `train!` as a wrapper around the `step!` function, while maintaining the same api.

To actually accumulate the loss as in https://github.com/FluxML/Flux.jl/issues/666#issuecomment-471307139, we could of course go with the loss function being a simple closure here, getting rid of the need to send around the mini batch.

With the changes to Zygote, it might be nice to actually have this in our interface.

cc @MikeInnes @oxinabox ",55262614
300,2020-02-01T14:39:45Z,2020-02-25T20:02:40Z,,,"I have implemented a Summary function to provide a brief model summary, which returns a struct with parameters as a list of trainable layers(with number of parameters in that layer) and the total number of trainable parameters. This function allows to have a summary about the number of parameters used by various layers, and by overall model. Kindly review this and suggest further changes required.",55262614
301,2020-01-29T15:21:12Z,2020-03-27T09:09:01Z,,,"A basic implementation inspired on https://pytorch.org/docs/stable/nn.html#bilinear

I haven't exported it, because I think this layer is a bit more esoteric compared with others.

It basically computes interactions between two sets of inputs.

I thought about augmenting it to also include the non-interaction terms (this can easily be done, eg. augmenting the data with a row of ones) but for now it simply mirrors PyTorch's one.

I had to use splatting `vcat(x...)` and `hcat(x...)` in the forward pass. I wanted to avoid it, but with `reduce` I couldn't get gradients. But I think this can be improved.",55262614
302,2020-01-16T14:08:30Z,2020-01-22T21:00:49Z,,,,55262614
303,2020-01-11T07:50:08Z,2020-02-25T18:57:45Z,,,"The current definition of `flip` uses `reverse`, which implemented using mutation, so it is not compatible with Zygote.
The proposed version uses `Zygote.Buffer` instead. As a consequence, the proposed version avoids unnecessary allocations so is also more efficient",55262614
304,2020-01-06T13:35:04Z,2020-01-10T18:02:38Z,,,"Fixes Issue #965 
Dense Layer with is able to take SparseArray as input",55262614
305,2020-01-06T00:59:06Z,2020-02-09T06:55:20Z,,,"**Ready**
Doesn't overlap with https://github.com/FluxML/Flux.jl/pull/853",55262614
306,2019-12-16T16:18:30Z,2019-12-18T08:52:04Z,,,This add the implementation of Lookahead optimiser from [this paper](https://arxiv.org/abs/1907.08610) and based on the [pytorch impl](https://github.com/lonePatient/lookahead_pytorch/blob/master/optimizer.py),55262614
307,2019-12-09T16:02:32Z,2020-03-25T14:13:20Z,,,This makes the behaviour of the LSTM and GRU recurrent layers consistent with other layers.,55262614
308,2019-12-05T12:46:36Z,2020-02-16T15:53:00Z,,,Good to add generic tests for tracking gradients through the various layers on the GPU.,55262614
309,2019-12-02T19:58:05Z,2019-12-10T20:07:38Z,,,"This is an example of adding a pull request labeler, as seen [here](https://github.com/actions/labeler).",55262614
310,2019-11-30T06:14:50Z,2019-12-08T07:38:08Z,,,"I am trying to add `groupcount` as a member for drafted changes here ->
https://github.com/FluxML/NNlib.jl/pull/146#issue-347196058

In short I am following @staticfloat 's suggestion here https://github.com/JuliaGPU/CuArrays.jl/pull/388#issuecomment-521697345",55262614
311,2019-11-19T18:22:43Z,2019-11-26T15:01:11Z,,,"Adds support for AMD GPUs via ROCArrays.jl

At the moment, ROCArrays doesn't have the equivalent of CUDNN/CURNN wired up (MIOpen is the ROCm equivalent), so `rocm/rocm.jl` is empty. I also blindly copied anything else referencing CuArrays/CUDA and made a ROCm equivalent, which could be the totally wrong thing to do.

I'm just posting this here so I don't forget about it, but this has not been tested yet. I expect to do a lot more work on this PR and ROCArrays before this PR is ready for review. That said, feel free to try it out and let me know if it does what you would expect :smile: 

Addresses #173 partially and #310 fully

TODO:
- [x] Get Flux to load correctly
- [x] Wire up ROCm GitLab CI
- [x] Add ROCArrays tests
- [ ] Make said tests pass
- [x] Update documentation",55262614
312,2019-10-23T16:45:06Z,2020-03-02T07:19:01Z,,,A lot of `@test_broken` here because of CUDAnative `log` fighting and scalar `getindex` being disallowed.,55262614
313,2019-10-20T11:50:36Z,2019-12-04T22:53:33Z,,,"Fixes #813 

This adds the possibility to set ""pad=SamePad()"" to automatically calculate the amount of padding to apply so that outputsize==inputsize (assuming stide == 1).

Comments on API more than welcome. I considered the following options:

* Call the type just Same and export it, but I was afraid to cause name collisions due to a too generic name
* Call the type Same and not export it
* Dispatch on type instead of instance (so that one can type pad=Same instead of pad=Same())
* Supply a method instead of a type, giving a similar API as above. 

Happy to change to any of the above or to anything else.

I don't think that same padding is common for pooling layers, but I added it just for the sake of consistency. It is a separate commit so it can easily be removed if not wanted.",55262614
314,2019-10-19T17:22:06Z,2020-02-24T14:00:26Z,,,"Fixes #89

I updated travis config file and added coverage tools to it. Also, I inserted their badges to readme file too. We should use travis-ci.com instead of travis-ci.org because of travis-ci announced that users and projects (both private and public) should only use travis-ci.com. ([ref1](https://blog.travis-ci.com/2018-05-02-open-source-projects-on-travis-ci-com-with-github-apps) [ref2](https://docs.travis-ci.com/user/migrate/open-source-on-travis-ci-com/))

You can see the results for my fork:
https://codecov.io/gh/hossein-pourbozorg/Flux.jl/branch/issue-89
https://coveralls.io/github/hossein-pourbozorg/Flux.jl

https://travis-ci.com/hossein-pourbozorg/Flux.jl/",55262614
315,2019-09-27T06:36:20Z,2020-03-07T10:43:45Z,,,"Addresses #868 

",55262614
316,2019-09-02T13:38:50Z,2020-03-18T15:51:41Z,,,"Fixes #282, by allowing `Dense((5, 3), (2, 7))` as suggested. Also closes #293, #617.

For `d = Dense(15, 14)` I wanted to leave the original behaviour alone, but attempting to do this by dispatch landed me in ambiguity hell, so it's an `if` statement (which I think ought to get compiled out). Possibly this should have `&& ndims(x)<=2` so that such `d` does allow higher-dimensional data. 

I also added a line to allow `Dense(15 => 14)` and `Dense((5, 3) => (2, 7))`, more like `Conv` and less confusing I think. But will remove if anyone objects / thinks this deserves its own issue. 

Note by the way the following `::Any` problem. Since this persists after `] free Flux`, I don't think it's my fault. (This is on Julia 1.1)
```
julia> d = Dense(5,2)
Dense(5 => 2)

julia> @code_warntype d(rand(Float32, 5))
Body::Any
```",55262614
317,2019-08-27T20:30:30Z,2020-02-09T06:58:21Z,,,"If you disagree with any of the changes, please tell me what to reverse or fix.
I am unsure about the docstrings I added to `src/utils.jl` for `unsqueeze` and
the `[un]stack` functions so please give those a more detailed look.

Update Documenter.jl version for new features, fix deprecation warnings in
`docs/make.jl` and import Flux for all doctests.
Add missing docstrings to `src/utils.jl`, `src/layers/stateless.jl` and `src/data/`; add
these and other missing functions to Markdown docs.

Improve docstrings by...
   - fixing typos,
   - removing trailing or double whitespaces,
   - using `jldoctest` blocks where applicable,
   - fixing, updating or correctly setting up existing doctests,
   - improving consistency (for example, always use ""# Examples"" instead
     of other variants),
   - removing empty lines between docstrings and functions,
   - instead of mentioning keywords, put them into the docstring,
   - adding some missing but useful keywords,
   - adding references (`@ref`),
   - using LaTeX math where applicable, and
   - linking papers.

Debatable stuff that is untouched:
   - BE/AE s/z irregularities (e.g. ""normalise"" versus ""normalize"") since
     most papers use the AE version while the Flux source code was
     written with BE spelling.
   - Names of normalization functions are capitalized
     (""Batch Normalization"" instead of ""batch normalization"").
   - Default values in argument lists have spaces around the equals sign (`arg = x` instead of `arg=x`).",55262614
318,2019-08-27T00:21:57Z,2019-11-08T15:30:10Z,,,"These fallbacks assume that if the first argument is a 1-element
`Vector` (as given by a model's output) and the second argument is a
scalar, the first argument should also be interpreted as a scalar.
Ref #850.

The use case is putting a model's scalar output right into the loss function instead of having to manually index into the 1-element `Vector` which is easily forgotten.

Not sure if this is desired, otherwise just close this. It does have the benefit of preventing not so obvious errors that – in my opinion – should not happen.",55262614
319,2019-08-22T00:49:33Z,2019-10-03T14:15:33Z,,,"some optimizer will print a super long `IdDict` by default, removed printing that in this PR.",55262614
320,2019-08-15T07:02:55Z,2019-10-04T09:23:37Z,,,This bug only appeared while taking gradients of networks is in testmode (`training=false`),55262614
321,2019-08-05T17:37:12Z,2019-10-01T12:28:56Z,,,"As I mention in #816, typeasserts of the form `get!(...)::typeof(data(x))` cause training to fail in some edge cases involving custom layers. This change would just remove the typeasserts from the definitions of `apply!` for the optimisers `Momentum`, `Nesterov`, `RMSProp`, and `ADAGrad` (the only optimisers where #816 is relevant).",55262614
322,2019-07-26T19:36:24Z,2019-10-24T15:23:41Z,,,,55262614
323,2019-07-09T21:45:42Z,2019-10-24T11:21:29Z,,,"Do not learn RNN initial state during backprop by default; leave this as an option
#807 ",55262614
324,2019-06-28T20:22:19Z,2019-10-01T12:28:57Z,,,"Implementation towards the discussion in #797 including the explicit/implicit parameterization tools discussed in #742. 

@MikeInnes Can you take a look at the `HyperNet` struct. In particular, do you think it should have type annotation like `HyperNet{H,M}` I don't know how to get such a thing working.

Also, I ran into a problem where `restructure` expects the parameters to be tracked going in. This is annoying as it fails silently, and the resulting model is just reparamterized with nothing as you can see in the broken test. If you supply `restructure` with an untracked array it will create the model with an empty `Any[]` array and fail silently. The check for `TrackedArray` was included in the DiffEqFlux implementation. @ChrisRackauckas do you remember why? If this is required it should produce an error possibly.",55262614
325,2019-05-16T16:30:12Z,2019-10-01T12:28:57Z,,,"In some of my tests, using proximal optimization for regularization functions (like L1 and L2 norms) has been quite efficient, ",55262614
326,2019-05-13T23:09:29Z,2020-01-31T00:34:59Z,,,"This layer can be helpful in the case when your model takes in multiple inputs. For a Q network you take in both the state and the action and try to predict the expected sum of discounted rewards. Typically you would have to write this by constructing the different inputs separately.

```
state, action = randn(4), randn(2)

state_input = Dense(4, 20)
action_input = Dense(2, 20)
model = Chain(Dense(40, 20, elu), Dense(20, 1))
Q = model(vcat(state_input(state), action_input(action)))
```

However, this makes it awkward when trying to collect all the parameters or pass around the entire model as one struct.

Instead we can have a combinator which allows for multiple inputs and the whole implementation becomes simpler.

```
state, action = randn(4), randn(2)

model = Chain(MultiInput(Dense(4, 20), Dense(2, 20)),
              x -> vcat(x...),
              Dense(40, 20, elu),
              Dense(20, 1))

Q = model((state, action))
```

Now we have something which looks like a function again, and it just takes in multiple inputs. You can query for the parameters and make it part of a longer chain if needed.",55262614
327,2019-05-12T18:36:40Z,2020-02-04T12:23:31Z,,,"This is aimed to simply the case when you wish to call multiple layers in parallel rather than in sequence. For example in the resnet style approach you apply a layer and the identity function then sum them. This can be written like so

```
model = Chain(Parallel(Conv((3, 3), 3=>3, pad=1),
                       x -> x),
              sum)

inputs = randn(28, 28, 3, 1)
model(inputs)
```

If you wish to have a set of convolutional layers that can extract different sized features and then concat them together of the feature dimension.

```
model = Chain(Parallel(Conv((3, 3), 3=>3),
                       Conv((5, 5), 3=>3, pad=1),
                       Conv((7, 7), 3=>3, pad=2)),
              x -> cat(x..., dims=3))


inputs = randn(28, 28, 3, 1)
model(inputs)
```",55262614
328,2019-05-04T15:55:49Z,2020-03-24T17:37:58Z,,,"Using `mapreduce` was returning an `Array` when used with `OneHotMatrix{CuArray}`. This causes issues down the chain. `map` avoids that, and also avoids the reduction giving slightly better speed (~40x).",55262614
329,2019-04-27T14:24:51Z,2020-01-04T00:31:16Z,,,"This works around #757.

The cause is as follows: BatchNorm gradients get promoted to Float64 is due to element-wise integer division. I don't know why Tracker is doing that - I could look into it but don't understand Tracker internals particularly well. Simply performing floating point division instead of integer division eliminates the issue.",55262614
330,2019-04-14T08:51:17Z,2019-10-01T12:28:58Z,,,Added Pixel Shuffle Operation used frequently in super-resolution networks for up-sampling.,55262614
331,2019-03-31T05:10:45Z,2019-10-01T12:28:59Z,,,"Addresses issue #671 . 
@MikeInnes is this what was needed?",55262614
332,2019-03-30T10:17:57Z,2019-10-01T12:28:59Z,,,"Added function for Local Response Normalization. Reference: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf. The previous PRs on adding LRNorm used for loops and indexing, due to which automatic differentiation was not supported. I have implemented a vectorized version of LRNorm. ",55262614
333,2019-03-29T18:28:13Z,2019-10-01T12:28:59Z,,,Added code and tests for the same,55262614
334,2019-03-10T21:03:09Z,2019-10-01T12:29:00Z,,,These Adaptive Pooling layers will be useful in the middle of a long convolutional chain when the user doesn't know the size of input but wants a required target output size.,55262614
335,2019-03-06T13:56:39Z,2019-10-01T12:29:00Z,,,"a peephole LSTM (`PLSTM`) and a fully connected LSTM (`FCLSTM`).

Both implementations are straight forward, as much of the original LSTM implementation was reused. see: `src/layers/recurrent.jl`

There are some very basic unit test under: `test/layers/recurrent.jl`",55262614
336,2019-03-06T13:49:45Z,2019-10-01T12:29:00Z,,,"@MikeInnes I've cleaned the parallel branch and separated the peephole LSTM into another branch for another [pull request](https://github.com/FluxML/Flux.jl/pull/663).

To enable a bidirectional model `Base.reverse()` is [defined](https://github.com/swiesend/Flux.jl/blob/258140f4ab1e9839015a7326cf89db8c67d3a9c4/src/layers/parallel.jl#L101-L186) for common Flux data structures.",55262614
337,2019-03-04T18:47:33Z,2019-10-01T12:29:01Z,,,#503 Provided aliases for unicode fields in batchnorm and dropout,55262614
338,2019-02-27T20:47:52Z,2019-10-01T12:29:01Z,,,train! was giving an error with TrackedReal saying grad member not found. This PR fixes the issue.,55262614
339,2019-02-14T15:38:24Z,2019-10-01T12:29:01Z,,,That I was too lazy to fix at the time. Need to add some tests though.,55262614
340,2019-02-11T15:15:03Z,2019-10-01T12:29:02Z,,,Another attempt at https://github.com/FluxML/Flux.jl/issues/282 (https://github.com/FluxML/Flux.jl/pull/293),55262614
341,2019-02-09T12:16:01Z,2019-10-01T12:29:02Z,,,"calling `back!` should not change if something `isleaf` or not.
and it should not  cause checking such to give missing related errors.

This solves https://discourse.julialang.org/t/typeerror-non-boolean-missing-used-in-boolean-context-when-collecting-parameters/20593

Wether or not this is a wise thing that a user should do or not,
the behavour with this PR is more sensible.

",55262614
342,2019-02-09T09:04:14Z,2019-10-01T12:29:02Z,,,"Let's say you have a batch of thousand 32x32 single-channel images. Then x will be a three-dimensional array and weights will be a four-dimensional array. So, in this case, we need the size(x) to be 32x32x1x1000. This takes care of it.",55262614
343,2019-02-06T04:51:59Z,2019-10-01T12:29:02Z,,,"See test cases. I hit these while taking third-order derivatives of
matrix multiplies (whose gradient definitions use transpose).",55262614
344,2019-02-05T14:24:45Z,2019-10-01T12:29:03Z,,,"e.g.
```julia
julia> m = Chain(Dense(10, 5, relu), Dense(5, 2)) |> gpu
Chain(Dense(10, 5, NNlib.relu), Dense(5, 2))

julia> y = m(gpu(rand(10)))
Tracked 2-element CuArray{Float32,1}:
 0.14124388f0
 1.0137802f0

julia> Tracker.mem(y)
511
```

This is pretty coarse grained for now (Zygote already has something much more advanced).",55262614
345,2019-02-05T00:41:13Z,2019-10-01T12:29:03Z,,,"This fixed higher order autodiff or getindex (and by extension vcat, since
it's backward pass uses getindex). This makes tracker able to handle
the third order derivatives needed to implement [1].

[1] Physics-Informed Generative Adversarial Networks for Stochastic Differential Equations. https://arxiv.org/abs/1811.02033",55262614
346,2019-02-02T17:28:59Z,2019-10-01T12:29:03Z,,,"This is a proposition that fixes #577. I basically delay the gradient computation to happen inplace in the `accum!()` call and not in the `@grad` definition, which allows me to avoid the copy. On an extreme cornercase:

```julia
using CuArrays
using Flux

Ea = gpu(param(randn(64, 1_000_000)));
Eb = gpu(param(randn(64, 65_535)));
i = UInt16.(collect(1:5_000));
loss(i,n) = sum(sum(Eb[:, i] .+ Ea[:, rand(1:size(Ea,2), 1)]) for _ in 1:n)

function g(n, t, i)
   for _ in 1:t
      print(""loss "")
      CuArrays.@time l = loss(i, n)
      print(""back "")
      CuArrays.@time Flux.back!(l)
   end
end

g(100, 10, i)
```

Before, I got the following timings:
```
loss   0.149092 seconds (35.33 k CPU allocations: 2.126 MiB) (600 GPU allocations: 245.150 MiB, 26.88% gc time of which 100.00% spent allocating)
back   7.784618 seconds (64.09 k CPU allocations: 3.001 MiB, 29.91% gc time) (900 GPU allocations: 25.882 GiB, 3.20% gc time of which 100.00% spent allocating)
```

And after:
```
loss   0.062988 seconds (32.28 k CPU allocations: 2.061 MiB) (600 GPU allocations: 245.150 MiB)
back   0.405314 seconds (78.89 k CPU allocations: 4.502 MiB, 24.22% gc time) (1.30 k GPU allocations: 734.404 MiB)
```

There is a downside: `getindex`ing into the sparse structure I use is very slow and I think there is some `getindex`ing happening in the jacobians (at least the jacobian tests were complaining). 

Please let me know what you think.
",55262614
347,2019-01-29T01:27:36Z,2019-10-01T12:29:03Z,,,"1. Parameterize OneHotVector on Integer type, to avoid using more memory
   than required for vectors of them.
2. Switch OneHotMatrix from storing a vector of OneHotVectors to only storing
   the data and the size of the vector (reconstructing the vector locally), thus
   saving half the memory required and eliminating a transpose operation for
   matmul with OneHotMatrix on TPU.",55262614
348,2019-01-19T13:45:15Z,2019-10-01T12:29:04Z,,,"Replaced calls to properties `.data` and `.grad` with their generic function calls so that `update!(opt, xs)` does not cause an `UndefRefError: access to undefined reference` error.",55262614
349,2019-01-10T20:41:38Z,2019-10-01T12:29:04Z,,,"This fixes the type mismatch in a constructor for Conv op.

Found when trying to create a 1d convolution like so:
```
c = Conv(Tuple(7),300=>128)
```
Which results in the following error:
```
MethodError: no method matching Conv(::TrackedArray{…,Array{Float32,3}}, 
::TrackedArray{…,Array{Float64,1}}, ::typeof(identity); stride=1, pad=0, dilation=1) ...
```
The important parts are :TrackedArray{…,Array{**Float32**,3}}, and ::TrackedArray{…,Array{**Float64**,1}}

",55262614
350,2018-12-20T15:42:55Z,2019-10-01T12:29:04Z,,,"
This is a new version of #334, aiming to solve these problems:

* `prod(xs)` has a gradient defined  `prod(xs) ./ xs .* Δ`  which fails when some x are zero.
* `prod(x,dims=1)`, `cumprod(x)` and `cumsum(x)` are all `Array{<:TrackedReal}`.
* `prod(x,1)`, in Julia 0.6 notation, has a custom gradient with various problems.

This custom gradient defined using `circshift` is I believe correct for the case `prod(x)`. In #334 I wrote a variant for the case `prod(x,dims=1)` which depends on `dims`. However both are very slow, and crash julia if called on arrays with 1000s of elements.

Compared to previous attempt:

* The function `∇prod` treats correctly cases where the product is zero due to rounding, but no `x[i]` was zero. It is much faster on the case where exactly one `x[i]` is zero, with thanks to @drvi for an idea.
* This is now applied using the built-in `mapslices`, which is a bit slow but will eventually improve.
* `∇cumprod` needs to be applied with variadic mapslices, here implemented with some ntuple-ing. I fixed stupid a bug in this, so now all test pass, and made it more compact.
* For more generic arrays, the fall-back is to call `ForwardDiff.gradient` instead of circshift funcitons.

ForwardDiff does work on CuArrays, but slowly via CPU I think, for now. However this is only called by `∇prod` when exactly one entry is zero, so other cases should be fast. (Ideally `∇prod_one` and `∇cumprod` could be written with CUDAnative, perahps, but I'm not sure where they would live.) The gradient for `cumsum` also goes via the CPU I think, and because `reverse(cu(rand(2,2)), dims=1)` is an error; I wrote a mapslices thing for that case.

Variadic mapslices https://github.com/JuliaLang/julia/issues/23306 would replace my `∇cumprod_d` function. You could also write this using Julia 1.1's `eachslice()` https://github.com/JuliaLang/julia/pull/29749/ as `cumprod` only works for `dims::Int`. But `prod` accepts `dim::Tuple` thus no help.

* There are a few benchmarks etc in [this gist](https://gist.github.com/mcabbott/9261c6f39969db9965a8d87d64d8b97f).

* Flux's tests fail locally for me, but in `curnn.jl` unrelated to this. 

* I just saw that #388 also defines a gradient for `cumsum`, however I believe it is missing a `reverse`.",55262614
351,2018-11-23T11:42:46Z,2019-10-01T12:29:04Z,,,"Couple of changes here:
 - `norm` requires `LinearAlgebra`
 - I believe l2 regularization is the sum of the squared two-norms of a model's parameters",55262614
352,2018-11-22T18:23:34Z,2019-10-01T12:29:05Z,,,It breaks some array interface assumptions to have indexing return a value that doesn't match the prescribed `eltype` of the array. This changes it so that way the `eltype` of a TrackedArray is the same as the type of the elements that come out.,55262614
353,2018-11-18T11:47:30Z,2020-02-03T22:07:58Z,,,"First shot at this.

The first commit is not directly related, but rather does some cleanups of the Flux.train! docs to reflect the new parameters.

The second commit adds callback info. I'm not too happy with the CallbackInfo struct's fieldnames. Also, maybe we should return `data(loss)` instead of  `loss`, since directly returning loss carries the risk that someone inadvertently messes with the parameters?

I tested this with the `xor1.jl` example in the modle-zoo, where I adapted the `evalcb` function:

```julia
evalcb = (info) -> begin
    dump(info)
    @show batch_loss(val)
end
```

Fixes #489.",55262614
354,2018-10-30T03:59:11Z,2019-10-01T12:29:05Z,,,"back propagation in Issue #472 
TODO:
- [ ] GPU support, now `gesvd!` in `CuArray.jl` is buggy, let us be patient.
- [ ] change numeric gradient check in test
- [ ] change file structure, now I put `svd.jl` under layers, which is not proper",55262614
355,2018-10-23T16:44:58Z,2019-10-01T12:29:05Z,,,"Man, autodiff code is convoluted. All the closures. Besides the issue with size `zero` of irrationals there was also another promotion issue where the irrational caused promotion to `Float64` even when the input is `Float32.
```julia
julia> Tracker.derivative(t -> π*t, Float32(1.0))
3.141592653589793
```
This should be fixed with this PR together with the `zero` issue.",55262614
356,2018-10-15T13:33:29Z,2019-10-01T12:29:06Z,,,"Follows the implementation in Keras

The slightly complicated way of getting the epsilon is so that yhat does not have to contain Float",55262614
357,2018-10-07T19:15:54Z,2019-10-01T12:29:06Z,,,"#424 
This PR adds support for kaiming normal and kaiming uniform initialization",55262614
358,2018-09-21T10:24:50Z,2019-02-04T10:37:15Z,,,"The current implementations of the ldiv and rdiv operations use an explicit inverse, which is generally understood to be sub-optimal numerically. The proposed implementations avoid the use of explicit inverses, and should produce reductions in both time- and memory-complexity.",55262614
359,2018-08-07T04:44:11Z,2020-01-28T14:13:11Z,,,"This is the CTC loss function and associated tests. it should interface with the tracker just like the other loss functions, and it uses the `@require` macro to allow the GPU functionality to be optional.",55262614
360,2018-07-24T09:38:35Z,2019-10-01T12:29:07Z,,,"This version is buggy and needs fixes before it can be merged. This directly depends on JuliaGPU/CuArrays.jl#100.
On the positive side the major speed issues discussed in avik-pal/DeepLearningBenchmarks#1 is resolved by this.",55262614
361,2018-07-09T11:50:30Z,2019-02-27T10:53:22Z,,,"Added a new LRNorm struct, which takes four hyper parameters and returns the normalized output.
Implemented by referring to the ImageNet paper which can be found [here](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). ",55262614
362,2018-06-15T22:39:18Z,2019-10-01T12:29:07Z,,,"`@einsum` as discussed in #297. The notation is based on the ""function of indices"" notation --

```julia
@einsum [i] -> a[i,j]                  # reduce dim 2
@einsum [i,k] -> a[i,j] * b[j,k]       # matmul
@einsum [i,k,N] -> a[i,j,N] * b[j,k,N] # batch matmul
@einsum [i,j] -> a[i] * b[j]           # outer product
```

You can enjoy the nice output, too:

```julia
julia> @expand @einsum [i,k] -> a[i,j] * b[j,k]
:(a * b)

julia> @expand @einsum [i,j] -> a[i] * b[j]
:(a .* (Flux.reshape)(b, (1, (Flux.size)(b, 1))))
```

There are various cases this doesn't handle properly yet, and I need to actually implement that batch-matmul primitive somewhere. Will probably throw together a naive CPU implementation, unless anyone knows a trick.",55262614
363,2018-06-12T10:12:51Z,2019-10-01T12:29:07Z,,,"By supporting additional dimensions, the interface of Dense layer would be like this.
 * Input Shape: (in_features, d1, d2, d3, ..., N)
 * Ouput Shape: (out_features, d2, d2, d3,..., N)

Related discuss: #282 ",55262614
364,2018-05-11T07:49:02Z,2018-09-17T17:30:38Z,,,Fixes : #260 ,55262614
365,2018-04-27T05:46:36Z,2019-10-01T12:29:07Z,,,"Fix #227 

these are all type unstable, because they either return `false` or `:stop`
I guess I could make them all return `:donotstop` or `:stop`.

",55262614
366,2018-04-18T12:20:56Z,2019-10-01T12:29:08Z,,,[ci skip],55262614
367,2018-04-02T18:59:36Z,2019-10-01T12:29:08Z,,,"This PR tries to address some inconsistencies (well, in my opinion at least :) in the present state of Flux's losses:
- 3 functions (`crossentropy`, `logitcrossentropy` and `mse`) perform reduction, 2 don't (`binarycrossentropy`, `logitbinarycrossentropy`)
- some names are too long
- the interface should encourage to use `logsoftmax` instead of `softmax` as the input of cross_entropy for numerical stability
- the `logit` prefix makes sense for binary cross entropy, less so for the standard one, I think.
- `crossentropy` can be used only in conjunction with a onehot target representation `y`. In other frameworks, such as pytorch and knet, tipically `y` is a vector of integers in 1:K or 0:K-1. 

The change I propose here is to mimic closely pytorch's interface http://pytorch.org/docs/master/nn.html#id46

TODO
[ ] improve docs
[ ] should the average in mse and bce be along all dimensions or just the last one (present state of the pr) ? 
",55262614
368,2018-03-18T10:42:58Z,2019-10-01T12:29:08Z,,,,55262614
369,2018-03-03T07:51:33Z,2019-12-03T07:14:56Z,,,Implements Local Response Normalisation. Fixes #192.,55262614
370,2018-01-21T16:47:20Z,2019-10-01T12:29:09Z,,,"This a model-parallel version of threading I have implemented and I use for Flux. I have created because I desperately wanted to move out of TensorFlow and wanted to be faster. I would be very if someone can give me feedback, how to implement this more cleanly.

Thanks a lot.
Tomas",55262614
371,2017-12-18T04:02:42Z,2019-10-01T12:29:09Z,,,This needs tests still,55262614
372,2017-12-17T23:29:12Z,2019-10-01T12:29:09Z,,,"Adds dispatched vecdot for TrackedArrays. Solves #114 for matrices too.
",55262614
373,2017-11-24T18:32:53Z,2019-10-01T12:29:10Z,,,,55262614
374,2017-10-17T20:46:30Z,2019-10-01T12:29:10Z,,,"`softmax` looks just like it did before:

```julia
julia> ps = softmax([1,2,3])
3-element Flux.Softmax{Float64,1,Array{Float64,1}}:
 0.0900306
 0.244728
 0.665241
```

But under the hood it's now lazy, and actually stores the original logits:

```julia
julia> ps.logits
3-element Array{Float64,1}:
 1.0
 2.0
 3.0
```

What this means is that functions like `crossentropy` can avoid computing the `softmax` entirely and be much faster; but this is completely transparent and models benefit from it for free. With our sparse matrix types this gets us fast code without memorising a bunch of horrific functions like `tf.nn.sparse_softmax_cross_entropy_with_logits`. `crossentropy(softmax(ŷ), y)` is identical.

Needs tests, gradient checks and some GPU support hacks.",55262614
375,2017-04-13T10:57:08Z,2019-10-01T12:29:10Z,,,"This adds a ""fake"" backend that doesn't perform actual training/prediction. Instead, it output a DOT string witch can be rendered as an image. For example:

```
using Flux

m = Chain(
         Input(784),
         Affine(784, 128), relu,
         Affine(128, 64), relu,
         Affine(64, 10), softmax)

dot, n_params = graphviz(m)(Array{Float32}(4, 784))

open(""some.dot"", ""w"") do f write(f, dot) end
```

then `dot -Tjpeg some.dot > some.jpeg`, or use `Graphviz.jl`(which unfortunatly broken on 0.6) to get this:

![image](https://cloud.githubusercontent.com/assets/7145046/25001905/2e23303c-207a-11e7-825f-6610ea8b60c4.png)

It also counts the elements of all tensors, which can be used to estimate the memory cost and can help users to select a batch size.",55262614
376,2020-03-26T21:15:42Z,2020-03-27T08:12:33Z,,,"By setting a client side timeout one can avoid net::ERR_NETWORK_IO_SUSPENDED errors caused by the OS network layer being suspended [1]. This allows the client to handle higher level errors.

The jQuery.getJSON() function does not have a parameter for timeout management.

[1]: https://cs.chromium.org/chromium/src/net/http/http_network_layer.cc?q=ERR_NETWORK_IO_SUSPENDED&sq=package:chromium&dr=C&l=42",6220644
377,2020-03-25T15:24:29Z,2020-03-26T04:04:13Z,,,"The for loop had a null checker, if the cell is empty it wouldn't be added to the Sheets Cell ArrayList, hence the empty cells weren't recognized and the remaining cells were shifted to the left. 

Removing the null checker allows for all the cells to be added to the Sheet, including the ones which are empty.",6220644
378,2020-03-24T15:58:24Z,2020-03-26T22:17:09Z,,,"Closes #2238 

Now splitting multi-cell values can have two more options:
* On transition from lowercase to uppercase
* Split between numbers and letters",6220644
379,2020-03-24T03:08:00Z,2020-03-25T13:07:11Z,,,"Substantial memory savings can be had when using G1GC and String Deduplication available in the JVM with OpenRefine, even before we move to Apache Spark.  I suggest that we enable this by default:

With:
`JAVA_OPTIONS=-XX:+UseG1GC -XX:+UseStringDeduplication`

![image](https://user-images.githubusercontent.com/986438/77384217-d1091280-6d52-11ea-86e7-88e40932ed14.png)

Without:

![image](https://user-images.githubusercontent.com/986438/77384240-df572e80-6d52-11ea-9264-ab20cf4b8a9e.png)
",6220644
380,2020-03-21T15:39:02Z,2020-03-21T18:21:44Z,,,Still testing this on Eclipse SaaS workspaces to find out issues and further customization's.,6220644
381,2020-03-20T16:28:00Z,2020-03-22T18:24:02Z,,,Fixes #2316 ,6220644
382,2020-03-20T16:03:56Z,2020-03-20T21:33:38Z,,,"- added link for ""Your first pull request""
- consolidated & reduced redundant information",6220644
383,2020-03-19T16:16:57Z,2020-03-20T08:59:18Z,,,"Fixes #2424 .

The application does not remember the data type of a cell as a cell only stores information related to the cell value. Hence, to show the correct data type on the front end, we can just use `typeof` to get the data type of the cell before rendering the HTML. This way of using `typeof` is also consistent with the rest of the application (e.g. the rest of the code in `cell-ui.js` etc).

This works with different languages as well since the `select` values are in English regardless how OpenRefine is localised.",6220644
384,2020-03-17T20:41:31Z,2020-03-27T08:32:18Z,,,"Attempt to fix issue #1164 .
**Source of error:**
The JacksonParser does not accept unquoted control chars (ASCII characters with value less than 32) unless udpate it's features. 
Parsing unquoted chontrol chars will cause an  `Illegal Unquoted character` exception. 

**Tests**:
1.  Implement new test cases to ensure parser works as expected.
2.  Using the attached test json file, illegal character (the tab) is converted into legal ones as shown in the preview.
![1](https://user-images.githubusercontent.com/32441682/76899191-d3143280-6875-11ea-8ede-14b9cd65753c.png)
![2](https://user-images.githubusercontent.com/32441682/76899192-d4455f80-6875-11ea-9f4f-77164e869c40.png)

testJson file: 
[testJson.txt](https://github.com/OpenRefine/OpenRefine/files/4345718/testJson.txt)


",6220644
385,2020-03-17T08:51:46Z,2020-03-17T10:48:17Z,,,Refers: #1312 ,6220644
386,2020-03-16T16:43:23Z,2020-03-16T18:25:26Z,,,"Cursor changes after the `merge selected and re-cluster` button is clicked and remains in the busy state till clustering is done.
resolves #2033",6220644
387,2020-03-08T21:52:25Z,2020-03-17T11:14:52Z,,,Refers #2332,6220644
388,2020-03-08T21:04:12Z,2020-03-10T14:19:13Z,,,"Signed-off-by: Agha Saad Fraz <agha.saad04@gmail.com>

I have added the link to the google extension page which will help the developers to configure the keys for signing into their Google account.
P.S: added the feature for different languages.

### Issue:
There was no help provided about how to set up the keys for signing in to the google account for the development version of the OpenRefine.

### Solution:
I have linked the google extension page with the OpenRefine, before signing in, the developer will visit the google extension page, will set up the required keys and will authorize the google account.",6220644
389,2020-03-07T20:09:13Z,2020-03-07T21:27:14Z,,,,6220644
390,2020-03-06T22:11:26Z,2020-03-20T13:15:26Z,,,"Signed-off-by: kushthedude <kushthedude@gmail.com>

Fixes #2359 ",6220644
391,2020-03-01T15:40:47Z,2020-03-24T16:18:12Z,,,"Signed-off-by: Agha Saad Fraz <agha.saad04@gmail.com>
I have added a feature of displaying non-printable characters. It is a fix for #1286.

## Steps:
- Import the messy data
- Create the project
- Toggling the non-printable character checkbox would show/hide the non-printable characters
## Demo:
#### Data grid without non-printable characters:
![image](https://user-images.githubusercontent.com/36513474/75628860-b5509780-5bfe-11ea-901d-6f0720560497.png)

#### Data grid with non-printable characters:    
![image](https://user-images.githubusercontent.com/36513474/75628583-42462180-5bfc-11ea-8b38-ef4b09dea0a9.png)",6220644
392,2020-02-29T19:22:14Z,2020-03-17T11:16:12Z,,,Fixes #745 ,6220644
393,2020-02-23T18:35:00Z,2020-03-02T09:21:30Z,,,"Can we move to using latest OpenJDK ?  (I'm using OpenJDK 13)
Regardless, we should use the newer ""release"" convention instead of source/target which wrecks havoc for me on Windows dev environment and switching Maven toolchains for other software.  Maybe Linux users are not prone to env issues as much (a reason Windows created WSL in the first place).  Anyways, some further context on using the ""release"" flag: https://stackoverflow.com/questions/43102787/what-is-the-release-flag-in-the-java-9-compiler

It's also possible to combine both properties and profile, and just use activation as shown: https://stackoverflow.com/questions/55404641/configure-maven-compiler-plugin-so-it-works-both-with-jdk-8-and-jdk-12-in-sprin",6220644
394,2020-01-17T10:15:06Z,2020-01-23T19:18:28Z,,,Opening a pull request to make it easier to draft this policy here.,6220644
395,2019-12-12T14:15:52Z,2019-12-26T17:09:05Z,,,"Closes #2224.

The importer now treats dates as strings - as a user I think I would prefer that rather than converting them to timestamps at midnight… I can still do that conversion later on with `toDate()` if necessary. @msaby, do you think this is the right approach?",6220644
396,2019-10-11T20:10:09Z,2019-10-11T20:21:05Z,,,Draft implementation of getFields function as per #2186. No tests yet,6220644
397,2019-07-03T09:02:04Z,2019-09-10T02:27:30Z,,,Just creating this pull request to keep track of the work even if it is not ready yet.,6220644
398,2019-06-19T17:15:14Z,2019-07-28T18:26:32Z,,,"Hi @thadguidry, sorry I took so long - I'm in the middle of exams right now, and need to split my time between repeating some material and coding. I wrote function to parse unix time to OffsetDateTime, and some tests for ToDate class. I couldn't figure out test case that would test [lines 165 and 166](https://github.com/impune-pl/OpenRefine/blob/0ecf28174683bf9f7c443abaef72586f81be7789/main/src/com/google/refine/expr/functions/ToDate.java#L165), so I left them untested.",6220644
399,2019-05-01T23:21:21Z,2019-05-06T17:16:01Z,,,Closes #2037.  See the ticket for details.,6220644
400,2019-04-30T08:37:09Z,2019-04-30T08:38:28Z,,,"This adds a Virtuoso connector to the database extension. This was developed by @smalinin in the OpenLinkSoftware fork of OpenRefine (released under the same license, so we can pull it back upstream).

Things to consider:
- [ ] can we have tests for this? (Does Virtuoso run on Travis and/or Appveyor)?
- [ ] can we avoid duplicating code from other connectors?

Any other thoughts?",6220644
401,2019-06-27T08:31:31Z,2020-03-15T05:55:26Z,,,"Welcome to [Renovate](https://togithub.com/renovatebot/renovate)! This is an onboarding PR to help you understand and configure settings before regular Pull Requests begin.

:vertical_traffic_light: To activate Renovate, merge this Pull Request. To disable Renovate, simply close this Pull Request unmerged.



---
### Detected Package Files

 * `app/views/notebook.scala.html` (html)
 * `build.sbt` (sbt)
 * `modules/git-notebook-provider/build.sbt` (sbt)
 * `modules/sbt-dependency-manager/build.sbt` (sbt)
 * `modules/sbt-dependency-manager/project/plugins.sbt` (sbt)
 * `project/plugins.sbt` (sbt)
 * `project/Dependencies.scala` (sbt)
 * `project/Shared.scala` (sbt)

### Configuration Summary

Based on the default config's presets, Renovate will:

  - Start dependency updates only once this onboarding PR is merged
  - Separate major versions of dependencies into individual branches/PRs
  - Do not separate patch and minor upgrades into separate PRs for the same dependency
  - Upgrade to unstable versions only if the existing version is unstable
  - Raise PRs immediately (after branch is created)
  - If semantic commits detected, use semantic commit type <code>fix</code> for dependencies and <code>chore</code> for all others
  - Keep existing branches updated even when not scheduled
  - Disable automerging feature - wait for humans to merge all PRs
  - Ignore `node_modules`, `bower_components`, `vendor` and various test/tests directories
  - Update existing lock files only when <code>package.json</code> is modified
  - Autodetect whether to pin dependencies or maintain ranges
  - Rate limit PR creation to a maximum of two per hour
  - Limit to maximum 20 open PRs at any time
  - Group known monorepo packages together
  - Use curated list of recommended non-monorepo package groupings

:abcd: Would you like to change the way Renovate is upgrading your dependencies? Simply edit the `renovate.json` in this branch with your custom config and the list of Pull Requests in the ""What to Expect"" section below will be updated the next time Renovate runs.

---
You have configured Renovate to use branch `master` as base branch.


### What to Expect

With your current configuration, Renovate will create 40 Pull Requests:

<details>
<summary>Update dependency com.eed3si9n:sbt-buildinfo to v0.9.0</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.eed3si9n-sbt-buildinfo-0.x`
  - Merge into: `master`
  - Upgrade com.eed3si9n:sbt-buildinfo to `0.9.0`


</details>

<details>
<summary>Update dependency com.sun.jersey:jersey-client to v1.19.4</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.sun.jersey-jersey-client-1.x`
  - Merge into: `master`
  - Upgrade com.sun.jersey:jersey-client to `1.19.4`


</details>

<details>
<summary>Update dependency com.timushev.sbt:sbt-updates to v0.5.0</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.timushev.sbt-sbt-updates-0.x`
  - Merge into: `master`
  - Upgrade com.timushev.sbt:sbt-updates to `0.5.0`


</details>

<details>
<summary>Update dependency com.typesafe.play:sbt-plugin to v2.8.1</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.typesafe.play-sbt-plugin-2.x`
  - Merge into: `master`
  - Upgrade com.typesafe.play:sbt-plugin to `2.8.1`


</details>

<details>
<summary>Update dependency com.typesafe.sbt:sbt-coffeescript to v1.0.2</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.typesafe.sbt-sbt-coffeescript-1.x`
  - Merge into: `master`
  - Upgrade com.typesafe.sbt:sbt-coffeescript to `1.0.2`


</details>

<details>
<summary>Update dependency com.typesafe.sbt:sbt-git to v0.9.3</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.typesafe.sbt-sbt-git-0.x`
  - Merge into: `master`
  - Upgrade com.typesafe.sbt:sbt-git to `0.9.3`


</details>

<details>
<summary>Update dependency com.typesafe.sbt:sbt-less to v1.1.2</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.typesafe.sbt-sbt-less-1.x`
  - Merge into: `master`
  - Upgrade com.typesafe.sbt:sbt-less to `1.1.2`


</details>

<details>
<summary>Update dependency com.typesafe.sbt:sbt-native-packager to v1.6.1</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.typesafe.sbt-sbt-native-packager-1.x`
  - Merge into: `master`
  - Upgrade com.typesafe.sbt:sbt-native-packager to `1.6.1`


</details>

<details>
<summary>Update dependency com.typesafe:config to v1.4.0</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.typesafe-config-1.x`
  - Merge into: `master`
  - Upgrade com.typesafe:config to `1.4.0`


</details>

<details>
<summary>Update dependency commons-io:commons-io to v2.6</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/commons-io-commons-io-2.x`
  - Merge into: `master`
  - Upgrade commons-io:commons-io to `2.6`


</details>

<details>
<summary>Update dependency io.get-coursier:coursier to v1.0.3</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/io.get-coursier-coursier-1.x`
  - Merge into: `master`
  - Upgrade io.get-coursier:coursier to `1.0.3`


</details>

<details>
<summary>Update dependency io.get-coursier:coursier-cache to v1.0.3</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/io.get-coursier-coursier-cache-1.x`
  - Merge into: `master`
  - Upgrade io.get-coursier:coursier-cache to `1.0.3`


</details>

<details>
<summary>Update dependency io.reactivex:rxscala to v0.27.0</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/io.reactivex-rxscala-0.x`
  - Merge into: `master`
  - Upgrade io.reactivex:rxscala to `0.27.0`


</details>

<details>
<summary>Update dependency mathjax to v2.7.7</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/mathjax-2.x`
  - Merge into: `master`
  - Upgrade [mathjax](https://togithub.com/mathjax/MathJax) to `2.7.7`


</details>

<details>
<summary>Update dependency org.apache.maven:maven-core to v3.6.3</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.apache.maven-maven-core-3.x`
  - Merge into: `master`
  - Upgrade org.apache.maven:maven-core to `3.6.3`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-continuation to v8.2.0.v20160908</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-continuation-8.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-continuation to `8.2.0.v20160908`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-http to v8.2.0.v20160908</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-http-8.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-http to `8.2.0.v20160908`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-plus to v8.2.0.v20160908</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-plus-8.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-plus to `8.2.0.v20160908`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-security to v8.2.0.v20160908</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-security-8.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-security to `8.2.0.v20160908`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-server to v8.2.0.v20160908</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-server-8.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-server to `8.2.0.v20160908`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-servlet to v8.2.0.v20160908</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-servlet-8.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-servlet to `8.2.0.v20160908`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-util to v8.2.0.v20160908</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-util-8.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-util to `8.2.0.v20160908`


</details>

<details>
<summary>Update dependency org.eclipse.jgit:org.eclipse.jgit to v4.11.9.201909030838-r</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jgit-org.eclipse.jgit-4.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jgit:org.eclipse.jgit to `4.11.9.201909030838-r`


</details>

<details>
<summary>Update dependency org.scala-lang.modules:scala-parser-combinators to v1.1.2</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.scala-lang.modules-scala-parser-combinators-1.x`
  - Merge into: `master`
  - Upgrade org.scala-lang.modules:scala-parser-combinators to `1.1.2`


</details>

<details>
<summary>Update dependency org.scala-lang.modules:scala-xml to v1.2.0</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.scala-lang.modules-scala-xml-1.x`
  - Merge into: `master`
  - Upgrade org.scala-lang.modules:scala-xml to `1.2.0`


</details>

<details>
<summary>Update dependency org.scalatest:scalatest to v2.3.0-SNAP2</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.scalatest-scalatest-2.x`
  - Merge into: `master`
  - Upgrade org.scalatest:scalatest to `2.3.0-SNAP2`


</details>

<details>
<summary>Update dependency org.slf4j:slf4j-api to v1.7.30</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.slf4j-slf4j-api-1.x`
  - Merge into: `master`
  - Upgrade org.slf4j:slf4j-api to `1.7.30`


</details>

<details>
<summary>Update dependency org.slf4j:slf4j-log4j12 to v1.7.30</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.slf4j-slf4j-log4j12-1.x`
  - Merge into: `master`
  - Upgrade org.slf4j:slf4j-log4j12 to `1.7.30`


</details>

<details>
<summary>Update dependency org.sonatype.aether:aether-api to v1.13.1</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.sonatype.aether-aether-api-1.x`
  - Merge into: `master`
  - Upgrade org.sonatype.aether:aether-api to `1.13.1`


</details>

<details>
<summary>Update dependency com.typesafe.sbt:sbt-git to v1</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/com.typesafe.sbt-sbt-git-1.x`
  - Merge into: `master`
  - Upgrade com.typesafe.sbt:sbt-git to `1.0.0`


</details>

<details>
<summary>Update dependency commons-io:commons-io to v20030203</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/commons-io-commons-io-20030203.x`
  - Merge into: `master`
  - Upgrade commons-io:commons-io to `20030203.000550`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-continuation to v9</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-continuation-9.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-continuation to `9.4.27.v20200227`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-http to v9</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-http-9.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-http to `9.4.27.v20200227`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-plus to v9</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-plus-9.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-plus to `9.4.27.v20200227`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-security to v9</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-security-9.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-security to `9.4.27.v20200227`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-server to v9</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-server-9.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-server to `9.4.27.v20200227`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-servlet to v9</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-servlet-9.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-servlet to `9.4.27.v20200227`


</details>

<details>
<summary>Update dependency org.eclipse.jetty:jetty-util to v9</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jetty-jetty-util-9.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jetty:jetty-util to `9.4.27.v20200227`


</details>

<details>
<summary>Update dependency org.eclipse.jgit:org.eclipse.jgit to v5</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.eclipse.jgit-org.eclipse.jgit-5.x`
  - Merge into: `master`
  - Upgrade org.eclipse.jgit:org.eclipse.jgit to `5.7.0.202003110725-r`


</details>

<details>
<summary>Update dependency org.scalatest:scalatest to v3</summary>

  - Schedule: [""at any time""]
  - Branch name: `renovate/org.scalatest-scalatest-3.x`
  - Merge into: `master`
  - Upgrade org.scalatest:scalatest to `3.3.0-SNAP2`


</details>

<br />

:children_crossing: Branch creation will be limited to maximum 2 per hour, so it doesn't swamp any CI resources or spam the project. See docs for `prhourlylimit` for details.


---

:question: Got questions? Check out Renovate's [Docs](https://docs.renovatebot.com/), particularly the Getting Started section.
If you need any further assistance then you can also [request help here](https://togithub.com/renovatebot/config-help/issues).


---

This PR has been generated by [WhiteSource Renovate](https://renovate.whitesourcesoftware.com). View repository job log [here](https://app.renovatebot.com/dashboard#spark-notebook/spark-notebook).
",23715842
402,2019-10-22T20:30:47Z,2019-10-22T20:30:47Z,,,"When installing edward using the default method (`pip install edward`) and using the tensorflow versions specified in the setup.py (tensorflow>=1.2.0rc0), I get the following for some of the get started code:
```
ModuleNotFoundError: No module named 'tensorflow.contrib'
```

Since Tensorflow released a v2.0 that has a different API, looks like the tensorflow dependency should be constrained to be <2.0 as is done here. This seems to work as expected but https://github.com/blei-lab/edward/issues/882 is showing up when I run examples from the notebooks directory and elsewhere so perhaps this should be constrained further or that issue could be fixed in a separate PR.

<details>
<summary>Stacktrace</summary>
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-2-9240d2ad9677> in <module>
      1 import tensorflow as tf
----> 2 from edward.models import Normal
      3 
      4 W_0 = Normal(loc=tf.zeros([1, 2]), scale=tf.ones([1, 2]))
      5 W_1 = Normal(loc=tf.zeros([2, 1]), scale=tf.ones([2, 1]))

~/.local/share/virtualenvs/ed-61OcBILj/lib/python3.7/site-packages/edward/__init__.py in <module>
      3 from __future__ import print_function
      4 
----> 5 from edward import criticisms
      6 from edward import inferences
      7 from edward import models

~/.local/share/virtualenvs/ed-61OcBILj/lib/python3.7/site-packages/edward/criticisms/__init__.py in <module>
      5 from __future__ import print_function
      6 
----> 7 from edward.criticisms.evaluate import *
      8 from edward.criticisms.ppc import *
      9 from edward.criticisms.ppc_plots import *

~/.local/share/virtualenvs/ed-61OcBILj/lib/python3.7/site-packages/edward/criticisms/evaluate.py in <module>
      7 import tensorflow as tf
      8 
----> 9 from edward.models import RandomVariable
     10 from edward.util import check_data, get_session, compute_multinomial_mode, \
     11     with_binary_averaging

~/.local/share/virtualenvs/ed-61OcBILj/lib/python3.7/site-packages/edward/models/__init__.py in <module>
      5 from __future__ import print_function
      6 
----> 7 from edward.models.dirichlet_process import *
      8 from edward.models.empirical import *
      9 from edward.models.param_mixture import *

~/.local/share/virtualenvs/ed-61OcBILj/lib/python3.7/site-packages/edward/models/dirichlet_process.py in <module>
      6 
      7 from edward.models.random_variable import RandomVariable
----> 8 from tensorflow.contrib.distributions import Distribution
      9 
     10 try:

ModuleNotFoundError: No module named 'tensorflow.contrib'


</details>",51468412
403,2019-01-03T00:02:00Z,2019-01-03T00:02:00Z,,,,51468412
404,2018-12-20T19:56:43Z,2018-12-20T19:56:44Z,,,,51468412
405,2018-10-31T17:12:08Z,2018-10-31T17:12:08Z,,,"I want to add black-box alpha as an inference method. I have added the `BBAlpha` inference object and the unit test `bb_alpha_test.py`. Please do help to make my code better or give suggestions. 

Here is the link to the Edward forum:
https://discourse.edwardlib.org/t/black-box-alpha-divergence-minimization/413/5",51468412
406,2018-10-22T18:45:49Z,2018-10-22T18:45:49Z,,,"_USE_C_SHAPES has been forced True for a while now
(since TensorFlow 1.9 in June, commit: https://github.com/tensorflow/tensorflow/commit/1d74a69443f741e69f9f52cb6bc2940b4d4ae3b7)",51468412
407,2018-10-17T00:17:05Z,2018-10-19T17:25:51Z,,,This pull request updates uses of tf.contrib.distributions with tfp.distributions (TensorFlow Probability: https://www.tensorflow.org/probability/).,51468412
408,2018-10-13T22:39:29Z,2018-10-13T22:39:29Z,,,"Recently I wrote my Bachelor's Thesis and I had to use Edward in order to build Bayesian Neural Network. In the evaluation I had to use CRPS (continuous ranked probability score) but the score is not one of the metrics. I've implemented it more or less the same was as in https://github.com/TheClimateCorporation/properscoring. 

",51468412
409,2018-07-25T22:20:57Z,2018-08-15T16:43:15Z,,,"See issues #882 and #893.
Another pull request (#894) apparently never got merged, maybe because it doesn't fix travis?",51468412
410,2018-07-16T12:13:59Z,2018-07-16T12:15:50Z,,,Create tf.Operation by passing in pre-created inputs instead of passing them in later. This is to make it compatible with C_API which is now enabled by default in Tensorflow. Also remove graph._add_op() since this is already done during tf.Operation creation in the most recent version.,51468412
411,2018-05-17T22:45:40Z,2018-10-23T18:25:08Z,,,"…according to the comment by larsr.
See: https://github.com/blei-lab/edward/issues/893#issuecomment-388792874
",51468412
412,2018-05-02T02:30:53Z,2018-05-27T23:54:00Z,,,"Follows on https://github.com/blei-lab/edward/issues/882

Specifically 

![2018-05-01-221624_1600x1668_scrot](https://user-images.githubusercontent.com/6968573/39502316-828baa78-4d8d-11e8-8d66-63ca3a04b0e4.png)

I'm assuming your comment about ""just removing it"" was tongue in cheek but I figured I'd give it a shot anyways

Running ```pytest tests```

I got 

```
78 failed, 248 passed, 49 warnings
```

where the relevant errors (basically anywhere there was a copy) are as follows

```

../edward/util/random_variables.py:321: in copy                                                                                                                                                  
    new_op = copy(op, dict_swap, scope, True, copy_q, False)                                                                                                                                     
../edward/util/random_variables.py:374: in copy                                                                                                                                                  
    op_def)                                                                                                                                                                                      
/home/ian/.virtualenvs/edward/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1732: in __init__                                                                                   
    op_def, inputs, node_def.attr)                                                                                                                                                               
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
                                                                                                                                                                                                 
self = <[AttributeError(""'Operation' object has no attribute '_c_op'"") raised in repr()] Operation object at 0x7fef20988e48>                                                                     
op_def = name: ""Mul""                                                                                                                                                                             
input_arg {                                                                                                                                                                                      
  name: ""x""                                                                                                                                                                                      
  type_attr: ""T""                                                                                                                                                                                 
}                                                                                                                                                                                                
input_arg {                                                                                                                                                                                      
  name: ""y""                                                                                                                                                                                      
  type_attr: ""T""                                                                                                                                                                                 
}                                                                                                                                                                                                
output_arg {                                                                                                                                                                                     
  name:...ype: DT_INT32                                                                                                                                                                          
      type: DT_INT64 
      type: DT_COMPLEX64                                                                                                                                                               
      type: DT_COMPLEX128
    }
  }
}
is_commutative: true

inputs = [], attrs = <google.protobuf.pyext._message.MessageMapContainer object at 0x7fef153103f0>                                                                                               

    def _reconstruct_sequence_inputs(self, op_def, inputs, attrs):
      """"""Regroups a flat list of input tensors into scalar and sequence inputs.
    
        Args:
          op_def: The `op_def_pb2.OpDef` (for knowing the input types)
          inputs: a list of input `Tensor`s to the op.
          attrs: mapping from attr name to `attr_value_pb2.AttrValue` (these define
            how long each sequence is)
    
        Returns:
          A list of `Tensor`s (corresponding to scalar inputs) and lists of
          `Tensor`s (corresponding to sequence inputs).
        """"""
      grouped_inputs = []
      i = 0
      for input_arg in op_def.input_arg:
        if input_arg.number_attr:
          input_len = attrs[input_arg.number_attr].i
          is_sequence = True
        elif input_arg.type_list_attr:
          input_len = len(attrs[input_arg.type_list_attr].list.type)
          is_sequence = True
        else:
          input_len = 1
          is_sequence = False
    
        if is_sequence:
          grouped_inputs.append(inputs[i:i + input_len])
        else:
>         grouped_inputs.append(inputs[i])
E         IndexError: list index out of range

/home/ian/.virtualenvs/edward/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1803: IndexError 
```",51468412
413,2018-03-04T02:00:27Z,2018-03-08T22:43:01Z,,,"When I use `log_likelihood` criticism method on the bayesian regression example, it fails with an error (message shown below):

Program:
```python

....

  # CRITICISM

  # Plot posterior samples.
  sns.jointplot(qb.params.eval()[FLAGS.nburn:FLAGS.T:FLAGS.stride],
                qw.params.eval()[FLAGS.nburn:FLAGS.T:FLAGS.stride])
  plt.show()

  # Posterior predictive checks.
  y_post = ed.copy(y, {w: qw, b: qb})

 ....

  print(""Log likelihood on test data:"")
  print(ed.evaluate('log_lik', data={X: X_test, y_post: y_test}))

  print(""Displaying prior predictive samples."")
  n_prior_samples = 10

  ....

if __name__ == ""__main__"":
  tf.app.run()
```

Error:
```python
 File ""bayesian_regression.py"", line 70, in main
    print(ed.evaluate('log_lik', data={X: X_test, y_post: y_test}))
  File ""/usr/local/lib/python2.7/site-packages/edward/criticisms/evaluate.py"", line 219, in evaluate
    evaluations += [log_likelihood(y_true, n_samples, output_key, feed_dict, sess)]
  File ""/usr/local/lib/python2.7/site-packages/edward/criticisms/evaluate.py"", line 478, in log_likelihood
    tensor = tf.reduce_mean(output_key.log_prob(y_true))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/distributions/distribution.py"", line 718, in log_prob
    return self._call_log_prob(value, name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/distributions/distribution.py"", line 700, in _call_log_prob
    return self._log_prob(value, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/distributions/normal.py"", line 189, in _log_prob
    return self._log_unnormalized_prob(x) - self._log_normalization()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/distributions/normal.py"", line 207, in _log_unnormalized_prob
    return -0.5 * math_ops.square(self._z(x))
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/distributions/normal.py"", line 232, in _z
    return (x - self.loc) / self.scale
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 925, in binary_op_wrapper
    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 946, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1036, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 879, in _TensorTensorConversionFunction
    (dtype.name, t.dtype.name, str(t)))
ValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(""copied/Normal_2/loc:0"", shape=(40,), dtype=float32)'

```
It seems like the `log_likelihood` module was missing the cast to `float32`. I have added it in this change and also refactored the code a little.",51468412
414,2018-01-27T19:53:37Z,2018-12-19T01:48:39Z,,,Adding `conda update -all` to update `numpy` to version 1.14. ,51468412
415,2018-01-16T06:25:08Z,2018-03-02T00:50:20Z,,,"This PR is a redesign of Edward for version 2.0. It adds/changes the following:

1. Core function for manipulating continuations: `call_with_manipulate`.
2. Inference algorithms are now pure functions (up to your usual TF side effects).

Details and features of design forthcoming.

 @csuter, @davmre, @derifatives, @jvdillon, @matthewdhoffman, @mattjj

## Breaking changes to API
+ Model and variational programs are callables.
+ Inference classes only have a functional equivalent and are Tensor-in Tensor-out.
+ All `edward.criticisms` is removed.
+ All `edward.util` is removed.
+ All custom Edward random variables are removed: `DirichletProcess`, `Empirical`, `ParamMixture`, `PointMass`.
+ `rv.value()` is renamed to `rv.value`.

## todos

+ [ ] Break up into smaller, self-contained PRs.
+ [x] Refactor all inferences (excluding Monte Carlo) to support traces.
+ [x] Refactor Monte Carlo (waiting on @jvdillon for go/hmc-new-interface)
+ [ ] Refactor `inferences/conjugacy/` (waiting on @matthewdhoffman, @mattjj)
+ [x] Add programmable docstrings.
+ [x] Update `criticisms` module.
+ [x] Update `util` module.
+ [ ] Update `tests/`. (only `tests/inferences/` left)
+ [ ] Update `examples/`, `docs/`, `notebooks/`.",51468412
416,2018-01-07T00:26:40Z,2018-06-27T11:34:31Z,,,WIP. Addresses [#379](https://github.com/blei-lab/edward/issues/379).,51468412
417,2017-11-22T22:19:36Z,2017-12-15T19:05:06Z,,,"This was a comment #211 which snowballed into a PR, which ports [PyMC3](https://github.com/pymc-devs/pymc3/blob/master/pymc3/distributions/timeseries.py#L247) implementation of Euler-Maruyama scheme for stochastic differential equations.  I'll follow up on 

- [ ] batch vs event shaping (I don't really understand this yet)
- [ ] sampling
- [ ] tests/examples

but feedback appreciated",51468412
418,2017-10-07T20:42:45Z,2017-10-07T21:02:43Z,,,Following merge of random variable supports from Edward -> tf.contrib.distributions.,51468412
419,2017-10-04T05:31:34Z,2017-10-05T04:04:17Z,,,Following decision to merge features in Edward <-> BayesFlow.,51468412
420,2017-10-02T09:42:41Z,2017-10-02T23:23:46Z,,,"Re: #759 this adds limited support for full graph sampling. Given an RV, `ed.marginal` will traverse its parent graph, replacing any root ancestor instances of `RandomVariable` with a sampled equivalent, so that each non-root RV in the graph is evaluated with a tensor of `batch_shape` of parameters. 

For example:
```python
import edward as ed
import numpy as np

from edward.models import Normal


ed.get_session()
loc = Normal(0.0, 100.0)
y = Normal(loc, 0.0001)
conditional_sample = y.sample(50)
marginal_sample = ed.marginal(y, 50)

np.std(conditional_sample.eval())
0.000100221

np.std(marginal_sample.eval())
106.55982
```

This current implementation does not work for graphs of RVs using the sample_shape arg. That will require some refactoring of how `RandomVariable` internally stores the sample_shape. I'm making this PR mostly because I'm confident that the API will be backwards compatible.

Beyond not allowing `sample_shape`, `ed.marginal` can fail in the following ways.

- Failure during graph construction, which could be caused by a successful broadcast in the source graph being broken by the prepended sample dimension.
- Failure after graph construction but before execution, as a result of `ed.marginal` detecting incorrect broadcasting (this prevents situations where sampling `10000` from a scalar RV produces some enthusiastically broadcasted `(10000, 10000, 1)` shaped tensor. ",51468412
421,2017-10-02T05:16:50Z,2017-10-02T05:17:46Z,,,"For example, we raise an error when instantiating an inference object that assumes explicit densities and the passed-in objects don't have a `_log_prob` method.

I was working on this until I started to wonder if we should even support such checks. Maybe we should throw away most of our static typing (#188, #198, #213) and follow the spirit of duck typing.",51468412
422,2017-10-02T04:52:54Z,2017-10-02T05:18:26Z,,,"Having a top navigation bar, API sidebar, and tutorials is unnecessary. The biggest change is that I removed all docs on the navigation bar and either made it explicitly part of the API docs (as docstrings) or threw it into the tutorials section.

![screen shot 2017-10-01 at 9 49 42 pm](https://user-images.githubusercontent.com/2569867/31064571-75cc845c-a6f2-11e7-833f-7486a7bff84e.png)

todo
+ [ ] change the sidebar and allow more fine-grained pages via a dynamic scrollbar with collapsible sections
+ [ ] deliberate on how to organize tutorials given API tutorials. should it be ""Tutorials"" and ""API tutorials""? ""Tutorials"" and ""Advanced tutorials""? Maybe shove all API tutorials into api/ sidebar like TensorFlow docs sometimes does?",51468412
423,2017-09-27T13:45:18Z,2017-09-29T17:57:33Z,,,"Here is an implementation of the Renyi divergence variational inference.
There's also an example on VAEs.

Here is a link to the edward forum with some more info:
https://discourse.edwardlib.org/t/renyi-divergence-variational-inference/366/3

ps: Sorry for the quite messy commit history.",51468412
424,2017-08-03T12:00:25Z,2017-08-14T21:52:10Z,,,"Hello to all !

This PR partially solve this issue https://github.com/blei-lab/edward/issues/541 which is about implementing [NUTS](https://arxiv.org/abs/1111.4246). As proposed in [this topic](https://discourse.edwardlib.org/t/any-plans-for-nuts/23), starting with the Dual Averaging is a good start.

The implemented inference method is in `edward/inferences/hmcda.py` and the corresponding test in `tests/test-inferences/test_hmcda.py`.

I hope you'll appreciate the PR ! :)",51468412
425,2017-06-24T09:30:10Z,2017-06-24T22:22:22Z,,,"In terminal work, printing Unicode characters can be problematic. In particular, using a Unicode character for the progressbar causes encoding errors if the environment is not set up properly:

```
UnicodeEncodeError: 'ascii' codec can't encode character '\u2588' in position 15: ordinal not in range(128)
```

This patch changes the progress bar character to be ASCII, so that we have less to worry about when running Edward.",51468412
426,2017-06-20T16:13:44Z,2017-09-24T13:12:21Z,,,I wrote some MCMC related examples in jupyter. Should these trials be as independently to the one of variational algorithm's note  or merged to existing variational algorithm's note?,51468412
427,2017-06-19T22:14:22Z,2017-09-26T21:20:48Z,,,"This PR makes explicit which class members and methods are public during inference. In particular, we prepend any private methods with underscores. We also prepend all class members with underscore and use the `@property` decorator as a getter method. This follows TensorFlow convention.",51468412
428,2017-06-12T12:01:48Z,2017-06-14T03:37:36Z,,,"With long simulations, storing every Monte Carlo sample is not going to fit in memory, nor really meaningful. For instance with minibatches, for me storing samples per minibatch is too much, only once per full data should be enough.

This is the simple if somewhat hacky solution that uses `floor_div` to only increment the empirical index once every iters_per_sample. I guess another solution could be to allocate a separate variable for current state, and have a separate ""sample"" index that could be conditionally incremented or something (this could also be used to store nothing during burnin). This would make sense if we don't want to store the latent parameters at all. (Rather than storing the model, it might in some cases make sense to just store predictions of the model). What do you think would be the best?",51468412
429,2017-05-01T13:40:54Z,2017-05-01T13:45:32Z,,,"This improves unit tests for GANs. For example, for `GANInference`, we can check more precise fits by using the optimal discriminator's closed form solution.

The PR also removes stochasticity after fixing the seed. This was caused by discriminative and generative updates being jointly run, and whichever terminated first depended on the machine's runtime. This made it difficult to reproduce exact results.",51468412
430,2017-02-27T00:23:23Z,2017-04-16T00:54:20Z,,,"I have written a few examples of using edward to fit some simple time series models. They are in Jupyter notebook form right now, but I'm happy to convert them to scripts if that's easier (though I think having them as notebooks with some annotations can also be useful for learning edward). Any suggestions for improvement are welcome. I am planning on adding at least two more examples, one that uses a VAR(1) model for multiple replicates of a series and another example demonstrating a VAR(p) process (p > 1).

One point to notice is that constructing the `Inference` objects takes a very long time. I haven't dug into why this is happening, but I'm guessing that there is a lot of copying going when when I connect the training data to the corresponding nodes in the computation graph. Now that ICML is over I should have some time to dig into what's actually happening during construction and ways that we could potentially speed it up.",51468412
431,2017-02-14T05:39:57Z,2017-10-02T21:53:37Z,,,"We use the KL function available in `tf.contrib.distributions`, which is registered for a number of distributions. This avoids duplicate effort from having to write our own functions. Using this function also lets us apply other analytic KL penalties during variational inference.

todo
+ [x] In `KLqp`, we need to determine whether the KL is registered for a pair of distributions. This lets us determine whether to use gradients of analytic KLs or more stochastic gradients. How?",51468412
432,2017-02-09T22:20:22Z,2017-04-16T00:54:19Z,,,"[Mescheder et al., 2017](https://arxiv.org/abs/1701.04722)

todo
+ [x] Gaussian posterior
+ [ ] Real data set
+ [ ] integration test",51468412
433,2016-12-17T03:03:52Z,2018-02-02T05:08:14Z,,,"A community repository is opened for pre-trained probabilistic models. (Fixes #28)

Part of the success of the neural net community is the ability to work with pre-trained models. The probabilistic modeling community should have this ability too.",51468412
434,2016-11-29T13:01:41Z,2017-04-16T00:54:19Z,,,"Notebook on the Latent Networks tutorial. It mimics the tutorial and the script. Similar to my question on the Classification notebook, I was wondering how to visualise this. The latent dimensionality here is `3`.",51468412
435,2016-11-28T11:36:36Z,2017-04-16T00:54:19Z,,,"Added the Gaussian process classification tutorial. This replicated the code found in the [script](https://github.com/blei-lab/edward/blob/master/examples/gp_classification.py) and the [example](http://edwardlib.org/tutorials/supervised-classification) on the website.

Thought it would be nice to visualise this as well - any ideas on how to go about doing so?",51468412
436,2016-07-19T18:05:29Z,2017-04-16T00:54:19Z,,,"This PR does the following:
1 - tutorial on VAE with a particular focus on the encoder part

Side Effects:
N\A

Reviewer Suggestions:
@dustinvtran 
",51468412
437,2020-03-22T18:05:36Z,2020-03-23T08:17:59Z,,,"Trying the idea from https://github.com/RaRe-Technologies/smart_open/issues/440.

#### Motivation

In general, seeing how much others use a package improves confidence in its robustness and maintenance. A kind of social proof.

I see no downsides, except if the service that provides this badge (https://pepy.tech/) proves unstable or unreliable.

Badges for gitter and wheels seem useless, so I removed them.

#### Checklist

Before you create the PR, please make sure you have:

- [x] Picked a concise, informative and complete title
- [x] Clearly explained the motivation behind the PR
- [x] Linked to any existing issues that your PR will be solving
- [x] Included tests for any new functionality
- [x] Checked that all unit tests pass
",1349775
438,2020-03-06T06:04:24Z,2020-03-24T21:08:32Z,,,"### Background
The inspiration for this contribution arose during my research for my senior thesis project in mathematics at Pomona College. In my project I have been investigating the generation of word embeddings for Medieval Latin using the tools offered by gensim. In order to determine the accuracy of these embeddings, I have relied primarily on three types of tasks--analogies, odd-one-out, and topk similarity. 

Testing on analogies has been quite straight forward due to the already implemented wv.evaluate_word_analogies() function. This method is makes large scale experimentation and testing of word embeddings easy because it provides a means for test cases to be generated from a custom file in the same style as [Mikolov et. al's analogy set](http://download.tensorflow.org/data/questions-words.txt). 

However, in addition to analogy testing, I also desired to evaluate my word embeddings on both the odd one out task as well as topk similarity. Though functions to perform these tasks already exist in the form of the wv.doesnt_match() and wv.most_similar(), there was no similarly convenient way to apply these methods to a large test set. For my own purposes, I set out to implement functions that would provide the flexible and scaled evaluation capabilities of the wv.evaluate_word_analogies() function  for the odd one out and topk similarity tasks. 

In this PR I add two functions: evaluate_doesnt_match() and evaluate_top_k_similarity(). Both seek to emulate the style of the evaluate_word_analogies() function in the type of parameters it takes and the format of the test set txt file. Details are provided below.

### Doesn't Match Evaluation on a File

This function expands the functionality of model.wv.doesnt_match() by allowing the user
to perform this evaluation task at scale on a custom .txt file of categories. It does this by
creating all possible ""odd-one-out"" groupings from the categories file. The groups are composed
of k_in words from one category and 1 word from a different category.

The function expects the .txt file to follow the formatting conventions of the 
evaluate_word_analogies() function where each category has a heading line (identified
with a colon) followed by a list of space separated words that belong to that category on the next line.

e.g.
:fruits
apple banana pear raspberry

Note that each category in the txt file must have at least k_in
words otherwise, comparison groups can't be created.

In the event that categories contain the same word, the function could produce comparison groups that contain duplicate words. 

For example, consider the following txt file.
:food
apple, hamburger, hotdog, soup
:fruit
apple, pear, banana, grape

Say k_in=3, then some comparisons would contain duplicate words.

[apple, hamburger, hotdog, apple]
[apple, hamburger, soup, apple]
[apple, hotdog, soup, apple]

By default, this function ignores these comparisons, unless
eval_dupes=True.

### Topk Similarity Evaluation on File

This function evaluates the accuracy of a word2vec model
using the topk similarity metric. 

The user provides the function with a txt file of words divided
into categories. It is expected the .txt file follows the formatting conventions of the 
evaluate_word_analogies() method where each category has a heading line (identified
with a colon) followed by a list of space separated words that belong to that category on the next line.

e.g.
:fruits
apple banana pear raspberry

For each word in the file, the function generates a topk similarity list. This list 
is compared to the other entries in the category of the word which generated the list
in order to find matches between the two. The number of matches is then used to compute 
one of two accuracy measures -- topk_in_cat or cat_in_topk.

### Summary
These two additional functions have been useful to me in my own experimentation with creating _good_ word embeddings by allowing me to evaluate their performance on the odd one out and similarity tasks with the same level of robustness as the analogy task. I believe this is an important tool because measuring the _goodness_ of embeddings can often be ambiguous. Access to multiple evaluation can help bring clarity to the task of assessment. 

Since I needed to implement these functions for my own work it seemed fitting to offer them back as a contribution to the project. I hope you find them useful.

Thanks!

Nate Stringham",1349775
439,2020-01-16T15:05:30Z,2020-01-23T07:50:57Z,,,"importing direct from keras raises error 
> 
AttributeError: module 'tensorflow' has no attribute 'get_default_graph'

as recommended by the tf community, we should import keras as tensorflow.keras",1349775
440,2019-12-05T07:28:56Z,2020-03-21T12:59:50Z,,,Removing some of the layers of unnecessary complexity from #1777 & related wrong-turns. Stripping out deprecations that were marked as to-be-removed in gensim-4.0.0. ,1349775
441,2019-10-28T19:36:22Z,2020-01-27T19:27:56Z,,,"Closes: https://github.com/RaRe-Technologies/gensim/issues/2535

This is my first PR for gensim so all comments are welcome. 
To be honest I have no idea how to test `restrict_vocab` for `most_similar_cosmul` or `most_similar` for `evaluate_word_analogies`. I wanted to write something similar to already existing tests for these keywords but did not find any (nor tests for `restrict_vocab` keyword in case of `most_similar` function and nor `most_similar` keyword in case of `accuracy` function)

Summary:

- Added new `restrict_vocab` parameter to `most_similar_cosmul`
- Improved `most_similar_cosmul` shorthand to handle both positive and negative cases
- Parameterized similarity function in `evaluate_word_analogies`",1349775
442,2019-10-17T23:48:32Z,2019-10-21T18:07:36Z,,,"Fix #2634

As reported in the issue, the last version of numpy that supported python2 was 1.16.5, so this requirement can be increased slightly from 1.16.1",1349775
443,2019-10-17T23:25:25Z,2020-01-23T08:13:02Z,,,"There are a couple of ways to go about preventing the infinite loop mentioned in the Issue from occurring. I thought maybe to user logger.error, or to skip the while loop if the corpus is empty. ButI saw in the _inference_ method that a RunTimeError is thrown when lda_alpha and lda_beta are not initialized. So I thought this seemed consistent.",1349775
444,2019-10-14T19:25:46Z,2020-01-25T12:52:28Z,,,,1349775
445,2019-10-14T18:11:09Z,2019-10-24T09:48:09Z,,,,1349775
446,2019-10-07T23:19:59Z,2019-10-16T00:45:04Z,,,Documentation for using minimum_probability parameter to obtain all topic distribution. Fixed #2489,1349775
447,2019-10-06T23:53:35Z,2019-10-24T09:49:12Z,,,Fix #2561,1349775
448,2019-10-05T06:30:41Z,2019-10-07T04:05:39Z,,,fixes #2523,1349775
449,2019-08-09T13:31:37Z,2019-10-01T02:05:20Z,,,,1349775
450,2019-07-04T06:09:51Z,2019-09-28T22:45:50Z,,,"Implemented as suggested by https://github.com/RaRe-Technologies/gensim/issues/2537#issuecomment-506584926
Fixes #2537",1349775
451,2019-06-18T22:31:17Z,2020-02-12T10:04:51Z,,,"The original function of `init_sim` is checking if there are no objects in `self.vectors_norm`, then it is calculated and saved by L2-normalizing `self.vectors`. 

However, if there are any newly added vectors in `self.vectors` that have not been pre-computed into L2-normalized vectors, we should pre-compute them then add to the `self.vectors_norm`.

This is extremely useful, because although KeyedVectors have ability to add new vectors (such as inferred vectors) and their tags, they cannot execute `self.most_similar()` method for newly added vectors because the method requires lookup of `self.vectors_norm`. ",1349775
452,2019-05-31T15:11:49Z,2020-01-05T02:04:16Z,,,I think it was just an oversight in #1915.,1349775
453,2019-05-07T14:31:40Z,2019-11-22T15:10:53Z,,,Made a rebase,1349775
454,2019-05-06T21:05:36Z,2019-07-08T04:10:28Z,,,"This feature is in accordance to this issue: [2459](https://github.com/RaRe-Technologies/gensim/issues/2459)

It does the following:
1. `predict_output_word` function take a document vector and returns a list of tuples having word and probability score.
2. The reference for this implementation has been taken from `Word2Vec.predict_output_word()` function.",1349775
455,2019-03-07T08:02:32Z,2019-04-28T03:28:08Z,,,"In current doc2vec's infer_vector implementation, it judges same content at `if` statement step by step at L926.
But I think it will be faster when it is judged at first and then train.",1349775
456,2019-02-21T09:08:52Z,2019-04-28T03:15:06Z,,,"Adding common used Chinese Word Similarity evaluating dataset to the repo those are wordsim240, wordsim296 and wordsim297.",1349775
457,2019-02-12T17:54:58Z,2019-04-28T03:14:56Z,,,Fixes #2380 .,1349775
458,2019-01-30T03:47:19Z,2019-04-28T03:14:33Z,,,"- ease check on fast and slow version
- make more relaxed for num_doc and num_terms arguments

TODO: 

- [x] tests
- [x] check on empty doc
- [x] benchmark

bench:
[corpus2csc.txt](https://github.com/RaRe-Technologies/gensim/files/2815531/corpus2csc.txt)

bench results:
```
CLASSIC, fast path
1.9 s ± 5.61 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
CLASSIC, slow path
2.63 s ± 19.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
NEW, fast path
1.42 s ± 8.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
NEW, slow path
2.45 s ± 6.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
NEW 2, fast path
1.41 s ± 15.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
NEW 2, slow path
1.43 s ± 10.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
NEW 3, fast path
1.94 s ± 14.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
NEW 3, slow path
1.95 s ± 7.24 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```",1349775
459,2019-01-30T02:12:57Z,2019-05-04T12:07:34Z,,,"+ fix doc for grouper
+ add argument in chunksize

Now word ids will not converse to float and it will be stored and pickled as an integer in memory and so there is no backward conversion issues + speed + size of pickled data.

benchmark:

``` python
import numpy as np
from pickle import dumps
bow = [(np.random.randint(1000000), np.random.rand()*10e6) for i in range(10000)]

def test(b):
    print('TRY:', len(b))

    print('pure python')

    %timeit dumps(b)

    print('np array without args')

    %timeit dumps(np.array(b))

    print('np array with simple dtype')
    %timeit dumps(np.array(b, dtype=np.float32))

    complex_dt = np.dtype([('1', np.int32), ('2', np.float32)])
    print('np array with complex dtype')
    %timeit dumps(np.array(b, dtype=complex_dt))

    print('-'*50)

test(bow[:50])
test(bow[:150])
test(bow[:500])
test(bow[:1000])
test(bow[:2000])
test(bow[:5000])
test(bow)
```

results (anyone else should recheck it):

```
TRY: 50
pure python
30.2 µs ± 407 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
np array without args
138 µs ± 4.54 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array with simple dtype
141 µs ± 4.92 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array with complex dtype
148 µs ± 3.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
--------------------------------------------------
TRY: 150
pure python
89.1 µs ± 520 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
np array without args
260 µs ± 6.07 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array with simple dtype
260 µs ± 5.11 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array with complex dtype
202 µs ± 4.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
--------------------------------------------------
TRY: 500
pure python
336 µs ± 5.43 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array without args
699 µs ± 11.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array with simple dtype
684 µs ± 7.86 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array with complex dtype
392 µs ± 5.77 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
--------------------------------------------------
TRY: 1000
pure python
660 µs ± 8.38 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array without args
1.33 ms ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array with simple dtype
1.32 ms ± 4.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
np array with complex dtype
689 µs ± 5.77 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
--------------------------------------------------
TRY: 2000
pure python
1.7 ms ± 76.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
np array without args
2.65 ms ± 61 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
np array with simple dtype
2.62 ms ± 20.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
np array with complex dtype
1.29 ms ± 11.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
--------------------------------------------------
TRY: 5000
pure python
4.11 ms ± 52.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
np array without args
6.58 ms ± 69.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
np array with simple dtype
6.56 ms ± 43.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
np array with complex dtype
2.92 ms ± 23.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
--------------------------------------------------
TRY: 10000
pure python
12 ms ± 95.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
np array without args
12.7 ms ± 35.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
np array with simple dtype
12.6 ms ± 34.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
np array with complex dtype
5.43 ms ± 43.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
--------------------------------------------------
```",1349775
460,2019-01-11T10:19:33Z,2019-04-28T03:17:56Z,,,,1349775
461,2019-01-08T00:03:25Z,2019-05-01T22:52:35Z,,,"3CosMul and 3CosAVG(set based vector offset) methods for solving analogies implemented.
Number of Topn similar words added as input in the evaluation of analogies.",1349775
462,2018-12-18T13:46:44Z,2020-01-05T12:18:13Z,,,"Communication between processes is not free due to pickling/unpickling.
So to reduce overhead we are trying to send a less data.",1349775
463,2018-12-03T11:06:59Z,2020-02-09T20:29:22Z,,,"**Tests**

~~We have some tests that we can also commit when this PR is of interest, I will polish them and embed them into the unittest module in that case.~~

They are commited.

**Materials**

As requested by @piskvorky, here are the materials that describe our module: 
[eLDA_algo_overview.pdf](https://github.com/RaRe-Technologies/gensim/files/2639134/eLDA_algo_overview.pdf)
[eLDA_motivation.pdf](https://github.com/RaRe-Technologies/gensim/files/2639135/eLDA_motivation.pdf)
[eLDA_when_to_use.pdf](https://github.com/RaRe-Technologies/gensim/files/2639136/eLDA_when_to_use.pdf)

**Code Review**

There are two places in ""gensim/models/ensemblelda.py"" for which I especially would like to have review:
- lines ~~691 and 883~~ 611 and 784: The error handling of multiprocessing
- line ~~1223~~ 1124 and following: The way I provide functions crucial to gensims ldamodel api. Those functions just forward the calls to an internal object. So are they too redundant or do you think they make things easier? Is there a better way to do so?
- ~~And another one for the ""tox -e flake8"" test, which does not pass here:~~
```
gensim/corpora/opinosiscorpus.py:78:22: W605 invalid escape sequence '\w'
                    processed = [
```
- ~~would you prefer the implemention not to use pandas, since it has not been a dependency of gensim before?~~",1349775
464,2018-11-21T13:00:43Z,2019-11-03T01:23:42Z,,,"Fixes #2260 

allows `most_similar()` to be called based on the newly trained vectors during multiple training iterations.",1349775
465,2018-09-03T13:26:18Z,2019-04-28T08:06:04Z,,,Solves issue https://github.com/RaRe-Technologies/gensim/issues/2172,1349775
466,2018-07-19T09:05:54Z,2019-11-13T17:47:42Z,,,"The current computation of word2vec losses has flaws. I address them in this PR.

This PR re-writes the computation of the loss for both CBOW and SG Word2Vec. The loss that is computed and reported is the running average NCE loss within the epoch. This means that for each new epoch, the counters are reset to 0, and the new average is computed. This was not the case before, and the loss was incremented during the whole training (but the total_examples used to average was only incremented during on epoch), which is not very informative, beside being also incorrect in the implementation. Moreover, reporting the average loss on an epoch appeared to me more in the spirit of what was trying to be achieved before.

The computation of the word2vec loss was flawed in many ways:
- race condition on the running_training_loss parameter (updated concurrently in a
  GIL-free portion of the code)
- As mentioned above, there was an incoherence between the accumulation of the loss on the whole run, and the reset of the averaging factor at each epoch.
- Even if the points above were fixed, remains the following: the dividing factor for the average in the case of SG is wrong. The averaging factor in the case of SG should not be the effective words, but the effective samples (a new variable I introduce), because the loss is incremented as many times as there are positive examples that are sampled for an effective word.

Additionnally, I add the logging of the current value of the loss in the progress logger, when compute_loss is set to True, and I add a parameter to the word2vec_standalone script to trigger the reporting of the loss.


As a hint towards the fact that the current implementation is now correct, one can look at the first reported values of the loss, when the word embeddings are still relatively uniformly distributed. In this situation, the expectancy of the NCE loss (for Skip-Gram) should be -(N+1)\log(\sigma(0)). Which is 5.545 for N=7, 14.556 for N=20... which corresponds to the reported loss values in the current implementation.


",1349775
467,2018-06-25T17:03:43Z,2019-09-29T06:27:48Z,,,"Render a nicer API reference page. Continues from #1809.

I had trouble merging `develop` into this PR -- there were files with trailing whitespace (`*_inner.c`), and even two files with Windows line endings: `lsi_dispatcher.py`, `lsi_worker.py`, which prevented me from committing the resolved conflicts.

I solved the conflicts by removing trailing whitespace, converting line endings to Unix and then finishing the merge commit.

I also fixed the content of the documentation of the distributed LSI and LDA files, since it used incorrect docstring style and introduced some minor documentation bugs, in PR #1912 (specifically commit 1611f3a477686bf9f462d0ed09b1daff2a58f09e).",1349775
468,2018-05-25T20:18:15Z,2019-03-02T19:29:43Z,,,"Fixed inconsistent shape of matrix returned by scipy2scipy_clipped, as described in issue #2065 
Solved by explicitly setting the second dimension of the new matrix to that of the input matrix.

Fixes #2065.",1349775
469,2017-10-10T13:52:12Z,2019-08-21T09:33:36Z,,,Rough initial code for sent2vec.,1349775
470,2017-05-12T08:10:12Z,2017-05-12T08:10:12Z,,,,33683694
471,2017-01-22T20:43:25Z,2017-01-22T20:43:25Z,,,"Hi,
I've wired up zoom in / zoom out / zoom to default
",33683694
472,2017-01-12T17:21:21Z,2017-11-18T04:16:18Z,,,Updated the readme to have the correct download links ( #580 ),33683694
473,2016-12-19T05:12:00Z,2016-12-19T16:42:11Z,,,"* make use of the find-port NPM package
* test `3001, 9000, 9001, 9002` for availability, make use of the first one returned",33683694
474,2020-03-26T20:13:04Z,2020-03-26T20:13:04Z,,,"This should fix #12102

I ran `iptest IPython.core.tests.test_completer` and it didn't seem to give any errors. Not sure if we want to add any new tests here, or if that is good enough.

According to jedi, `Multiple signatures are typical if you use Python stubs with @overload.` I made my change to account for all signatures, but not sure how this plays out IRL since I've never seen a function with multiple signatures.
",658518
475,2020-03-18T17:43:17Z,2020-03-18T17:43:17Z,,,"Hey ! 

You, yes you, you want to contribute to IPython but do't know where to start ? Take inspiration from this Pull Request, try to understand it, and see if it could bee done better, or just cleanup what I did and add tests/documentation.

--- 
Description:

Prompt toolkit has the ability to show extra informations on completions. RIght now we are using  it to show types. For unicode/latex, we could show what character we would complete to.

Example:


<img width=""206"" alt=""Screen Shot 2020-03-18 at 10 39 30 AM"" src=""https://user-images.githubusercontent.com/335567/76990327-03ef7880-6905-11ea-8c9a-6bca7be3b3a2.png"">


Here we can see the wiggly thing in the bottom left.",658518
476,2020-03-17T19:57:43Z,2020-03-17T23:06:05Z,,,"See https://twitter.com/jakevdp/status/1239979963920445441 for background.

-- 


![hmm](https://user-images.githubusercontent.com/335567/76896327-dd6f0600-684e-11ea-8030-11596cd68a01.gif)",658518
477,2020-03-02T20:42:26Z,2020-03-16T19:40:03Z,,,"Together with https://github.com/zeromq/pyzmq/pull/1368, this will fix #10516 

There were two issues (I was wrong about pyzmq):

1. `IPython.core.debugger` would swallow KeyboardInterrupts and not let them get raised. This fixes that issue.
2. `pdb` had the same issue. The ipykernel PR (which I will open next) fixes that.

I believe both need to be merged to fix the problem.
To get this fix in, the following needs to happen, in order:

* [ ] Merge this PR
* [ ] New IPython release
* [ ] Merge https://github.com/ipython/ipykernel/pull/490
* [ ] New IPykernel relesae

Next I will investigate why the tests are failing.",658518
478,2020-02-29T20:57:11Z,2020-02-29T22:56:48Z,,,A bit hacky but should help with #12151,658518
479,2020-02-10T13:44:03Z,2020-02-27T05:29:02Z,,,"Intended to close #8015 by making the suppression of the captured result optional. 

I want this so that I can capture cell output and use it within the same cell, while still displaying it to the screen. I eventually want to wrap that non-suppressing capture magic within a new `%%save_output` cell magic in order to close [this jupyter issue](https://github.com/jupyter/notebook/issues/3039).

However, it's not working yet - executing this cell in jupyterlab
```python
%%capture --return-result out

x = 5
x
```
returns
```
<ExecutionResult object at 7f72d0a49f90, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f72d0a499d0, raw_cell=""
x = 5
x
"" store_history=False silent=False shell_futures=True> result=None>
```
which makes sense, except that the `ExecutionResult` object has attribute `result=None`. I would have expected it to have `result=5`. Obviously I can't return the result if it's always being set to `None`.

Confusingly this does work in the interactive ipython shell, just not in the notebook:
```python
In [1]: from IPython.core.interactiveshell import InteractiveShell
In [2]: InteractiveShell().run_cell(raw_cell=""x=5 \nx"")                                                                                                                                                                        
Out[1]: 
<ExecutionResult object at 7fd41a854210, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fd41a854e10, raw_cell=""x=5 
x"" store_history=False silent=False shell_futures=True> result=5>
```

This problem might be related to #12090 ?
",658518
480,2020-02-01T20:21:24Z,2020-02-13T00:38:25Z,,,"This is more interesting to look at on a per commit basis.

It runs some auto-reformatting; and then add the commits hash to a list of hashes to ignore.
It is not (yet) suported by github/gitlab, but works great locally where blame is not screwed up.

The goal would be to always automatically reformat the source code.",658518
481,2019-11-25T20:24:16Z,2019-11-27T10:27:59Z,,,"A significant chunk of the startup time is spent loading pygments
lexers.

This make it lazy which slightly improve the hot and cold startup time.

time `ipython -c exit()` cold goes from ~0.7 to ~0.65 seconds on my work
laptop I expect much more on old or slower machines.",658518
482,2019-08-12T14:31:48Z,2019-08-17T06:34:17Z,,,"Currently the tab completion in %cd relies on glob.glob(), which strangely case sensible on macOS, while case-insensitive was actually the default setting of macOS.
This PR trying to address it, while keeps original behavior as default, simple add 'c.TerminalInteractiveShell.completion_sensitive = False' to config file enables case-insensible completion in %cd command.",658518
483,2019-06-01T01:22:14Z,2019-06-09T02:33:55Z,,,"Addresses #11749. If autoformat (done with `black`) is not desired, I can remove
that.

If it *is* desired, then I still need to add it to Travis to enforce that people
use it. One issue is that it requires 3.6+ to run.",658518
484,2019-05-16T23:10:46Z,2019-05-16T23:10:46Z,,,"getargspec is likely going to be removed one of those days in CPython (deprecation has been delayed a couple of times. 

It would be nice to remove this from IPython, though it may affect the Jupyter `inspect_reply`, as it removes `argspec` from it. Though I could not find anything actually using the info in `'argspec'`

One should study the effect of this patch , if more could be removed, or replaced by using `inspect.signature()`


 --- 

Feel free to take over this Patch/PR, I don't have a particular intention of moving it forward soon.",658518
485,2019-04-08T03:41:02Z,2019-05-09T14:16:40Z,,,"Currently, IPython's sphinx directive doesn't really use the `ast.parse` to decide where to break lines, except for functions. This encourages the directive to do just that. 

This will likely break a bunch of people's docs, so I would suggest we do this right and give some warning. 

Additionally, I think currently bad syntax gets ignored by the code. This will now crash.

xref #11362",658518
486,2019-04-03T13:59:05Z,2019-04-15T11:05:29Z,,,"This allows the rerun magic to be run in quiet mode, which suppresses
the `=== Executing: ===` and `=== Output: ===` headings as well as the
text of the cell being re-run. This means that cells that print can
optionally be re-run as part of a larger control structure like a loop
without IPython-specific noise.",658518
487,2019-01-22T01:54:55Z,2019-05-26T03:52:26Z,,,"Currently ipython stores the traceback on exception. The problem is that it creates a circular reference to locals() in each frame of the traceback. And until next exception arrives gc.collect() will fail to collect. If locals() happened to be huge variables, like 8GB of GPU RAM, the user has no choice but to restart ipython/jupyter to recover.

Solution - cleanse the saved tb from any references to `locals()` by running it through: `traceback.clear_frames(tb)`

My fix was made to only one such location in the ipython code and there is a dozen other places where it does that. So please kindly review for other similar situations.

I wrote a test case that demonstrates the problem and putting this fix in that particular location fixed that leak. But I don't know whether they are other situations where a similar fix needs to be applied.

You can see the test case here (run it with Alt-F so it runs all cells ignoring the errors)
https://hub.mybinder.org/user/stas00-fastai-misc-xbrsnjae/notebooks/debug/ipython/locals_leak_on_exc.ipynb
It shows how if there is a local variable that is of 128MB in size, an exception involving that function will leak that much RAM. And it shows how the next exception will release the previously held locals() allowing gc.collect() to recover that leaked memory.

The source of the test case is here:
https://github.com/stas00/fastai-misc/blob/master/debug/ipython/locals_leak_on_exc.ipynb

and here it is in straight ipython's output:

```
In [1]: import numpy as np                                                                                                                                                  

In [2]: def consume_cpu_ram(n): return np.ones((n, n)) 
   ...: def consume_cpu_ram_128mb():  return consume_cpu_ram(2**12)                                                                                                         

In [3]: import gc, os, sys, time, psutil 
   ...: process = psutil.Process(os.getpid()) 
   ...: def cpu_ram_used():  return process.memory_info().rss                                                                                                               

In [4]: def fail(): 
   ...:     x = consume_cpu_ram_128mb() 
   ...:     raise ValueError(""Ouch"") 
   ...:                                                                                                                                                                     

In [5]: before = cpu_ram_used() 
   ...: fail()                                                                                                                                                              
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-3b8b03759f74> in <module>
      1 before = cpu_ram_used()
----> 2 fail()

<ipython-input-4-e846ddbe1c18> in fail()
      1 def fail():
      2     x = consume_cpu_ram_128mb()
----> 3     raise ValueError(""Ouch"")
      4 

ValueError: Ouch

In [6]: gc.collect() 
   ...: after = cpu_ram_used() 
   ...: print(f""Difference { (after-before)/2**20}"")                                                                                                                        
Out[6]: 1105
Difference 128.62890625

In [7]: # force ipython to reset its %tb 
   ...: raise ValueError(""Reset"")                                                                                                                                           
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-7-31374e7d7384> in <module>
      1 # force ipython to reset its %tb
----> 2 raise ValueError(""Reset"")

ValueError: Reset

In [8]: gc.collect() 
   ...: after = cpu_ram_used() 
   ...: print(f""Difference { (after-before)/2**20}"")                                                                                                                        
Out[8]: 418
Difference 0.77734375
```
",658518
488,2019-01-09T19:14:15Z,2019-01-09T19:14:15Z,,,"Fixes #11548.
```
$ ipython3 script.ipy --force
['/home/techtonik/p/rirror.github.io/script.ipy', '--force']
$ ipython3 script.ipy -- --force
['/home/techtonik/p/rirror.github.io/script.ipy', '--force']
```",658518
489,2018-10-13T06:00:20Z,2018-11-08T01:35:28Z,,,"Basic clean up, formatting, unnecessary if/else removal and putting the style overrides separately to make the functions smaller.",658518
490,2018-10-12T19:11:00Z,2019-01-18T17:03:58Z,,,"This change allows setting the minimum/maximum brightness. It helps making text more readable on terminals with either dark or light background.

It requires the latest commit from prompt_toolkit master (which is not yet released). I'll probably push a new release soon.
See: https://github.com/jonathanslenders/python-prompt-toolkit/commit/0fef4be17836f444a533715e27a56e6ef75c8584
",658518
491,2018-10-04T13:06:03Z,2018-10-24T23:32:34Z,,,"Handled whitespaces and quotes based filenames using regex

closes #11309",658518
492,2018-09-30T23:32:56Z,2018-10-13T15:48:58Z,,,"prompted by #11336, in which I realized that `patch_stdout` was mostly meant for async based application. 

This run the all IPython shell into an eventloop, which has the side-effect of making async bg task work. ",658518
493,2017-11-21T10:34:35Z,2017-11-22T16:19:57Z,,,"Since people want the to-be-removed functionality as default, we should at least keep the ability to configure it!",658518
494,2017-05-28T19:36:16Z,2018-03-10T11:03:40Z,,,"# Introduction

There are nowadays several terminal emulators which support showing images in the terminal.
On such terminal emulators the terminal IPython version can support the image/png mime type.

The following is a summary of such terminal emulators to the extend that I am aware of them.
- ITerm2 on macOS
- Terminology, the terminal emulator of Enlightenment
- All terminal emulators which support the [Sixel protocol](https://en.wikipedia.org/wiki/Sixel)

In all these cases there is a simple program which takes the name of the image file and just displays
it on the terminal. E.g. for ITerm2 there is  the [catimg script](https://github.com/asmeurer/catimg) which used like this:
`catimg someimage.png`
Similar (but unfortunately different) programs exist for the other terminal emulators mentioned.

This pull request introduces an environment variable IPYTHON_IMAGE_VIEWER . When set, the terminal IPython will use the named program to view an image of type image/png .

# Unresolved issues

- I decided to use an environment variable and not a configurable. This is because IPYTHON_IMAGE_VIEWER really describes a property of the terminal and is therefore a bit analogous to TERM. However this is probably something to be discussed.
- It should now be possible  to use `%matplotlib inline` in the terminal but I couldn't figure out how to enable that.
- I am unsure how to write a testcase for this.",658518
495,2016-12-21T23:00:39Z,2018-03-29T08:59:37Z,,,"After QT is imported the SIP API functions can no longer be
called to set the QT API. This change ensures that all of
the APIs are either set to 1 or 2 so that other packages
that may be imported after this function have a consistent
interface. This is consistent with other packages such as
pyside and is particularly relevant for IPython as the
%matplotlib magic means that one of the first set of imports
made in an IPython session is often from the %matplotlib
magic.

For example:
```
%matplotlib qt
from pyface.qt import QtGui, QtCore
```
fails without this change as only some of the required APIs have been set to version 2.",658518
496,2020-01-31T16:53:14Z,2020-01-31T16:55:19Z,,,"Adds testing for python 3.8
",102908804
497,2019-11-14T07:47:43Z,2019-11-14T07:50:46Z,,,"### Pull Request Description
based on  #791, tqdm shows multiple progress bars if a cell is interrupted then run again.
tqdm has a submodule to support native Jupyter: https://pypi.org/project/tqdm/#ipython-jupyter-integration
using the submodule solves the issue.

-----
*After creating the pull request: in order to pass the **changelog_updated** check you will need to update the ""Future Release"" section of* `docs/source/changelog.rst` *to include this pull request.*
",102908804
498,2019-10-24T15:28:38Z,2020-03-25T18:45:20Z,,,"Allows users to supply either a pandas dataframe or a dask dataframe when creating an entity with `es.entity_from_dataframe`.

### Summary of Changes 
- Updated `df[col].tolist()` syntax to `list(df[col])` since dask dataframes do not support `.tolist()`
- Changed `frame.shape[0]` syntax to `len(frame)` as calling `.shape()` on a dask dataframe returns a delay object for the number of rows. Must use `len(frame)` to get the total number of rows.
- Changed `if df.empty` syntax to `if len(df) == 0` as dask dataframes to not have a `.empty` attribute.
- Updated syntax because dask dataframes do not support `inplace` parameter for dropping variables or renaming columns.
- In order to determine if a dask dataframe index has unique values, you must first call `.compute()` before `.is_unique`, so added logic to add this step for dask dataframes.
- You cannot assign a list or np.array to a dask dataframe column as you can with pandas, so you must first create a dask dataframe from a dask array and then perform the assignment
- Dask dataframes do not support using `.iloc` to select rows, so in order to get the first row of data, you must first call `.head()` or `.compute()` on the dataframe. `.head()` is better if you only want the first row, as `.compute()` will compute the full dataframe, which isn't needed.
- Calling `.set_index()` on a dask dataframe generates an error if the index column passed is of type `categorical`. This can be fixed by calling `.cat.as_ordered()` on the index column passed in or by casting the column to type `object` with `.astype(object)` first. On a simple test, there was only a slight difference in performance but `.astype(object)` was 4% faster.
- Updated sampling process in `utils/entity_utils.py` as dask dataframes do not support sampling by specifying the number of samples.
- Many more...",102908804
499,2019-08-18T03:33:44Z,2019-11-14T21:14:45Z,,,"Add a transform primitive and two variables. It transforms US states into US regions (Northeast, Midwest etc.)

It's failing the tests that serialize the mock data into the AWS container; I don't know how to change that.

-----
*After creating the pull request: in order to pass the **changelog_updated** check you will need to update the ""Future Release"" section of* `docs/source/changelog.rst` *to include this pull request.*
",102908804
500,2019-08-05T01:22:29Z,2020-03-11T21:12:33Z,,,"Prior to handling NaN's, the code first checks if any are present (to save calculation time). Time with old method that failed on NaNs was 650 ms per 1 million rows. Time for data with no NaNs is now 750ms per 1 million rows. When NaNs are present, time is 1.5 seconds per 1 million rows, even using np.where to fix NaNs in a vectorized way.

### Pull Request Description
This allows the Haversine function to handle NaN values as inputs. It does so in a vectorized way so that performance is affected as little as possible.

-----
*After creating the pull request: in order to pass the **changelog_updated** check you will need to update the ""Future Release"" section of* `docs/source/changelog.rst` *to include this pull request.*
",102908804
501,2019-07-31T18:40:40Z,2020-03-11T21:11:33Z,,,"### Pull Request Description
Fix the issue that ft.show_info() doesn't work in Jupyter.

-----
*After creating the pull request: in order to pass the **changelog_updated** check you will need to update the ""Future Release"" section of* `docs/source/changelog.rst` *to include this pull request.*
",102908804
502,2019-07-31T16:54:15Z,2020-03-11T21:12:37Z,,,"### Pull Request Description

Adding logic for the default value in _calculate_direct_features. Now, if the parent missing, the calculated value will be the default value (not a 'nan').

-----
*After creating the pull request: in order to pass the **changelog_updated** check you will need to update the ""Future Release"" section of* `docs/source/changelog.rst` *to include this pull request.*
",102908804
503,2019-05-19T21:03:10Z,2020-03-11T21:12:36Z,,,"- As described in issue 487, FT expects cutoff_time dataframe's columns to be in instance_id_time, ... order. This commit addresses this issue by first moving the expected columns to the first and second position before cracking on with the rest of the code 

- A unittest has been added to check the functionality of the added code",102908804
504,2019-04-10T16:44:26Z,2020-01-28T19:32:22Z,,,"Fixes #486 

When creating an empty dataframe, we could cast types using base dtypes.",102908804
505,2018-10-31T23:48:29Z,2020-03-11T21:12:43Z,,,"Addresses https://github.com/Featuretools/featuretools/issues/248

Adds `featuretools.io.from_sql()`

 * This function takes an `id` and `connection_string` or an SQLAlchemy connection object `connection`.
 * Ingests each table (all tables by default, or a subset specified by `tables` parameter) as an entity, using database fk constraints to infer and generate relationships.
 * Uses the primary key of the table as the `id` for the Entity, or generate one if not available.

Other parameters for `entity_from_dataframe()` can also be passed using a table-keyed dictionary. Ex: 
```python
> from_sql(id='my_es', connection=my_connection, variable_types={""widgets"": {'name': ft.variable_types.Categorical}}
```

There are a few requests for feedback in-line, but I'm especially eager for thoughts about:
 * Handling generating an index for Entities without corresponding primary keys (right now I'm just creating a 'featuretools_sql_import_id' field)
 * Opinions on using an instantiated helper class (current) vs. one function vs. many functions",102908804
506,2018-02-17T00:24:11Z,2020-03-11T21:10:40Z,,,Resolves #79 and does better inferring of Boolean types by checking if nunique() of the input column is 2.,102908804
507,2020-03-21T08:49:18Z,2020-03-23T16:43:51Z,,,"- Modified codebase to use typescript instead of javascript
- There are some places where we need to stop using `any` annotations that will require further analysis.
- Tested locally",131619646
508,2020-03-20T20:56:01Z,2020-03-20T21:01:24Z,,,"Hi there! In an attempt to run my Dagster pipelines on a distributed Dask cluster, I noticed that spawned dask-workers did not have the `DAGSTER_HOME` environment variable set (even when the original shell used to start them did). In my case, Dask is running on top of a CfnCluster with the SGE resource manager. SGE has a `qsub -v` option that lets you pass individual environment variables to the jobs. This PR adds a bit of information to the ""Deploying on Dask"" docs about how to configure Dask to do this automatically.",131619646
509,2020-03-19T05:35:11Z,2020-03-19T19:44:56Z,,,"This PR adds a small section of documentation to include installing `pandoc`, which is required for developing docs.",131619646
510,2020-02-26T23:28:22Z,2020-03-04T22:48:14Z,,,,131619646
511,2020-01-08T18:59:29Z,2020-02-25T23:04:03Z,,,"this lets you avoid solid creation boilerplate by creating a solid from any function

default parameters and varargs are dropped for simplicity

this has been massively helpful for my own pipeline development, but not sure of the community's views on using dynamic functions in this way

have discussed a bit with @prha ",131619646
512,2020-03-23T11:42:02Z,2020-03-26T12:41:56Z,,,"This PR adds support for conditional expectations, which are used as an extension of the standard expectations. It does the following: adds a `condition` argument to all expectations, which one can use to narrow the scope of whatever expectations one is stating. For example, it is possible to expect values in some column to not be null _in general_ but only if other column or columns take on certain values.
It is only implemented for pandas dataset yet, and the `condition` argument should take a query string (expression) as stated in [pandas docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html).
The code also makes it possible to reflect conditional expectations in the Data Docs (e.g. in the following form: ""if _condition_, then values must not be null"").",103071520
513,2020-03-20T23:30:45Z,2020-03-26T15:35:54Z,,,Nothing to see here yet other than evaluationparameters addition to run_validation_operator,103071520
514,2020-03-20T21:05:53Z,2020-03-25T04:20:55Z,,,"## What's Here

- A small bugfix where `DataSource.list_batch_kwarg_generators` was returning a dictionary with empty values instead of an empty list.
- A major rename of all things generator to batch kwarg generator
- A substantial slash and burn of legacy python hacks and workarounds

**Note this is a breaking PR**
﻿",103071520
515,2020-03-17T14:44:45Z,2020-03-27T06:44:11Z,,,,103071520
516,2020-03-12T06:36:00Z,2020-03-25T06:30:06Z,,,"This PR replaces CONTRIBUTING.md with a full-fledged community contributions section.

It's still a draft---some sections are still mostly empty.

I'm creating a draft PR (1) so that the rest of the team can start to orient to a working model with more clarity and predictability for community members, and (2) to progressively get feedback on the sections that are ready for it.",103071520
517,2020-03-07T06:19:28Z,2020-03-26T22:33:58Z,,,"**What's in the PR**

This is a docs PR, not a code one.
It contains a new tutorial that shows how GE can be integrated with a Airflow/dbt data pipeline.
The example that is used in this tutorial is this: https://github.com/superconductive/ge_tutorials
This tutorial is the third item in the getting_started section of the docs. The intended user journey is to run 'init', read the typical workflow article and then dive into this article.

**How to review**

Reviewers, please ""play a user""  - read the article, watch the videos and install and run the example.

**Limitations**

My highest priority is to release this ASAP in order to start getting user feedback.

I voiced the videos myself. We can replace them later when we get ""a better talent"" :), but if they are acceptable, let's release them.

The last 2 sections are place holders (search for ""Coming soon...""). We will populate them later. Again, we want to judge if this content helps users get GE into their data pipeline.

Thank you!",103071520
518,2019-08-10T10:58:01Z,2020-02-11T01:33:14Z,,,"Following PR is a _work in progress_ PR to make GE work with Apache Hive.

The changes introduced are the following:
 - change the temporary table creation to be `ge_tmp_` followed by letters (no numbers or underscore characters any more)
 - custom `_get_dialect_regex_fn` for `Hive`
 - import Hive dialect from `pyhive`


/cc @abegong @jcampbell",103071520
519,2020-03-27T08:41:11Z,2020-03-27T10:00:02Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

In GCS Fault Tolerance proposal, we use redis for pub-sub.
Here is the implementation of RedisMessagePublisher.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
520,2020-03-27T06:01:39Z,2020-03-27T08:02:28Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

When creating a Worker, we now assert that it uses the TorchPolicy class iff use_pytorch is set in the config.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
521,2020-03-27T05:26:39Z,2020-03-27T06:51:16Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->
There is no need to ensure 'ascii' encode here. it will cause encoding error in python3 if user using a non ascii character in remote function.

In **python2**, user need to add `# -*- coding: UTF-8 -*-` to get non-ascii code run, and the dis.dis in python2 will return the ascii-ensured disassemble result, for example
```
# -*- coding: UTF-8 -*-

import dis

def a():
  return 'あ'

print(dis.dis(a))
```
will give
```
  6           0 LOAD_CONST               1 ('\xe3\x81\x82')
              3 RETURN_VALUE        
```

But **python3** is different, following code
```
import dis

def a():
  return 'あ'

print(dis.dis(a))
```
will give
```
  4           0 LOAD_CONST               1 ('あ')
              2 RETURN_VALUE
```
and if you still using 'ascii' to encode the disassemble result, encoding error will occurs.

It it seems that ray already drop out the support for python2, so I think there is no reason to use 'ascii' here.

## Related issue number
#7755 
<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
522,2020-03-27T04:04:54Z,2020-03-27T05:05:49Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

For `GetAll` and `DeleteByIndex`.

<!-- Please give a short summary of the change and the problem this solves. -->

#7675 

<!-- For example: ""Closes #1234"" -->

## Checks

- [X] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
523,2020-03-27T03:51:49Z,2020-03-27T04:56:33Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Tune Tab wasn't visible because tune stats thread wasn't started.

## Related issue number


## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
524,2020-03-27T03:50:35Z,2020-03-27T05:02:37Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
When sleep_time = 10 * initial_lease_period_ms_, test case fails, because the AsyncAddTaskLease function is expected to be called four times, but only three times. It's hard to determine the sleep_time value, so let's double it for now.
<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
525,2020-03-27T03:11:37Z,2020-03-27T07:32:39Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
Fixes dashboard init error

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number
Closes #7763 

<!-- For example: ""Closes #1234"" -->

## Checks

Local checked. It works fine after this patch.
ray.init()
```
In [2]: ray.init(webui_host=""0.0.0.0"")
2020-03-27 10:57:48,739 INFO resource_spec.py:212 -- Starting Ray with 31.79 GiB memory available for workers and up to 15.9 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2020-03-27 10:57:49,142 INFO services.py:1120 -- View the Ray dashboard at 10.239.10.98:8265
Out[2]:
{'node_ip_address': '10.239.10.98',
 'redis_address': '10.239.10.98:27344',
 'object_store_address': '/tmp/ray/session_2020-03-27_10-57-48_737122_12798/sockets/plasma_store',
 'raylet_socket_name': '/tmp/ray/session_2020-03-27_10-57-48_737122_12798/sockets/raylet',
 'webui_url': '10.239.10.98:8265',
 'session_dir': '/tmp/ray/session_2020-03-27_10-57-48_737122_12798'}

```

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
526,2020-03-26T15:58:21Z,2020-03-27T08:02:20Z,,,"## Why are these changes needed?
rollout.py did not work with custom trainables, becaus it was not using the tune repository.
Replaces `ray.rllib.agents.registry.get_agent_class` with `ray.tune.registry.get_trainable_cls`

## Related issue number
Closes #7757 

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
527,2020-03-26T12:24:58Z,2020-03-26T23:53:10Z,,,"## Related issue number

#7587

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
528,2020-03-26T11:38:38Z,2020-03-27T06:43:23Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

If `SomeID::Nil().Hash()` was called, `SomeID::FromRandom()` and `SomeID::FromBinary()` would reuse the cached hash value, which was calculated in `SomeID::Nil().Hash()`. The root cause is that a copy of [the static nil ID](https://github.com/ray-project/ray/blob/ca6eabc9cb728a829d5305a4dd19216ed925f2e4/src/ray/common/id.h#L454) is used when generating a new ID.

Another way of fixing this is to make `BaseID<T>::Nil()` returns `T` instead of `const T &`.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
529,2020-03-26T09:22:14Z,2020-03-27T08:02:54Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

Currently, we have two ways to set envs:

- Set system level. This is not always true for different purpose remote functions or Actors.

- For each function/actor, we set by `os.envrion`. This's not very convenient.

One common usage for this is set `OMP_NUM_THREADS` for trainer/rolloutworker in rllib. Because the `OMP_NUM_THREADS` can influence the TensorFlow performance a lot when training with CPU. I will add the support for rllib in the following patch.

API changes as following:
```python
@ray.remote(extra_envs={""OMP_NUM_THREADS"": ""1""})
def func():
    # ...
 
@ray.remote(extra_envs={""OMP_NUM_THREADS"": ""2""})
class Worker:
    # ....
```

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
530,2020-03-26T05:14:39Z,2020-03-26T05:55:56Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)",71932349
531,2020-03-25T23:16:52Z,2020-03-27T06:23:28Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Running a worker on head (locally, not as a Ray actor) allows for easier handling of stateful stuff like logging and for easier debugging.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
532,2020-03-25T20:52:32Z,2020-03-27T01:53:12Z,,,,71932349
533,2020-03-25T06:01:01Z,2020-03-25T21:44:01Z,,,"## Why are these changes needed?

Fixes various bugs on Windows.

## Related issue number

#631

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
534,2020-03-25T05:59:52Z,2020-03-25T07:13:01Z,,,"## Why are these changes needed?

Addresses various issues related to processes on Windows.

## Related issue number

#631

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
535,2020-03-25T00:36:32Z,2020-03-25T18:12:43Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Depends on #7690.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
536,2020-03-24T16:14:46Z,2020-03-25T17:23:29Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

These changes give the user the ability to choose which parameters and metrics they wish to show in the table on the Tune Dashboard if there are more than 3 to choose from each. 

<img width=""1247"" alt=""Screen Shot 2020-03-24 at 8 52 34 AM"" src=""https://user-images.githubusercontent.com/31830056/77449789-a5684580-6daf-11ea-870d-ad24a60372c1.png"">

The Tune Dashboard will also now show error logs associated with Tune Trials, both in the regular Tune Table with other information about the Trial and in a new Errors Tab.



<img width=""1243"" alt=""Screen Shot 2020-03-24 at 8 55 24 AM"" src=""https://user-images.githubusercontent.com/31830056/77449930-d47eb700-6daf-11ea-8803-2d3d756f531f.png"">
<img width=""1241"" alt=""Screen Shot 2020-03-24 at 8 55 31 AM"" src=""https://user-images.githubusercontent.com/31830056/77449934-d6e11100-6daf-11ea-8d5c-0ebb0755a7ab.png"">
<img width=""1244"" alt=""Screen Shot 2020-03-24 at 8 55 41 AM"" src=""https://user-images.githubusercontent.com/31830056/77449938-d8123e00-6daf-11ea-8439-93cf1d3be6dd.png"">
 


- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
537,2020-03-24T11:36:05Z,2020-03-24T23:19:57Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

Provide a pytorch version of our SAC algorithm (discrete and cont. action spaces).

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
538,2020-03-23T21:52:45Z,2020-03-26T04:38:19Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Improves Azure deployment as component dependencies are handled by Azure vs python polling code.
",71932349
539,2020-03-23T21:04:24Z,2020-03-23T22:18:42Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
Part of Tune documentation revamp.
<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)",71932349
540,2020-03-23T12:45:06Z,2020-03-23T14:01:17Z,,,"Documents for Actor Group Design and API:
* [Actor Group Design](https://docs.google.com/document/d/1G_BKoVP2vsCgCrD3C5mmVHPXwHfn60Rmyq6WMYVrpv8/edit?usp=sharing);

<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Ray has provided a mechanism for managing individual Actors, while the relationship between Actors is rarely considered in Ray design. It is hard to manipulate multiple Actors as a whole, such as creating 3 Actors in the same physical machine, etc.

This PR introduces Actor Group API, including Placement Group and Life Cycle Group, to manage the placement and the life cycle of Actors in group manner.

[WIP] There is also a common library API called Actor Group in this PR, to which some auto-generated codes are not added yet.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [X] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
541,2020-03-23T08:56:30Z,2020-03-27T06:27:11Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Currently, only bytes can be passed across different languages. This PR implements cross language serialization for primitive data types. The design doc: https://docs.google.com/document/d/1KZPJtL2uMrc63bc-PCzuSsP2EQuCCC_-GUobyWnXRF4/

This PR do:
- cross language serialization infrastructure.
  - Java and Python serialize / deserialize data by MessagePack.
  - support user defined cross language type. extension id: 100
  - support language specific type. extension id: 101
- primitive types can be transferred across different languages.
- optimized for Pickle5 in Python.
- raise exception if send non-deserializable data to another language. e.g. send Python specific data to Java is not allowed.

## example

# Java call Python

```python
# a.py
@ray.remote
def foo(x):
    return x
```

```Java
RayObject<String> res = Ray.call(
    new PyRemoteFunction<>(""a"", ""foo"", String.class),
    ""Hello World!"");
Assert.assertEquals(res.get(), ""Hello World!"");

RayObject<Integer> res = Ray.call(
    new PyRemoteFunction<>(""a"", ""foo"", Integer.class),
    123);
Assert.assertEquals(res.get(), 123);
```

# Python call Java

```Java
// org.ray.api.test.Example.java
public class Example {
    public static Object[] pack(int i, String s, Object[] o) {
        return new Object[]{i, s, o};
    }
}
```

```Python
f = ray.java_function(""org.ray.api.test.Example"", ""pack"")
input = [100, ""hello"", [1, ""2"", 3.0]]
r = f.remote(*input)
assert ray.get(r) == input
```

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
542,2020-03-23T08:00:28Z,2020-03-27T07:10:08Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

Goal: Move hard-coded parameter noise feature from DQN into new Exploration API.

The new methods ...

on_episode_start
on_episode_end
before_forward_pass
postprocess_trajectory

... have been added to enable the Exploration classes to control the ModelV2 before a forward pass (which is executed by the Policy) and to setup things before and after each episode.

NOTE: THIS PR DOES NO LONGER INCLUDE CHANGES TO DQN REGARDING PARAMETER-NOISE. This will be a separate PR.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
543,2020-03-23T02:03:13Z,2020-03-23T12:21:48Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

@sven1977 Dear Mr Mika, this is my first PR if you want to make more tests, please help me to do it.

There is no implementation on exporting checkpoint and export model in eager model in eager_tf_policy.py

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

https://github.com/ray-project/ray/issues/7674

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
544,2020-03-22T20:25:29Z,2020-03-24T18:20:28Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

We now only support direct call mode and assume that all IDs are direct call IDs - plasma objects are differentiated by having an `InPlasmaErr` in the memory store. Will remove the transport type entirely in a follow-up PR.

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
545,2020-03-21T19:24:31Z,2020-03-27T02:38:24Z,,,"Using `__dict__` to serialize objects improve the overall noop query latency

Rough benchmark

before
  25% in 0.0054 secs
  50% in 0.0060 secs
  75% in 0.0066 secs
  90% in 0.0068 secs

after
  25% in 0.0044 secs
  50% in 0.0046 secs
  75% in 0.0048 secs
  90% in 0.0059 secs

<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
546,2020-03-21T17:17:57Z,2020-03-26T10:43:20Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
1. Add 3 Github action jobs to build all in one jars:
`build_all_in_one_jars` job depends on `build_native_osx` and `build_native_linux`.(Use `needs` feature of Github action.)
2. Make Java Worker loads the corresponding native dependencies according to its platform.
3. We use the OSS to store our jars cached. Once CI succeeded to finish `build_all_in_one_jars`, it will upload the generated jars to the OSS.


#### This PR is not able to cover:
Deploying jars to maven central repository. Because we have not do any maven preparations yet.


## Related issue number
#7291
<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
547,2020-03-20T19:38:46Z,2020-03-26T04:32:31Z,,,"
## Why are these changes needed?
To solve issue #7677 

## Related issue number

#7677 

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
548,2020-03-20T11:45:52Z,2020-03-26T16:58:21Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Deprecate _remote function.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

Closes #7040
## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
549,2020-03-20T09:58:36Z,2020-03-26T05:29:09Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

[This PR is for GCS Fault Tolerance](https://docs.google.com/document/d/1sUp18ZLwGvv_WiDv_ik7R8QXU3u-9cBosIxcJieQS3I/edit?usp=sharing):

GCS Service use Persistent Storage to store key information. This PR implement the RedisStoreClient which use Redis as Persistent Storage.

Some complex functions are not implemented and will be completed in subsequent PRs.
<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [X] I've run `scripts/format.sh` to lint the changes in this PR.
- [] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [X] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
550,2020-03-20T01:27:03Z,2020-03-27T08:40:30Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
551,2020-03-20T01:22:36Z,2020-03-26T00:40:42Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
552,2020-03-19T21:29:28Z,2020-03-22T22:03:36Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)",71932349
553,2020-03-19T11:58:17Z,2020-03-20T06:45:36Z,,,"## Why are these changes needed?

Boost has generic sockets as an abstraction between TCP and UNIX domain sockets. This helps avoid `#ifdef`s and `template`s around the codebase as well as making it easier to work with different socket types in a generic manner.

## Related issue number

#631

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
554,2020-03-18T21:55:56Z,2020-03-26T02:03:13Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

This PR merges object store manager and plasma store, and now plasma store can work as a thread.

We only keeps the necessary and changed plasma store code (and apply windows patches to them), and unused files are borrowed from upstream when compiling.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
555,2020-03-16T21:51:26Z,2020-03-20T06:19:43Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
Simple build script refactor to remove dead code to improve maintainability/readability, and to fix now-incorrect hard-coded supported python versions error message (via re-use of existing SUPPORTED_PYTHONS variable).

## Related issue number
Unknown/None

## Checks

- [X] I've run `scripts/format.sh` to lint the changes in this PR.
- [N/A] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [X] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)

Manually validated that a properly formatted error message is emitted when using an unsupported python version that enumerates all supported python versions, and ensured that builds still complete successfully using expected Bazel-managed job concurrency.",71932349
556,2020-03-16T16:16:32Z,2020-03-25T10:26:32Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
This is the first PR to support multiple core workers in one process.

What are included in the PR:
* Introducing the `CoreWorkerProcess`. The lifecycle management of core workers will be taken over by it.
* Python adaptation. We will keep using 1 worker for a Python worker process.
* Java adaptation. We will delete the `RayMultiWorkerNativeRuntime` (https://github.com/ray-project/ray/pull/5505) and re-implement the multi-worker feature based on the new implementation in core worker.

What are not included in the PR:
* Connection sharing between workers like the GCS client.
* IO thread sharing.

## Related issue number

<!-- For example: ""Closes #1234"" -->

TODO

- [x] Core worker refactoring
- [x] Python adaptation
- [x] Java adaptation
- [x] Streaming adaptation

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
557,2020-03-16T08:42:55Z,2020-03-16T15:36:48Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
558,2020-03-15T16:36:07Z,2020-03-26T15:06:57Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Currently when PPO is used with a bounded (continuous) action space, action samples are simply drawn from an unbounded normal distribution, and then clipped to the bounds.   The entropy is calculated directly on the normal.  Because PPO gives a reward for higher entropy, then there exists a failure mode where the algorithm can learn to push most of the mass outside of the action range and increase the variance, thus increasing entropy despite there being little change in selected actions.

The direct way to fix this, ie. calculating the entropy of the clipped distribution doesn't work since the clipped distribution actually has undefined entropy.  Another way to fix it is to use a ""soft clip"" such as the existing SquashedGaussian distribution which maps samples through a (scaled) tanh function in order to ensure samples lie within the desired range.  The problem here is that the entropy here is hard (impossible?) to compute analytically which is required by PPO when using a non-zero `entropy_coeff`.

In this PR I have implemented the `GaussianSquashedGaussian` which instead of mapping through tanh maps through the normal CDF.  When scaled appropriately it closely approximates tanh:

![image](https://user-images.githubusercontent.com/1709642/76705674-a1fefb00-66d9-11ea-8f93-10127964c485.png)

However, it has the benefit that the entropy is analytically tractable.  In fact, the entropy is just -KL(N1 || N2), where N1 is the normal being squashed, and N2 is the normal corresponding with the CDF used for squashing.

This should be considered a draft review for now, since I'd like to get a second opinion on how I've structured the catalog -> action space mapping, and I have also touched the existing SquashedGaussian and I'm unsure if these changes will break anything.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
559,2020-03-14T21:53:01Z,2020-03-23T12:36:46Z,,,"## Why are these changes needed?

We currently split command-lines on spaces.

## Related issue number

#631

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
560,2020-03-13T12:05:55Z,2020-03-25T04:33:50Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
When ray throws a RayWorkerException or RayActorException, it is hard to find which actor causing the problem. So I wrap an objectId to all exceptions in ObjectSerializer, it will help a lot for debugging.
<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
561,2020-03-13T11:56:30Z,2020-03-22T19:13:51Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

Implement a DQN torch version.

<!-- Please give a short summary of the change and the problem this solves. -->

Addresses and solves issue #4371 

<!-- For example: ""Closes #1234"" -->

Closes #4371 

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
562,2020-03-12T14:26:23Z,2020-03-23T21:11:30Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

Move hard-coded parameter noise feature from DQN into new Exploration API.

The new methods ...

on_episode_start
on_episode_end
before_forward_pass
after_forward_pass
postprocess_trajectory

... have been added to enable the Exploration classes to control the ModelV2 before/after a forward pass (executed by the Policy) and to setup things before and after each episode.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
563,2020-03-12T07:25:13Z,2020-03-12T13:57:23Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

This PR is for [GCS Fault Tolerance](https://docs.google.com/document/d/1sUp18ZLwGvv_WiDv_ik7R8QXU3u-9cBosIxcJieQS3I/edit?usp=sharing).

For GcsClient(raylet or worker): GCSServiceDiscoveryClient is used to watcher GCS Service, if GCS Service changes, GcsClient will receive notification.

For GcsService: GCSServiceDiscoveryClient is used to register GcsService as a new service.

StoreBasedGcsServiceDiscoveryClient is an implementition of GCSServiceDiscoveryClient which use storage as backend.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
564,2020-03-12T03:43:05Z,2020-03-16T04:24:34Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
This part is focus on the ```JobWorker``` of streaming runtime.

For basic concepts, please refer to [doc](https://docs.google.com/document/d/1rchL3M0Dp_jzOzT4qJplb1cIV1TyU6GTbN4bh3szkJM/edit#heading=h.mkl92ec2a6wo).
For JobWorker part, please refer to [JobWorker](https://docs.google.com/document/d/1rchL3M0Dp_jzOzT4qJplb1cIV1TyU6GTbN4bh3szkJM/edit#heading=h.vtowosq14m8o) part.

**Notice:**

To keep the main functional.

**The main modifications are:**

- ```JobWorker``` and ```Task``` related part refactor.
- Transfer config replace map configuration with owner.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number
#6184
<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
565,2020-03-12T01:52:54Z,2020-03-21T10:14:43Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

After merging https://github.com/ray-project/ray/pull/7544, it seems that now the gRPC handlers are sometimes returning `Status::Unavailable` (see https://grpc.github.io/grpc/core/md_doc_statuscodes.html) when under load, presumably because requests are taking longer than expected (but haven't found any documentation indicating that this should happen). Our gRPC client currently doesn't handle this status by retrying, resulting in the workers or raylet dying due to a check failure when there is any backpressure (can trigger this by running `ray microbenchmark`.

This PR adds a retry mechanism to the RPC client to address this. Unfortunately, it also seems to introduce a new issue with a segfault in the raylet that needs to be addressed before merging (found while running `ray microbenchmark`, happens ~1/3 of the time during `multi client put calls per second`):

```
  1 *** Aborted at 1583977925 (unix time) try ""date -d @1583977925"" if you are using GNU date ***
  2 PC: @                0x0 (unknown)
  3 *** SIGSEGV (@0x0) received by PID 32511 (TID 0x114c34dc0) stack trace: ***
  4     @     0x7fff713eab1d _sigtramp
  5     @ 0xf685c0007000100c (unknown)
  6     @        0x1094a7f12 ray::rpc::ServerCallImpl<>::HandleRequestImpl()
  7     @        0x1094a7e5f _ZN5boost4asio6detail18completion_handlerIZN3ray3rpc14ServerCallImplINS4_25NodeMan    agerServiceHandlerENS4_19PinObjectIDsRequestENS4_17PinObjectIDsReplyEE13HandleRequestEvEUlvE_E11do_complete    EPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
  8     @        0x109b36759 boost::asio::detail::scheduler::do_run_one()
  9     @        0x109b2a0e2 boost::asio::detail::scheduler::run()
 10     @        0x109b29f7c boost::asio::io_context::run()
 11     @        0x109432e92 main
 12     @     0x7fff711e92e5 start
```

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
566,2020-03-11T22:24:12Z,2020-03-25T04:23:49Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
Ray will segfault if pyarrow is imported before it because exported symbol are colliding . This fixes that bug and adds a test to ensure that future changes do not reintroduce it. 
<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->
Fixes Issue #7393 

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
567,2020-03-11T13:08:36Z,2020-03-12T00:58:57Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->
This is for GCS Fault Tolerance. Ref[ here for more detail of GCS Fault Tolerance Proposal](https://docs.google.com/document/d/1sUp18ZLwGvv_WiDv_ik7R8QXU3u-9cBosIxcJieQS3I/edit?usp=sharing).

ObjectLocator is used in GCS Server. Raylets or workers will get object location from GCS Server. GCS Server will lookup object location from ObjectLocator. ObjectLocator is thread-safe.

Still working on UT.
<!-- For example: ""Closes #1234"" -->

## Checks

- [X] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
568,2020-03-11T06:34:46Z,2020-03-23T09:01:43Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

According to the naming convention found in the internet, e.g. wikipedia, ververica(https://www.ververica.com/what-is-stream-processing), It makes sense to rename the StreamingContext to StreamContext. Relevant discussion please refer to: PR: https://github.com/ray-project/ray/pull/7512

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [ ] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)
",71932349
569,2020-03-11T02:00:34Z,2020-03-25T04:03:03Z,,,"## Why are these changes needed?
This PR introduces two new changes.

1. A new API for tuning TorchTrainer. We leverage RayTune for this.
2. A new API for save/restore. Instead of capturing serialization within the class, we expose state.

What's left to do:

1. `state_dict` and `load_state_dict`, though @maximsmol should impl this when he does the head=trainer pr.
2. I need to test if PBT works here on a cluster. Chances are it may not.

Thoughts:

There's quite a bit of layering. You need to create like, 6 different creator functions to get TorchTrainer to work. 

## Related issue number


## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
- Testing Strategy
   - [x] Unit tests
   - [ ] Release tests
   - [ ] This PR is not tested (please justify below)",71932349
570,2020-03-09T05:58:26Z,2020-03-16T07:25:54Z,,,"As @JingGe commented [here](https://github.com/ray-project/ray/pull/7348#discussion_r387051523) it is a better name `StreamRuntimeContext` for current `RuntimeContext` in ray-streaming subproject. Because we are building fusion computing in Ray with multiple computer engines. Each of them might have its own RuntimeContext.

Thus here created a separated pull request *just* for renaming to avoid pollute refactor changes into feature PR. 
",71932349
571,2020-03-05T13:44:03Z,2020-03-17T09:37:09Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
This PR introduced streaming cross-lang api based on the [design doc](https://docs.google.com/document/d/1QpZe7q3b2csoHUOjnP_CA58IqPFOAnF-RgTruiyV_iY/edit?usp=sharing). 
Streaming cross-Lang api can be used to build a job consisting of both java and python workers, which will be required by some applications like online learning or can be used to share java/python connectors in python/java workers.

# Example
Java Example
```java
ds.filter(Example::filter)
 .map(Example::mapper1)
 .asPython() 
 .map(""module1"", ""func1"")
 .keyBy(""module1"", ""func2"")
 .reduceByKey(""module1"", ""func3"")
 .asJava()
 .map(Example::map)
 .asPython()
 .sink(new Sink());
```
Python Example
```python
ds.filter(func1)
   .map(func2)
   .as_java()
   .map(""com.example.Mapper"")
   .key_by(""com.example.KeyExtractor"")
   .reduce_by_key(""com.example.ReduceFunction"")
   .as_python()
   .map(func3)
   .as_java()
   .sink(""com.example.Sink"")
```

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number
#6184
#6755
#6689
#7532
<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
572,2020-02-28T20:13:45Z,2020-03-06T00:54:08Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Often it's useful to know when actors have finished initializing and can execute tasks. We don't currently have an API for this, so users often resort to defining an ""empty"" method and calling that and waiting for the result.

This PR adds the ability to call `ray.wait()` on `ActorHandle` objects in the same way as `ObjectID`s.

A few design considerations:

- This currently overloads the same API call as the existing `ray.wait()` - should we instead add a new call?
- If we want to use the same API call, should we allow calling `ray.wait()` on a list containing both `ObjectID`s and `ActorHandle`s?
- This currently waits for an actor by defining an internal `__ray_ready_check__` method and calling it the first time each worker calls `ray.wait()` an actor. This is somewhat problematic as it might not return quickly if the actor is busy processing other tasks. We could instead modify the actor creation task to return an ID that is either exposed to the user or not exposed to the user and only used for `ray.wait()`.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
573,2020-02-27T12:56:11Z,2020-03-24T07:38:03Z,,,"# Why are these changes needed?
add state module to support more complicated streaming requirements.
The design doc https://docs.google.com/document/d/1wGF3GgrxN4yc6i9wHFG9d4pFB75XBHLNMEu4rzW4sLg/edit?usp=sharing

# Related issue number
#6184",71932349
574,2020-02-27T00:46:01Z,2020-03-27T03:34:23Z,,,Just changing the runner so we don't get the test failed messages ,71932349
575,2020-02-26T19:46:44Z,2020-03-24T18:40:09Z,,,"Previously any actor failure would cause all outstanding actor tasks to
get marked as failed, leading to an exception in the user program.
This patch changes the behavior such that as long as the actor has
remaining reconstructions, all tasks which haven't been completed previously
will get resubmitted to the new instance automatically.

The new semantics allow to handle failures in a more transparent way, but
also requiresthe user to make sure that the actor will always restart in a
state in which the repeated calls are still valid. If this is not the desired
behavior the workaround is to disallow automatic reconstruction and implement
manual handling of actor failures in the user program.

## Related issue numbers

#6670, #6773

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
576,2020-02-22T02:25:19Z,2020-02-23T10:00:10Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
577,2020-02-21T11:57:14Z,2020-02-24T19:55:44Z,,,"## Why are these changes needed?

Inline definitions introduce dependencies on headers and slow down compilation.

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
578,2020-02-20T22:28:02Z,2020-03-22T05:36:35Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Fixing some small code quality errors with #6955 
<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
579,2020-02-18T02:35:40Z,2020-02-18T03:42:33Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->
Currently the `get_command_info` function returns only the `command_name`, ignoring the arguments if the shell flag is set. This is an issue because the `command_name` variable is only the first string of the actual command intended to be run. This PR returns the full concatenated command to run as the output instead of just the first string.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
580,2020-02-17T22:31:14Z,2020-02-17T23:48:09Z,,,,71932349
581,2020-02-17T19:36:05Z,2020-03-04T04:45:17Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

The autoscaler runs all of its commands in interactive mode with `bash --login -c -i 'true <command>`. This expects a tty to be allocated, so it doesn't make sense to have it as a separate option (this causes ioctl errors to be printed). Instead, we should simply have an interactive option which runs commands interactively and allocates a tty at the same time.

## Related issue number

N/A

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
582,2020-02-14T08:29:14Z,2020-02-14T09:40:50Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [X] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
583,2020-02-13T18:16:19Z,2020-02-14T00:02:41Z,,,"This PR allows user to add arbitrary path prefix for their dashboard. For example, http://localhost:8265/ray_dashboard. This mode is required when dashboard is served behind a reverse proxy for path based proxying.

The implementation uses two template variable:
- `%PUBLIC URL%` is provided by react to inject path prefix for built assets and their relative linking. However this works only in build time. At dashboard startup time, we will replace with custom routes
- `<base>` tag is needed for other relative url based assets and HTTP API path. This is also replaced on dashboard server startup time.

Made sure the following works:
- [x] dev server, no prefix
- [x] production build, no prefix
- [x] production build, has prefix",71932349
584,2020-02-12T01:23:31Z,2020-02-12T17:52:04Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

This PR brings back some of the capabilities from https://github.com/ray-project/ray/pull/5888. Back then we removed the token since it was annoying for users. We now bring it back in a way that should be safe and convenient for most users. There are three modes:

- By default (`webui_host=None`), the dashboard listens to `127.0.0.1` which is the safe default, the same way as it currently is. If you define `RAY_DASHBOARD_TOKEN` on the head node to a token (a UUID
that was generated in a cryptographically safe way), the dashboard listens to `0.0.0.0` (i.e. it is public to the outside), but will need the token to access.

- If you want the webui to listen on localhost irrespective of whether `RAY_DASHBOARD_TOKEN` is set, use `webui_host=""127.0.0.1""`, in which case the dashboard will always listen on localhost.

- If you specify `webui_host=""0.0.0.0""` without setting `RAY_DASHBOARD_TOKEN`, the WebUI is public without a token. In this case you are responsible to secure it in some other way. **You should not run the Dashboard in an open way without proper authentication, especially because going forward we will expose more functionality in the Dashboard that can modify your cluster.**

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
585,2020-02-09T17:51:39Z,2020-02-20T19:09:53Z,,,"## Why are these changes needed?

Otherwise, users could read the old blog page and not realized it's stale.

## Related issue number

N/A

## Checks

This change only affects a static HTML page.

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
586,2020-02-09T06:28:05Z,2020-02-10T18:56:20Z,,,"This documents the current actor checkpointing behavior.

Some details are in https://docs.google.com/document/d/1P0sOFzikMkT4rW35qI97ZrZEjNptZf7rQIBSOTfAG0Q/edit#heading=h.8nhx5qmjmdxn.",71932349
587,2020-02-06T15:44:01Z,2020-02-11T04:16:32Z,,,"## Why are these changes needed?
Currently, the `restore` argument of `tune.run` only takes a single trial. When the training task is sensitive to initialization, it can be useful to initialize a population's weights from a population of working models. This is to avoid scenarios where a significant portion of trials are ""dead"" right from the beginning due to poor initialization. This change permits the use of `tune.run(restore=tune.choice(my_list_of_checkpoints))`, as per @richardliaw's suggestion (thanks!)

The work entailed removing the path resolution from `restore`, which required it be a string. I have only tested with absolute paths, and this removal may prevent use of relative paths. This concern needs addressing.

### Additional notes
New trials created with particularly ugly names:
```
~/ray_results/PPO$ ls
experiment_state-2020-02-05_22-28-43.json
experiment_state-2020-02-05_23-13-23.json
experiment_state-2020-02-06_14-42-39.json
experiment_state-2020-02-06_15-33-35.json
PPO_MyEnv_09b61f96_0_restore=_restore_PPO_PPO_MyEnv_4b3d3a46_2020-02-04_22-47-17x1p6dcm6_checkpoint_200_checkpoint-200_2020-02-06_15-33-36umu05v_h
PPO_MyEnv_09b8c4bc_1_restore=_restore_PPO_PPO_MyEnv_4b46f572_2020-02-05_05-49-14kfpsvc1v_checkpoint_200_checkpoint-200_2020-02-06_15-33-36rxrol44w
PPO_MyEnv_09b97f38_2_restore=_restore_PPO_PPO_MyEnv_4b489422_2020-02-05_12-52-415hjqghl6_checkpoint_200_checkpoint-200_2020-02-06_15-33-36lw64jt7x
PPO_MyEnv_09ba13c6_3_restore=_restore_PPO_PPO_MyEnv_4b4913de_2020-02-05_12-55-12kzzf6dcn_checkpoint_200_checkpoint-200_2020-02-06_15-33-37b7trculd
PPO_MyEnv_09bcbc84_4_restore=_restore_PPO_PPO_MyEnv_4b403caa_2020-02-04_22-47-179k2l_6a3_checkpoint_200_checkpoint-200_2020-02-06_15-33-37qfzb169_
```

Presumably, the name will grow every time this process is repeated. I personally do not like that the restore path is inserted into the trial name, and would much prefer this information be available in a json file. Please inform.

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
*I'm having difficulty with this one on MacOS.*

- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
*Inline comments are changed for `tune.run`, will make changes to docs upon request.*

- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
*Currently running - meant to create this PR on my master but created it on the official.*
",71932349
588,2020-02-04T01:55:20Z,2020-03-26T19:29:47Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

- Support aggregated results (#6994). 
- Separate execution concerns from search algorithm.
- Make it easier to implement search algorithm restoring.

TODO:
  - Improve and update tests.
  - Improve documentation.


## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.",71932349
589,2020-02-03T12:51:46Z,2020-02-07T16:52:52Z,,,,71932349
590,2020-02-01T14:51:11Z,2020-03-05T02:12:57Z,,,"# Why are these changes needed?

We should be ""forgiving"" and not throw an exception if the user specifies a `num_returns` that's > `len(ids)`. It's analogous to allowing `str.substring(5, 10000000)` to work a string much shorter than the 2nd argument. 

I also greatly expanded the tests for ray.wait, extracting the previous test with the new tests into a new test file.

## Related issue number

Fixes #6667 

## Checks

- [] I've run `scripts/format.sh` to lint the changes in this PR. 
   I would have, but the `flake8` command fails because of an unrecognized argument `--inline-quotes`.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
591,2020-01-30T10:54:46Z,2020-02-04T19:31:20Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Those additions are not mandatory, although useful to remove boilerplate and arrange the metric callbacks code in a more modular manner - decoupling each metric's code into a separate class such that metrics can be added and removed with ease.

## Related issue number

No issue here :) just a nice feature

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
592,2020-01-28T13:34:45Z,2020-01-28T14:17:53Z,,,"…e looking for wrong types in config dict

<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Using Tune with recursive config, one gets the following exception:

> 2020-01-28 15:30:18,878	ERROR trial_runner.py:487 -- Error processing event.
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py"", line 436, in _process_trial
>     result = self.trial_executor.fetch_result(trial)
>   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py"", line 323, in fetch_result
>     result = ray.get(trial_future[0])
>   File ""/usr/local/lib/python3.6/dist-packages/ray/worker.py"", line 2195, in get
>     raise value
> ray.exceptions.RayTaskError: ray_worker (pid=1138, host=7e2ac1920dad)
>   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer_template.py"", line 87, in __init__
>     Trainer.__init__(self, config, env, logger_creator)
>   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py"", line 323, in __init__
>     Trainable.__init__(self, config, logger_creator)
>   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py"", line 87, in __init__
>     self._setup(copy.deepcopy(self.config))
>   File ""/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py"", line 424, in _setup
>     self._allow_unknown_subkeys)
>   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/util.py"", line 94, in deep_update
>     deep_update(original[k], value, True, [])
>   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/util.py"", line 96, in deep_update
>     deep_update(original[k], value, new_keys_allowed, [])
>   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/util.py"", line 96, in deep_update
>     deep_update(original[k], value, new_keys_allowed, [])
>   File ""/usr/local/lib/python3.6/dist-packages/ray/tune/util.py"", line 88, in deep_update
>     for k, value in new_dict.items():
> AttributeError: 'str' object has no attribute 'items'

## Related issue number

Closes #6847

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
593,2020-01-25T02:20:50Z,2020-02-22T04:04:41Z,,,"## Why are these changes needed?

Boost inclusions slow down performance considerably.  

We use forward declarations and pointer members to gradually avoid including expensive Boost headers in files where they're not needed.

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
594,2020-01-25T01:40:30Z,2020-01-25T05:04:22Z,,,"![image](https://user-images.githubusercontent.com/21118851/73114558-9446a980-3ed0-11ea-8dee-31c7cfdf2755.png)
",71932349
595,2020-01-21T15:30:53Z,2020-03-08T22:24:43Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
596,2020-01-21T02:01:00Z,2020-01-21T08:40:54Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Log4j 1.2 has vulnerability issues
<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
597,2020-01-20T05:09:55Z,2020-01-20T09:16:33Z,,,,71932349
598,2020-01-20T03:42:53Z,2020-02-05T12:03:25Z,,,"Signed-off-by: Ce Gao <gaoce@caicloud.io>

<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
599,2020-01-19T06:33:49Z,2020-01-20T15:38:37Z,,,"## Why are these changes needed?

Right now TaskSpec is copied when the task is submitted, because we need to retry the task if it fails for direct task calls. Right now the copy is done for both direct actor and direct task call. There's also a copy on the receiver side when translating the task request to task spec.

This PR removes the copy for task submission by allowing `TaskSpecification` to hold a shared_ptr of `PushTaskRequest`. The copy is only required when the failed task is retried, it's actually on-demand and doesn't impact the main code path.  It also removes copy for task execution using the same way.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
600,2020-01-16T18:39:31Z,2020-03-19T19:26:12Z,,,"## Why are these changes needed?

Sometimes (non-deterministically) there is a bogus `SwapFree` value in `/proc/meminfo` with a space missing after the colon, preventing `split()` from working correctly.

**Users:** If you encounter this issue, please react/comment here so we know how widespread this issue is.

## Related issues

#631

Once giampaolo/psutil#1666 is resolved and merged into PyPI, this patch can be removed.

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
601,2020-01-11T22:03:52Z,2020-01-11T22:55:32Z,,,,71932349
602,2020-01-10T06:44:38Z,2020-03-27T09:51:38Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
Pls see the <[Design Document](https://docs.google.com/document/d/1EAWide-jy05akJp6OMtDn58XOK7bUyruWMia4E-fV28/edit#heading=h.qyadc2fbpmni)> first.

<!-- Please give a short summary of the change and the problem this solves. -->
This PR implements the creation and reconstruction of actors based on gcs server.

**Changes on gcs server side**

Several important classes are added: `GcsNode`, `GcsActor`, `GcsActorManager`, `GcsLeasedWorker`,`GcsActorSchedulingStrategy`.

- **GcsNode**: An abstraction of raylet at GcsServer side, which provides an interface to lease wroker from remote raylet. 
- **GcsLeasedWorker**: An abstraction of leased worker from raylet at GcsServer side, which provides an interface to CreateActor on the remote worker.
- **GcsActor**: An abstraction of actor at GcsServer side, which contains the actor table data as well as the leased worker weak reference.
- **GcsActorManager**: It is responsible for handling the CreateActor request from Driver/Worker and scheduling the actors to the registered nodes.
- **GcsActorSchedulingStrategy**: It is an abstract class, it provides an interface to select a node to lease worker for a specified actor. Simply, we just provides a implementation to random select a node as the raylet has strategy to lease resources. In the long run, we expect GCS Server to be able to make decisions.

In addition, this PR has also made some changes to `GcsNodeManager`, it is responsible for monitoring and manage nodes.

**Changes on raylet side**
- In the old actor management scheme, raylet will be responsible for updating ActorTableData, while in the new GCS-Based actor management scheme, we expect that GCS will be responsible for updating all ActorTableData. So, you will see that all logic about updating ActorTableData will be get ride off.
- Besides, the raylet should cache the relationship of actor and leased worker because that the raylet should fast reply gcs server without lease anything when gcs server rebuild actors after restart. Pls see the <[Design Document](https://docs.google.com/document/d/1EAWide-jy05akJp6OMtDn58XOK7bUyruWMia4E-fV28/edit#heading=h.qyadc2fbpmni)>.

**Chages on worker side**
- invoke the gcs_rpc_client.CreateActor on the callback of ResolveDependencies.
- Fast reply the gcs server without create anyting if it is already bound with an actor when gcs server rebuild actors rebuild actors after restart.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
603,2020-01-10T03:45:05Z,2020-01-16T03:14:45Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
It's necessary to have a human readable name for job.  

The code in this comment https://github.com/ray-project/ray/issues/6715#issuecomment-571470306  is a good usage of job name.
Not only this, but it also helps in xlang to set the jars or py-packages for a job according the job name.

## Changes in this PR
1. Add a `job_name` as a parameter to `ray.init`.
Add an index `JOB_NAME_2_JOB_ID` in redis.
2. If it's not set, a default name will be generated.
3. We clean up the job name when a driver exits.

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
604,2020-01-06T10:05:40Z,2020-02-11T12:30:16Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->
We plan to implement a Resource Provision Service (RPS) for Ray, you can read the design doc [here](https://docs.google.com/document/d/1Isx6I5dF2jRdNh9RVIRVmYb4Rd0QxOlAd_glq_l0eJo).

We have implemented a kubernetes operator for Ray (and has been merged #6328 ). As we know ""Desired State Oriented"" is an important core concept in Kubernetes, it means we need to interact with Kubernetes using declarative API, so we need a translator to translate user commands(from ray autoscaler or any other user client) to a desired ray cluster state.

On the other side, we can abstract the interaction with Kubernetes and Yarn to make the deployment and management more concise and unified.

In this plan, We will 
1. provide the abstraction / interface between Ray and different platforms like Kubernetes and Yarn. (This is what this Pull Request for).
2. provide a k8s-implementation.
3. adapt the ray-operator for the RPS.

## Related issue number

<!-- For example: ""Closes #1234"" -->
Merged #6328 
Merged #6501 

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
605,2019-12-19T10:26:09Z,2020-03-24T21:05:01Z,,,"## Why are these changes needed?

Creates Bazel rules for Redis so we can integrate it into the build system and don't have to call an external Makefile.

- **This removes `jemalloc`.** While complex to incorporate, `jemalloc` can be added back in the future if needed, but [others have found](https://github.com/tensorflow/tensorflow/commit/8b21130fb6a9575db953ae46a9cc353bb9de8d57#diff-781a53e648f3df8d16a08ec083b04bf4R34) its performance to be comparable to that of the system allocator, so this may be unnecessary.

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
606,2019-12-13T03:37:21Z,2020-03-13T22:52:11Z,,,"## Why are these changes needed?
Adds attention layers that can be used in custom models.

To do:
- [x] ~Supervised learning example~
- [ ] RL example

## Related issue number
Closes #6030 ",71932349
607,2019-12-12T13:24:33Z,2020-01-08T01:42:27Z,,,"## Why are these changes needed?

Right now we cache task specs in task manager in order to allow resending the task when it fails. Since we only support retry for non-actor tasks, there's no need to cache for actor tasks. This pr removes the unnecessary caching.

## Related issue number

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
608,2019-12-11T12:41:39Z,2020-02-05T14:45:27Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

If the value of `--num-cpus` parameter specified in 'ray start' command is greater than `maximum_startup_concurrent`, Some workers are not started initially. We should ignore `maximum_startup_concurrent` while starting initial workers.

The fix:

I introduced two new internal configs - `num_initial_py_workers` and `num_initial_java_workers`. Raylet will start the specified count of initial workers of that language, regardless of `num_cpus` and `maximum_startup_concurrent`. The default value is `0`, so by default no workers will be started during startup.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
609,2019-12-11T06:44:50Z,2019-12-11T15:20:48Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?
There is no difference in the priority of events inner boost::asio::io_context, but sometimes we expect some events(e.g. timer event of heatbeat) could be executed with high priority. 

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [x] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [x] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
610,2019-12-11T04:45:45Z,2019-12-11T12:23:41Z,,,"Build are getting slow again, this PR is used to see where does travis spend most of its time. ",71932349
611,2019-12-10T08:25:02Z,2019-12-15T02:52:35Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
612,2019-12-06T09:01:49Z,2019-12-06T16:39:29Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
613,2019-12-04T05:32:14Z,2019-12-13T01:31:50Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
614,2019-12-02T18:00:37Z,2019-12-02T22:31:08Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
615,2019-11-21T14:50:28Z,2019-11-22T13:38:16Z,,,"This patch introduces a new directory, `clib`. `clib` includes a standalone C header (`clib/ray.h`) and builds an associated C-compatible `libray`. Compiled applications that wish to interact with Ray can use the C header and link against the generated `libray`. They do not need to be included in the Ray build pipeline. `libray` is generated using a thin C++ shim layer (`clib/lib.cpp`) which presents C function signatures and opaque types and internally makes the appropriate C++ calls to Ray.

The bindings are not complete, but cover the main basic building blocks needed to interact with Ray, including constructing a `CoreWorker`, and calling the main methods like `Wait`, `Get`, `Put`, and `SubmitTask`. The API is also overly simplified in places, such as object metadata always being empty, and various task options not being possible to specify. This should probably be fixed before an eventual release.

The patch also adds `clib/main.c`, a binary that uses only `ray.h` and `libray` to implement a proof-of-concept of a compiled Ray client.

See the commit messages on the individual commits for much more detail about the patches.",71932349
616,2019-11-18T01:17:55Z,2020-01-30T18:01:24Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

The current config format consists of `initialization_commands`, `setup_commands` and `*_start_ray_commands`. The format is inconsistent, with some but not all having variants for head and worker nodes, and the distinction between `initialization_commands` and `setup_commands` is blurry.

This PR refactors the config to consist of three entries:
  - `setup_commands`, run when the node is created.
  - `boot_commands`, whenever the node is powered on.
  - `start_ray_commands`, whenever Ray is (re)started.

Each of these entries is a dict that can contain keys `common`, `head` and `worker`. `common` is always executed first, followed by the node-specific commands, if specified.

A particular pain point, described in #6128, is that node re-use is incompatible with Docker. This PR fixes this by giving the user control over which commands get executed in Docker at different stages, by use of an `IN_DOCKER` macro.

## Related issue number

Fixes #6128. The changes broadly follow the format agreed in the discussion; the main difference is I opted for a hierarchical config format rather than creating `{head,worker}_setup_commands`, `{head,worker}_restart_commands`, etc.

## Checks

- [X] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
617,2019-11-13T15:13:53Z,2019-11-28T13:42:14Z,,,"## Why are these changes needed?

See https://github.com/ray-project/ray/pull/6066#issuecomment-552646897

## Related issue number

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
618,2019-11-11T22:16:08Z,2019-11-12T01:26:01Z,,,some remaining cleanups didn't get merged into #5978,71932349
619,2019-11-09T09:56:37Z,2020-03-27T10:00:06Z,,,"## Why are these changes needed?
- This is a c++ worker implementation from Ant Financial.
- We just support single process mode in this pull request. Single box mode and cluster mode will be implemented in future work.
- Design docs is written in [google docs](https://docs.google.com/document/d/1NSp7cEVxZY5rO0hiYaPjRJrwLBkbbm_outrrZZn1fJQ/edit#heading=h.vdm0makpsddw).
- How to build and run c++ worker test?
```
bazel test //cpp:all
````
- Contributors for this work:
    Yiming Yu, Zhenyu Guo, Guyang Song

## Related issue number
- No one

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
620,2019-11-07T03:41:55Z,2020-01-13T03:53:20Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

This PR makes it so the core worker test is run with the address sanitizer.

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
621,2019-11-04T19:09:18Z,2019-11-04T21:38:32Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
622,2019-11-04T13:03:14Z,2020-03-02T04:14:32Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

Bazel is aiming to get rid of the MSYS2 dependency on Windows. Since Python will stay a requirement, we should use Python. It will also be more robust against platform differences than shell scripting.

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
623,2019-10-28T00:06:15Z,2019-10-31T03:13:38Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

rules_jvm_external v1.2 is now broken due to unforeseen upstream changes (some libraries were moved/removed)

## Related issue number

Fixes #6011 

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
624,2019-10-24T18:49:18Z,2019-10-25T04:11:56Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

This integrates the changes that @danyangz did in https://github.com/apache/arrow/pull/5626

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failure rates at https://ray-travis-tracker.herokuapp.com/.
",71932349
625,2019-09-27T23:24:58Z,2020-01-14T09:19:50Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

## Why are these changes needed?

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [ ] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
",71932349
626,2019-09-25T07:23:01Z,2020-03-26T04:34:44Z,,,"<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->

Recently added save/restore methods to searcher algs of Tune; this commit hooks these into `trial_runner.py` checkpoint and resume methods.

<!-- Please give a short summary of the change and the problem this solves. -->

## Related issue number

<!-- For example: ""Closes #1234"" -->

## Checks

- [x] I've run `scripts/format.sh` to lint the changes in this PR.
- [ ] I've included any doc changes needed for https://ray.readthedocs.io/en/latest/.
",71932349
627,2020-03-27T08:23:01Z,2020-03-27T10:42:20Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
This PR contributes an opensource tool from [ArangoDB](https://www.arangodb.com/machine-learning/) to manage data from machine learning experiments. Examples of how this tool is used are provided with colab notebooks in the .arangopipe folder. Please let us know off any feedback or questions you may have.

Thank you,
Rajiv 
",843222
628,2020-03-26T19:30:10Z,2020-03-27T10:39:54Z,,,"This is a prototype of what [SLEP014](https://github.com/scikit-learn/enhancement_proposals/pull/37) may look like with pandas/xarray + sparse support. The configuration flag, `array_out`, only controls what comes out of `transform`. 

[This usage notebook](https://nbviewer.jupyter.org/github/thomasjpfan/sklearn_slep_014/blob/0.0.2/usage.ipynb) contains how this API can be used with various transformers.

### Notes

1. This implementation assumes that the feature names in fit and transform are the same.
2. The `_DataAdapter` is able to get the names of any of the support inputs and wrap a dense or `scipy.sparse` matrix with feature names in either format.

I am going to put together some benchmarks to to compare the following:

1. Dense ndarray vs pandas dataframe vs xarray dataarray
2. scipy.sparse vs pandas dataframe with sparse arrays vs xarray + pydata/sparse

**WIP**: Feel free to look at the usage notebook. The internal implementation and its API is flux.",843222
629,2020-03-25T23:59:52Z,2020-03-26T19:43:27Z,,,"- add references throughout the text
- put examples in their respective sections
- update docstring examples to use a StandardScaler
- add detail about shrinking param. I don't know how to detail `tol` and `max_iter` though (seems really libsvm-specific).
- add short descriptions to figures
- add details in math section and reference the hinge and eps-insensitive losses
- a few fixes / clarifications here and there",843222
630,2020-03-25T17:19:36Z,2020-03-25T19:55:36Z,,,"
#### Reference Issues/PRs
Issue Number: 13990 Issue: Add 3-fold split method train/val/test

Issue Link: https://github.com/scikit-learn/scikit-learn/issues/13990 We had created issue number 16342 which was closed and marked as duplicate Link For Closed Issue:https://github.com/scikit-learn/scikit-learn/issues/16342

#### What does this implement/fix? Explain your changes.
We have introduced an additional functionality in the existing function of train_test_split. Now the Target(Y) and Response(X) can be split three ways i.e. into train set / validation set / test set.

This is additional optional functionality and does not hamper the existing functionality.

only if function is called with the optional parameter ""validation_size"", then the function returns 3 subsets. Otherwise, the existing functionality to return two subsets for each input array is retained.
The test_split.py is also changed in order to test the new functionality

Changes impact following four files: 4a) sklearn / base.py 4b) sklearn / model_selection / _split.py 4c) sklearn / model_selection / tests / test_split.py 4d) skelarn / utils / fixes.py

#### Any other comments?
Fixes were made in the file fixes.py to resolve import issues due to different scipy versions.

Source Code link: https://github.com/rbewoor/scikit-learn

Different use cases were tested using the script Test_Cases_SciKit-Learn.ipynb in the folder Dedicated_Test_Cases Link https://github.com/rbewoor/scikit-learn/blob/master/Dedicated_Test_Cases/Test_Cases_SciKit-Learn.ipynb

On 24.04.2020: Uploaded new ""sklearn/model_selection/_split.py"" and ""sklearn/model_selection/tests/test_split.py"" versions with code indentation correction for 80 characters per line. No such changes required for ""sklearn/utils/fixes.py"" and ""sklearn/base.py"" as all lines within 80 character limit.

On 25.04.2020: Uploaded further corrections to test_split.py and _split.py.",843222
631,2020-03-25T09:24:25Z,2020-03-25T09:26:16Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #16613

#### What does this implement/fix? Explain your changes.
Add fitted check to ```score_samples``` and ```sample``` for kde

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
632,2020-03-23T06:28:12Z,2020-03-24T08:24:58Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Towards #11996. Fixed #12025.  See also #13028 and #15009.
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

#### What does this implement/fix? Explain your changes.
Tackle `handle_missing` specifically for NaNs. `handle_missing` can be
- None: kept as default for now to make sure all previous testings all passed as well as consistent behaviour compared to previous versions. Thinking of using ['warn'](https://scikit-learn.org/stable/developers/contributing.html?highlight=contribute#deprecation) to alert users about the changes.
- 'all-zero'
- 'indicator'

Tests implemented for all three options, including `inverse_transform`

**Pending**: documentation
**Suggestion**: can utilize `pd.isna` to tackle both NaNs and None

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
633,2020-03-23T02:42:06Z,2020-03-23T02:44:53Z,,,"#### Reference Issues/PRs
#16556

#### What does this implement/fix? Explain your changes.
Added support to use pre-fit model in `StackingClassifier` and `StackingRegressor`

Similar to `CalibratedClassifierCV`, I added the option to make `cv = ""prefit""` to use fitted estimators into a stacking model. 

#### Any other comments?
There are a few things I am unsure about. I feel it is probably better to have the PR up so I can consult with the community. 

1. I am not sure if we are fine with using `copy.deepcopy` on estimators. We can't use `clone` here as fitted models will lose their attributes after cloning. If there is any preferred alternative, please let me know. 

2. Another issue is wrt to testing. I have used a synthesized random data set together with a dummy classifier/regressor to check that stacker with prefit estimators work the same way as we expect. Are we ok with this approach or should I include tests using actual datasets like iris and diabetes?

",843222
634,2020-03-22T19:06:39Z,2020-03-25T20:29:19Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #10772
References PR #9131
References PR #10775

#### What does this implement/fix? Explain your changes.

This PR adds a new variable alphaCorrection in classes in naive_bayes.py, which is set to True by default and if set to False, then for alpha=0 (or greater, but still smaller than _ALPHA_MIN) alpha is not being rounded up to _ALPHA_MIN.

#### Any other comments?

I decided to propose a solution to a problem mentioned in issues above.
This makes possible for user to use functions with smaller alpha, even equal 0, which was earlier impossible, because after setting it to something smaller than _ALPHA_MIN it was changed to _ALPHA_MIN in _check_alpha(self).
By adding a new variable I hope to ensure that small (or equal 0) alpha is used by user intentionally.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
635,2020-03-22T17:32:00Z,2020-03-25T20:23:08Z,,,"Hello!

I added some extra functionality to KDTree, which is mix between query and query radius.

I'm fine with abandoning it if you think it would just bloat the API, but if it is welcome, I'm happy to touch up the documentation and whatever else is necessary a bit.

However I've found that there is no great tool for doing this kind of filter+search combination efficiently, and it cannot be done with the current API.

It's probably easiest to understand how it works from the first part of the test:

```python
    X = np.array(
        [
            [0, 2, 1],
            [2, 2, 3],
            [10, 3, 1],
            [4, 5, 6],
            [0, 0, 0],
            [-10, -3, 2],
            [1, -5, 2],
        ]
    )
    tree = KDTree(X, leaf_size=1)
    queries = np.array([[1, 1, 1]])

    dist, ind = tree.query_filtered(queries, [[1, -1, -1]], k=2)

    assert ind[0][0] == 0
    assert ind[0][1] == 4
    assert dist[0][0] == 1
```

",843222
636,2020-03-22T17:04:04Z,2020-03-23T12:58:16Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
637,2020-03-21T20:54:51Z,2020-03-23T13:09:46Z,,,"Towards #16155

Use Ames housing data for `plot_transformed_target.py`.

Old plots:
![image](https://user-images.githubusercontent.com/23182829/77236306-9457e280-6bbd-11ea-867a-ba15c04f5761.png)
![image](https://user-images.githubusercontent.com/23182829/77236312-9cb01d80-6bbd-11ea-9034-8f17a235d2dc.png)

New plots:
![image](https://user-images.githubusercontent.com/23182829/77236415-86569180-6bbe-11ea-8923-f6575892eeef.png)

Hopefully `n_quantiles` I used is reasonable. Ames data has 1460 samples.
",843222
638,2020-03-21T14:05:53Z,2020-03-23T00:55:42Z,,,"This is a follow up to #16713 to better tackle #16702 and #16703.

I found the this specific case when loading the Ames housing dataset from `fetch_openml`:

```python
X, y = fetch_openml(""house_prices"", as_frame=True, return_X_y=True)
```

Note that #11996 also discusses how to natively handle missing values in categorical encoders which could be an alternative to this PR, assuming we support both None and np.nan as missing value markers in object-dtyped columns.
",843222
639,2020-03-21T10:39:45Z,2020-03-21T12:45:05Z,,,"Part of work in #15426.

I think there shouldn't have been anything added in files:
sklearn/compose/_column_transformer.py
sklearn/ensemble/_voting.py
but I decided to include these changes too. I will just remove them if necessary.",843222
640,2020-03-20T21:59:42Z,2020-03-23T11:50:36Z,,,Also parametrized a test while we're at it,843222
641,2020-03-20T02:29:18Z,2020-03-27T10:50:54Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes https://github.com/scikit-learn/scikit-learn/issues/12800


#### What does this implement/fix? Explain your changes.
Does not convert pandas dataframe into a dense array if ALL its columns are sparse arrays.

#### Any other comments?
Since we have pandas sparse support on our minds: CC @rth

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
642,2020-03-19T22:04:14Z,2020-03-24T11:20:06Z,,,"Solves mypy errors and closes https://github.com/scikit-learn/scikit-learn/issues/12953

Solving these errors is a pre-requisite to consider adding some light type annotations https://github.com/scikit-learn/scikit-learn/issues/16705

The full error log on master can be found below (58 errors),
<details>

```
mypy sklearn/ --ignore-missing-imports
sklearn/externals/_arff.py:331: error: Incompatible types in assignment (expression has type ""str"", base class ""ArffException"" defined the type as ""None"")
sklearn/externals/_arff.py:335: error: Incompatible types in assignment (expression has type ""str"", base class ""ArffException"" defined the type as ""None"")
sklearn/externals/_arff.py:349: error: Incompatible types in assignment (expression has type ""str"", base class ""ArffException"" defined the type as ""None"")
sklearn/externals/_arff.py:386: error: Incompatible types in assignment (expression has type ""str"", base class ""ArffException"" defined the type as ""None"")
sklearn/externals/_arff.py:390: error: Incompatible types in assignment (expression has type ""str"", base class ""ArffException"" defined the type as ""None"")
sklearn/externals/_arff.py:394: error: Incompatible types in assignment (expression has type ""str"", base class ""ArffException"" defined the type as ""None"")
sklearn/utils/fixes.py:54: error: Name 'pinvh' already defined (possibly by an import)
sklearn/utils/fixes.py:151: error: All conditional function variants must have identical signatures
sklearn/utils/fixes.py:173: error: Name 'MaskedArray' already defined on line 159
sklearn/__init__.py:63: error: Cannot determine type of '__SKLEARN_SETUP__'
sklearn/utils/_pprint.py:327: error: ""Type[PrettyPrinter]"" has no attribute ""_dispatch""
sklearn/model_selection/_split.py:2147: error: ""Callable[[VarArg(Any), KwArg(Any)], Any]"" has no attribute ""__test__""
sklearn/svm/_classes.py:973: error: Decorated property not supported
sklearn/svm/_classes.py:980: error: Decorated property not supported
sklearn/svm/_classes.py:1307: error: Decorated property not supported
sklearn/svm/_classes.py:1314: error: Decorated property not supported
sklearn/svm/_base.py:6: error: Module 'sklearn.svm' has no attribute '_libsvm'
sklearn/svm/_base.py:7: error: Module 'sklearn.svm' has no attribute '_liblinear'
sklearn/svm/_base.py:8: error: Module 'sklearn.svm' has no attribute '_libsvm_sparse'
sklearn/linear_model/_stochastic_gradient.py:290: error: Decorated property not supported
sklearn/linear_model/_stochastic_gradient.py:296: error: Decorated property not supported
sklearn/linear_model/_stochastic_gradient.py:302: error: Decorated property not supported
sklearn/linear_model/_stochastic_gradient.py:308: error: Decorated property not supported
sklearn/linear_model/_least_angle.py:22: error: Module 'sklearn.utils' has no attribute 'arrayfuncs'
sklearn/linear_model/_coordinate_descent.py:28: error: Module 'sklearn.linear_model' has no attribute '_cd_fast'
sklearn/covariance/_graph_lasso.py:23: error: Module 'sklearn.linear_model' has no attribute '_cd_fast'
sklearn/manifold/_isomap.py:170: error: Decorated property not supported
sklearn/manifold/_t_sne.py:25: error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne'
sklearn/ensemble/_base.py:109: error: Need type annotation for '_required_parameters' (hint: ""_required_parameters: List[<type>] = ..."")
sklearn/dummy.py:398: error: Decorated property not supported
sklearn/dummy.py:625: error: Decorated property not supported
sklearn/tests/test_random_projection.py:28: error: Unsupported operand types for + (""List[Callable[[Any, Any, Any, Any], Any]]"" and ""List[Callable[[Any, Any, Any], Any]]"")
sklearn/tests/test_random_projection.py:33: error: Unsupported operand types for + (""List[Type[SparseRandomProjection]]"" and ""List[Type[GaussianRandomProjection]]"")
sklearn/experimental/enable_iterative_imputer.py:18: error: Module has no attribute ""IterativeImputer""
sklearn/svm/tests/test_svm.py:31: error: Module 'sklearn.svm' has no attribute '_libsvm'
sklearn/manifold/tests/test_t_sne.py:24: error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne'
sklearn/impute/tests/test_impute.py:21: error: Module 'sklearn.impute' has no attribute 'IterativeImputer'
sklearn/impute/tests/test_common.py:12: error: Module 'sklearn.impute' has no attribute 'IterativeImputer'
sklearn/experimental/enable_hist_gradient_boosting.py:29: error: Module has no attribute ""HistGradientBoostingClassifier""; maybe ""GradientBoostingClassifier""?
sklearn/experimental/enable_hist_gradient_boosting.py:30: error: Module has no attribute ""HistGradientBoostingRegressor""; maybe ""GradientBoostingRegressor""?
sklearn/ensemble/tests/test_forest.py:105: error: Argument 1 to ""update"" of ""dict"" has incompatible type ""Dict[str, Type[ForestRegressor]]""; expected ""Mapping[str, Type[ForestClassifier]]""
sklearn/ensemble/tests/test_forest.py:106: error: Argument 1 to ""update"" of ""dict"" has incompatible type ""Dict[str, Type[RandomTreesEmbedding]]""; expected ""Mapping[str, Type[ForestClassifier]]""
sklearn/ensemble/tests/test_forest.py:109: error: Argument 1 to ""update"" of ""dict"" has incompatible type ""Dict[str, Type[ForestRegressor]]""; expected ""Mapping[str, Type[ForestClassifier]]""
sklearn/ensemble/tests/test_forest.py:1262: error: Variable ""sklearn.ensemble.tests.test_forest.DEFAULT_JOBLIB_BACKEND"" is not valid as a type
sklearn/ensemble/tests/test_forest.py:1262: note: See https://mypy.readthedocs.io/en/latest/common_issues.html#variables-vs-type-aliases
sklearn/ensemble/tests/test_forest.py:1262: error: Invalid base class ""DEFAULT_JOBLIB_BACKEND""
sklearn/tests/test_pipeline.py:38: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingClassifier'; maybe ""GradientBoostingClassifier""?
sklearn/tests/test_pipeline.py:1146: error: ""_BaseComposition"" has no attribute ""steps""
sklearn/tests/test_docstring_parameters.py:36: error: Module has no attribute ""__path__""
sklearn/model_selection/tests/test_search.py:71: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingClassifier'; maybe ""GradientBoostingClassifier""?
sklearn/feature_selection/tests/test_from_model.py:14: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingClassifier'; maybe ""GradientBoostingClassifier""?
sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py:12: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingRegressor'; maybe ""GradientBoostingRegressor""?
sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py:13: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingClassifier'; maybe ""GradientBoostingClassifier""?
sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py:12: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingRegressor'; maybe ""GradientBoostingRegressor""?
sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py:13: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingClassifier'; maybe ""GradientBoostingClassifier""?
sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py:9: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingRegressor'; maybe ""GradientBoostingRegressor""?
sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py:10: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingClassifier'; maybe ""GradientBoostingClassifier""?
sklearn/inspection/tests/test_partial_dependence.py:19: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingClassifier'; maybe ""GradientBoostingClassifier""?
sklearn/inspection/tests/test_partial_dependence.py:20: error: Module 'sklearn.ensemble' has no attribute 'HistGradientBoostingRegressor'; maybe ""GradientBoostingRegressor""?
Found 58 errors in 31 files (checked 600 source files)
```

</details>

and generally errors are due to mypy not being able to determine type. That's happens very rarely as by default anything unknown is of type `Any`. In particular following type of errors are found,
 - mypy needing some help with specifying types manually
 - unsupported features that can also be ignored such as,
   - decorated properties
   - importing C extensions (only some, as far as I can tell). Maybe it's due to the way we build them and could deserve more investigation.
   - some non standard edge cases in tests that don't matter and where typing can be ignored.

mypy is also added in CI. I don't have strong feeling about it. pandas does this. I would propose to try it and if it's too annoying for any reason in PRs disable it.  

CC maybe @thomasjpfan @jnothman ",843222
643,2020-03-19T13:52:10Z,2020-03-24T20:17:29Z,,,"towards #15005

tackles the ensemble module",843222
644,2020-03-19T12:35:43Z,2020-03-19T15:31:13Z,,,"towards #15005

tackles the decomposition module.",843222
645,2020-03-19T11:58:36Z,2020-03-22T16:34:42Z,,,"The equation in the original paper is undersimplified:

    c(n) = 2 H(n-1) - 2 (n-1) / n,

where H(k) ~ (ln(k) + gamma)

The definition of the harmonic number is:

    H(n) = sum_{j=1}^{n} (1 / j)

then it follows that:

    H(n) = H(n-1) + 1 / n

and then:

    c(n) = 2 H(n-1) - 2 (n-1) / n = 2 ( H(n-1) - 1 + 1 / n ) =
         = 2 (H(n) - 1)

Here we use simplier equation to save calculations.

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
646,2020-03-18T16:58:42Z,2020-03-19T09:56:58Z,,,"A part of #15005

This PR makes the dataset loader functions kwonly mostly.

I'm not quite certain about a few of them though.

TODO: fix the decorator for functions

ping @thomasjpfan ",843222
647,2020-03-17T13:11:48Z,2020-03-19T10:02:12Z,,,"This makes it possible to plug in algorithms like TSNE in the
pipeline, although they do not support `transform`.  A warning will be
raised that the pipeline is not reusable.

#### Reference Issues/PRs
Fixes  #16710

#### What does this implement/fix? Explain your changes.
This allows algorithms that only implement a `fit_transform` function, but not `transform` (such as TSNE) to be used in a Pipeline in sklearn.
",843222
648,2020-03-16T16:03:52Z,2020-03-24T16:22:40Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
One of the TODO's in #3846, the `DataConversionWarning` class needed an example in the doc. Therefore I took inspiration from the example in the `FitFailedWarning` class doc. 


#### What does this implement/fix? Explain your changes.
Adds an example to the class description where the warning is triggered. 

#### Any other comments?
This is my first PR to scikit-learn so I'm happy to hear if any changes are necessary!


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
649,2020-03-16T13:26:50Z,2020-03-17T15:16:12Z,,,The to do list in issue #3846 mentioned that the `OneVsOneClassifier` classifier needed an example in its documentation. I saw the was already an example in the [user guide](https://scikit-learn.org/stable/modules/multiclass.html#id1) so I added this example to the documentation.,843222
650,2020-03-15T04:26:05Z,2020-03-27T06:42:25Z,,,"#### Reference Issues/PRs
Fixes https://github.com/scikit-learn/scikit-learn/issues/16426

#### What does this implement/fix? Explain your changes.
Removes some masks for missing features.
",843222
651,2020-03-14T18:28:02Z,2020-03-26T07:23:13Z,,,"#### Reference Issues/PRs
This PR partly resolves #16668 and #5975.

#### What does this implement/fix? Explain your changes.
This PR implements the Poisson loss for `HistGradientBoostingRegressor`, i.e. splitting based on improvement in Poisson deviance.",843222
652,2020-03-14T14:39:00Z,2020-03-18T17:01:40Z,,,"#### What does this implement/fix? Explain your changes.
This fixes a test for the binary cross entropy loss function in the module for `HistGradientBoostingClassifier`.

#### Any other comments?
See added comments in commits.

",843222
653,2020-03-14T06:43:13Z,2020-03-14T06:46:09Z,,,"Added `mean_absolute_relative_error`. Fixes #16688 .
@ogrisel , @agramfort , @jnothman , @thomasjpfan - Please review this one.",843222
654,2020-03-11T02:02:47Z,2020-03-16T10:03:49Z,,,"Fixes #16593


#### What does this implement/fix? Explain your changes.

`OneHotEncoder.get_feature_names` now handles `integer` column names.


",843222
655,2020-03-08T17:42:42Z,2020-03-18T14:56:40Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #16646
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create a link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This PR is an improvement to the docstrings of some Transformer classes in `sklearn.preprocessing._data`.
As described in issue #16646, `PolynomialFeatures` does not mention the possibility to pass sparse matrices. The same holds for other classes in the same module, such as `StandardScaler`, `MaxAbsScaler`, `RobustScaler`, `Binarizer`.

With this PR all the descriptions of `X` are changed to
```
array-like, shape (n_samples, n_features)
```
or
```
{array-like, sparse matrix, dataframe} of shape (n_samples, n_features)
```
depending on whether the corresponding methods accept sparse matrices and dataframes respectively.

#### Any other comments?
I took the liberty of formatting other occurrences of `[n_samples, n_features]` to `(n_samples, n_features)`, for consistency within the module.

Further fixes: docstrings with no `Returns` are completed; every `.fit` method documents the ignored argument `y`, and the default values are described in a consistent format throughout the module.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
656,2020-03-06T01:09:52Z,2020-03-18T14:42:29Z,,,"Suggested improved calibration plots with density plots, which I thought are easier to tell where the peaks are than histograms.

https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html
and
https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html

The new ones would look like
![plot_calibration_curve_1](https://user-images.githubusercontent.com/527862/76040329-e6cda980-5f03-11ea-8cfa-d15d3e6cd0e4.png)
![plot_calibration_curve_2](https://user-images.githubusercontent.com/527862/76040333-e9300380-5f03-11ea-8fd0-c337390605fe.png)
![plot_compare_calibration](https://user-images.githubusercontent.com/527862/76040335-e9c89a00-5f03-11ea-8569-e302d25b3d66.png)



",843222
657,2020-03-05T18:28:10Z,2020-03-17T21:21:50Z,,,"This rewrite is motivated by the following:

- use headers to split the content into sections;
- simplify the data-loading, display the data-frame content (an excerpt) and histograms for the target variable;
- use HistGradientBoostingRegressor (with the default least squares loss) now that it supports sample weights: it's better and much faster than RandomForestRegressors and it makes it possible to use the full dataset.
- break the top level import block to progressively introduce scikit-learn component when needed;
- contrast calibration and discriminative power of the models in the narrative;
- add a take-aways section at the end.",843222
658,2020-03-04T12:15:34Z,2020-03-12T15:08:01Z,,,"#### Reference Issues/PRs
In reference to this issue: #16153.


#### What does this implement/fix? Explain your changes.
We want to add the first threshold decision to the edges of the decision tree. So for the first split, we add the decision criteria for the left child, and right child.

#### Additional comments
Added a new PR since my previous repository was deleted.",843222
659,2020-03-03T18:13:38Z,2020-03-16T16:59:51Z,,,"#### Reference Issues/PRs
Closes #10488
Fixes #10144
Fixes #8234

#### What does this implement/fix? Explain your changes.

This implements a top-k accuracy classification metric, for use with predicted class scores in multiclass classification settings. A prediction is considered top-k accurate if the correct class is one of the k classes with the highest predicted scores.
",843222
660,2020-03-03T17:00:26Z,2020-03-19T09:37:53Z,,,"Follow up to #16112 

This PR adds a `is_supervised` tag and uses it to raise an error in `_validate_data()` if:

- a supervised estimator is passed y=None
- ~a non-supervised estimator is passed y!=None~

This PR is mostly motivated by the fact that since #16112, supervised estimators that are passed y=None will fail with a tuple unpacking error instead of a nicer error message.

**This comes with a bunch of complications**:

- This forces supervised estimators to call `_validate_data(X, y)`, instead of validating X and y separately. Since calling `check_array(X, ...)` and then `check_array(y, ...)` isn't in general equivalent to calling `check_X_y(X, y, ...)`, I had to introduce a way to check X and y separately when calling `_validate_data(X, y, ...)`. This is really ugly. It will also definitely not work for third-party estimators that inherit from ours.

- ~Some  estimators like OneClassSVM, IsolationForest, and RandomTreesEmbedding  are unsupervised, but they generate a random `y` that is passed down to their base classes and validated there with `_validate_data(X, y)`. This makes `_validate_data` fail because of the check mentioned above. So we need custom checks in the validation of the base classes in each of these. This is really, really ugly.~

- Since `_validate_data` assumes the tag exists, this means that all estimators using `validate_data` must also support the tag. IMO, that's fine.",843222
661,2020-03-03T05:43:27Z,2020-03-24T14:32:45Z,,,"This PR adds support for Individual Conditional Expectation (ICE) plots. 

#### Reference Issues/PRs
Fixes #14126
Closes #16164


#### What does this implement/fix? Explain your changes.
After the discussion in #16164, it was decided to implement support for ICE plots with a parameter names `individual`. Since the decided method is much different from what's implemented in #16164, I am starting afresh. 


#### Any other comments?

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
662,2020-03-02T04:50:06Z,2020-03-26T05:56:55Z,,,"**What does this implement/fix? Explain your changes.**
Create one example that combines both LOF and IF ROC curves, according to the last discussion in a merged PR [#9798](https://github.com/scikit-learn/scikit-learn/pull/9798).

**Other Comments**
Also fixed some data input typos in the `plot_annomaly_comparison.py`",843222
663,2020-03-01T20:01:47Z,2020-03-16T07:01:42Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Issue
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Diabetes dataset from `sklearn.datasets` had incorrect values (the target is fine).

For instance:

```
from sklearn.datasets import load_diabetes

load_diabetes().data
```

returns 

```
array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,
         0.01990842, -0.01764613],
       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,
        -0.06832974, -0.09220405],
       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,
         0.00286377, -0.02593034],
       ...,
       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,
        -0.04687948,  0.01549073],
       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,
         0.04452837, -0.02593034],
       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,
        -0.00421986,  0.00306441]])
```
#### What does this implement/fix? Explain your changes.
The compressed csv file was incorrect https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/data/diabetes_data.csv.gz

I uploaded data from the source file indicated in the description (https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt)

Testing the same code locally yields:

```
array([[ 59.    ,   2.    ,  32.1   , ...,   4.8598,  87.    , 151.    ],
       [ 48.    ,   1.    ,  21.6   , ...,   3.8918,  69.    ,  75.    ],
       [ 72.    ,   2.    ,  30.5   , ...,   4.6728,  85.    , 141.    ],
       ...,
       [ 60.    ,   2.    ,  24.9   , ...,   4.1271,  95.    , 132.    ],
       [ 36.    ,   1.    ,  30.    , ...,   5.1299,  85.    , 220.    ],
       [ 36.    ,   1.    ,  19.6   , ...,   4.5951,  92.    ,  57.    ]])
```

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
664,2020-02-28T09:34:25Z,2020-03-02T18:56:38Z,,,"#### Reference Issues/PRs
Closes #16517 
(note that the feature proposal wasn't approved yet, but well, I need it for my job :) )

#### What does this implement/fix? Explain your changes.
It implements the Extended Isolation Forest Algorithm by:
- Adding an ""extended"" argument to the Isolation Forest class,
- Add a new ""random_oblique"" mode to the ExtraTreeRegressor class, which uses a new splitter,
- Add this new RandomObliqueSplitter to split instances with an oblique hyperplane.

#### Any other comments?

This is a WIP, several things still need to be implemented or require a discussion:
- The sparse version of the splitter is not implemented yet,
- What should be done with the `max_features` , `min_samples_leaf` and `min_weight_leaf` arguments needs to be discussed. With the oblique splitter, there is not really a way to select the best split out of several ones, unless we use `max_features` as the number of retries to find the best split ? The other two parameters could then be used to drop incorrect split then.
",843222
665,2020-02-27T22:38:23Z,2020-03-05T15:34:38Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
After discussing with @seberg, it was concluded that a prototype of using CuPy with sklearn can help with seeing how libraries would use NEP 37.

- [NEP 37](https://numpy.org/neps/nep-0037-array-module.html)
- https://github.com/scikit-learn/scikit-learn/issues/11447
- https://github.com/scikit-learn/scikit-learn/pull/14963
- Using `enable_duck_array` as a config flag for now

#### What does this implement/fix? Explain your changes.
For reference, this is how this will look like for the user:

https://gist.github.com/thomasjpfan/da54f874f3d434eb4ae360b5e6fa4c12

### With numpy

```py
import numpy as np
from sklearn.decomposition import PCA

rng = np.random.RandomState(42)
X = rng.rand(15, 5)

pca = PCA(n_components=3, random_state=0)
X_trans = pca.fit_transform(X)
type(X_trans)
# numpy.ndarray
```

### With cupy

```py
X_cp = cupy.asarray(X)
type(X_cp)
# cupy.core.core.ndarray

X_cp_trans = pca.fit_transform(X_cp)
# ValueError

with sklearn.config_context(enable_duck_array=True):
    X_cp_trans = pca.fit_transform(X_cp)
type(X_cp_trans)
# cupy.core.core.ndarray
```


#### Any other comments?
This is a quick prototype on how we can use NEP 37 in PCA. (There is still work for other solvers)
Note the `get_array_module` is a hack for now. It would be called with numpy if NEP 37 gets accepted.

CC: @amueller @seberg

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
666,2020-02-27T17:35:33Z,2020-03-05T20:42:13Z,,,"This is a bit of a follow-up on #16262 and inspired by a talk by @nmsanchez at the Chan Zuckerberg kickoff meeting.

I think we all know that there's a diversity problem in OSS in general an sklearn in particular. I think anything we can do, we should do.

Inviting a more diverse contributor base explicitly might also give an additional anchoring point for discussing how we communicate and treat each other.",843222
667,2020-02-27T13:03:19Z,2020-03-02T18:56:39Z,,,"Fixes https://github.com/scikit-learn/scikit-learn/pull/15506#discussion_r385093644

To follow up on https://github.com/scikit-learn/scikit-learn/pull/15506#discussion_r341828520 there was indeed a change of behaviour in OneHotEncoder in 0.20 (cf [what's new](https://scikit-learn.org/dev/whats_new/v0.20.html#id37)) but I'm not convinced about the added value of something like,
>  .. versionchanged:: 0.22  OneHotEncoder now handle encoding of all feature types (also handles string-valued features) and derives the categories based on the unique values in the features instead of the maximum value in the features.

added in 0.23 (i.e. mid 2020) for something that was relevant for the  0.19 -> 0.20 migration (i.e. mid 2018). For the vast majority of users that's not relevant. So I prefer to remove the versionchanged altogether.

cc @jnothman ",843222
668,2020-02-26T10:50:15Z,2020-03-07T18:23:53Z,,,"README.rst logo addition

Fixes #15617 issue: #15617

**What does this implement/fix? Explain your changes.**
Logo addition

**Any other comments?**
super simple update, did not add anything except logo; Learned reStructuredText.
",843222
669,2020-02-24T13:32:24Z,2020-02-24T13:44:15Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Towards #16155
It changes the Boston dataset for diabets dataset and adds more through explanation on what's in the example

#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
670,2020-02-24T08:02:00Z,2020-03-06T03:44:58Z,,,"#### Reference Issues/PRs
This PR is the related work for #15962.


#### What does this implement/fix? Explain your changes.
In the old PR above, I proposed an AVX512 version of SVM kernel function dot 
and k_function. There is around 40% on our CLX machine on MLpack benchmark.
However, @rth mentioned that writing AVX512 code without run-time detection 
was of limited use since most users don't build packages from sources 
(with custom compile flags). Accordingly, I choose replacing my AVX512 implementation
svm kernel function with scipy blas api as suggested by @jeremiedbb . My implemenation
is similar to that in liblinear, which is to pass a pointer to the blas function ``` kernel::dot```
method. Please help to review the patch and thanks for your great advise.
 

#### Any other comments?
The test data is attached.
[profile final.xlsx](https://github.com/scikit-learn/scikit-learn/files/4243508/profile.final.xlsx)
The precision and recall file is also attached.
[training accuracy.xlsx](https://github.com/scikit-learn/scikit-learn/files/4243511/training.accuracy.xlsx)


",843222
671,2020-02-23T19:48:05Z,2020-03-02T18:56:46Z,,,"Added ability to specify custom objective functions for minimizing kernel hyperparameters in _gpr.py and _gpc.py. Added tests for this functionality.

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Implements #16523 (Add ability to specify custom objective function in GaussianProcessRegressor and GaussianProcessClassifier)

#### What does this implement/fix? Explain your changes.

I allow the user to pass in a function as a keyword argument (obj_func) to GPR and GPC. If obj_func is None, the objective function is assigned to the default negative log likelihood function. Otherwise, the user-passed function is used as the objective function. To allow the user to access parameters in the GPR/GPC, the user-supplied function should take self as the first parameter and theta as the second, and the instance of GPR/GPC will be passed to the function during optimization. I also added tests for this functionality.

#### Any other comments?
The `types` library is part of the python STL, and the functionality I use has been in place since before 2.7. However, I have not seen this used elsewhere, and if there is a better way to implement this (e.g. by exposing obj_func as a public method to allow overloading or taking in an objective function in fit) I will certainly rework this request.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
672,2020-02-23T15:41:34Z,2020-03-20T22:07:22Z,,,"closes #8614
closes #10117
supersedes #10117 

# Description

This meta-estimator is intended to find the decision threshold which will maximize an objective metric. There are 2 use-cases:

* maximize a metric such as `balanced_accuracy_score`, `f_beta`, `f1_score`, etc. In this case, we are required to maximize the score.
* find the decision threshold for a couple of metrics for which one of the metric value will be fixed. For instance, for precision-recall, one would like the threshold maximizing the recall for a given precision or vice-versa. I think that we can support precision/recall and TPR/FPR at first.

# Additional work

* in the future, we could think of adding support for passing a cost-sensitive matrix of the size of the confusion matrix. It will allow injecting business-oriented information regarding the different classification errors done by a  learner.",843222
673,2020-02-21T12:21:31Z,2020-03-26T16:58:55Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

towards #16155

This PR is towards removing Boston dataset from the Sklearn.
It exchanges Boston dataset for California dataset in missing values example

Before:
![before](https://user-images.githubusercontent.com/2219642/75034211-064ce700-54ad-11ea-8987-8b09fd0227bb.png)


After:
![after](https://user-images.githubusercontent.com/2219642/75034144-d867a280-54ac-11ea-8f8b-c7a40d127398.png)


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
674,2020-02-21T00:12:40Z,2020-03-02T18:56:48Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Resolves #16498


#### What does this implement/fix? Explain your changes.
Uses the name of the dtype to check if the dtype is a pandas `IntegerArray`.

#### Any other comments?
The `__array__` interface for panda's `IntegerArray` results in a object:

```py
import pandas as pd
X = pd.Series([1, 2, pd.NA])

np.asarray(X)
# array([1, 2, <NA>], dtype=object)

np.asarray(X, dtype=float)
# ValueError

np.asarray(X.astype(float))
# array([ 1.,  2., nan])
```

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
675,2020-02-20T23:14:09Z,2020-03-05T14:58:53Z,,,"Continues and closes https://github.com/scikit-learn/scikit-learn/pull/15015

Closes https://github.com/scikit-learn/scikit-learn/pull/5515
 
This merges the common check which ensure that setting sample_weight to 0 is equivalent to removing samples. Estimators that currently fail it are listed in https://github.com/scikit-learn/scikit-learn/issues/16298 and are marked as a known failure.  

We use the [`_xfail_test` estimator tag](https://scikit-learn.org/dev/developers/develop.html#estimator-tags) to mark estimators that xfail this test.

Also related to https://github.com/scikit-learn/scikit-learn/issues/11316, https://github.com/scikit-learn/scikit-learn/issues/15657


",843222
676,2020-02-20T17:22:52Z,2020-03-03T14:49:55Z,,,"follow up on #11950. I just realized that the initialization was included in the threadpoolctl context, which can degrade perfs if the init is 'k-means++'.

I moved the context manager as close as possible to the nested prans/BLAS part of the code.

ping @ogrisel @NicolasHug it should still be fresh in your minds :)",843222
677,2020-02-20T04:33:00Z,2020-03-02T18:56:49Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #16490

#### What does this implement/fix? Explain your changes.
Changed the default value of max and min to `np.inf` and `-np.inf` respectively.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
678,2020-02-19T10:44:12Z,2020-03-02T18:56:50Z,,,"#### Reference Issues/PRs

#### What does this implement/fix? Explain your changes.

This PR is the continuation of scikit-learn#15289.

I simply implemented the method to inverse transform the response variables Y back to the original space. Indeed, the current method only allows to inverse transform the X data array.
Within this new implementation, the user has now the choice to either inverse transform X, Y or both of them.

I adapted the example code from scikit-learn#15289 :

```
from sklearn.cross_decomposition import PLSRegression

X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]
Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]

pls2 = PLSRegression(n_components=2)
x, y = pls2.fit_transform(X, Y)

# back transform to the original space:
X_reconstructed = pls2.inverse_transform(X=x)
Y_reconstructed = pls2.inverse_transform(Y=y)
# or:
X_reconstructed, Y_reconstructed = pls2.inverse_transform(X=x, Y=y)
```

#### Any other comments?

I missed this feature for my work so I added it here.

Cheers,

Robin

",843222
679,2020-02-19T09:02:35Z,2020-03-02T18:56:51Z,,,"**Reference Issue**
[scikit-learn] Feature Request: Hellinger split criterion for classificaiton trees #9947
[scikit-learn-contrib] [WIP] ENH: Hellinger distance tree split criterion for imbalanced data classification [#437](https://github.com/scikit-learn-contrib/imbalanced-learn/pull/437)  

**What does this implement/fix? Explain your changes.**
Hellinger Distance as tree split criterion, cython implementation compatible with sklean tree based classification models

**TODO**
- [ ] tests
- [ ] documentation
- [ ] examples",843222
680,2020-02-18T22:12:06Z,2020-03-02T18:56:51Z,,,"When calculating cross entropy, we are comparing ints with floats.

https://github.com/scikit-learn/scikit-learn/blob/3e92edbab8b1d210523e775d2050039055dda1d7/sklearn/ensemble/_hist_gradient_boosting/_loss.pyx#L88

https://github.com/scikit-learn/scikit-learn/blob/3e92edbab8b1d210523e775d2050039055dda1d7/sklearn/ensemble/_hist_gradient_boosting/loss.py#L283

https://github.com/scikit-learn/scikit-learn/blob/3e92edbab8b1d210523e775d2050039055dda1d7/sklearn/ensemble/_hist_gradient_boosting/loss.py#L293

This PR encodes the target as floats for classification.

CC: @NicolasHug ",843222
681,2020-02-17T16:24:32Z,2020-03-02T18:56:54Z,,,"#### Reference Issues/PRs
Fixes #14257  See also #16236

#### What does this implement/fix? Explain your changes.
completed 3 test cases for GroupTimeSeriesSplit

#### Any other comments?
Test cases from a prev. [WIP] PR #14914 from @mfcabrera
and implementation code side is in #16236 by @getgaurav2 
I'm only doing the test side",843222
682,2020-02-17T15:39:55Z,2020-03-02T18:56:54Z,,,"
#### Reference Issues/PRs
Fixes #16461.

",843222
683,2020-02-17T14:23:11Z,2020-03-13T10:49:49Z,,,"Introduced suggestions made by @glemaitre to the example of permutation importance

#16223

hope it works out this time. looking foreword to feedback. all the best!

together with @fraboeni ",843222
684,2020-02-16T22:10:41Z,2020-03-10T07:53:04Z,,,"This PR removes the StreamHander which was attached to the sklearn logger on import.

Relevant: #15122 and https://github.com/scikit-learn/scikit-learn/issues/15122#issuecomment-537831009 which suggests that this was added for Python 2.
Please note that since Python 3.2, a [logging.lastResort](https://docs.python.org/3/library/logging.html#logging.lastResort) default handler was added to the logging standard library, which means that by default your messages will show up for users, even if you don't add this streamhandler.

Note that the standard advice for Python libraries is to not attach handlers:
https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library

Why is it bad to have the handler?

Because it causes log messages to appear twice, if people add a root logger, which is added by calling e.g. `logging.info`, like this:
```
>>> import sklearn
>>> import logging
>>> logging.info(""123"")
>>> sklearn.logger.info(""123"")
123
INFO:sklearn:123
```

Note that the problem of duplicate log messages is an old one, here's a blog post on the issue, and nice tool to figure how how logging works, and which libraries do what:
https://rhodesmill.org/brandon/2012/logging_tree/

Thoughts?",843222
685,2020-02-16T15:12:44Z,2020-02-17T04:01:23Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

Breiman, Friedman, Ohlsen, Stone use a T with a tilde to denote the terminal nodes of a tree. A simple T would imply we are counting the nodes in the whole tree.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
686,2020-02-16T12:49:19Z,2020-02-16T19:26:28Z,,,"#### Reference Issues/PRs
Partially solves #3702: Adds sample_weight to `ElasticNetCV` and `LassoCV`, but only for dense feature array X.
It is a follow up of PR #15436.

#### Any other comments?
**DO NOT MERGE BEFORE #15436** as it is based on that branch.",843222
687,2020-02-13T14:47:45Z,2020-02-14T11:06:11Z,,,"Continuation of #6641

Here's a benchmark of the speedup using 2 threads. When the number of components is large enough the speed-up approaches 2x (maybe a bit better than 2x because I switched to a blas `dot` instead of a manual loop).
![image](https://user-images.githubusercontent.com/34657725/74445723-24946080-4e77-11ea-9924-f3bf6ce36fdf.png)

Since it now uses a BLAS function inside an OpenMP loop, we first need to prevent oversubscription with threadpoolctl before we can merge this pr.",843222
688,2020-02-11T23:57:30Z,2020-02-20T23:39:58Z,,,Caches repeated compilations with [ccache](https://ccache.dev/). This only applies to Linux and Mac OS.,843222
689,2020-02-10T08:12:28Z,2020-03-10T07:53:53Z,,,"#### Reference Issues/PRs
Fix https://github.com/scikit-learn/scikit-learn/issues/15144


#### What does this implement/fix? Explain your changes.
`fit_transform` do the projection whenever algorithm=""randomized"" or when algorithm=""arpack"" with tol > 0

#### Any other comments?
",843222
690,2020-02-09T21:07:33Z,2020-02-09T21:07:33Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Ref PR: #1288

#### What does this implement/fix? Explain your changes.
- adds an example of generating distance metrics from RandomTreesEmbedding and uses it on unsupervised task. 

The distance metrics used are: 
1) the depth of the nearest common ancestor of the leaf nodes sample i and j land into (take the reciprocal to make it a distance)
2) the length of the shortest path of the leaf nodes sample i and j land into
3) the probability of sample i and j landing into the same lead node(take 1 minus the value to make it a distance)

#### Any other comments?
The plot now looks like this: 
![image](https://user-images.githubusercontent.com/55062863/74109846-23042900-4b55-11ea-960b-cac5d680474a.png)


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
691,2020-02-09T00:07:45Z,2020-03-26T13:45:03Z,,,"Added Bayesian Ridge Regression attribute documentation for  X_offset_ , X_scale_ , and y_offset

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs

Issue #14312
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This adds documentation for attributes for the Bayesian Ridge function. The attributes are x_offset, y_offset, and x_scale.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
692,2020-02-08T13:25:01Z,2020-02-08T13:25:01Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Fixes: #14081 

#### What does this implement/fix? Explain your changes.

I've created Common Methodological Pitfalls section in user guide and made the changes suggested in the review of https://github.com/scikit-learn/scikit-learn/pull/14643

#### Any other comments?

While generating html files, I was getting some warnings about ModuleNotFound errors and some other as well. Is that normal or I'm missing something?

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
693,2020-02-08T10:02:30Z,2020-03-02T18:57:00Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #16398
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Added warning in RidgeCV when the optimal value found for the alpha is at the boundary of the given values.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
694,2020-02-07T17:32:39Z,2020-03-02T18:57:01Z,,,"…h.py

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
See issue 
Unable to Take Cartesian Products of Many Arrays
#16350 .

#### What does this implement/fix? Explain your changes.
This fixes the problem of only accepting 32 arrays into the cartesian function.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
695,2020-02-07T15:36:50Z,2020-03-04T14:34:15Z,,,"
#### Reference Issues/PRs



#### What does this implement/fix? Explain your changes.

The current grid_to_graph function only defines voxel neighbors with the 6-connectivity definition. I would like to add 18 and 26 connectivity. (https://en.wikipedia.org/wiki/Pixel_connectivity#26-connected)

#### Any other comments?

I would like feedback on my test coverage. Thanks!
",843222
696,2020-02-05T22:00:28Z,2020-03-02T18:57:04Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Partly addresses https://github.com/scikit-learn/scikit-learn/issues/11198


#### What does this implement/fix? Explain your changes.
Deprecates `grid_scores_` in `GraphicalLassoCV`

#### Any other comments?
Working toward less issues!

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
697,2020-02-05T09:15:06Z,2020-03-03T07:59:33Z,,,"This PR adds an implementation of lightweight random indexing (LRI) as a vectorization method.
LRI is a stateless indexing method based on hashing. It can be used in out-of-core scaling and in all the cases in which the feature set may not be entirely defined at the start of the indexing process (e.g. when repeatedly fitting with partial_fit).
LRI is based on random indexing theory, and it is more robust than the hashing trick in handling collisions.
The raw difference between hashing trick and LRI is that LRI uses two hashes to index a feature.
As detailed in this [paper](https://www.jair.org/index.php/jair/article/view/11025), the LRI hashing method supports the quasiorthogonality of the resulting vectors even when the indexing space is sensibly smaller than the original feature space.
The contributed code mainly consists of a modification of the `_hashing_fast.pyx` code so as to implement the LRI hashing method (`_lri_fast.pyx`). The two classes `FeatureLightweightRandomIndexing` and `LightweightRandomIndexingVectorizer` make the LRI method available similarly to the `FeatureHasher` and the `HashingVectorizer` classes for the hashing trick.",843222
698,2020-02-03T05:01:20Z,2020-03-02T18:57:07Z,,,"This PR implements conditional density estimation loss as described in T. Pospisil, and A. Lee, ""RFCDE: Random Forests for Conditional Density Estimation"", arXiv:1804.05753 [stat.ML], 2018. The key advantages of adding the CDE Loss criterion into sklearn is for making it easy for users to incorporate this algorithm in scikit-learn pipelines and serialize this estimator. 

Conditional Density Estimation is a useful branch of machine learning with many applications. The random forest implementation in scikit-learn is robust, in comparison to the C++ implementation provided by the authors of the above paper.

While users can easily implement their own cython class for CDE Loss, I believe this criterion is useful enough for CDE-type problems (and similar enough to the existing MSE criterion), that it should be supported within scikit-learn. ",843222
699,2020-02-02T11:29:49Z,2020-03-02T18:57:08Z,,,Fixes Issue #16254,843222
700,2020-02-02T11:09:43Z,2020-02-09T22:48:59Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
#16157 - Comparison of ML Algorithms

#### What does this implement/fix? Explain your changes.
In continuation of the ongoing discussion, I have added a doc containing a comparison of the 3 most basic and popular Machine Learning algorithms today. This is in accordance with my last comment in the group discussion If this doc seems feasible and useful to everyone, I would love to expand on it and add more algorithms .
#### Any other comments?
Even if not, I would love for us to find an agreeable solution for this issue! 
<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
701,2020-01-31T21:08:28Z,2020-02-17T21:41:28Z,,,"I'm trying to make https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html more illustrative.
I find the equal spacing of groups makes it hard to understand the difference with k-fold.
![Figure_8](https://user-images.githubusercontent.com/449558/73574456-ccf00100-4443-11ea-812e-4e57ce9e4a6e.png)
![Figure_5](https://user-images.githubusercontent.com/449558/73574458-ce212e00-4443-11ea-93c1-5d69177e6c2a.png)

I'm sure there's other ways to generate unequal group sizes but this seemed like the Bayesian way ;)
",843222
702,2020-01-31T17:18:40Z,2020-03-02T18:57:10Z,,,"## Reference Issues/PRs

Works on  #11000 for BernoulliRBM.

#### What does this implement/fix? Explain your changes.

Prevent the transformer from converting float32 to float64.

#### Any other comments?

Maybe we should wait to merge a generic test for dtype consistency (see #16290) before merging this one.",843222
703,2020-01-31T15:17:46Z,2020-03-02T18:57:10Z,,,"closes #16336 

Refactor `SparseCoder`, `DictionaryLearning`, `MiniBatchDictionaryLearning` such that we follow properly our API.

There is a bit of boilerplate but I am not sure how to do better.",843222
704,2020-01-31T14:59:26Z,2020-02-27T12:30:26Z,,,"References to #12730

This PR adds the possibility of running a two sided paired t-test between the test scores of each pair of candidates in a grid-search. The p-values obtained from the t-test are returned in the cv_results dictionary, in a column named ttest_test_{scorer}. For each candidate-row, a list containing a masked array is returned. The length of this array equals to the total number of candidates, and reflects the comparison between the candidate of that row and every other possible candidate (masking the comparison of the candidate with itself).

This is an example of the implementation so far: 

```
X, y = make_blobs(random_state=0, centers=2)

search = GridSearchCV(SVC(), {'C': [0.1, 10, 100]}, scoring='accuracy', cv=5, return_stats=True).fit(X, y)

```

In this case `search.cv_results_['ttest_test_score']`  would return the following list:

```
[masked_array(data=[--, 0.2079999999999997, 0.3375018565403647],
              mask=[ True, False, False], fill_value=1e+20), 
 masked_array(data=[0.2079999999999997, --, 0.7040000000000004],
              mask=[False,  True, False], fill_value=1e+20), 
 masked_array(data=[0.3375018565403647, 0.7040000000000004, --],
              mask=[False, False,  True], fill_value=1e+20)
]
```

*To-Do/Needs-discussion*:

- This implementation still lacks a multiple-comparisons correction. Before adding this feature it would be good to get your feedback on what has been implemented so far: should the pairwise comparisons be returned inside the cv_results_ attribute dictionary? if so, should it be returned in the proposed masked arrays? Or should the pairwise comparisons be returned as a separate attribute of the grid search?
- I think the answer to the above questions will also depend on the following: This implementation only performs pairwise comparisons, should we provide an overall test statistic like the Friedman test (see https://github.com/scikit-learn/scikit-learn/issues/9631) when having more than three candidates, followed by a posthoc test?
- This implementation only performs a parametric t-test. Should we provide non-parametric (e.g. Wilcoxon signed ranks test) or variance corrected (e.g. http://papers.nips.cc/paper/1661-inference-for-the-generalization-error.pdf) alternatives?
- What type of multiple-comparison correction for the pairwise comparisons should we provide? (e.g. Bonferroni and the more traditional ones?)
- Depending on these choices, this implementation might also lack a warning to the user when the number of splits its too low to approximate a normal distribution (less than 10?).

This PR was done during the Paris sprint and with the help of @adrinjalali ",843222
705,2020-01-31T14:55:26Z,2020-03-25T17:41:10Z,,,"#### Reference Issues/PRs
Continuation of PR #9978

#### Any other comments?
I will try this implementation on several datasets.

",843222
706,2020-01-31T11:22:47Z,2020-03-03T14:50:57Z,,,"#### Reference Issues/PRs

Fixes #15409.

#### What does this implement/fix? Explain your changes.

Add `dtype`parameter to the class `KBinsDiscretizer`in order to cast the output of `transform` and `inverse_transform` methods. 

#### Any other comments?

The dtype of the output is independent from the dtype of the data used to fit `KBinsDiscretizer`. Default is `np.float64`.",843222
707,2020-01-30T19:37:38Z,2020-03-02T18:57:14Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
closes #16028 
closes #16045 


#### What does this implement/fix? Explain your changes.
This PR provides a way to make predictions with `CategoricalNB` with data that contains categories that were not observed in the training data. E.g., data for a problem can possibly have 2 categories, but training data only contains samples with one category observed:

```
>>> import numpy as np
>>> from sklearn.naive_bayes import CategoricalNB
>>> X_train = np.array([[0], [0], [0]])
>>> y_train = np.array([0, 1, 0])
>>> clf = CategoricalNB(min_categories=2)
>>> clf.fit(X_train, y_train)
>>> X_test = np.array([[1]])
>>> clf.predict(X_test)
array([0])
>>> clf.n_categories_
array([2])
```

This adds the additional Parameters/Attributes to `__init__` of `CategoricalNB`:
```
    Parameters
    ----------
    min_categories : int or array-like or None, (default=None)
        Minimum number of categories per feature:
        - int : Sets the minimum number of categories per feature to
          `n_categories` for each features.
        - array-like : `n_categories[i]` holds the minimum number of categories
          for the ith column of the input.
        - None : Determines the number of categories automatically from the
          training data.

    Attributes
    ----------
    n_categories_ : ndarray (n_features,)
        Number of categories for each feature. This value is provided
        inferred from the data or set by the minimum number of categories.
```

This feature has uses in instances where a given category may be rare, and then by random chance is observed in a test/application set, but not in the training set.

In the current version, such an instance will raise an error (as detailed in #16028).

#### Any other comments?
* This differs slightly from the solution I proposed in #16028 . Implementation in this way allows a minimum number of categories to be specified, but be overridden if the data has more categories, which I thought may be user-side.
* This fix will still result in the same unhelpful error message mentioned #16028 if category `i` has a greater value in a non-training set than the value of `n_categories[i] - 1`. Is #16045 considered stalled at this point?  Would it be worth wrapping a helpful error message for that case into this PR?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
708,2020-01-30T14:33:20Z,2020-03-06T11:20:31Z,,,"Fixes #16065. 

#### What does this implement/fix? Explain your changes.

It ignores weights equal to zero when called by precision_recall_curve function.

#### Any other comments?

I'm not sure if there is a more elegant solution.
",843222
709,2020-01-30T11:04:17Z,2020-03-14T12:08:04Z,,,"#### Reference Issues/PRs
Proposes one fix for #16143.

#### What does this implement/fix? Explain your changes.

Allow the user to disable the parallelisation or to set check_input=False when calling predict for a RandomForestRegression. The prediction time is divided by 3 by parallelisation is disabled, check_input=False with 100 trees, 10 features, and 1 observation.

```
parallel=T check=T: r=1000 e=100 n=1 f=10 time=4.597580
parallel=F check=T: r=1000 e=100 n=1 f=10 time=3.539385
parallel=F check=F: r=1000 e=100 n=1 f=10 time=1.461284
```

Profile with py-spy:

![image](https://user-images.githubusercontent.com/4695832/73444304-a0ab8600-4358-11ea-861b-1f37eab9b9d3.png)
",843222
710,2020-01-30T09:19:23Z,2020-03-02T18:57:16Z,,,"#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
WIP It tries to answer #16093
ColumnTransformer assumes elements of get_feature_names() will be strings



#### What does this implement/fix? Explain your changes.

It removes the assumption that the elements returned by get_feature_names() will be strings

#### Any other comments?

I tried to create a test with an error using both ColumnTransformer as well as DictVectorizer unsuccessfully 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
711,2020-01-29T16:33:56Z,2020-03-02T18:57:17Z,,,"## Reference Issues/PRs

Fixes #16191. See also #8723.

#### What does this implement/fix? Explain your changes.

Use `DataFrame.take` instead of `iloc` in`_safe_indexing`. In `DataFrame.take`, there is a parameter to ensure the `_is_copy` is not set, so you can ensure that the result is a ""real"" copy that will not generate those warnings.

#### Any other comments?

Parameter `is_copy` from `DataFrame.take` will be deprecated in pandas 1.0.0. The PR should be compatible with older version of pandas, without defining pandas as a scikit-learn's requirement.",843222
712,2020-01-29T16:12:45Z,2020-03-02T18:57:18Z,,,"#### Reference Issues/PRs
Part of #11000.

#### What does this implement/fix? Explain your changes.
It will make testing and implementing of #11000 easier. Also added a new tag `preserve_32bit_type`, which skips the test if false.

#### Any other comments?
Verifying that values and results are similar as without conversion is needed to be added. 
Also it's part of Paris sprint.
",843222
713,2020-01-29T15:58:12Z,2020-03-27T08:12:42Z,,,"I want to reconstruct fitted stepwise interpolation function of isotonic regression in other programming languages (Scala, Go, ...).

Therefore, I want to get interpolation points (input to function `f_`).

However, interpolation points (`_necessary_X_`, `_necessary_y_`) are internal variable of IsotonicRegression class according to PEP8 (leading underscores).

This PR changes interpolation points to attributes of IsotonicRegression class.",843222
714,2020-01-29T15:40:37Z,2020-03-02T18:57:19Z,,,"#### Reference Issues/PRs
Closes #9173

Addressing this in the context of Paris Sprint of the Decade.

#### What does this implement/fix? Explain your changes.
Merged current master into https://github.com/aarshayj/scikit-learn/commit/01a63a7a5ad870a7693491d71377550575890893 :

* Resolved the conflicts choosing for latest (current master).
* Added test for GridSearchCV with trianing-score, since the default behaviour changed in the meantime

Ping @amueller @thomasjpfan 

- [ ] test check GridSearch Display class
- [ ] test check `params` with more than 2 (not implemented) values
- [ ] test check `params` with wrong values (right dimensionality, but non-existent value)
- [ ] Use `NotImplementedError` when `params=None` and `cv_results` dim > 2
- [ ] Review/adjust examples
",843222
715,2020-01-29T14:50:15Z,2020-03-02T18:57:20Z,,,"contributes to #14312
adds documentation for attributes of HistGradientBoostingClassifier, except classes_ (see #16277)

@NicolasHug attribute `bin_mapper_` is of private class `_BinMapper` but is exposed, is that ok?
",843222
716,2020-01-29T14:29:57Z,2020-03-20T00:29:11Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Fixes #15077. See also #10856.

#### What does this implement/fix? Explain your changes.

This implements general naive Bayes (`GeneralNB`) in addition to the [existing naive Bayes implementations](https://scikit-learn.org/stable/modules/naive_bayes.html) like `GaussianNB` and `BernoulliNB`. 

This implementation allows multiple assumptions on the features, namely the Bernoulli, Gaussian, Multinomial, and Categorical distributions. In the API, the user will be able to specify these distributions and their respective features.

I have divided this description into 3 sections as below:

1. Design and usage
2. Under the hood
3. Runtime checks

#### 1. Design and usage

The design of the API is similar to that of `ColumnTransformer` and `Pipeline`. To specify that columns 0-2 and 3-4 are to be modelled with Gaussian and categorical naive Bayes respectively, indicate these in the `GeneralNB` constructor and fit accordingly:

```python
>>> clf = GeneralNB([
...     (""gaussian"", GaussianNB(), [0, 1, 2]),
...     (""categorical"", CategoricalNB(), [3, 4])
... ])
>>> clf.fit(X, y)
```

It also accepts a list of strings of column names if the data to be fitted are pandas DataFrames:

```python
>>> clf = GeneralNB([
...     (""gaussian"", GaussianNB(), [""a"", ""b"", ""c""]),
...     (""categorical"", CategoricalNB(), [""d"", ""e""])
... ])
```

Lastly, similar to `ColumnTransformer`, it also accepts callables like `make_column_selector` to specify DataFrame columns:

```python
>>> from sklearn.compose import make_column_selector
>>> clf = GeneralNB([
...     (""gaussian"", GaussianNB(), make_column_selector(pattern=r""[abc]"")),
...     (""categorical"", CategoricalNB(), make_column_selector(pattern=r""[de]""))
... ])
```

The attributes of the fitted estimators can be accessed using the `self.named_models_` attribute. For example, to access the `theta_` parameter of the `bernoulli` model, 

```python
clf.named_models_.bernoulli.theta_
```

#### 2. Under the hood

For the `GeneralNB.predict()` function, we sum the `_joint_log_likelihood()` for each naive Bayes estimator, then subtract (n-1) log P(c) from this sum. Here is a pseudocode:

```python
jlls = [model._joint_log_likelihood() for model in models]
jlls = jlls - log_prior
jll = jlls.sum(axis=0) + log_prior
```

#### 3. Runtime checks

Check `self.models`:
- Duplicate specification of column is not tolerated. 
- No. of cols specified must match the no. of cols in `X`.

Checks on parameter consistency across naive Bayes estimators are performed to ensure that specific parameters across the estimators stay the same. Otherwise, the calculation of the joint log likelihood will be wrong. Such parameters are:
- `class_prior`* or `priors`^
- `fit_prior`*
- `class_log_prior`* or `class_prior`^

*used in BernoulliNB, MultinomialNB, ComplementNB, CategoricalNB
^used in GaussianNB

Data checks:
Methods like `_check_X_y()` and `_check_X()` check if the data type used during fitting is the same during prediction (i.e. NumPy array and pandas DataFrame). 

#### 4. Others

Partial fitting is not supported.
GeneralNB implements a restriction that the models should be from sklearn.


#### Progress:

- [x] API design
- [x] Main code
- [x] Docs
- [x] Tests

#### Any other comments?

PR submitted.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
717,2020-01-29T13:13:31Z,2020-03-02T18:57:22Z,,,"#### Reference Issues/PRs
Fixes #12542 

#### What does this implement/fix? Explain your changes.
With the changes we'll be now able to call `score_samples` on the estimator with the best found parameters in *SearchCV instances.
",843222
718,2020-01-28T15:31:18Z,2020-03-20T07:32:32Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Continuation of #11527.


#### What does this implement/fix? Explain your changes.

This PR proposes a new implementation for the computation of the explained variance for the Sparse PCA.
As such, we add two more attributes to the SparsePCA & MiniBatchSparsePCA classes: `explained_variance_` and `explained_variance_ratio_`.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
719,2020-01-27T15:32:34Z,2020-03-19T20:19:51Z,,,"- more detailed docstrings for parameters and attributes
- explain that the output of decision_function is the log posterior
- explain what coef_ and intercept_ actually are 
- detailed math section to give intuition behind LDA
- detailed (some) solvers. There are still some of them that I don't understand",843222
720,2020-01-26T09:01:08Z,2020-03-13T17:05:47Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Fixes #14257 
1. Split the time series like regular Time Series Split
2. Loop through all the train , test splits
3. If a ""group"" in the test split is already in the training split , do not include that  pair of Train ,Test arrays in the final result 
#### Any other comments?

Test cases from a prev. [WIP] PR #14914 from @mfcabrera  

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
721,2020-01-25T18:42:50Z,2020-03-11T13:15:56Z,,,"Towards #15426

finished v0.19.1 (added as main version v0.19 as per Adrin's advice), still working through v0.19

@adrinjalali #wimlds #scikitlearnsprint ",843222
722,2020-01-25T15:41:52Z,2020-03-02T18:57:25Z,,,"#### Reference Issues/PRs

Fixes #14567, closes #14627 and closes #15529.

#### What does this implement/fix? Explain your changes.

It adds an option for n_features_to_select to be a float between 0 and 1, in which case it is treated as a percentage of features to select, rather than the absolute number. If a float larger than 1 is given, it is converted to an int and considered an absolute number of features to select.

#### Any other comments?

Worked with @gelavizh1 @lschwetlick at scikit-learn sprint.

CC @adrinjalali @noatamir #WiMLDS",843222
723,2020-01-25T14:17:24Z,2020-02-28T13:20:34Z,,,"referencing #15426 
together with @hhnnhh, @shravaniCD
for version 0.18: added labels versionadded and versionchanged
#wimlds #ScikitLearnSprint @noatamir @adrinjalali 
",843222
724,2020-01-25T13:39:27Z,2020-02-26T17:02:00Z,,,"#10548 
fix docstrings for random_state in theses files:

sklearn/dummy.py 
sklearn/multioutput.py 
sklearn/kernel_approximation.py
sklearn/multiclass.py 
sklearn/random_projection.py",843222
725,2020-01-25T13:08:17Z,2020-03-03T14:20:25Z,,,"…er inspecting changelog


Reference Issue #15426 
Partially adding version added/changed to fixes/features implemented in this version (0.21)

@Schindst
@WiMLDS
@adrinjalali 





",843222
726,2020-01-25T12:26:59Z,2020-02-28T13:14:53Z,,,"with @lopusz and @magda-zielinska

#### Reference Issues/PRs
#15761


#### What does this implement/fix? Explain your changes.
Deletes optional specifier and changes array to array-like


",843222
727,2020-01-25T10:46:24Z,2020-02-25T17:45:44Z,,,"#### Reference Issues/PRs

Fixes #15761 in the random_projection.py

With @lopusz

@adrinjalali 

",843222
728,2020-01-25T10:42:49Z,2020-03-02T15:54:18Z,,,"Fixes #14117
Fixes the warning about the `MLPClassifier` not being able to converge, now it does.

@adrinjalali, @noatamir",843222
729,2020-01-25T10:37:32Z,2020-03-09T09:22:09Z,,,"Towards to #15426

This PR will add versionchanged and versionadded label to parameters changed in v0.20

With @ellenkoenig

@WiMLDS_Berlin
@adrinjalali
@noatamir",843222
730,2020-01-24T20:06:22Z,2020-03-02T18:57:27Z,,,"#### Reference Issues/PRs

Followup of my quest to make vaex work with sklearn via NEP18 #14963 / https://github.com/vaexio/vaex/pull/415


#### What does this implement/fix? Explain your changes.

PolynomialFeatures uses `np.empty`, if it uses np.empty_like, I can intercept it via NEP18, so that vaex can use this transformer.

#### Any other comments?

I could push more of these changes on this branch, but I first want to see if this is what people like to see or not.",843222
731,2020-01-24T19:06:40Z,2020-03-02T18:57:28Z,,,Fixes parts of #15761.,843222
732,2020-01-24T17:25:09Z,2020-03-02T18:57:29Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs

Implements changes pursuant to Issue #16171

#### What does this implement/fix? Explain your changes.

MeanShift predictions are made via MeanShift.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
733,2020-01-24T12:38:02Z,2020-02-11T16:01:26Z,,,"There are (at least ?) 2 ways to specify the compiler to build the project:
- CC=\<compiler\> python setup.py build_ext -i
- python setupy.py build_ext --compiler=\<compiler\> -i

In the second option, \<compiler\> must be one of the predefined compilers in distutils or in numpy.distutils. For example, to build with icc, it's `--compiler=intelem`.

Currently the checks before building sklearn (compile a test program and compile a test program using openmp) ignore the compiler specified through the second option.

I also updated `get_openmp_flag` to better handle icc on linux (although it accepts `-fopenmp`, it's recommended to use `-qopenmp`).

I temporarily modified a ci job, `Linux_Runs_pylatest_conda_mkl`, to build sklearn with icc (installed through intel oneAPI) to check it works as expected. The install of icc is actually pretty fast and the job is not longer than the other ones. We could keep a job where we build with icc, not necessarily this one. wdyt ?

@oleksandr-pavlyk I'd like your feedback on this. Especially, do you use the same command to build sklearn with icc ?",843222
734,2020-01-24T07:43:14Z,2020-03-02T18:57:31Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
735,2020-01-23T16:18:54Z,2020-03-02T18:57:32Z,,,"closes #5563
closes #9696
closes #16137 

Using estimator tags to avoid a deep copy of parameters when cloning.",843222
736,2020-01-23T00:51:23Z,2020-03-02T18:57:33Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs  Fixes #14851
<!--
Example: Fixes #14851 . See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Whitening transform in partial_fit() has integer overrun in calculation that causes NaN to be injected into the data.  See detail in the Issue.

#### Any other comments?  Patch as requested.  Also included is a small test case.


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
737,2020-01-21T10:22:25Z,2020-02-03T19:35:11Z,,,"#### Reference Issues/PRs

Fixes #16145

#### What does this implement/fix? Explain your changes.

Implements the Platt99 cross-validation procedure, also [adopted by libsvm](https://github.com/scikit-learn/scikit-learn/issues/16145#issuecomment-575986059), which is different from the ensemble-based one implemented in `CalibratedClassifierCV` until now.
As [suggested](https://github.com/scikit-learn/scikit-learn/issues/16145#issuecomment-575998124), I added an `ensemble` parameter to the class, so that both procedures are available.
By default it will use the ensemble method, so that existing code would run exactly the same.
By setting `ensemble=False` it uses the Platt99/libsvm cross-validation procedure.
",843222
738,2020-01-21T04:53:24Z,2020-03-02T18:57:34Z,,,"This PR adds support for Individual Conditional Expectation (ICE) plots

#### Reference Issues/PRs
Fixes #14126 

#### What does this implement/fix? Explain your changes.
##### API
Following how the partial dependence is implemented, this PR adds the following methods and classes to the public API
`sklearn.inspection.individual_conditional_expectation`,
`sklearn.inspection.plot_individual_conditional_expectation`,
`sklearn.inspection.IndividualConditionalExpectationDisplay`.

##### Implementation
Since partial dependence calculation is closely related to ICE, relevant common methods have been moved to and common code segments have been extracted into `_base.py`. 

##### Tests
Since PDP and ICE share quite a bit of code, most of the code in `_ice.py` is already covered by existing unit tests. Common unit tests have been moved to `test_plot_pdp_ice.py`. Additional unit testing has been added in `test_plot_ice.py` and `test_ice.py`. 

##### Examples and Doc
Examples on how to plot ICEs, centered ICEs and comparison of ICEs with PDPs have been added.

#### Any other comments?
Would appreciate reviews and suggestions.",843222
739,2020-01-20T19:33:57Z,2020-03-02T18:57:35Z,,,"Fixed documentation for _dbscan.py based on documentation guidelines https://scikit-learn.org/stable/developers/contributing.html#guidelines-for-writing-documentation
",843222
740,2020-01-15T04:18:54Z,2020-03-02T18:57:36Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
See #15725

#### What does this implement/fix? Explain your changes.
This implements version of OPTICS algorithm that allow to chose which heap should be used to manage candidates. This change require small change if API (adding parameter with heap name).
I think it should be discussed.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
741,2020-01-15T02:34:39Z,2020-03-02T18:57:36Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--

-->
See issue #12394

#### What does this implement/fix? Explain your changes.
This implement settings weight coefficients in OPTICS algorithm. Changes are similar to solution in DBSCAN.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
742,2020-01-12T05:14:31Z,2020-03-02T18:57:37Z,,,Fixes: #16097 ,843222
743,2020-01-11T21:42:31Z,2020-01-14T21:32:19Z,,,"Hi there! 

@amueller Here is Neuraxle, as we've talked about. 

Note that I ticked ""Allow edits from maintainers"" in case I was asking to merge to the wrong branch (it's my first PR here in sklearn).

Thanks! ",843222
744,2020-01-10T06:50:48Z,2020-03-02T18:57:38Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Addresses #15808 for MDS case, which was also mentioned earlier in #2887. Adds tranform method on manifold.MDS learner.

#### What does this implement/fix? Explain your changes.
I've added a transform method on the manifold.MDS learner, for embedding of out-of-sample points from a pre-fitted model. The method maintains the exact embedding of the original points, and finds an optimal fit for the additional points, as in 

        ""The Out-of-Sample Problem for Classical Multidimensional Scaling""
        Trosset, M., Priebe, C. Computational Statistics and Data Analysis
        52 (2008)

[http://www.cis.jhu.edu/~parky/CEP-Publications/TP-CSDA2008.pdf](url)

#### Any other comments?
The method relies on an optimization in the Frobenius norm, similar to the guttman transform used to minimize stress in smacof(). I chose not to modify that code, but to invoke a minimizer directly in the method.

The optimization leads to an alternative to the smacof/fit method, that seems to be more accurate, but a bit slower( 30 ms vs 12 ms for a typical 5x5 problem). I'll do some testing to see if there is additional use.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
745,2020-01-09T21:22:48Z,2020-03-02T18:57:39Z,,,"I've not benchmarked yet. This should reduce memory requirements when fetching from OpenML.

This no longer explicitly closes the open URL handler, since it is required until the ARFF has been completely read. We could return the stream object from _download_data_arff if we want to close it.",843222
746,2020-01-09T21:05:21Z,2020-03-02T18:57:40Z,,,"The current implementation of Elkan's algorithms had several issues that negatively affected performance. This fixes several of them:

* lower_bounds were not always initialized, when left at 0, this would require additional distance computations, making the algorithm less effective
* points were reassigned first, then the means updated; but they were already assigned in the first iteration. So the entire pass in the first iteration was wasted.
* If no more cluster assignments change, the algorithm can terminate. This required changing some of the unit tests because the ""full"" algorithm does one unnecessary additional iteration, and hence the test that both need the *same* number of iterations fail. Here, the ""full"" algorithm needs to be improved, then the unit tests can be adjusted for it, too.
* When not converged, another additional assignment step was executed at the end.
* `verbose=True` causes expensive computations of inertia. Now only a bound for inertia is reported which is much cheaper. 
* `bounds_tight` is reduced from an array of size N to a single boolean, so this saves O(N) memory, too.

I also fix the iteration number reported in verbose mode, to begin with 1.",843222
747,2020-01-09T17:02:32Z,2020-02-11T07:08:44Z,,,"Taking a stab at proposal 4 of the sample prop proposal (https://github.com/scikit-learn/enhancement_proposals/pull/16).

Clear WIP, just putting it here for now for us to see how it'd look.

The first change I've made compared to the proposal is that the dict
storing the required props, includes a key as which method requires
which props. This way score and fit and predict can all have different
requirements.",843222
748,2020-01-09T11:22:00Z,2020-03-02T18:57:41Z,,,"closes #16016

Add a `n_jobs` parameter to set the number of threads (OpenMP threads) to be used when finding split. By default, we limit to 16 threads to avoid oversubscription issue.",843222
749,2020-01-09T06:52:07Z,2020-03-26T06:59:43Z,,,"#### Reference Issues/PRs
Fixes #15272

#### What does this implement/fix? Explain your changes.
Adds implementation of Multidimensional scaling (MDS), which uses Singular Value Decompostion (SVD) method. SVD works much faster and far more accurate for Euclidean matrixes than SMACOF algorithm (current implementation of MDS in sklearn/manifold module). 

#### Any other comments?
I'm putting simple benchmark script and it's results in comment (not sure if it's best practice).
",843222
750,2020-01-09T06:46:50Z,2020-03-02T18:57:42Z,,,"#### Reference Issues/PRs
Partially adresses: #15601


#### What does this implement/fix? Explain your changes.
Method partial_fit in StandardScaler can be used multiple times to incrementally update mean and variance, but there was no proper method to do it with sample_weights. This PR introduce _incremental_weighted_mean_and_var, which does the exact thing we want. 

#### Any other comments?
1. Basic equations I used can be found [here](https://fanf2.user.srcf.net/hermes/doc/antiforgery/stats.pdf), but I haven't found any (free) papers with derivation leading to batch version of them. It's basic math, but code may seem confusing without that sort of explanation. If it's really necessary I can provide brief paper.
2. NaNs handling is somehow inelegant, but I haven't found any 'weighted' method similar to np.nanvar/np.nanmean.",843222
751,2020-01-08T21:27:26Z,2020-03-10T21:15:33Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->
This PR adds `plot_decision_boundary` to the inspection module.

#### What does this implement/fix? Explain your changes.
Many examples were touched because they can take advantage of the new function.


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
752,2020-01-08T00:31:09Z,2020-03-02T18:57:44Z,,,"

#### Reference Issues/PRs
Fixes #16028

#### What does this implement/fix? Explain your changes.
Raises an error when CategoricalNB encounters new categories present during testing but never seen during training.

#### Any other comments?

",843222
753,2020-01-06T18:08:01Z,2020-03-02T18:57:45Z,,,"required merging #14848 first 

@agramfort pointed out that Ridge*CV should pick up the most regularized model in case of a tie to be consistent with LassoCV as well.

This PR intends to solve this issue by sorting the `alphas` in descending order before to perform the search.",843222
754,2020-01-03T19:30:04Z,2020-03-10T21:48:19Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Resolves https://github.com/scikit-learn/scikit-learn/issues/12153
Supersedes https://github.com/scikit-learn/scikit-learn/pull/13833 

#### What does this implement/fix? Explain your changes.
This PR adds the infrequent categories to `OneHotEncoder` by:

1. Adds `return_counts` to `_encode` to return the counts of the categories.
2. Adjust `_BaseEncoder._fit` and `_BaseEncoder._transform` to accept callables, which can adjust the processing of categories and unknown categories.
3. Deprecates 'ignore' in favor of 'auto'. 'auto' behaves like 'ignore' when there are no infrequent categories in training. If there are infrequent categories in training, then unknown categories are considered infrequent.
4. For the inverse transform and feature names, the infrequent category with the highest cardinality will be used.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
755,2020-01-03T05:56:17Z,2020-01-28T16:11:31Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

#### What does this implement/fix? Explain your changes.
Added support for all kernels in the ""kernel cookbook"" [https://www.cs.toronto.edu/~duvenaud/cookbook/](https://www.cs.toronto.edu/~duvenaud/cookbook/) 
except the low-rank projection (could be added, superseded by variational methods such as implemented in GPy, which would require additional likelihood functions).

Especially note the use of `ProjectionKernel` with sets of indicator variables, (i.e. one-hot-encoded) for use with categorical variables.  `ProjectionKernel` is comparable in utility to GPy's `active_dims`.  Typically one constructs X from continuous variables concatenated with one or more sets of dummy-coded columns.  constructs an RBF kernel over the projection onto each dummy-coded set, constructs any desired kernel over the projection onto the continuous variables, and these may then be combined using Sum, Product, etc.  

For example if X has 3 continuous variables concatenated with three columns output by one-hot encoding, using a RationalQuadratic kernel over the continuous variables and defaulting to RBF over the encoded categorical variable:
`k = ProjectionKernel(columns=[0,1,2], name='continuous', kernel=RationalQuadratic()) * ProjectionKernel(columns=[3,4,5], name='categorical')`
(The `name` parameter is used to label the parameters.)

`CompoundKernel` is extended to include `DirectSum` and `Tensor` (DirectProduct) using `reduce`.
- `CompoundKernel` was not fully implemented; the missing methods have now been added.
- fixed a bug in `set_param` referencing undefined `.k1`
- added `.n_dims`
- added support for `deep` in `get_param`

#### Any other comments?
- No unit tests at present.
- Comments 
-- are not yet complete,
-- do not yet follow standards everywhere,
-- some are out of date.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
756,2019-12-31T05:02:46Z,2020-03-02T18:57:46Z,,,"This is a quick and dirty fix for my use case, but looking at the astropy decorator I suspect there may be some corner cases that still doesn't work.


#### Reference Issues/PRs

closes #15994 



",843222
757,2019-12-30T09:09:35Z,2020-03-19T17:43:41Z,,,"#### Reference Issues/PRs
Fixes #13605
Fixes #11943
Fixes #11770

#### What does this implement/fix? Explain your changes.

I think the way the docs are worded has caused confusion about the role of the alpha parameter in Gaussian Process regression. 

**In brief:**

alpha is there to ensure that the kernel matrix is positive definite, so that it can be inverted without numerical error.

It does not, however, represent measurement noise variance. For problems with measurement noise, WhiteKernel should be used instead. 

**In detail:** 

alpha is used to add a 'ridge' to the kernel matrix. The ridge ensures that the columns of the kernel matrix are linearly independent which guarantees that the kernel matrix is full rank. In the current version of the code, the ridge is added here:

`K = self.kernel_(self.X_train_)`
`K[np.diag_indices_from(K)] += self.alpha`

The cholesky decomposition is then used to find the inverse of K when predictions are made, here:

`L_inv = solve_triangular(self.L_.T, np.eye(self.L_.shape[0]))`
`self._K_inv = L_inv.dot(L_inv.T)`

alpha does not, however, represent 'measurement noise'. If we have a problem with noise (say it has variance sigma^2), training inputs X and we are predicting at a new input x*, the variance of our prediction (y*) should be given by

Var[y* | x*] =  k(x*, x*) + sigma^2 - k^T(X, x*) (K + I \sigma^2)^{-1} k(X, x*) 

(where K is kernel matrix). In the code, however, the predictive variance is 

Var[y* | x*] =  k(x*, x*) - k^T(X, x*) (K + I alpha)^{-1} k(X, x*) 

which is implemented by the code:

`y_var = self.kernel_.diag(X)`
`y_var -= np.einsum(""ij,ij->i"", np.dot(K_trans, self._K_inv), K_trans)`

so alpha doesn't represent measurement noise variance, it is simply there to help us find the inverse of K. If the user wants to address a problem with (Gaussian) noise, they should use WhiteKernel. 

Hopefully that's clear... 

#### Any other comments?

One more thing actually, there's currently the option that alpha can be entered as an array so that the 'observation uncertainty' can change over the input x. To address these sorts of problems properly we would usually have to infer how the noise variance changes with x. It is possible to do this with a 2nd GP - this approach is usually called a Heteroscedastic GP. As the current implementation is not Heteroscedstic (as far as I'm aware anyway), we should perhaps disable this feature in the future. Anyway, one PR at a time! 
",843222
758,2019-12-28T16:41:24Z,2020-03-02T18:57:48Z,,,"I was trying out some models on the 20 newsgroup dataset by using the `fetch_20newsgroups` method, but was geting some blank samples, which was causing my preprocessing and models to fail. Then I figured out that the `remove` option may remove all the content of some samples, as can be seen bellow. So I'm proposing to filter out those empty samples after removing the metadata to avoid silently generate blank samples.

```python
from sklearn.datasets import fetch_20newsgroups

full = fetch_20newsgroups(subset='train')
removed = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))

print(len(full.data[155])) # => 849
print(len(removed.data[155])) # => 0
```",843222
759,2019-12-28T11:05:46Z,2020-02-05T16:15:14Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Corrected the 'fac` multiplier in the case of between variance scaling. It really doesn't affect the singular vectors but it should affect the singular values.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
760,2019-12-28T09:27:24Z,2020-03-02T18:57:49Z,,,"Reference Issues/PRs
Note: Since my last pull request  for this feature was too old, I have redone it.
I would love if someone can guide me so I can finish this feature.

What does this implement/fix? Explain your changes.
This implements a preference_percentile parameter for the affinity propagation clustering module.
It gives control over the number of classes regardless of the size of the similarities.

Any other comments?
The authors of Affinity propagation suggest using a function to find the min and max of the similarities in order to compute the range of the preference parameter and thus controlling the number of clusters. implementing the preference_percentile allowing to change the preference variable within the range, without knowing computing the similarities matrix outside of the affinity propagation module.

",843222
761,2019-12-24T06:14:25Z,2020-03-02T18:57:49Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->



#### What does this implement/fix? Explain your changes.
We found the heated point of the svm workflow was in the kernel function with the help of perf and program instrumentation technique. The further investigation showed that the kernels heavily used a certain function named dot and a similar part in k_function. The dot function mainly takes two arrays and sums up the in-place multiplication value. For acceleration, we use avx512 instructions to replace the old one like FMA (fused multiply add) instruction and add data alignment part for profiling the avx512 code. The whole acceleration is around 40% on our CLX machine on MLpack benchmark. The detail configuration is as follows.
- CPU, 2 x Intel Xeon Platinum 8280L @ 4.00GHz
- MLpack, https://github.com/mlpack/benchmarks
         command: python3 run.py -l SCIKIT -m SVM -c svm.yaml
         datasets: ['datasets/webpage_train.csv', 'datasets/webpage_test.csv', 'datasets/webpage_labels.csv']

",843222
762,2019-12-22T20:23:37Z,2020-02-05T21:16:46Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #15836

#### What does this implement/fix? Explain your changes.
Ensures that `sample_weight` is applied to the output of the `loss_function` when determining inliers/outliers at each iteration.

#### Any other comments?
I'm not too sure of the best way to add a non-regression test for this behavior. I originally tried fitting two RANSAC estimators with/without weights and then compared their output coefficients (which were indeed different) but there was no way to know that the difference was from `sample_weight` being used during the underlying estimators' `.fit()` or if it was because of `sample_weight` being applied to the output of the `loss_function`.

Would it be useful to run an example with/without weights, checking the resulting `.inlier_mask_` (or something similar), and then writing a test that asserts the `.inlier_mask_` is the same?

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
763,2019-12-22T05:15:36Z,2020-03-02T18:57:52Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #12489

#### What does this implement/fix? Explain your changes.

Calling get_precision in PCA uses a division by noise variance. When noise variance is 0, this causes a division by zero and raises an error. Noise variance is 0 when either n_components == n_features < n_samples or n_components == n_samples < n_features. The current code handled the first case, but not the second case. Checking for self.n_components == min(n_samples, n_features) solves the issue.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
764,2019-12-19T22:18:49Z,2020-03-02T18:57:52Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Forest ensembles take the mean of class weights from each tree, which may sometimes be problematic. For example, in a trained forest at prediction time, the input sample falls into a leaf with 5 samples from the training set in the first tree while it falls into a leaf with 5000 samples from the training set in the second tree. With the current implementations, this difference in sample size from the training examples are not taken into consideration when making predictions. The method **predict_proba_with_sample_weight** will allow all forest ensembles to predict class probabilities with each tree's vote weighted by how many times it's seen similar data points in the training set (number of samples in the leaf the input sample falls into). Users can also call the predict function in classification problems with sample_weight set to True, this will call the function **predict_proba_with_sample_weight** in place of **predict_proba**.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
765,2019-12-19T14:47:58Z,2020-03-02T18:57:53Z,,,"Rewriting implementation to remove redundant calculations.
Old implementation:
1. R = R + w_ii * X[:,ii]
2. tmp = X[:,ii] dot R
3. f(tmp,...)
4. R = R - w[ii] * X[:,ii]

New implementation:
substitute R from 1 into 2 ->
tmp = X[:,ii] dot (R + w_ii * X[:,ii]) 
tmp = X[:,ii] dot R + w_ii * X[:,ii] dot X[:,ii]
2. tmp = X[:,ii] dot R + w_ii * norm_cols_X[ii]
Then to update R:
4. R = R + (w_ii - w[ii]) * X[:,ii]

This removes step one, and rewrites step 2 and 4, improving loop speed.
The method here is probably also extendable to the other 3 functions.

Sadly my python skills are not good enough to build and test this, I did however test this in C++, so everything should work.
Help with building, testing, and extending is very much appreciated.

Tasklist:
- [ ] Build and validate results.
- [ ] Extend to sparse, gram, and multi_task",843222
766,2019-12-18T23:39:40Z,2020-03-02T18:57:54Z,,,"#### Reference Issues/PRs
This pull request adds another named kernel option, `sparse-rbf` for `semi_supervised.LabelSpreading`.

Loosely related to #15868.

#### Rationale
This kernel provides RBF weights for only the k-nearest-neighbors of each point.

Currently, the only kernel options are `knn` and `rbf` - but for a large dataset, the dense `rbf` kernel option is not feasible (for `N` items, we must compute a dense `NxN` matrix). For any non-trivial dataset, computing the dense kernel matrix will be infeasible, so my purpose here is to give a kernel that can perform better than `knn`, and still be feasible for a large dataset. 

The intuition is that a weighted adjacency matrix gives more information about the graph structure of our dataset, and therefore with appropriate parameter tuning, such a kernel should perform better than a binary adjacency matrix. Furthermore, filling with the RBF weights is cheap, once we have found the k-nearest-neighbors, so the additional runtime cost is minimal.

In particular, I believe that the performance difference will be more clear for datasets with more ""difficult"" structure; therefore I included CIFAR10 as an option for the example script (the easiest way I know to obtain this is via https://pytorch.org/docs/stable/torchvision/datasets.html#cifar), though I have not yet had time to experiment with a significant fraction of that dataset.

It might also be useful to try this on some toy datasets like https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html, but I have not experimented with this yet.

#### Notes and Questions
I included tests to show:
1. this kernel ""works"" (the learned classifier has high accuracy)
2. the weights found using `sparse-rbf` are indeed the same as the top-k entries of the kernel that you get using the dense `rbf` option

Since I am making a claim/intuition about performance, I also included an example that does a hyperparameter grid search for the two available sparse kernels (`knn`, already provided, and the new `sparse-rbf`), and then uses the optimal parameters across a range of % supervision to compare:

1. transductive accuracy (guessing labels for the unlabeled training data)
2. inductive accuracy (labeling test data)
3. runtime

I'm opening this PR to get some feedback on the following items:

1. Does this kernel seem helpful, and could I make it more helpful somehow?
2. Is the example I provided clear enough, and would other examples be useful to show the potential benefit of this kernel?
3. For sklearn examples in general, I'm not sure (practically speaking) how to get the results to be visible as embedded plots, so is there something I should do to structure my example appropriately?
4. Are there other experiments/examples that would be useful for evaluating this kernel that folks would like to see?

#### Caveats about the hyperparameter search
- The search is done using only the **inductive accuracy** because of the API used by `GridSearchCV`. It might be possible (with some headache) to run the grid search using transductive accuracy, but this also might not matter.
- The search is done at a fixed % supervision, again because of the API for `GridSearchCV`.
- Notice that the exact hyperparameters found depend on what dataset and what fraction of that dataset is used for the grid search.

#### Example results

Here are the results of using 20% of MNIST, with the fixed parameters included at the bottom of the example script.:
![sparse_kernel_comparison](https://user-images.githubusercontent.com/29764926/71132447-79905f00-21c5-11ea-9e7d-f3dc9b0abde7.png)


Please let me know if you need more info.
Thanks!
",843222
767,2019-12-17T04:03:19Z,2020-03-02T18:57:55Z,,,"Fix for #13478

This allows the user to extract predictions or prediction probabilities using 'return_predictions' argument for cross_validate. The argument takes 'predict_proba' or 'predict' as a string to return the predictions.

The approach has been to have `_fit_and_score` run inference on the cross-validation split and return it with the indices. Once all the indices and predictions have been returned to the `cross_validate` function, they are ordered according to the dataset and returned.

```
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate
from sklearn.datasets._samples_generator import make_classification

n_samples = 25
n_classes = 2
clf = LogisticRegression(random_state=0)
X, y = make_classification(n_samples=n_samples, n_classes=n_classes)

ret = cross_validate(clf, X, y, return_train_score=False,
                     return_estimator=True, return_predictions='predict')
ret['predictions']
>>> array([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
       0, 1, 1])
```",843222
768,2019-12-17T01:54:50Z,2020-03-02T18:57:55Z,,,"#### What does this implement/fix? Explain your changes.
Previously, classes with methods decorated with `if_delegate_has_method` still had their instances display the method via `dir` and code auto-completion even when the delegate did _not_ have the method. This uses `__set_name__` to dynamically adjust `__dir__` for a class instance to conceal methods that would throw an `AttributeError` due to a delegate not implementing the method.

#### Any other comments?
`__set_name__` was introduced in Python3.6, but if Scikit-Learn is installed with Python3.5, the `__set_name__` protocol will not be invoked. This should not be an issue other than this pull request not affecting builds under that version.

```python
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.neighbors import KNeighborsClassifier
>>> pipe = make_pipeline(KNeighborsClassifier())
>>> [attr for attr in dir(pipe) if not attr.startswith('_')]
['classes_',
 'fit',
 'fit_transform',
 'get_params',
 'inverse_transform',
 'memory',
 'named_steps',
 'predict',
 'predict_proba',
 'score',
 'set_params',
 'steps',
 'transform',
 'verbose']
```
Note for convenience, the methods in `Pipeline` that are decorated with `if_delegate_has_method` are:
- `predict`
- `fit_predict`
- `predict_proba`
- `decision_function`
- `score_samples`
- `predict_log_proba`
- `score`

#### TODO
- [x] Not all pipelines pass due to Deprecation errors and Mount errors beyond my control:
    - __Linux pylatest_pip_openblas_pandas__: all of these are due to SciPy `scipy.rand` deprecation errors
    - __Windows py37_conda_mkl__: all of these are due to `Error in atexit._run_exitfuncs:  Traceback (most recent call last):    File ""c:\miniconda\envs\testvenv\lib\ntpath.py"", line 562, in relpath      path_drive, start_drive))  ValueError: path is on mount 'D:', start on mount 'c:'`
- [ ] Increase code coverage:
    - I did add extra unit tests to address the new `__dir__`, but don't know how to address the insufficient codecov since the red lines are applied to code that I directly emulated from the existing codebase.
",843222
769,2019-12-13T14:20:29Z,2020-03-02T18:57:56Z,,,"Allow passing predicted labels `y_pred` to `plot_confusion_matrix` that will be used instead of `estimator` and `X`. In order to maintain backwards compatibility, `y_pred` is added as an optional keyword argument.
When both `estimator` and `X` aswell as `y_pred` are passed, a ValueError is raised since it is unclear which of both should be used.

For the discussion see #15880 ",843222
770,2019-12-10T21:43:23Z,2020-03-02T18:57:57Z,,,"Setting tighter tolerance (`tol=1e-6`) allows the test to pass in daal4py, where computation of logistic/cross_entropy  loss and its gradient is performed using Intel DAAL.

With current test:

```
        for est_cv_3, est_cv_5 in zip(stacker_cv_3.estimators_,
                                      stacker_cv_5.estimators_):
>           assert_allclose(est_cv_3.coef_, est_cv_5.coef_)
E           AssertionError: 
E           Not equal to tolerance rtol=1e-07, atol=0
E           
E           Mismatch: 100%
E           Max absolute difference: 0.53488994
E           Max relative difference: 2.78663169
E            x: array([[ 1.330649,  0.023872,  0.128831, -0.002925, -0.053261, -0.241014,
E                   -0.337852, -0.144228, -0.074394, -0.014633,  0.055071,  0.642667,
E                    0.242799, -0.109252, -0.004939, -0.048658, -0.069445, -0.018589,...
E            y: array([[ 1.839001,  0.088849, -0.072108,  0.002609, -0.076382, -0.341382,
E                   -0.477864, -0.204314, -0.106559, -0.021162,  0.077972,  0.911786,
E                    0.38237 , -0.115388, -0.007024, -0.068589, -0.097992, -0.026255,...

miniconda/envs/bld/lib/python3.6/site-packages/sklearn/ensemble/tests/test_stacking.py:474: AssertionError
```

as run on Travis CI in daal4py.",843222
771,2019-12-10T21:11:04Z,2020-03-02T18:57:57Z,,,"This PR tweaks ``sklearn/svm/tests/test_svm.py::test_svm_equivalence_sample_weight_C`` to specify tighter SVC tolerance parameter, specifically `tol=1e-6`, as well as absolute tolerance in `assert_allclose` to `atol=1e-5`.

This change fixes the failure of the test, when SVC class is patched to use DAAL in daal4py project.

```
_____________________ test_svm_equivalence_sample_weight_C _____________________

    def test_svm_equivalence_sample_weight_C():
        # test that rescaling all samples is the same as changing C
        clf = svm.SVC()
        clf.fit(X, Y)
        dual_coef_no_weight = clf.dual_coef_
        clf.set_params(C=100)
        clf.fit(X, Y, sample_weight=np.repeat(0.01, len(X)))
>       assert_allclose(dual_coef_no_weight, clf.dual_coef_)
E       AssertionError: 
E       Not equal to tolerance rtol=1e-07, atol=0
E       
E       Mismatch: 100%
E       Max absolute difference: 0.00413014
E       Max relative difference: 0.01060894
E        x: array([[-0.44591 , -0.393437, -0.445884,  0.391153,  0.447039,  0.447039]])
E        y: array([[-0.448086, -0.389307, -0.447859,  0.387865,  0.448694,  0.448693]])

miniconda/envs/bld/lib/python3.6/site-packages/sklearn/svm/tests/test_svm.py:493: AssertionError
```
",843222
772,2019-12-10T13:23:26Z,2020-03-02T18:57:58Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
closes #13998


#### What does this implement/fix? Explain your changes.

The predictions returned by `RidgeCV` are not in the original space but in the normalized space (mean removed and divided by the std. dev.). One has to always denormalize them to be able to compute a score. This PR fixes this issue.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
773,2019-12-06T21:05:28Z,2020-03-02T18:57:59Z,,,"Modify `silhouette_score()`, `calinski_harabasz_score()` and `Davies-Bouldin Index()` to be compatible with GridSearchCv() when the numbers of clusters is equal to 1 or equal to n_samples.

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
This pull request is to fix this issues https://github.com/scikit-learn/scikit-learn/issues/15717


#### What does this implement/fix? Explain your changes.
For `silhouette_score(X, y)`: instead of raise an `ValueError` when the len(classes_) = 1 or len(X), I return the min possible value = -1.

For `calinski_harabasz_score(X, y)`:  instead of raise an `ValueError` when the len(classes_) = 1 or len(X), I return the min possible value = 0.

For `Davies-Bouldin Index()`: instead of raise an `ValueError` when the len(classes_) = 1 or len(X), I return the max possible value = np.inf. This scorer does not follow the convention that higher return values are better than lower return values used along scikit-learn. It follows the opposite for that I return np.inf (could be fixed to a very big value)

I change few lines in sklearn/metrics/clusters/tests/test_unsupervised.py and passed all the tests.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
774,2019-12-04T14:30:04Z,2020-03-02T18:57:59Z,,,"I added a `vlines_` attribute to the PDP Display so that we can hide the deciles vertical lines. But maybe there's a way to access the lines via the `ax`? I couldn't find anything.

@thomasjpfan please LMK if it's worth it, and I'll add test or close",843222
775,2019-12-04T04:23:35Z,2020-03-02T18:58:00Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #15772


#### What does this implement/fix? Explain your changes.
Checks to see if the `ylabel` is present before writing ""Partial dependence"".

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
776,2019-12-03T02:03:53Z,2020-03-02T18:58:00Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes https://github.com/scikit-learn/scikit-learn/issues/15616
Fixes https://github.com/scikit-learn/scikit-learn/issues/15726

#### What does this implement/fix? Explain your changes.
Adds support for string dtypes in the `Encoders`.

#### Any other comments?
This PR opens up `'U'` dtypes to be encoded the same way as `'O'` dtypes.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
777,2019-11-30T10:29:27Z,2020-03-02T18:58:01Z,,,"#### Reference Issues/PRs
Fixes issue #14398

#### What does this implement/fix? Explain your changes.
By default, sklearn's GMM is initiated by k-means. When the features are of very different scales, this can lead to a poor choice of initial conditions for the GMM parameters and, as a result, a poor fit once trained. Have added:

- Automatic scaling of data before k-means is used to initiate GMM.

- Test where the GMM is trained on data where the features have very different scales.

#### Any other comments?

- For the test, data is not generated using the RandomData class as I couldn't see an obvious way of setting the covariance matrix to what we need it to be for this example. My code that generates samples isn't elegant but is hopefully readable.

- The test reflects the example raised in the original issue. 

- The test only looks at the ability of the GMM to estimate the means of the example, as this seemed sufficient. I could add a test of the covariance matrices if we think it's needed?

- Over 1000 runs, old code failed the test every time and new code passed every time.
",843222
778,2019-11-28T20:43:29Z,2020-03-02T18:58:02Z,,,"
#### Reference Issues/PRs

Resolves #15696 Generate the pairs of most confused classes in multiclass classification

#### What does this implement/fix? Explain your changes.

This pull request implements the function most_confused_classes in the metrics module. This function computes the classes have been confused the most often by the classifier. Applies for both binary and multiclass classification problems.
It returns a np.ndarray with each row containing: [class_1, class_2, count] where count denotes the number of times class_1 was misclassified as class_2.

*Example:*

    >>> from sklearn.metrics import most_confused_classes
    >>> y_true = [2, 0, 1, 2, 1, 2]
    >>> y_pred = [0, 0, 2, 0, 1, 1]
    >>> most_confused_classes(y_true, y_pred)
*will return:*
```
    array([[2, 0, 2],
           [2, 1, 1],
           [1, 2, 1]], dtype=int64)
```

#### Any other comments?

This function was suggested by @regstrtn in the issue linked earlier.
I found this issue and thought that it will be a good first issue for me to learn the scikit-learn contribution workflow, so I worked on it.
I don't know if this function has the potential to be used enough to make it to scikit-learn, but it was fun working on it!
Please let me know how I could potentially improve this.",843222
779,2019-11-28T11:56:21Z,2020-03-02T18:58:02Z,,,"#### Reference Issues/PRs
Fixes #15672.

#### What does this implement/fix? Explain your changes.
* Fix `RuntimeWarning: divide by zero encountered in true_divide` arising when the standard deviation of some of the random variables (regressors and target) is zero. The Pearson correlation coefficient is undefined for those cases. To solve this issue, a `ValueError` is raised.

* Ensure that the degrees of freedom used for the F-tests is at least one (for degrees of freedom less than or equal zero, `np.nan` values were returned for the F-scores and p-values). To solve this issue, a `ValueError` is raised.

* Filter `RuntimeWarning: divide by zero encountered in true_divide` when computing the F-scores between random variables with maximum correlation (either -1 or 1). In that cases, the returned F-scores and p-values are `np.inf` and `0`, respectively (that is correct).",843222
780,2019-11-28T03:49:20Z,2020-03-02T18:58:03Z,,,"#### Reference Issues/PRs
Please see my issue at https://github.com/scikit-learn/scikit-learn/issues/15731

#### What does this implement/fix? Explain your changes.
`sklearn.metrics.pairwise_distances` returns error when using `metric='seuclidean'` and input is not of type `np.double`. 

#### Any other comments?
This is my first contribution to scikit-learn.  I have read through the contributions page, but please let me know if I have done anything wrong.


",843222
781,2019-11-27T19:16:18Z,2020-03-02T18:58:04Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
See #12442 

#### What does this implement/fix? Explain your changes.
This add OPTICS benchmark. See #12442. I used heap to manage expansion candidates in OPTICS algorithm.

#### Any other comments?
On test with 10000 samples current OPTICS run about 15s. I think it is possible to improve this using heap. I will try to run test with larger number of samples.
EDIT: New code run few seconds faster. Locally on tests with 20000 samples previous code run 36s and new code 28s. Please note that running on different computers may get different results.
When profiling code I noticed that pairwise distance function takes a lot of time (about 1/3). I am not familiar with code of pairwise distance function. Could someone tell me if it is possible to speed up it?

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
782,2019-11-25T22:06:41Z,2020-03-02T18:58:04Z,,,"This, is a proposal for the addition of 2 methods to 'TimeSeriesSplit' class. They are both based on rolling windows. 

Examples were added in section 'Examples' of the class' docstring, so that you can examine how they both work.",843222
783,2019-11-24T05:28:24Z,2020-03-02T18:58:05Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This allows the user to create multi-labeled encodings for dict-like objects with classes and associated values. My use-case is encoding a histogram with string-like keys. MultiLabelBinarizer does not support dictionaries with fuzzy values, only discrete `{0,1}` sets. 

Examples:
```
    mlh = MultiLabelHistogram()
    y = [{1: 5.5, 2: -3.0}, {3: 999}]
    Y = np.array([[5.5, -3., 0.],
                  [0., 0., 999.]], dtype=np.float32)
    assert_allclose(mlh.fit_transform(y), Y)
```
```
    mlh = MultiLabelHistogram()
    y = [{'sci-fi': -2.0, 'thriller': 5.0}, {'comedy': 0.1}]
    Y = np.array([[0., -2., 5.],
                  [0.1, 0., 0.]], dtype=np.float32)
    assert_allclose(mlh.fit_transform(y), Y)
```

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
784,2019-11-21T23:32:32Z,2020-03-02T18:58:06Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Resolves https://github.com/scikit-learn/scikit-learn/issues/15581 (better)


#### What does this implement/fix? Explain your changes.
After a discussion with @tacaswell, it was concluded that adding attribute to the axes would be solution to figuring out

This allows use cases in #15581 to work:

```py
fig, ax = plt.subplots(figsize=(10, 6))
tree_disp = plot_partial_dependence(tree, X, [""LSTAT""], ax=ax)
mlp_disp = plot_partial_dependence(mlp, X, [""LSTAT""], ax=ax,
                                   line_kw={""c"": ""red""})
```

#### Any other comments?
Storing the display object in the axes so it can be referenced later is a small hack, but it improves the API quite a bit.

CC @tacaswell @amueller @NicolasHug @glemaitre @qinhanmin2014 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
785,2019-11-21T07:55:22Z,2020-03-02T18:58:07Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
####Reference Issues/PRs
FIxes #15684 


#### What does this implement/fix? Explain your changes.
Add return_centers parameter to make_blobs. 

#### Any other comments?
This can be useful for comparing with e.g. GaussianMixture.means_ or KMeans.cluster_centers_, when the centers are randomly generated by make_blobs

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
786,2019-11-21T00:34:30Z,2020-03-02T18:58:07Z,,,"Changing `OneVsRestClassifier`, `OneVsOneClassifier` and `OutputCodeClassifier` multiclass learning algorithms within *multiclass.py*, by replacing `n_jobs` parameter with keyworded, variable-length argument list, in order to allow any `Parallel` parameter to be passed, as well as support `parallel_backend` context manager.

`n_jobs` remains one of the possible parameters, but other ones can be added, including `max_nbytes`, which might be useful in order to avoid *ValueError* when dealing with a large training set processed by concurrently running jobs defined by `n_jobs > 0` or by `n_jobs = -1`.

More specifically, in parallel computing of large arrays with ""loky"" backend, [Parallel](https://joblib.readthedocs.io/en/latest/parallel.html#parallel-reference-documentation) sets a default 1-megabyte [threshold](https://joblib.readthedocs.io/en/latest/parallel.html#automated-array-to-memmap-conversion) on the size of arrays passed to the workers. Such parameter may not be enough for large arrays and could break jobs with exception **ValueError: UPDATEIFCOPY base is read-only**.
`Parallel` uses `max_nbytes` to control this threshold.

Through this fix, the multiclass classifiers will offer the optional possibility to customize the max size of arrays.

Fixes #6614

See also #4597

------------
Edited text and title, to reflect the support of `parallel_backend` context manager",843222
787,2019-11-20T07:24:24Z,2020-03-02T18:58:08Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
788,2019-11-18T11:52:19Z,2019-11-20T19:23:41Z,,,"Enable arm64, ppc64le (little endian power9), s390x (mainframe)",843222
789,2019-11-18T10:06:33Z,2020-03-02T18:58:09Z,,,"Fixes #4667 Fixes #4790 Fixes #13998 Fixes #15182 Fixes #15183

This PR focus on RidgeCV
- add best_score_ to RidgeCV, in order to write tests (#4667 #4790)
- do not store all cv values when store_cv_values == False (#15182 #15183 )
- when scoring = None
    - correct the errors (previously they are erroneously weighted)
    - correct the scores (previously they are erroneously weighted)
- when scoring != None
    - correct the predictions (previously they are erroneously weighted and scaled)
    - correct the scores (previously they are erroneously weighted and scaled)
    - handle multioutput correctly when calculating the scores

TODO in this PR: update the doc, update what's new

TODO: issues regarding RidgeClassifierCV
ping @glemaitre feel free to edit or push",843222
790,2019-11-15T19:33:38Z,2020-03-02T18:58:09Z,,,"Trying to fix issue: https://github.com/scikit-learn/scikit-learn/issues/15611

Lots of algorithms that rely on randomization offer an argument `random_state` through which it's possible to pass them RNG seeds that would ensure reproducible results. The `IterativeImputer` class will take some regressor/classifier and set their attribute `random_state` to a NumPy MT19937 object class if the object has such attribute, in such a way that this object is modified implicitly once the regressor/classifier to which it was assigned produces some random number.

This is fine for SciKit-Learn’s own classes, but other libraries which provide different regressors/classifiers which are SciKit-Learn-compatible might not be able to use such `RandomState` objects (e.g. if they generate random numbers in C++ with some method other than MT19937).

This PR addresses the issue by setting the `random_state` of the estimator to ~~a random integer, which is drawn from the imputer's `RandomState` object~~ the same value that was passed to the imputer, but does so only if it's not already set in the estimator. This way:
* It doesn't override the `random_state` which is passed to the constructor of the estimator used for imputation (if it was passed).
* Allows to use a wider array of SciKit-Learn-compatible objects from external libraries as estimators for `IterativeImputer`. Example:
```python
import numpy as np
from lightgbm import LGBMRegressor
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

np.random.seed(123)
X = np.random.normal(size = (100, 5))
X[np.random.randint(100, size = 10),
  np.random.randint(5,   size = 10)] = np.nan

imputer = IterativeImputer(LGBMRegressor(), random_state = 456)
X_full = imputer.fit_transform(X)
```",843222
791,2019-11-15T18:26:15Z,2020-03-02T18:58:10Z,,,"In the cases where none of the K neighbors of a given point have a label then normalizer equals 0 and generates divide by zero erros. This creates NaNs in the label distribution and blocks label propagation.
By replacing 0 values of the normalizer by 1 this problem is avoided.

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
792,2019-11-13T02:09:01Z,2020-03-02T18:58:12Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Partially addresses #14228
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Add documentation for `n_jobs` in `sklearn.cluster._spectral.SpectralClustering`.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
793,2019-11-10T06:13:53Z,2020-03-02T18:58:13Z,,,"Partially addresses #15556.

Right now only for l-bfgs.
I added a parameter for people to confirm that it behaves as expected and easier debugging, but I think we should not add a parameter and always do this.

Might be a bit late but honestly I would like to avoid users having another 6 month of convergence warnings whenever they fit logistic regression. Also the results are much more stable / better and faster.

To demonstrate the effect:
```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.linear_model._logistic import _logistic_loss, _logistic_loss_and_grad
import numpy as np

X, y = make_classification(n_samples=100, n_features=60, random_state=0)
X[:, 1] += 10000
X[:, 0] *= 10000

lr = LogisticRegression(tol=1e-8).fit(X, y) # convergence warning
lr_pre = LogisticRegression(tol=1e-8, precondition=True).fit(X, y) # no warning
lr1000 = LogisticRegression(max_iter=1000, tol=1e-8).fit(X, y) # no warning, still worse solution!

print(_logistic_loss(np.hstack([lr.coef_.ravel(), lr.intercept_]), X, 2 * y - 1, 1))
# 14.289
print(_logistic_loss(np.hstack([lr1000.coef_.ravel(), lr1000.intercept_]), X, 2 * y - 1, 1))
# 13.478
print(_logistic_loss(np.hstack([lr_pre.coef_.ravel(), lr_pre.intercept_]), X, 2 * y - 1, 1))
# 12.354
```
in other words: it converges faster and to a better solution.

Multinomial
```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
import scipy
from sklearn.linear_model._logistic import _multinomial_loss, _multinomial_loss_grad
import numpy as np
from sklearn.preprocessing import label_binarize

X, y = make_classification(n_samples=100, n_features=60, n_classes=3, random_state=0, n_informative=6)
sample_weight = np.ones(X.shape[0])
Y = label_binarize(y, [0, 1, 2])
X[:, 1] += 10000
X[:, 0] *= 10000

lr = LogisticRegression(max_iter=100).fit(X, y) # convergence warning
lr10000 = LogisticRegression(max_iter=10000).fit(X, y) # no convergence warning, still worse. 1000 is not enough.
lr_pre = LogisticRegression(precondition=True, max_iter=100).fit(X, y)
# convergence warning, though max_iter=200 would fix it

loss, p, w = _multinomial_loss(np.hstack([lr.coef_, lr.intercept_.reshape(-1, 1)]), X, Y, 1, sample_weight=sample_weight)
print(loss) # 31.075

loss_10000, p, w = _multinomial_loss(np.hstack([lr10000.coef_, lr10000.intercept_.reshape(-1, 1)]), X, Y, 1, sample_weight=sample_weight)
print(loss_10000)  # 18.972

loss_pre, p, w = _multinomial_loss(np.hstack([lr_pre.coef_, lr_pre.intercept_.reshape(-1, 1)]), X, Y, 1, sample_weight=sample_weight)
print(loss_pre) # 18.308
```
For multinomial, it still warns with the defaults, but produces a better result than master with 10000 iterations (in which case master doesn't warn).
So this doesn't remove all warnings, but makes the solutions much more stable, and allows solving by increasing ``max_iter`` within a reasonable range (in this example).

Changing to ``n_samples=10000`` the above script produces
```
max_iter=100: 7999.571 & warn
max_iter=10000: 7999.5717 & doesn't warn
max_iter=100, precondition=True: 7734.327 & warn
max_iter=200, precondition=True: 7733.976 & doesn't warn
```
",843222
794,2019-11-09T11:49:57Z,2020-03-02T18:58:14Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

Implements #15578

#### What does this implement?
Implements a new criterion for the `DecisionTreeClassifier` model. This criterion is called `absolute_error` and represents the absolute error rate of a node as defined in #15578.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
795,2019-11-06T22:49:02Z,2020-03-02T18:58:15Z,,,"#### Reference Issues/PRs
Make bugs in #15438 visible by introducing tests (with xfail).

#### What does this implement/fix? Explain your changes.
So far, this PR only adds tests for `Ridge` and `LinearRegression`.
Branch is based on #15530.

#### Any other comments?
Would be nice to have the generalizable part of those tests in `sklearn/utils/estimator_checks.py`:
- `sample_weight = 0` equivalent of removing that sample (data row). Edit: Ongoing effort in  #15015
- combination of sample weights and sparse input.",843222
796,2019-11-03T14:41:14Z,2020-03-02T18:58:15Z,,,"I am trying to resolve sameshl/rfe_percentage #14627  and resolve #14567 . Adding option for 'n_features_to_select` to be percentage.

I improved the docstrings of the parameter `n_features_to_select`  and the initialization of n_features_to_select in the ._fit()  function. 
",843222
797,2019-11-03T05:22:02Z,2020-03-02T18:58:16Z,,,"Fixes #15409

Add dtype parameter in KBinsDiscretizer. Returns transformed array with data_type as dtype.
Add tests to assert the returned data type.",843222
798,2019-11-03T03:00:30Z,2020-03-02T18:58:17Z,,,"#### Reference Issues/PRs
Closes #8642 

#### What does this implement/fix? Explain your changes.
It implements `__sizeof__` for Estimators as mentioned in https://github.com/scikit-learn/scikit-learn/issues/8642#issue-216883385.

TODO:

- [x] implement and test `__sizeof__`
- [ ] implement `__sizeof__` for trees",843222
799,2019-11-03T00:27:58Z,2020-03-02T18:58:17Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Adding Fall-out, Miss rate, specificity as metrics #5516
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Implemented a function which returns fpr, tpr, fnr, tnr. 

#### Any other comments?
Implementation of seperate functions for each metric which calls this function is still pending. 
Co-authored by @ddhar1 @samskruthireddy

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
800,2019-11-02T23:59:39Z,2019-11-03T00:26:37Z,,,"#### Reference Issues/PRs
This References the Issue: Ensure all attributes are documented #14312.


#### What does this implement/fix? Explain your changes.
This adds documentation and alphabetizes:
**Feature Selection**

- RFE
- RFECV

**SVM**

- NuSVR
- OneClassSVM
- SVR

**Neighbors**

- KNeighborsClassifier
- KNeighborsRegressor
- KNeighborsTransformer
- NearestNeighbors
- RadiusNeighborsClassifier
- RadiusNeighborsRegressor
- RadiusNeighgborsTransformer

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
801,2019-11-02T23:55:27Z,2020-03-02T18:58:18Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
--> Issue #14312 : updated RandomTreesEmbedding attributes 


#### What does this implement/fix? Explain your changes.
Added in the following attribute documentation for RandomTreesEmbedding:
- base_estimator_
- feature_importances_
- n_features_
- n_outputs_
- one_hot_encoder_

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
802,2019-11-02T23:15:03Z,2020-03-02T18:58:19Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
803,2019-11-02T23:03:52Z,2020-03-02T18:58:19Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #15440 for `RandomForestClassifier` and `RandomForestRegressor`


#### What does this implement/fix? Explain your changes.
Applies numpydoc validation to `RandomForestClassifier` and `RandomForestRegressor` docstrings

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
804,2019-11-02T22:45:19Z,2019-11-02T22:59:04Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
#14312
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
I added documentation for the attributes, random_weights_ and random_offset_ to the doc-string for SkewedChi2Sampler. 

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
805,2019-11-02T21:52:30Z,2020-03-02T18:58:20Z,,,"This adds the loss function attribute to Perceptron and alphabetizes all of the attributes.

Original problem:
as discussed in #13385 we need to ensure all attributes are documented.",843222
806,2019-11-02T21:17:59Z,2019-11-04T23:10:52Z,,,"
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #14124 

#### What does this implement/fix? Explain your changes.
The estimators are traversed in reversed order to be consistent with the value returned on arg-max of decision function.

#### Any other comments?
The reproducer in the original issue, now runs successfully.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
807,2019-11-02T19:48:34Z,2020-03-02T18:58:21Z,,,"#### What does this implement/fix? Explain your changes.
Format doc strings according to the NumPy documentation format for NearestNeighbors.",843222
808,2019-11-02T19:00:28Z,2020-03-02T18:58:22Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Address part of #15440.


#### What does this implement/fix? Explain your changes.
Address numpydoc validation to GradientBoostingClassifier and GradientBoostingRegressor

#### Any other comments?
No.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
809,2019-11-02T18:20:13Z,2020-03-02T18:58:22Z,,,"Fixes #10529. See also #10546.
Adds warning if CV test scores from function GridSearchCV are non-finite

Done during Man AHL London Hackathon

",843222
810,2019-11-02T16:59:02Z,2020-03-02T18:58:23Z,,,"#### Reference Issues/PRs
<!--

-->

Contribution to fix part of #15440


#### What does this implement/fix? Explain your changes.
<!--

-->
Ensuring LabelBinarizer methods pass NumPy docstring validation",843222
811,2019-11-02T14:54:32Z,2020-03-02T18:58:24Z,,,"*Reference Issues/PRs*
Contribution to fix part of #15440

*What does this implement/fix? Explain your changes.*
Ensuring `KNeighborsClassifier` methods pass NumPy docstring validation",843222
812,2019-11-02T12:32:24Z,2020-03-02T18:58:24Z,,,"Reference Issues/PRs
Contribution to fix part of #15440

What does this implement/fix? Explain your changes.
Ensuring TSNE methods pass NumPy doc validation",843222
813,2019-11-01T19:01:25Z,2020-03-02T18:58:25Z,,,"Related to PR #11635, this adds support for NaN/Inf values in univariate feature selectors. These feature selectors were removed from that PR due to the unresolved question: how do we properly determine the `allow_nan` tag when it depends on the underlying score function, which does not have tags to indicate its handling of NaN/Inf.",843222
814,2019-11-01T02:35:22Z,2020-03-02T18:58:26Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #12971 

#### What does this implement/fix? Explain your changes.
1. Merges in changes from #13229, which raises a ValueError if the token pattern regex has more than one capturing group and documents the general behavior of capturing groups in the token pattern regex
2. Improves the doc for token_pattern per requested changes on #13229
3. Changes the test spec from testing 0 capturing groups to testing 1 capturing group per requested changes on #13229
4. Modifies the test corpus and regexes for `test_countvectorizer_custom_token_pattern` so they represent a more ""realistic"" type of custom token pattern than used in #13229.  The previous test used an example from #12971 where the custom token pattern behaved the same as the default token pattern on that particular corpus.  My new test is an adapted version of those examples that uses some of the same selectors but also (a) behaves differently than the default and (b) gives an example of what you might actually use a capturing group for.  I have modified both regexes so they meet these requirements and are almost identical, but the first has 1 capturing group and the second has 2 capturing groups.

#### Any other comments?
Same as #13229, this does not address deprecation or backward compatibility

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
815,2019-11-01T00:30:26Z,2019-11-01T11:14:47Z,,,"#### Reference Issues/PRs

This is relevant to SLEP006.  I took code from #9566 and merged it with my code to also support routing to transform, scorers, and prediction functions within `Pipeline` and `*SearchCV`.

I also added my own hack to get feature properties routing and subsetting to work within `Pipeline` and `*SearchCV`.  It's a hack because the only solution I could currently see is to fix in the source code what the name of the feature properties kwarg is, but I needed it now for my own work...

#### Any other comments?

This is intended to be exploratory to see what can work and how designs look when implemented.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
816,2019-10-31T03:48:13Z,2020-03-02T18:58:27Z,,,"Seems that people often specify pos_label when y_true is str :)
```
import numpy as np
from sklearn.metrics import brier_score_loss
y_true = np.array([""neg"", ""pos"", ""pos"", ""neg""])
y_pred = np.array([0.8, 0.6, 0.4, 0.2])
brier_score_loss(y_true, y_pred)
```
```
TypeError                                 Traceback (most recent call last)
<ipython-input-1-0a4406be1adf> in <module>
      3 y_true = np.array([""neg"", ""pos"", ""pos"", ""neg""])
      4 y_pred = np.array([0.8, 0.6, 0.4, 0.2])
----> 5 brier_score_loss(y_true, y_pred)

d:\github\scikit-learn\sklearn\metrics\_classification.py in brier_score_loss(y_true, y_prob, sample_weight, pos_label)
   2487             pos_label = 1
   2488         else:
-> 2489             pos_label = y_true.max()
   2490     y_true = np.array(y_true == pos_label, int)
   2491     return np.average((y_true - y_prob) ** 2, weights=sample_weight)

D:\Anaconda3\envs\dev\lib\site-packages\numpy\core\_methods.py in _amax(a, axis, out, keepdims, initial)
     26 def _amax(a, axis=None, out=None, keepdims=False,
     27           initial=_NoValue):
---> 28     return umr_maximum(a, axis, None, out, keepdims, initial)
     29 
     30 def _amin(a, axis=None, out=None, keepdims=False,

TypeError: cannot perform reduce with flexible type
```",843222
817,2019-10-30T08:32:46Z,2020-03-02T18:58:28Z,,,"A patch for @GregoryMorse to benchmark.

Fixes #15388.",843222
818,2019-10-29T23:21:29Z,2019-10-29T23:21:29Z,,,"### What does this implement/fix? Explain your changes.

The example plotted the best model, but the code only worked if the best model found had a full covariance matrix. Some other parts, like the title for the plot of the best model, were also hard-coded. This change reduces the amount of fixed code to work in more cases.",843222
819,2019-10-29T18:28:02Z,2020-03-02T18:58:29Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes https://github.com/scikit-learn/scikit-learn/issues/14953
Related to https://github.com/scikit-learn/scikit-learn/pull/15050


#### What does this implement/fix? Explain your changes.
Adds categories='dtypes' option to OrdinalEncoder and OneHotEncoder. With this option enabled, the dtypes are remember during fit and checked during transform.

#### Any other comments?
1. Uses pip to install pandas on the python3.5 conda env to get a newer version of pandas.
2. A follow up to this PR would be to add missing value support for pandas dataframes.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->

CC @NicolasHug @jnothman @jorisvandenbossche @adrinjalali ",843222
820,2019-10-28T20:25:22Z,2019-10-30T21:36:38Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes https://github.com/scikit-learn/scikit-learn/issues/15122


#### What does this implement/fix? Explain your changes.
For reference: https://github.com/scikit-learn/scikit-learn/pull/9240#discussion_r130809777

Adding NullHandler based on https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
821,2019-10-28T20:03:58Z,2020-03-02T18:58:31Z,,,"Temporary fix for https://github.com/scikit-learn/scikit-learn/issues/14959

We ""gracefully"" error instead of providing wrong results when the variance of the transformed data is too small in the YeoJohnson transformer.

Not sure that needs a whatsnew?",843222
822,2019-10-25T12:06:43Z,2020-03-02T18:58:32Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Fixes #15312
#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
823,2019-10-23T23:57:29Z,2020-03-02T18:58:33Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

An implementation to auto adjust the number of clusters based on the paper of Tutorial on Spectral Clustering, 2007 Ulrike von Luxburg http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323. 

It may need massaging into the prefered style of this module, although the code serves its purpose with minimal interference to anything else in the algorithm. 


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
824,2019-10-16T05:54:52Z,2019-10-20T11:43:39Z,,,"This PR fixes issue ##15171

I have added a .github folder and a FUNDING.yml file to support a sponsor button. However, the PayPal link and any additional links should be updated/double checked (got the PayPal link from https://scikit-learn.org/) and updated as need. Additionally, the repo settings need to be updated in Github for this functionality to be added.",843222
825,2019-10-14T15:10:26Z,2020-03-19T06:19:05Z,,,"Toward #3846

this PR adds an example to `exceptions.DataDimensionalityWarning`
ping @adrinjalali",843222
826,2019-10-13T20:31:53Z,2020-03-18T22:47:01Z,,,"This implements a Stratified version of the GroupShuffleSplit and GroupKFold cross-validators.  Note that GroupKFold differs from StratifiedGroupKFold in that GroupKFold attempts to approximately balance the number of groups in each fold regardless of group class, whereas StratifiedGroupKFold attempts to stratify the group class percentages in each fold to be the same as that of the entire data.

There are two important points regarding the implementation logic:
1. All samples in each group are of the same class
2. Stratification is done on the group class level

This makes the logic straightforward and covers a lot of use cases (at least the ones I needed them for in my work). 

TODO:
1. Tests
",843222
827,2019-10-13T12:22:24Z,2020-03-02T18:58:35Z,,,"Fixes a bug with `SparseCoder.set_params`. One of the consequences
of this bug is that, for instance, grid searching on the dictionary has no effect.

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklistThe way that SparseCoder was initialized led to a bug so that
set_params on the dictionary had actually no effect. I added a test
that breaks on the current master, and the fix to make the test pass.

-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

The way that `SparseCoder` was initialized led to a bug so that
`set_params` on the dictionary had actually no effect. I added a test
that breaks on the current master, and the fix to make the test pass.

#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
828,2019-10-13T11:51:54Z,2020-03-02T19:51:57Z,,,"PR to add example to SparseCoder
see #3846",843222
829,2019-10-13T09:47:29Z,2020-03-02T18:58:37Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Adding tests as discussed in #15218.

#### What does this implement/fix? Explain your changes.

Add unit tests for `CheckingClassifier`, improve its docstring.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->

I would propose renaming the `foo_param` argument to something more
useful, e.g. `score_param` since it affects the `score` method. Since
`CheckingClassifier` is located in `_mocking.py`, such a change should
not break the public API.
",843222
830,2019-10-13T08:33:29Z,2020-02-05T16:16:31Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #11793
Closes https://github.com/scikit-learn/scikit-learn/pull/12387

#### What does this implement/fix? Explain your changes.

Add the parameter `metric_params` to `TSNE`, to be used as metric parameters in the pairwise distance computation and `NearestNeighbours` inside TSNE. This solves the missing V and VI parameters for Mahalanobis distance, that can be reproduced with

```python
from sklearn.datasets import load_boston
X = load_boston().data

from sklearn.manifold import TSNE
tsne = TSNE(verbose=1, 
            perplexity=40, 
            n_iter=250,learning_rate=50, 
            random_state=0,
            metric='mahalanobis')
tsne_results = tsne.fit_transform(X)
```
#### Any other comments?

Add tests that show that the solution fixes the issue and that the parameter `metric_params` is correctly used also with other metrics.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
831,2019-10-12T18:54:21Z,2019-10-13T20:39:38Z,,,"Adds a paragraph in the contributors documentation about the general guidelines to follow when taking over issues, as discussed in #13188",843222
832,2019-10-12T14:30:56Z,2020-03-02T18:58:38Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Partly addresses #12384

#### What does this implement/fix? Explain your changes.

This PR makes it possible to pass `fit_params` to the fit method of the
`CalibratedClassifierCV`, which are then routed to the underlying base
estimator.

*Note*: I implemented `predict_proba` on `CheckingClassifier` for the
new unit test to work.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->

While working on this issue, I discovered these lines, which may be problematic:

https://github.com/scikit-learn/scikit-learn/blob/86aea99159c944fae1f519ac61c8b8c484449130/sklearn/calibration.py#L168-L176

In fact, if the base estimator doesn't have `sample_weight` in
its signature, the `sample_weight` argument is not routed to
it. However, one could argue that it should still be routed to it
if `fit_params` are part of the signature.

This can be relevant, for instance, when the base estimator is
just a meta estimator that delegates all `fit_params` to the true
estimator. With the current implementation, `sample_weight` would
not be passed on.

",843222
833,2019-10-12T13:44:25Z,2020-03-02T17:59:37Z,,,"#### Reference Issues/PRs

Issue #3846

#### What does this implement/fix? Explain your changes.

Example for `multioutput.RegressorChain`

#### Any other comments?

The naming of this class is wrong, it should be `MultiLabelClassifierChain`, it is not a regression but a multi-label classification",843222
834,2019-10-12T13:25:36Z,2020-03-02T18:58:39Z,,,Added example for Chebyshev distance metric. See #3846.,843222
835,2019-10-12T13:20:24Z,2020-03-02T10:33:37Z,,,"#### Reference Issues/PRs

Relates to #3846 

#### What does this implement/fix? Explain your changes.

Adds an example to the docstring of ```multioutput.ClassifierChain```.

#### Any other comments?

The code example is a simplification of what is used here:
https://scikit-learn.org/stable/auto_examples/multioutput/plot_classifier_chain_yeast.html
",843222
836,2019-10-12T13:14:21Z,2020-03-02T18:58:40Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
 See #3846 

#### What does this implement/fix? Explain your changes.
Add an example for the OAS covariance example

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
837,2019-10-12T09:47:36Z,2020-03-02T18:58:41Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

#3846 

#### What does this implement/fix? Explain your changes.

Add examples to `mixture.GaussianMixture` and `mixture.BayesianGaussianMixture`

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
838,2019-10-12T09:26:31Z,2019-11-18T06:22:55Z,,,"Reaction to #15185 since Travis CI is not used anymore, I would recommend removing its config so it it does not confuse developers...",843222
839,2019-10-11T14:25:02Z,2020-03-02T18:58:43Z,,,"fixes #15182 

in master the `_RidgeGCV` stores the LOO predictions and the dual coefficients for all hyperparameters during `fit`, which can take a lot of memory. This PR only stores the best score and coefficients when `store_cv_values == False`",843222
840,2019-10-11T01:02:56Z,2020-03-02T18:58:44Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
See #2746. 

#### What does this implement/fix? Explain your changes.

Following the discussion in the above issue, this PR adds a `jitter` keyword argument to the `Lars` and `LassoLars` classes. The argument, defaulted to 0.0001, applies uniformly distributed noise, bounded by `jitter`, to the `y` variable when fitting. 

#### Any other comments?

Main question:
- By adding this random noise, we start to get failing tests in `test_least_angle.py` from differences in floating point estimates for model coefficients. One way to address this would be loosening the precision of the `assert_array_almost_equal()` statements. Is this how we'd like to proceed? 
- Does it make sense to set a `np.random.seed()` and ""lock in"" the noise so that the model gives predictable results once it's been imported, even if the user keeps instantiating and fitting it? I think no, but... I'm not sure.

Thanks to all the maintainers of sklearn! It's a great project. Thanks for the contributing guidelines also, those were very helpful. 

",843222
841,2019-10-10T17:11:40Z,2020-03-02T18:58:45Z,,,"This PR builds on top of #14300.

I plan to rebase the last 2 commits on top of #14300 from time to time.

TODO:

- [ ] Add tests
- [ ] Add documentation
- [ ] Add a helper plot_lorenz_curve function?",843222
842,2019-10-06T20:21:02Z,2020-03-02T18:58:45Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
I opened another PR #15111 to add tests, b/c I was having trouble forking this repository onto my local machine, was getting an error about file corruption, but I finally managed to get it forked. Probably better to go back to this PR now and close the other one out.

#### What does this implement/fix? Explain your changes.
This PR adds tests for the VotingRegressor class in the file sklearn/ensemble/tests/test_voting.py for the new voting logging verbose flag

#### Any other comments?
I still need to do some more local testing on my machine, but I wanted to get a WIP Pull request opened up with the Regressor tests on it. I will get to finishing this as soon as I'm able to.


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
843,2019-10-04T22:43:58Z,2020-03-02T18:58:46Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Enhancement #14713
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Trying to simplify the logic for selecting a solver in spectral_clustering.
The scheme for deciding which solver to use remains the same. The refactoring goal is not to change the algorithm for chosing the solver. Rather, it tries to make the code more clear.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
844,2019-10-04T20:25:34Z,2019-10-06T19:53:37Z,,,"#### Reference Issues/PRs
Fixes #15133, towards #3846

#### What does this implement/fix? Explain your changes.
- Rewrites the docstring summary to more fully explain how and when crossvalidation is used
for training and calibrating a classifier
- Adds two examples to demonstrate and compare usage any other cv option and cv=""prefit""

",843222
845,2019-10-04T02:17:28Z,2020-03-02T18:58:47Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes https://github.com/scikit-learn/scikit-learn/issues/15021


#### What does this implement/fix? Explain your changes.
1. Refactors `_fit_and_score` to return a dictionary.
2. Refactors `_check_multimetric_scoring` to only handle list of strings or a dictionary.
3. Because the return of the callable is only known after being called, `_check_fit_and_score_results` was added to normalize the output of parallelize `_fit_and_score`.
4. `_aggregate_list_of_dicts` is a helper that combines list of dicts.
5. Updates docs, introduction a third way to do multi-metric scoring.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
846,2019-09-28T13:55:06Z,2020-03-02T18:58:48Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

This change adds the functionality of handling new values in the OrdinalEncoder class.
It implements the handle_unknown variable on construction as the OneHotEncoder does.
Returns -1 when a new class is seen by the encoder.

#### Any other comments?

```python
import pandas as pd

df = pd.DataFrame(['ola', 'k', 'ase'])
df_test = pd.DataFrame(['ase', 'k', 'ase'])
df_test_unknown = pd.DataFrame(['tu', 'k', 'ases'])

from sklearn.preprocessing import OrdinalEncoder

encoder = OrdinalEncoder(handle_unknown='ignore')
encoder.fit(df)
print(f'Good shape: {encoder.transform(df_test)}')
print(f'Bad shape: {encoder.transform(df_test_unknown)}')
```

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
847,2019-09-27T13:59:51Z,2020-03-02T18:58:50Z,,,"
I implemented a multiprocessing option to make a speedup on MinCovDet estimator. This is a very simple approach where I used joblib to parallelize some for loops present in the code.
",843222
848,2019-09-26T17:19:15Z,2020-03-02T18:58:50Z,,,"The `get_memview_*` Cython helpers in KDTree and BallTree were used because at the time numpy/Cython didn't fully support memoryviews and now can be removed.

This was adapted from larsmans's refactoring PR in https://github.com/scikit-learn/scikit-learn/pull/4217",843222
849,2019-09-25T15:05:26Z,2020-03-18T14:04:00Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
For Issue #14614


#### What does this implement/fix? Explain your changes.
It adds precomputed feature to MissingImputer and implements on SimpleImputer 

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
850,2019-09-23T19:16:17Z,2020-03-02T18:58:52Z,,,"

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
none
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

removing extra tests, that raised the same errors

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->

<!-- Reviewable:start -->
---
This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/scikit-learn/scikit-learn/15069)
<!-- Reviewable:end -->
",843222
851,2019-09-21T23:23:52Z,2019-09-24T06:59:31Z,,,"#### Reference Issues/PRs
Based on Issue #15047 proposed by @amueller 

#### What does this implement/fix? Explain your changes.
As proposed by @amueller, I added support for multimetric_scoring using `GridSearchCV` to the given example

#### Any other comments?
~~Note:-
Because [predict is only available if refit=True or I believe specified](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.predict), I set `refit` to 'precision_score_macro'.
This is a purely personal decision and it could be changed to recall_score_macro as well.
It affects predict function for now.~~

It's set to refit=False
We calculate Best Params for a given scorer not based on the return value of the attribute, but rather by
1. Finding the Lowest Valued Index/Best Index by Test Scores for given scorer
2. Finding the Params at the Lowest Ranked Index/Best Index

Now that we have the Best Params for a scorer, we use this for:
1. Finding the SVC Estimator based on the Params
2. Using this estimator to perform predict function
3. Printing out the values

Reference Code at https://github.com/scikit-learn/scikit-learn/blob/89332d3e90e8643bef63bc05986cc922d7d718e5/sklearn/model_selection/_search.py#L726

I used the following source as a reference:- https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
852,2019-09-21T16:23:38Z,2020-03-02T18:58:53Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Resolves https://github.com/scikit-learn/scikit-learn/issues/14953

#### What does this implement/fix? Explain your changes.
Gets the `dtypes` in `_fit` and checks only when it has ""categories"".

#### Any other comments?
The fun part begins when we try to get the encoders to respect pandas categories (when they are numerical) :)

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
853,2019-09-20T16:45:12Z,2020-03-02T18:58:54Z,,,"replaced the use of `assert_raises`, `assert_raises_regex`, and `assert_raise_message` with `pytest.raises` context manager.

related to #14216",843222
854,2019-09-19T16:35:12Z,2020-03-02T18:58:54Z,,,"### Reference Issues/PRs

Fixes #14034 

#### What does this implement/fix? Explain your changes.

Instead of saving a RandomState instance, which is mutated after use, an integer is saved. This ensures. A small test is added to check that the random seed is the same or different (depending on the parameters).

#### Any other comments?

There are currently 6 failing tests:
* 4 are about performance, which is worse in settings with no warm starting
* 2 are hard-coded tests

I also fixed the random seed for the random mask (out-of-bag samples, [relevant lines](https://github.com/johannfaouzi/scikit-learn/blob/5012dac2cc65b21518cbbfb7453a1150e61a0732/sklearn/ensemble/gradient_boosting.py#L1587-L1588)) (I was a bit scared at looking at the Cython function). I don't know if it is necessary or if it is a mistake.

Feedback is welcomed.
",843222
855,2019-09-18T18:31:51Z,2020-03-02T18:58:55Z,,,"Adding a test following #10873.
This is a pretty simple sample_weight test that says that a weight of 0 is equivalent to not having the samples.
I think every failure here should be considered a bug. This is:

- [ ] SVR
- [ ] SGDRegressor
- [ ] RANSACRegressor
- [ ] OneClassSVM
- [ ] NuSVR
- [ ] MiniBatchKMeans
- [ ] LinearSVR
- [ ] LinearSVC
- [ ] LogisticRegressionCV
- [ ] KernelDensity
- [ ] KMeans
- [ ] CalibratedClassifierCV
- [ ] IsolationForest
- [ ] RidgeClassifierCV

I wonder if for the SGD based algorithms there's an issue where we shrink ``w`` if we see a sample with sample weight 0.",843222
856,2019-09-18T09:27:08Z,2020-03-02T18:58:56Z,,,"#### Reference Issues/PRs
partially fix https://github.com/scikit-learn/scikit-learn/issues/11996 (With a current PR https://github.com/scikit-learn/scikit-learn/pull/12045) 
and partially fix https://github.com/scikit-learn/scikit-learn/issues/11997 (this PR will supersed https://github.com/scikit-learn/scikit-learn/pull/13028)


#### What does this implement/fix? Explain your changes.
This minimum PR will allows `_encode()` (in label.py) to handle np.nan.
That will help OneHot- and OrdinalEncoder to deal with np.nan values in next PRs.

#### Any other comments?


",843222
857,2019-09-18T06:03:48Z,2020-03-20T01:02:25Z,,,"I have faced problem when working with any quantitative modelling algorithm. Scikit-learn doesn't have any function such **mean_absolute_percentage_error**. Everytime, when I want to use this function, then I have to implement it myself. But, if it is included in sklearn itself then it will be great. I have written code for this function and I have also added tests for it. @agramfort , Please Review my PR.
![mape](https://user-images.githubusercontent.com/20843596/65117948-26611180-da08-11e9-837d-68af24949e57.png)
This PR Fixes #10708 .  This PR is continuation of the work done by @mohamed-ali in #10711 PR.",843222
858,2019-09-16T12:47:14Z,2020-03-03T13:00:15Z,,,"replaced the use of `assert_raises`, `assert_raises_regex`, and `assert_raise_message` with `pytest.raises` context manager.

related to #14216",843222
859,2019-09-16T11:14:33Z,2020-03-02T18:58:58Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Add **predict_params to the predict method to pass them to the predict of the underlying regressor , see discussion  #14890

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
860,2019-09-15T18:08:52Z,2020-03-02T18:58:59Z,,,"replaced the use of `assert_raises`, `assert_raises_regex` , and `assert_raise_message` with `pytest.raises` context manager.

related to #14216

",843222
861,2019-09-15T14:42:20Z,2020-03-02T18:58:59Z,,,"#### Reference Issues/PRs
Fixes enhancement request #14304 ""manhattan_distances for sparse matrices is slow""

#### What does this implement/fix? Explain your changes.
This PR affects primarily metrics/pairwise_fast.pyx. 
The new version provides a faster Cython implementation of _sparse_manhattan(), but requires that the matrices have sorted indices.
I also improved the implementation of the dense matrix case, making it less memory demanding. 
In both cases, the implementation is now multithreaded, using all cores available (it uses Cython's prange).

I ran existing tests via:
pytest -v metrics/tests/test_pairwise.py
There are 2 issues but they have nothing to do with what I touched.

#### Any other comments?
This is a first attempt, so I welcome comments and suggestions.
Questions
1. Is it OK to make it multithreaded by default?
2. How do I provide non-regression tests? Any tutorials/examples with recommended best practices?

",843222
862,2019-09-14T16:55:15Z,2020-03-02T18:59:00Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #14954

<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? 
Added the option `categories='lexicographic'` and added corresponding warning

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
863,2019-09-14T10:26:45Z,2020-03-02T18:59:02Z,,,"
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #14934


#### What does this implement/fix? Explain your changes.
Add case for throwing exception as expected.

#### Any other comments?
NA

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
864,2019-09-12T08:43:27Z,2020-03-02T18:59:03Z,,,"# Intro

At EuroScipy we (@ogrisel, @adrinjalali @JovanVeljanoski and me) discussed the options of 'dataframe in (same) dataframe out' for sklearn. Also, this was a popular topic at the dataframe summit (https://datapythonista.github.io/blog/dataframe-summit-at-euroscipy.html cc @datapythonista). 

The idea was, to experiment first with some transformers, so see if we can get that to work with a vaex dataframe. The proof of concept is in this PR: https://github.com/vaexio/vaex/pull/415 

The result is that we/vaex can reuse sklearn without any modification and it works out of core (works on a billion rows for instance).

# How does it work?

## fit
By monkey patching (or using this PR), we pass through objects in check_array that are non-ndarray, but follow [NEP13](https://numpy.org/neps/nep-0013-ufunc-overrides.html) and [NEP18](https://numpy.org/neps/nep-0018-array-function-protocol.html). The idea being that if you want to behave like numpy, we will not try to convert you.

Vaex implements NEP13 and NEP18 in the referenced PR, and for instance will intercept np.sum/np.std/np.var/np.isnan etc. During for instance StandardScalar.fit, it will use vaex for the computations.

## transform
Executing StandardScalar.transform, the dataframe gets modified by building up virtual columns (we track what operations are being performed, we don't compute). The result is a dataframe with virtual columns that will be computed on the fly when needed.

# What do we get out of this?

During the fit, no memory copies are being made (at least for the examples in the vaex PR) and we get multithreading for free. The standard scalar is 5x faster out of the box and will take virtually no memory (memory-mapped data).

After the transform, we don't have just the numerical result, we have the expressions that will lead to the numerical result. These expressions can be jitted using Numba, Pythran, of by using CuPy for extra performance or GPU acceleration. Also, since we don't compute the result, we don't take up any memory.

Also, instead of having a 'nameless' ndarray, we now have a dataframe with meaningful column names.

# Conclusion / Question

Should sklearn pass through object following NEP13/NEP18? If yes, there probably needs to be a way to explicitly say 'now i need a real in memory c/fortran-contiguous array'. 

I doubt that this PR will be the final step in getting for instance vaex and sklearn or other dataframe/numpy like libraries to work together*, I hope that at least it gives food for thought. I can also imagine having an even stricter opt-int than just NEP13 and NEP18 (another magic dunder?).


*) One issue I see is in the PCA, which is using scipy.linalg.svd, which vaex cannot intercept. If numpy.linalg.svd was used, vaex could intercept that and pass it on to for instance dask. 

# Demo notebook screenshot
![image](https://user-images.githubusercontent.com/1765949/64768010-0aadc500-d549-11e9-8c9b-a501d66c5e99.png)
",843222
865,2019-09-10T11:46:13Z,2020-03-02T18:59:04Z,,,"Related to #13570, closes #13541.

This PR proposes adding a warning for parameters such as `dbscan.eps` which have no good default value.

It also adds a utility function which can be used potentially for other classes/parameters we'd like the user to explicitly specify a value for.

Since it's a deviation from our usual no warning with defaults policy, pinging @scikit-learn/core-devs ",843222
866,2019-09-09T01:46:11Z,2020-03-26T12:33:56Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Example: Fixes #13969  Add non-strict mode to check_estimator
<!--
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This implementation provides 'strict mode' for check_estimator function. If strict mode is false, then check_estimator function will assert only based on error type, not on error message. 
By default, value of this variable is True. So, it won't change existing behavior of any function.


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
867,2019-09-08T23:32:24Z,2020-03-02T18:59:05Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Fixes should add the option for users to add a third option to the as_frame argument in fetch_openml. It simply adds an additional conditional to check for the option and if so, checks to see if the given dataset is appropriate as an array or as a Dataframe and returns the corresponding option.

#### Any other comments?
This is my first open source project, so if there are any irregularities I do apologize and I very much look forward to hearing your feedback.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
868,2019-09-06T15:03:56Z,2020-03-02T18:59:06Z,,,"### Reference Issues/PRs
Fixes #14257

#### What does this implement/fix? Explain your changes.
Combines TimeSeriesSplit with the Group awareness of other CV strategies such as GroupKFold.

#### Any other comments?
Sprint PR
",843222
869,2019-09-06T03:42:10Z,2020-03-02T18:59:07Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Incomplete information on documentation for returns section of sklearn.model_selection.cross_validate.

Reference: #14831 

#### What does this implement/fix? Explain your changes.

Added information to documentation on validation.py for name changes when there are multiple scoring metrics in the scoring parameter for the cross_validate method.

#### Any other comments?

No other comments.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
870,2019-09-03T20:30:08Z,2020-03-02T18:59:08Z,,,"Fixes #14874.

This makes use of the digitize creating ""out of bounds"" indices to the left and right to create open intervals on both sides.
That removes the need for additional clipping.

I'm not sure how detailed the tests for these functions are, but I assume this version is more stable than the previous versions, as it contains no parameters or thresholds.",843222
871,2019-09-03T15:38:31Z,2019-09-03T21:07:35Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->


#### What does this implement/fix? Explain your changes.

We don't yet know if 3.10 or 4.0 will follow Python 3.9, but whichever it is, it will probably happen in [2020 when Python 3.9 reaches beta](https://www.python.org/dev/peps/pep-0596/#schedule) and work begins on Python 3.9+1.

There's some code which checks the Python major version is exactly 3:

```python
PY3 = sys.version_info[0] == 3
if PY3:
    # Python 3+ code
```

When run on Python 4, this will run the Python 2 code! Instead, check the version is >= 3.

* One is in external `_arff.py`. This PR makes the same fix I have proposed upstream: https://github.com/renatopp/liac-arff/pull/98

* Another is in the external `pyparsing.py`:

```python
system_version = tuple(sys.version_info)[:3]
PY_3 = system_version[0] == 3
if PY_3:
    # Python 3+ code
```

This has since been fixed upstream, this PR updates it to the latest pyparsing 2.4.2.

* A third can be found in the external `six.py`. I've not updated it here, but have made an upstream PR: https://github.com/benjaminp/six/pull/297.


#### Any other comments?

Found using https://github.com/asottile/flake8-2020:
```console
$ pip install -U flake8-2020
...
$ flake8 --select YTT
./sklearn/externals/six.py:39:7: YTT201 `sys.version_info[0] == 3` referenced (python4), use `>=`
./sklearn/externals/six.py:458:8: YTT203 `sys.version_info[1]` compared to integer (python4), compare `sys.version_info` to tuple
./sklearn/externals/_arff.py:307:7: YTT201 `sys.version_info[0] == 3` referenced (python4), use `>=`
```

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
872,2019-09-02T08:58:23Z,2020-03-02T18:59:09Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
873,2019-09-01T09:18:38Z,2020-03-02T18:59:10Z,,,"#### Reference Issues/PRs
Fixes #14860
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This fixes the ""sklearn.utils.multiclass.type_of_target"" function for sparse matrices.


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
874,2019-08-31T22:39:16Z,2020-03-02T18:59:10Z,,,"#### Reference Issues/PRs

Example: Fixes #7343. To replace PR #4975.

Blocked by #14860, requires #14862 merge

#### What does this implement/fix? Explain your changes.
Implement ""recall_at_k"" and ""precision_at_k"" measures for multiclass/multilabel classification, accepting sparse format.

#### Any other comments?
Thx guys for your awesome work.
I initially checked this PR: https://github.com/scikit-learn/scikit-learn/pull/4975 and decided to make a new PR to adapt it to sparse representations, and benefit from features from already present 'precision_score' and 'recall_score' functions (types of averages, labels, sample_weights).

About the context in which it is useful:
- ""recall_at_k"" in multiclass context will give insight about the number of guess you have to make to have the right answer. It is very useful when you want to narrow down the number of classes that a person will then manually validate: with a 0.95 recall at 3, you are 95% certain of displaying the right choice to the user if you display your 3 best guesses.

- ""precision_at_k"" in multilabel context will give you insight of the proportion of valid predictions you make when you have a fixed number : 
Note: for samples with nb_labels < k, there will necessary have a negative impact (you can't have a good guess when there is no valid option)

As a whole these are useful metrics when you make predictions in multiclass/mutilabel context and that you have a tolerance on false-positives and want to tune how many wrong guesses you're allowing youself to make.
",843222
875,2019-08-30T19:55:09Z,2020-03-02T18:59:11Z,,,"When designing `fetch_openml` in https://github.com/scikit-learn/scikit-learn/pull/11419 we decided to go with fine grained cache of individual HTTP requests. In retrospective that was not the right decision.

The issue is that HTTP fetching is not the only bottleneck. The (pure Python) ARFF parser we use is quite slow. A side note: there could be potentially a gain of 3x or so by using to compiled parser ([here in Rust](https://github.com/mbillingr/arff/issues/5#issuecomment-515011120)) but that would require a lot of work (on build and to reach feature parity). We are also doing a significant amount of post-processing to convert the parsed arff to something usable. ARFF is a problem overall.

A better solution is to cache the output of `fetch_openml` directly with `joblib.Memory`. This PR implements that. 

This also has the advantage of delegating most of the caching logic to joblib.

**Performance**

Loading MNIST from cache with
```py
fetch_openml('mnist_784', version=1)
```
takes 19s on master and 0.3s in this PR (on SSD). 

Loading it the first time with an empty cache will probably also be marginally faster (due to pickling ndarray being more efficient as compared to storing gzipped HTTP responses). My internet is not good enough to try at the moment.

Initial motivation was to make examples in https://github.com/scikit-learn/scikit-learn/pull/14300 faster,

**TODO**
 - [ ] possibly add tests to ensure coverage: we have somewhat less control on where files are stored in the cache, so a bit more tricky to test.
 - [ ] see if the re-trying mechanism on corrupted/outdated pickles can be done in joblib.",843222
876,2019-08-27T15:00:58Z,2020-03-02T18:59:12Z,,,"Hello.

This is proof of concept of GeneticSearchCV. It implements genetic algorithm for hyper parameters search.

This is only initial commits to show concept. More docs, tests and code improvements will be added later, as well as removing copy-paste. :)

I just wanted to know you opinion about idea to implement it. :)",843222
877,2019-08-27T05:25:43Z,2020-03-02T18:59:14Z,,,"#### Reference Issues/PRs
https://github.com/scikit-learn/scikit-learn/blob/master/examples/model_selection/plot_grid_search_refit_callable.py
See also #11269. See also #11354. See also #12865. See also #9499.

#### What does this implement/fix? Explain your changes.
As discussed briefly with @amueller and @NicolasHug last week, this is an enhancement to the latest implementation of sklearn's refit callable functionality. The aim is to provide a more generic set of methods for balancing model complexity with CV performance via model selection 'smoothing'.  In the context of a highly versatile package such as sklearn, where it becomes possible to fit the vast majority of model types using an extensive set of parameters and scorers, the risk of overfitting may be more pressing. To ameliorate this, OneSE might be especially valuable for final model estimation in CV.

#### Any other comments?
The challenge with implementing such a tool has always been with generalizing its functionality to all types of models, parameters, and scoring methods. In particular, defining model 'complexity' is also relatively context-dependent.  This PR lays out a prototype that allows these definitions to be user-determined, rather than hard-coded defaults. Currently, both _error and _score type scorers are supported, along with multi-metric scoring. Either 1 SD bounds can be used, or a percentile tolerance can be specified. Feel free to revise, rewrite, and discuss!

Expanding upon the excellent example recently created by @jiaowoshabi :
```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection._search import OneSE
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC

pipe = Pipeline([
        ('reduce_dim', PCA(random_state=42)),
        ('classify', LinearSVC(random_state=42)),
])

param_grid = {
    'reduce_dim__n_components': [2, 4, 6, 8]
}

# Here, we set our OneSE parameters, though ideally, we would make them explicit arguments for the actual OneSE callable below:
param='n_components'
greater_is_complex=True
refit_scoring = 'accuracy'
tol = None

grid = GridSearchCV(pipe, cv=10, n_jobs=1, param_grid=param_grid,
                    scoring=['accuracy', 'neg_mean_squared_log_error'], refit=OneSE)
digits = load_digits()
grid.fit(digits.data, digits.target)

n_components = grid.cv_results_['param_reduce_dim__n_components']
test_scores = grid.cv_results_['mean_test_accuracy']

plt.figure()
plt.bar(n_components, test_scores, width=1.3, color='b')

plt.axhline(np.max(test_scores), linestyle='--', color='y', label='Best score')

plt.title(""Balance model complexity and cross-validated score"")
plt.xlabel('Number of PCA components used')
plt.ylabel('Digit classification accuracy')
plt.xticks(n_components.tolist())
plt.ylim((0, 1.0))
plt.legend(loc='upper left')

best_index_ = grid.best_index_

print(""The best_index_ is %d"" % best_index_)
print(""The n_components selected is %d"" % n_components[best_index_])
print(""The corresponding accuracy score is %.2f""
      % grid.cv_results_['mean_test_accuracy'][best_index_])
plt.show()
```

Let me know what you think
@dPys",843222
878,2019-08-24T21:11:05Z,2020-03-02T18:59:15Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #11816 


#### What does this implement/fix? Explain your changes.

When fetching an open-ml dataset, metadata about the dataset is fetched via `https://openml.org/api/v1/json/data/`, which includes the latest file version to download, its md5-checksum, etc.

This PR adds functionality to verify the md5-checksum of the file downloaded to the one provided via the api. It does this ONLY when the file is being downloaded and not when it is loaded from cache. If the validation fails, it produces a `ValueError` stating the same.

#### Any other comments?
- Validation is done by default unless explicitly overriden
- Most files in the local tests `sklearn.datasets.tests.data.openml` folder do not match their checksums. This may be because of differences between versions of metadata vs actual file downloaded.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
879,2019-08-24T16:15:35Z,2020-03-02T18:59:16Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #10905 
Fixes #10931. Worked on the branch https://github.com/maskani-moh/scikit-learn/tree/fix-10905.

Closes #14769



#### What does this implement/fix? Explain your changes.
Added user guide documentation for permutation_test_score.

#### Any other comments?
Worked with @vmanisha at the NYC scikit-learn sprint 2019.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
880,2019-08-24T14:08:06Z,2020-03-02T18:59:17Z,,,"The TfidfTransformer and TfidfVectorizer use an idf that is not the standard textbook definition. I added a parameter to both as an option to use the standard definition, while leaving the previously defined idf term as the default. I created some test cases to make sure it works but I didn't include them in tests/test_text.py. Should I include them there?",843222
881,2019-08-23T04:01:43Z,2020-03-02T18:59:17Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

See https://github.com/scikit-learn/scikit-learn/pull/12028#issuecomment-523813165, #11982

#### What does this implement/fix? Explain your changes.

Make `OPTICS.fit(self, X, y=None)` support the sparse matrix `X`.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
882,2019-08-20T18:11:03Z,2020-03-02T18:59:19Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Resolves https://github.com/scikit-learn/scikit-learn/issues/13405


#### What does this implement/fix? Explain your changes.
1. Adds section to user guide.
2. Updates permutation feature importance example with this correlation feature selection.

#### Any other comments?
This should be useful when placed a `Pipeline` in the final estimator when stacking: https://github.com/scikit-learn/scikit-learn/pull/11047

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
883,2019-08-19T01:09:02Z,2020-03-02T18:59:20Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes  #14483

#### What does this implement/fix? Explain your changes.

I've added 2 common tests that provide estimators with a nominally large sparse matrix and check the memory utilization. Both tests use different approaches and I'm hoping that the reviewers can help me decide which way to go. The current code is WIP and my goal is to get feedback on the approaches. After the reviewers help me decide which way to go, I'll clean up the code more.

The first check is **check_estimator_sparse_data_memory_growth**. As the name indicates, this check aims to assert on the growth in memory requirements for an estimator. It uses the Python builtin profiler hook and checks for peak memory consumption within a context. It then uses the estimator's `fit` and `predict` methods for a baseline input and for 2 inputs that are larger in either the number of rows or number of features. It asserts that the memory requirement does not increase linearly.

The second check is **check_estimator_sparse_data_process_maxrss**. This check uses the `resource` module to check for the max RSS of the process. It does not deal with growth, but just absolute value of the max RSS. I don't know of a way of resetting this max-RSS value for a process; I'm not sure it's possible. Because of this, this check suffers from a few issues: (1) a previous test that uses a lot of memory can raise the max-rss of the process enough that this test completely misses all densifications, (2) If one estimator increases the max-rss sufficiently and causes a failure, it can mask the failures of succeeding checks for estimators.

In these 2 ways, I wanted to not only show 2 techniques for measuring memory consumption, but also 2 ways of asserting on memory (growth vs. absolute).

I also wanted to mention some additional aspects of the 2 approaches:

**check_estimator_sparse_data_memory_growth**

* Adding a profiling hook causes the code to slow down during the context of the memory profiler.
* To do cross-platform memory consumption measurement, I've added a third-party dependency (`psutil`). Maybe if the reviewers think that that's a problem, I can look into removing it.
* To check whether the memory growth is sub-linear, I probably need to measure at more than 2 points per axis. That would probably cause this check to take a while. I'm not sure what kind of runtime is acceptable.

**check_estimator_sparse_data_process_maxrss**

* I'm not sure how one would select an absolute threshold for memory. Probably better to go with the growth idea from the check above.
* As I mentioned, using max-rss seems like a flawed idea because test isolation is an issue.

##### Results

There are a fair number of failures using the memory growth idea. If the idea receives positive feedback, I'd be happy to look into it. A lot of these failures are genuine and would probably be fun to fix 😄 
Also note that it takes 85.19 seconds to run the growth tests and that 34 out of 162 tests fail.

On the other hand, the RSS test is a lot faster at 24.64 seconds. It also has only 2 failures, but that doesn't actually mean anything because it's based on an arbitrary threshold that I've added to it.

Note that the regular sparse data check (`check_estimator_sparse_data`) takes only 11.04 seconds to run on my machine. So the memory tests are going to add a fair amount of time to the CI pipeline.

#### Any other comments?

There are more approaches to solving this issue:

1. I could just create a very very large sparse matrix and see if it blows up the process. As was mentioned by @rth, this would be a very annoying dev-experience.
2. I could try to mock methods in sparse matrices that densify it. This approach is going to get very complex very fast as we try to keep track of the internal methods of sparse matrices that could detect this. Also something as simple as cloning the matrix could thwart this and to avoid that, we'll have to add further complexity. I'm not a fan of this approach.

Obviously the `LeakyEstimator` exists only for development purposes in case any reviewers want to try running the tests themselves. I'll delete it if and when we're ready to merge. 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
884,2019-08-18T11:48:23Z,2020-01-10T20:20:47Z,,,"For reviewing https://github.com/numpy/numpydoc/pull/221

Should check doc rendering for how signatures look in method listings, at the tops of object/function pages, etc.

",843222
885,2019-08-17T16:41:39Z,2020-03-02T18:59:21Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs

Fixes [#14671](https://github.com/scikit-learn/scikit-learn/issues/14671)
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This PR fixes the bug referenced above. 
Initially, 
```
from sklearn.utils.multiclass import type_of_targets
import numpy as np
x = [[1, 2]]
assert type_of_targets(x) == assert type_of_targets(np.array(x)) 
```
results in an error, although it shouldn't. My changes fixes this. 

I fixed the bug my refactoring the function `multiclass.is_multilabel` (which type_of_tagets depends on)  to accept numpy arrays  as well as lists. I also added tests to verify that the bug is fixed. 



<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
886,2019-08-15T11:56:01Z,2019-08-16T15:16:23Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

In our use case (Bayesian optimization) - ""get_params"" takes about 30% of the total time of the run. From a quick look it seems like it's using signature which can be very heavy when run multiple times. Since signature is class based and not object based (obviously) this can be cached (not too many classes around so no worries about memory implications). for us it improved our runtime by 30%.
Let me know what you think! 

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
887,2019-08-14T15:35:36Z,2020-01-10T09:49:01Z,,,"#### Reference Issues/PRs
Fixes #9245
Fixes #13338
Fixes #13339

#### What does this implement/fix? Explain your changes.
`predict_proba` and `decision_function` of `ClassifierChain` retuns now a list of `n_outputs` arrays, each of  shape `(n_samples, n_classes)`. This format is consistent with that of `MultiOutputClassifier`. 

#### Any other comments?
`multiouput_only` tag has now been removed. I wonder whether we should keep the `_skip_test` tag. Tests pass even when this tag is removed. 

This change will break backward compatibility, since the functions previously returned arrays rather than lists. Any ideas on how to handle this?
",843222
888,2019-08-14T15:12:34Z,2020-03-02T18:59:22Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
889,2019-08-13T15:32:24Z,2020-02-07T14:02:48Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
#14081 

#### What does this implement/fix? Explain your changes.
Created a Pitfalls section at the end of Tutorials and added SelectKBest example.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
890,2019-08-13T03:49:17Z,2020-03-02T18:59:23Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

superseded  #8842
closes  #8842
closes #8834


#### What does this implement/fix? Explain your changes.

Add a variable named ""copy"" to perform in-place laplacian calculation in spectral clustering

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
891,2019-08-11T06:26:28Z,2020-03-02T18:59:24Z,,,"add option to specify `n_features_to_select` as float between 0.0 and 1.0
which would be treated as the percentage of of available features to be
selected. Eg. To select 25% of the available features, you could give
`n_features_to_select` = 0.25

closes #14567
",843222
892,2019-08-10T15:38:40Z,2020-03-02T18:59:25Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Partially address #14351

#### What does this implement/fix? Explain your changes.

Remove redundant max_iter in examples and tests


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
893,2019-08-09T23:31:33Z,2020-03-02T18:59:25Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Fixes #8245 

#### What does this implement/fix? Explain your changes.
When all the `y_true` labels are negative, `precision_recall_curve` returns `nan` because of `recall` being set to `nan` instead of `1`. This is because of the direction division of the `tps` vector by `tps[-1]` which happens to be `0`.

This fix checks if `tps[-1]` is `0` and if yes, sets the recall to `1` directly since there are no True Positives or False Negatives, else we calculate `recall` as normal.


#### Any other comments?
I had to update `test_precision_recall_curve_toydata` since this test was expecting the `TrueDivide` exception to be raised which is no longer the case as a result of this fix. I added 2 test cases, one to check when all truth labels are negative and the other to check when all truth labels are positive to ensure precision calculation is accurate.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
894,2019-08-09T19:38:33Z,2019-08-29T13:28:21Z,,,"Changes made:
* declared the estimator first and operate fit_transform with the timing around
* corrected that we apply TruncatedSVD and not PCA

closes #14226

",843222
895,2019-08-09T04:02:02Z,2020-03-02T18:59:26Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
896,2019-08-07T16:08:42Z,2020-03-02T18:59:27Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs 14571
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Fixes #14571
MLP Regressor only had `squared_loss` as a loss function. This PR adds functionality
for `mae_loss` in addition to the existing `squared_loss`

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
897,2019-08-02T21:37:49Z,2020-03-02T18:59:28Z,,,"Follow up on #4757 with a much simplified implementation.
Fixes #4757.

If anyone has an idea for a better/shorter name I'm all ears!",843222
898,2019-08-02T16:43:55Z,2020-03-02T18:59:28Z,,,"As far as I can tell the `sklearn.utils.graph_shortest_path` is the same implementation originally in 2011 by Jake Vanderplas as [scipy.sparse.csgraph.shortest_path](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.shortest_path.html) (available since v0.11), except that the scipy version (unlike the one in scikit-learn) has received [several improvements since](https://github.com/scipy/scipy/blob/master/scipy/sparse/csgraph/_shortest_path.pyx).

This removes the scikit-learn versions in favor of using the one from scipy.

I have not benchmarked the run time or audited the code in detail, but given that it's used in scikit-learn only once in Isomap I think it's worth using the scipy implementation in any case.

Removing without deprecation warning per https://github.com/scikit-learn/scikit-learn/issues/6616#issuecomment-516479413

",843222
899,2019-08-01T16:24:18Z,2020-03-02T18:59:29Z,,,"Closes https://github.com/scikit-learn/scikit-learn/issues/13783

This makes `KNeighborsClassifier.predict` faster by re-writing `scipy.stats.mode` as an argmax of a sparse array as discussed in the parent issue.

Using the example provided in https://github.com/scikit-learn/scikit-learn/issues/13783#issue-440225690,

**On master**
```
%timeit knn.predict(X_grid)                                                 
2.47 s ± 37.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
**With this PR**
```
%timeit knn.predict(X_grid)                                                 
793 ms ± 39.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

so in this particular case, the `KNeighborsClassifier.predict` is 3.1x faster.

It works in a straightforward way both on weighted an unweighted data making `sklearn.utils.weighted_mode` no longer necessary. 

The downside is that it makes `RadiusNeighborsClassifier.predict` slower by about 30% on the following example,
<details>

```
In [1]: import numpy as np 
   ...: from sklearn.datasets import make_blobs 
   ...: from sklearn.neighbors import RadiusNeighborsClassifier 
   ...:  
   ...: X, y = make_blobs(centers=2, random_state=4, n_samples=30) 
   ...: knn = RadiusNeighborsClassifier(algorithm='kd_tree', radius=2).fit(X, y) 
   ...:  
   ...: x_min, x_max = X[:, 0].min(), X[:, 0].max() 
   ...: y_min, y_max = X[:, 1].min(), X[:, 1].max() 
   ...:  
   ...: xx = np.linspace(x_min, x_max, 100) 
   ...: # change 100 to 1000 below and wait a long time                             
   ...:               
   ...: yy = np.linspace(y_min, y_max, 100)                                         
   ...:   
   ...:  
   ...: X1, X2 = np.meshgrid(xx, yy)                                                
   ...:    
   ...: X_grid = np.c_[X1.ravel(), X2.ravel()]                                      

In [2]: %timeit knn.predict(X_grid)                                                 
1.27 s ± 9.02 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
</details>
The difference is mostly that in `RadiusNeighborsClassifier` we call this function repeatedly on 1D arrays, as opposed to a single 2D array.  In worst case, I could revert back to `scipy.stats.mode` and `sklearn.utils.weighted_mode` for `RadiusNeighborsClassifier` but it's annoying to use two different implementations.

**TODO**
 - [ ] fix the remaining 2 test failures on the comparison between the multi-output and single-output case.
 - [ ] investigate RadiusNeighborsClassifier.predict performance regression.",843222
900,2019-07-31T20:56:44Z,2020-03-02T18:59:30Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Addresses #13488. 


#### What does this implement/fix? Explain your changes.
This allows the user to specify an error policy of ""virtual"" when instantiating the OrdinalEncoder. When the encoder transforms data that contains values it has not seen in fit, instead of throwing an error, when handle_unknown == 'virtual', the transform will run and send all unknown values to a virtual additional category, last in the ordinal progression of categories, i.e 0,1,2 before, now 0,1,2,3, where 3 maps to any value not previously encountered. Upon inverting the transformation, these values will appear as None, which is consistent with the current behavior of OneHotEncoder when handle_unknown == 'ignore'.

The None category is created upon instantiation of the encoder, so the encoder does not have to be changed over the course of its use. This means that a drawback of this approach is that any encoder that has been fitted will indicate a none category in its category list

This is important because it is a common use case to attempt to transform categorical data that may contain categories not in the original fitted data. The behavior of sending to an additional virtual category seems like a reasonable solution, and is in line with the solution currently implemented for OneHotEncoder.

#### Any other comments?
This is my first ever pull request, I'm very excited and nervous. I personally would really like to use this feature, I hope others will find it useful as well. 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
901,2019-07-30T21:07:01Z,2020-03-02T18:59:31Z,,,"[Feature] This commit implements an additional model selection split strategy names LabelAndGroupKFold which treats the label and group combination as something that can not be split across multiple folds, while at the same time attempting to evenly distribute labels across folds so that each fold has a similar amount of each label type. This is a extension of the GroupKfold, which does not attempt to evenly distribute the classes, only the groups.
",843222
902,2019-07-30T20:54:16Z,2020-03-02T18:59:31Z,,,"#### Reference Issues/PRs

Closes #14444

#### What does this implement/fix? Explain your changes.

Respects `fit_prior` and `class_prior` parameters when computing joint log-likelihood.

#### Benchmark Script

```python
from time import time
from sklearn.datasets import fetch_20newsgroups_vectorized
from sklearn import metrics
from sklearn.utils.extmath import density
from sklearn.naive_bayes import ComplementNB as CNB


def fetch_data():
    train = fetch_20newsgroups_vectorized(
        subset='train', 
        remove=(), 
        data_home=None, 
        download_if_missing=True)

    test = fetch_20newsgroups_vectorized(
        subset='test', 
        remove=(), 
        data_home=None, 
        download_if_missing=True)

    return train.data, train.target, test.data, test.target, train.target_names

X_train, y_train, X_test, y_test, target_names = fetch_data()


def benchmark(clf):
    print('_' * 80)
    print(""Training: "")
    print(clf)
    t0 = time()
    clf.fit(X_train, y_train)
    train_time = time() - t0
    print(""train time: %0.3fs"" % train_time)

    t0 = time()
    pred = clf.predict(X_test)
    test_time = time() - t0
    print(""test time:  %0.3fs"" % test_time)

    score = metrics.accuracy_score(y_test, pred)
    print(""accuracy:   %0.3f"" % score)

    if hasattr(clf, 'coef_'):
        print(""dimensionality: %d"" % clf.coef_.shape[1])
        print(""density: %f"" % density(clf.coef_))
    
    print(""classification report:"")
    print(metrics.classification_report(y_test, pred,
                                        target_names=target_names))

    print(""confusion matrix:"")
    print(metrics.confusion_matrix(y_test, pred))

    print()
    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


if __name__ == '__main__':
    results = []
    for clf, name in (
            (CNB(), ""Complement NB""),):
        print('=' * 80)
        print(name)
        results.append(benchmark(clf))
```

#### Benchmark Output

Complement NB (original)
________________________________________________________________________________
Training: 
ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)
train time: 0.136s
test time:  0.028s
accuracy:   0.832
dimensionality: 130107
density: 1.000000
classification report:  precision    recall  f1-score   support

             alt.atheism       0.76      0.69      0.72       319
           comp.graphics       0.80      0.75      0.77       389
    comp.os.ms-windows.misc    0.79      0.77      0.78       394
    comp.sys.ibm.pc.hardware   0.71      0.77      0.74       392
    comp.sys.mac.hardware      0.88      0.80      0.84       385
          comp.windows.x       0.84      0.82      0.83       395
            misc.forsale       0.85      0.87      0.86       390
               rec.autos       0.92      0.90      0.91       396
         rec.motorcycles       0.93      0.97      0.95       398
      rec.sport.baseball       0.92      0.93      0.92       397
        rec.sport.hockey       0.88      1.00      0.94       399
               sci.crypt       0.83      0.97      0.89       396
         sci.electronics       0.83      0.64      0.72       393
                 sci.med       0.89      0.86      0.88       396
               sci.space       0.85      0.96      0.90       394
    soc.religion.christian     0.67      0.95      0.78       398
      talk.politics.guns       0.69      0.95      0.80       364
    talk.politics.mideast      0.94      0.95      0.95       376
      talk.politics.misc       0.92      0.51      0.66       310
      talk.religion.misc       0.93      0.30      0.45       251

                accuracy                           0.83      7532
               macro avg       0.84      0.82      0.82      7532
            weighted avg       0.84      0.83      0.82      7532


Complement NB (priors)
________________________________________________________________________________
Training: 
ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)
train time: 0.129s
test time:  0.026s
accuracy:   0.804
dimensionality: 130107
density: 1.000000
classification report:  precision    recall  f1-score   support

             alt.atheism       0.85      0.45      0.59       319
           comp.graphics       0.81      0.74      0.77       389
    comp.os.ms-windows.misc    0.78      0.78      0.78       394
    comp.sys.ibm.pc.hardware   0.71      0.78      0.74       392
    comp.sys.mac.hardware      0.91      0.78      0.84       385
          comp.windows.x       0.84      0.82      0.83       395
            misc.forsale       0.86      0.86      0.86       390
               rec.autos       0.90      0.91      0.91       396
         rec.motorcycles       0.91      0.98      0.94       398
      rec.sport.baseball       0.89      0.93      0.91       397
        rec.sport.hockey       0.85      1.00      0.91       399
               sci.crypt       0.80      0.97      0.88       396
         sci.electronics       0.84      0.66      0.74       393
                 sci.med       0.86      0.87      0.87       396
               sci.space       0.83      0.96      0.89       394
    soc.religion.christian     0.50      0.97      0.66       398
      talk.politics.guns       0.71      0.88      0.79       364
    talk.politics.mideast      0.91      0.93      0.92       376
      talk.politics.misc       1.00      0.32      0.49       310
      talk.religion.misc       1.00      0.04      0.08       251

                accuracy                           0.80      7532
               macro avg       0.84      0.78      0.77      7532
            weighted avg       0.83      0.80      0.79      7532",843222
903,2019-07-29T14:50:36Z,2020-03-02T18:59:32Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
I added default values to documentation of each parameter which were missing in both `PassiveAggressiveRegressor` and `PassiveAggressiveClassifier` functions.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
904,2019-07-27T07:38:13Z,2020-03-02T18:59:33Z,,,"Added default value documentation for some parameters of `KFold`, `StratifiedKFold` and `cross_validate`, just like it was previously done for parameters of some linear models (see #14452).",843222
905,2019-07-26T15:25:13Z,2020-03-02T18:59:34Z,,,calling fit or transform fails for most estimators having a `copy` or `copy_x` or `copy_X` parameter (see CI).,843222
906,2019-07-25T02:21:52Z,2019-08-17T00:15:19Z,,,"#### What does this implement/fix? Explain your changes.
In the entry, I paraphrased the first paragraph and added the required link.

#### Any other comments?
",843222
907,2019-07-24T23:51:24Z,2020-03-02T18:59:36Z,,,"Added clarifying comments to spectral.py and spectral_embedding_.py on use of normalized Laplacian and the exact eigenproblem being solved. 

@GaelVaroquaux would you mind checking if you approve of the changes?",843222
908,2019-07-24T07:12:49Z,2020-03-02T18:59:37Z,,,"Addresses #14452

- Improves the documentation for all the model in coordinate_descent.py by correctly documenting default values for various parameters 
- This includes models such as Lasso, ElasticNet and ElasticNetCV 
",843222
909,2019-07-23T12:45:37Z,2020-03-06T16:25:48Z,,,"Fixes #14436 
Plug-in any covariance estimator into Fisher discriminants
This PR allows the user to plug-in any sklearn covariance estimator into LDA and QDA. This option is only possible with the ""lsqr"" and ""eigen"" solver of LDA and for the ""lsqr"" solver of QDA (initially only an ""svd"" solver was available for QDA so I made this explicit and added the ""lsqr"").",843222
910,2019-07-17T08:18:50Z,2020-03-02T18:59:38Z,,,"This PR refactors the three base scorers (_PredictScorer, _ProbaScorer, _ThresholdScorer) into _BaseScorer. These are all private classes so we can remove them directly.
Honestly I'm not sure whether we want this (vote +0 for myself :))

- Advantages:
  * avoid duplicate code, easier to maintain
  * avoid inconsistency between different base scorers (e.g.,  previously, for binary y_true, _ProbaScorer requires score function to accept 2d y_pred, but _ThresholdScorer requires score function to accept 1d y_pred, see #14318)

- Disadvantages:
  * Maybe more difficult if we want to add more base scorers?",843222
911,2019-07-17T01:28:02Z,2020-03-02T18:59:38Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This PR allows for sampling a subset of features [start_feature, end_feature) from a libsvm file without loading all the features.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
912,2019-07-15T19:32:27Z,2020-03-02T18:59:39Z,,,"I have prepared a PR with replacing np.zeroes to np.empty for performance improvement on multi-core systems.
Original issue: https://github.com/scikit-learn/scikit-learn/issues/14306
",843222
913,2019-07-13T17:53:51Z,2020-03-02T18:59:40Z,,,"Added random_state parameter to allow user to select the seed.

Relates to issue #14302 ",843222
914,2019-07-13T17:13:42Z,2019-08-07T21:47:49Z,,,"This warning is an sklearn internal warning. No available flags in class parameters to intentionally silence. Most feasible solution, without created an additional parameter to the class definition was to just add a filter warning to the top of that example script.

Relates to issue #14117",843222
915,2019-07-13T13:50:52Z,2020-03-02T18:59:41Z,,,"…with constant features


#### Reference Issues/PRs
Example: Fixes #14311 ""Add common test for constant features""


#### What does this implement/fix? Explain your changes.
Implements a common test that estimators work with constant features or, in case of failure,  return the correct error message

",843222
916,2019-07-12T19:35:46Z,2020-03-02T18:59:42Z,,,"This is based on #13307 by @amueller, related SLEP: https://github.com/scikit-learn/enhancement_proposals/pull/18

This PR explores the idea of passing around the column names with the X object. It is the (b) option in https://github.com/scikit-learn/enhancement_proposals/pull/18#issuecomment-507792196

The idea is to pass the column names with the object, and for that we can either use an `xarray.DataArray`, or a `sklearn.NamedArray`. This PR uses xarray, exploring the idea, and the focus is to see how it would look like if we do it.

There are certain aspect we need to consider, deciding which one of xarray or an internal NamedArray to choose:
- xarray depends on `pandas`! I haven't checked/asked if it'd be easy/doable to remove that hard dependency, but for now it's there.
- On the other hand, `NamedArray` requires careful discussion on whether we really wanna have such an object, and what it should include as _features_.

The code behaves as such ATM:

input:
```python
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.compose import make_column_transformer
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest
import pandas as pd

iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
pipe = make_pipeline(StandardScaler(), PCA(), SelectKBest(k=2), LogisticRegression())
pipe.fit(X, iris.target)
pipe[-1].feature_names_in_
```
output:
```python
array(['pca0', 'pca1'], dtype='<U4')
```
input:
```python
pipe = make_pipeline(make_column_transformer((make_pipeline(StandardScaler(),
                                                            PCA()), X.columns),
                                             (StandardScaler(), X.columns[:2])),
                     SelectKBest(k=2), LogisticRegression())
pipe.fit(X, iris.target)
pipe[-1].feature_names_in_
```
output
```python
array(['pipeline__pca0', 'standardscaler__sepal length (cm)'],
      dtype='<U33')
```
input:
```python
pipe = make_pipeline(make_column_transformer((PCA(), X.columns),
                                             (StandardScaler(), X.columns[:2])),
                     SelectKBest(k=2), LogisticRegression())
pipe.fit(X, iris.target)
pipe[-1].feature_names_in_
```
output
```python
array(['pipeline__pca0', 'standardscaler__sepal length (cm)'],
      dtype='<U33')
```

__EDIT__: an update/summary is written under https://github.com/scikit-learn/scikit-learn/pull/14315#issuecomment-514673854",843222
917,2019-07-05T19:44:20Z,2020-03-02T18:59:43Z,,,"### What does this implement/fix? Explain your changes.

It is sometimes useful to extract statistics and visualize the features in a libsvm file that is too large to be loaded entirely in memory. This PR adds an option to load_svmlight_file cython code to select rows randomly while reading the data from file. It also adds an option to select some features / columns while discarding the other.",843222
918,2019-07-05T13:31:58Z,2020-03-02T18:59:44Z,,,"Alternative/complement to #14246 

Check that `sample_weight` change the prediction of the estimator. In case this is not true, it could be because of a bug.",843222
919,2019-07-05T11:55:27Z,2020-03-03T14:43:14Z,,,"Fixes #14249 

LinearModelsCV perform inplace operations on input which can cause an error when using loky or multiprocessing backends if the input is sufficiently large to cause a memmapping (1MB).",843222
920,2019-07-03T14:35:55Z,2020-03-02T18:59:46Z,,,"closes #14191 

It seems that we don't have a common test checking that over-sampling or under-sampling `X` give equivalent results as `sample_weigth`. This PR introduces this new common tests.

In addition, it fixes the estimators which do not follow this constraint if they should.",843222
921,2019-07-02T21:19:38Z,2019-09-25T21:29:51Z,,,"This should be easier once we have an ``n_features_in_`` attribute #13603, and use a ``OneToOneMixin`` or something like that.",843222
922,2019-07-02T19:59:19Z,2019-08-06T21:01:36Z,,,"This is an alternative to #12627 that I proposed in https://github.com/scikit-learn/enhancement_proposals/pull/18

Basically I think now it's better to have the feature names as close to X as possible, so they are not out of sync, and I want the user interface to be as small as possible.

This PR adds ``feature_names_in`` as a parameter to ``fit``, and adds ``feature_names_in_`` as an attribute to every estimator, and ``feature_names_out_`` as an attribute to all transformers.

Other alternatives that do basically the same but don't require an attribute to fit are:

- require the user to set the ``feature_names_in`` attribute manually
- pass around objects that have feature names attached to X, i.e. use dataframes or a dataset object or a subclass of ndarray that adds feature names.",843222
923,2019-06-25T02:18:38Z,2020-03-25T20:23:53Z,,,"#### Reference Issues/PRs
Closes https://github.com/scikit-learn/scikit-learn/issues/14061

#### What does this implement/fix? Explain your changes.

You can demo the visualization here: https://thomasjpfan.github.io/sklearn_viz_html/index.html

This PR implements a HTML visualization for estimators with a focus on displaying it in a Jupyter notebook or lab. This implementation is in pure HTML and CSS (no javascript or external dependencies): 

![Screen Shot 2019-06-28 at 4 16 20 PM](https://user-images.githubusercontent.com/5402633/60368922-19f88a00-99c0-11e9-9397-06acf766390d.png)

1. We can hover over elements to see an estimators parameters (`print_changed_only=True` is the default for `export_html`):

<img width=""549"" alt=""Screen Shot 2019-06-24 at 10 11 36 PM"" src=""https://user-images.githubusercontent.com/5402633/60064008-14075e00-96cd-11e9-9fc1-c1b4c4de6484.png"">

2. All the labels in bold can be hovered over to get more information.
3. `_type_of_html_estimator` returns how to layout metaestimators, (`ColumnTransformer` and `FeatureUnion` is ""parallel"", while `Pipeline` is ""serial"") If there are any other metaestimators to add, we just need to add it to `_type_of_html_estimator`)
4. There is a hidden div `sk-final-spacer` as a hack to provide enough space for the information displayed while hovering over elements.

<details>
<summary>Code to Create HTML (In jupyterlab or a notebook)</summary>

```py
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.pipeline import FeatureUnion
from sklearn.feature_selection import SelectPercentile
from sklearn.inspection import display_estimator

# We create the preprocessing pipelines for both numeric and categorical data.
numeric_features = ['age', 'fare']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median'))])

feat_u2 = FeatureUnion([(""pca"", PCA(n_components=1)),
                      (""svd"", Pipeline([('tsvd1', TruncatedSVD(n_components=2)), 
                                        ('select', SelectPercentile())]))])

numeric_transformer2 = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('scaler', StandardScaler(with_std=False)),
    ('feats', feat_u2)
])
categorical_features = ['embarked', 'sex', 'pclass']
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', missing_values=""missing"")),
    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num1', numeric_transformer, numeric_features),
        ('num2', numeric_transformer2, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

feat_u = FeatureUnion([(""pca"", PCA(n_components=1, whiten=True, svd_solver='full')),
                      (""svd"", TruncatedSVD(n_components=2, n_iter=10))])
clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',
                         random_state=1, max_iter=200)
clf2 = RandomForestClassifier(n_estimators=50, random_state=1, max_depth=8, warm_start=True, n_jobs=3, oob_score=True)
clf3 = GaussianNB()
eclf1 = VotingClassifier(estimators=[
        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')

clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('feat_u', feat_u),
                      ('classifier', eclf1)])
display_estimator(clf)
```
</details>",843222
924,2019-06-24T14:12:37Z,2020-03-02T18:59:48Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Hi there, this is a PR for #13555 .

#### What does this implement/fix? Explain your changes.
- Moved `_get_param_names` out of `BaseEstimator` as utility function now called `get_param_names_from_constructor`.
    - This is meant for reuse in `gaussian_process.kernels.Kernel` .
    - Added a `SignatureError` class to pass the signature from the utility function up to the appropriate exception message in `BaseEstimator` and `gaussian_process.kernels.Kernel`.
- Modified `gaussian_process.kernels.Kernel` to use the new utility function and remove duplication.
- Changed `GenericUnivariateSelect._make_selector` to use `get_params` instead of the new utility function so that it raises the proper error in case of a selector with inappropriate signature.
- Also refactored `get_params` to call from a new `_get_params_from` method. This is so `get_params` can be easily overridden to take a signature of, for example, the parent class in custom subclasses of client code.

#### Any other comments?
- Docstrings will be added later.
- Other that raising `SignatureError` (subclassed from `RuntimeError`) these changes don't add or change current behavior, so I haven't added any new tests. If new tests might seem necessary, please let me know.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
925,2019-06-17T07:20:14Z,2020-03-02T18:59:49Z,,,"#### What does this implement/fix? Explain your changes.

This PR implements `SelectFromModelCV`, bringing the equivalent of what `RFECV` is to `RFE`.
This addresses the need of having a cross validation of feature importances for estimators having no intrinsic cross validation like `LassoCV`or eli5's `PermutationImportance`.

#### Any other comments?

I'll add doc and tests if you are interested in continuing with the integration of this feature.",843222
926,2019-06-08T23:40:49Z,2020-03-06T09:10:27Z,,,"Currently, if ColumnTransformer is called with remainder='passthrough', then get_feature_names() will raise a NotImplementedError, but this pull request adds in that functionality. 

In this pull request, if the ColumnTransformer was fitted on a DataFrame and remainder='passthrough' then the columns which passed through will appear in get_feature_names() as their column names in the DataFrame, and if it was not fitted on a DataFrame, then it will be the indices of the columns which will appear in get_feature_names().

Also, if someone explicitly passes a transformer as the text 'passthrough', then the feature names will be name__{column name}, where column_name is whatever have defined when they passed it.
e.g.
`ct = ColumnTransformer([('trans', 'passthrough', ['col0', 'col1'])])`
will produce features 
`['trans__col0', 'trans__col1']`
which is in keeping with the existing behavior for other transformers.

The behaviour for when a transformer does not have a get_feature_names method has also been changed. Now, instead of an error being raised, the feature names will be given as 'name_x0', ..., 'name_xN'. This allows get_feature_names to be useful by giving at least some indication of where the features came from, even if some of the transformers do not give explicit feature names.

Don't believe this fixes any currently open issues.
",843222
927,2019-05-31T17:27:10Z,2020-03-02T18:59:49Z,,,"#### Reference Issues/PRs
https://github.com/scikit-learn/scikit-learn/issues/14000

#### What does this implement/fix? Explain your changes.
Isolation Forest is executed in parallel during fitting. But, during prediction it is running single-threaded.

In this PR, I parallelised the execution during prediction, more precisely in the _compute_score_samples method. Each ITree was being called in sequence, using a for loop. I created an auxiliary internal function that executes each tree, and this function can be run in parallel. I used joblib for parallelisation.

#### Any other comments?
I run the tests for Isolation Forest and it passed.

",843222
928,2019-05-30T09:48:36Z,2020-03-02T18:59:50Z,,,"Partially addresses #13986 and #13923 

This avoids one memory copy in linear models when `copy_X=True` (default) and `fit_intercept=False`.

For instance, this makes `Ridge(fit_intercept=False)` around 10% faster on the following dataset,
```py
from sklearn.datasets import make_blobs
  
X, y = make_blobs(n_samples=100000, n_features=10, random_state=42)
```

Maybe we should change the default value of `copy_X` from `True` to `None` but I'm not sure it's worth a deprecation cycle.

I have checked that this doesn't break anything with the following common test,
<details>

```diff
diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py
index 3b09ec287..72961bd73 100644
--- a/sklearn/utils/estimator_checks.py
+++ b/sklearn/utils/estimator_checks.py
@@ -124,6 +124,11 @@ def _yield_classifier_checks(name, classifier):
     # basic consistency testing
     yield check_classifiers_train
     yield partial(check_classifiers_train, readonly_memmap=True)
+
+    sig = signature(classifier.__class__.__init__)
+    if ""copy_X"" in sig.parameters:
+        yield check_extra_copy_fit_intercept
+
     yield check_classifiers_regression_target
     if not tags[""no_validation""]:
         yield check_supervised_y_no_nan
@@ -171,6 +176,11 @@ def _yield_regressor_checks(name, regressor):
     yield check_regressor_data_not_an_array
     yield check_estimators_partial_fit_n_features
     yield check_regressors_no_decision_function
+
+    sig = signature(regressor.__class__.__init__)
+    if ""copy_X"" in sig.parameters:
+        yield check_extra_copy_fit_intercept
+
     if not tags[""no_validation""]:
         yield check_supervised_y_2d
     yield check_supervised_y_no_nan
@@ -190,6 +200,7 @@ def _yield_transformer_checks(name, transformer):
     yield check_transformer_general
     yield partial(check_transformer_general, readonly_memmap=True)
 
+
     if not _safe_tags(transformer, ""stateless""):
         yield check_transformers_unfitted
     # Dependent on external solvers and hence accessing the iter
@@ -956,6 +967,25 @@ def check_transformer_general(name, transformer, readonly_memmap=False):
     _check_transformer(name, transformer, X.tolist(), y.tolist())
 
 
+@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+def check_extra_copy_fit_intercept(name, transformer):
+    """"""Check linear models work on RO arrays when fit_transform=False""""""
+
+    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
+                      random_state=0, n_features=2, cluster_std=0.1)
+    X = StandardScaler().fit_transform(X)
+    X -= X.min()
+
+    X, y = create_memmap_backed_data([X, y])
+
+    estimator = clone(transformer)
+    estimator.set_params(copy_X=False)
+    sig = signature(estimator.__class__.__init__)
+    if ""fit_intercept"" in sig.parameters:
+        estimator.set_params(fit_intercept=False)
+    estimator.fit(X, y)
+
+
 @ignore_warnings(category=(DeprecationWarning, FutureWarning))
 def check_transformer_data_not_an_array(name, transformer):
     X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
```
</details>

but of course this only checks the default solver and parameters (and we don't yet have a way to programmatically check all solvers I think?). I'm not sure it's worth adding it in this PR.",843222
929,2019-05-29T17:43:28Z,2019-08-06T20:16:13Z,,,"This adds a consistent blurb to the landing page and the readme.
This is helpful for people who arrive at either of them and wonder what sklearn is.
It also helps people writing about scikit-learn, or introducing speakers, or in a number of settings where someone might ask ""what's scikit-learn"".

I removed the BSD license from the sentence in the readme, if we think that's important I would probably change it to ""scikit-learn is an open-source Python...""",843222
930,2019-05-24T21:43:51Z,2020-03-02T18:59:51Z,,,"#### Reference Issues/PRs

Fixes #12055

#### What does this implement/fix? Explain your changes.

This implements a Bernoulli mixture model. It reuses much of the code in the 
`GaussianMixture` class, mainly differing in the log-likelihood calculation.

#### Any other comments?

An example of applying this to images is provided. The example is motivated 
by the discussion in Chapter 9 of *Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning*.",843222
931,2019-05-24T13:30:28Z,2020-03-02T18:59:52Z,,,"#### Reference Issues/PRs

Fix for bug reported in #13836 by @smsaladi
See also related issues #12940, #13847

#### What does this implement/fix? Explain your changes.

- [x] Added assertion test for when input to `normalized_mutual_info_score()` is sparse to check the returned score is between 0 and 1.
- [x] Control structure to return fixed value in the case that `normalizer` < `eps`.
",843222
932,2019-05-23T20:45:35Z,2020-03-02T18:59:53Z,,,"#### What does this implement/fix? Explain your changes.
The `decision_path` method is currently available only for `RandomForest`/`ExtraTrees` Classifier/Regressor, but not for `IsolationForest` and `GradientBoosting` Classifier/Regressor. 
The `apply` method is currently available only for `RandomForest`/`ExtraTrees`/`Gradient Boosting` Classifier/Regressor but not for `IsolationForest`.

I have added a mixin, `ForestMixin`, in `sklearn.ensemble.base` to provide a common implementation of the methods `decision_path` and `apply` for `RandomForest`/`ExtraTrees` Classifier/Regressor and `IsolationForest`.
However, `Gradient Boosting` Classifier/Regressor require specific implementations of the methods `decision_path` and `apply`. The `apply` method is already available. Work in progress for `decision_path` ...",843222
933,2019-05-20T02:56:55Z,2020-03-02T18:59:53Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fix #13852


#### What does this implement/fix? Explain your changes.
I fixed the constant terms of the objective function in BayesianRidge.
In addition, the timing of convergence was changed to make the objective function value ​​and the result ``coef_`` consistent.

#### Any other comments?
- Also the objective function of ARDRegression ~~can be~~ was fixed.
- ~~The description of ``scores_`` and the name of the method may be altered as discussed in #13852 .~~
- This fix may conflict with PR #13618, but can be easily resolved.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
934,2019-05-17T20:16:29Z,2020-03-12T20:02:01Z,,,"Closes #12538 

This implements hyper parameter search with [successive halving](https://amueller.github.io/dabl/dev/user_guide.html#searching-optimal-parameters-with-successive-halving)

This builds upon #13145, whose changes are required.

This is a port of what we implemented in dabl with @amueller.

- [X] main functional tests
- [X] examples
- [X] user guide
- [x] better integration into user guide to avoid redundancy
- [x] a few more doc here and there
- [x] some more thorough tests about input checking, etc


~~Still WIP but very advanced, and would appreciate some feedback before I start tackling the last few bullet points, so I'll mark as MRG.~~

ping @ogrisel ;)

------

## Benchmarks

Please check out [dabl benchmarks](https://github.com/amueller/dabl/tree/master/benchmarks) for source.

- On 20_news_group datset:

```
                    training time   test score   best CV score
---------------------------------------------------------------
GridSearchCV             19984.9 s      0.8567          0.9262
GridSuccessiveHalving      598.4 s      0.8514          0.8811
---------------------------------------------------------------
Best Params GridSuccessiveHalving
{'clf__C': 1000.0, 'vect': TfidfVectorizer(), 'vect__ngram_range': (1, 1)}
Best Params GridSearchCV
{'clf__C': 1000.0,
  'vect': TfidfVectorizer(ngram_range=(1, 2)),
  'vect__ngram_range': (1, 2)}
```

- on digits dataset:

```

Training Time Successive Halving 3.1159074306488037
Test Score Successive Halving:  0.9911111111111112
Parameters Successive Halving:  {'C': 100.0, 'gamma': 0.1}

Training Time Grid Search:  39.42753505706787
Test Score Grid Search:  0.9911111111111112
Parameters Grid Search:  {'C': 10.0, 'gamma': 0.1}
```",843222
935,2019-05-16T22:45:01Z,2020-03-02T18:59:57Z,,,"#### Reference Issues/PRs
Handles new values when encountered after fit. This has been referenced in issue #10465 and #11997. Although it does no passthrough NaNs or None.


#### What does this implement/fix? Explain your changes.
This PR uses the basic features implemented for one hot encoding to add the same functionality to OrdinalEncoder. This handles both the transform and inverse_transform. In transform category -1 is returned for new values, and None is returned in the inverse transform for -1 values.

#### Any other comments?
This PR might be conflicting with the work of PR #12045.

The unknown category and unknow value could be configured at intanciation.",843222
936,2019-05-16T12:30:43Z,2020-03-02T18:59:57Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #8245
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
I was using _average_precision_score_ for multilabel classification problems and I was getting nan result in many datasets, I found a similar issue in #8245 and I checked the code for the function.
I found a bug in _precision_recall_curve_ function, precision is set to 0 in case a nan is obtained but the same is not done for recall, so I added that, because it is needed for average_precision_score to  work properly.


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
937,2019-05-01T21:23:24Z,2020-03-02T18:59:58Z,,,"Time series have temporal dependence, which may cause information leaks during the cross-validation.
One way to mitigate this risk is by introducing **gaps** between the training set and the testing set.
This PR implements such a feature for **leave-p-out**, **K-fold**, and the **naive train-test split**.
As for the walk-forward one, @kykosic is implementing, among others, a similar feature for the class `TimeSeriesSplit` in #13204. I reckon his implementation promising, so I refrain from reinventing the wheel.

Concerning my implementation, I ""refactored"" the whole structure while still keeping the same public API. `GapCrossValidator` replaces `BaseCrossValidator` and becomes the base where `GapLeavePOut` and `GapKFold` derive from. Although not tested, all other subclasses, I believe, can derive from the new `GapCrossValidator`. I put the quotation marks on the word refactor because I didn't really touch the original code. Instead, my code currently *coexists* with the original one.

### Classes and functions added:
-  GapCrossValidator
    -  GapLeavePOut
    -  GapKFold
-  gap_train_test_split

### Related issues and PRs
#6322, #13204 

### Related users
@kykosic, @amueller, @jnothman, @cbrummitt 
",843222
938,2019-04-26T17:14:05Z,2020-03-02T18:59:59Z,,,"This PR adds M5P model (model trees) to scikit-learn, as mentioned in #13106.

It is still work in progress. The two example files in the `examples/` folder ""work"" already so you can try them, but there are **many** things that need to be thoroughly reviewed/tested/improved.

#### Implementation philosophy

It is an attempt to leverage the Cythonized tree growing procedure from scikit-learn, while implementing the M5P algorithm from Quilan (M5) +Wang (M5P), inspired by the M5P Weka classes. Note that there are significant discrepanties between the Weka implementation choices and the papers. But since Weka is definitely the reference for this kind of models, I am wondering if we should maybe offer both sets of parameters to let the users choose.

Note that other reference implementations also exist in R (cubist) and in python (LearningX).

#### Scientific references

Ross J. Quinlan: Learning with Continuous Classes. In: 5th Australian Joint Conference on Artificial Intelligence, Singapore, 343-348, 1992.

Y. Wang, I. H. Witten: Induction of model trees for predicting continuous classes. In: Poster papers of the 9th European Conference on Machine Learning, 1997.

#### Reference Issues
Fixes #13106 
",843222
939,2019-04-25T10:21:31Z,2019-08-06T17:03:01Z,,,"#### Reference Issues/PRs
Fixes #10912.

#### What does this implement/fix? Explain your changes.
The original issue #10912 is about 2D sample weight for MultiOutputRegressor.
I made modifications in MultiOutputEstimator, so that this feature is available for not only MultiOutputRegressor but also MultiOutputClassifier.

#### Any other comments?
2D sample weight is useful in my use case, because different outputs have different numerical ranges.
",843222
940,2019-04-24T14:34:13Z,2020-03-02T19:00:00Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Bonification of sklearn.metrics.classification.precision_recall_fscore_support so it can take many beta values in input. If so, it returns an F-score for each beta.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
941,2019-04-21T03:34:46Z,2019-08-06T16:59:48Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs

Partial fix to #13383

#### What does this implement/fix? Explain your changes.

I propose two modifications to the example `examples/decomposition/plot_pca_vs_fa_model_selection.py` mentioned in #13383 : 

- Increase the tolerance of the FactorAnalysis: `FactorAnalysis(tol=1)`
- Do not run CV for `n_components = 45`.

The build time (on my computer) decreases from 11.846 to 3.437 seconds.

These modifications are motivated by the fact that among the 3 dimensional reduction techniques trained, only Factor Analysis for 40 and 45 components takes some significant time to fit:

```
Total running time of the script: ( 0 minutes 11.846 seconds)

I - Homoscedastic noise
0 components...
    Time PCA: 0:00:00.022663
    Time FA: 0:00:00.042642
5 components...
    Time PCA: 0:00:00.018533
    Time FA: 0:00:00.042504
10 components...
    Time PCA: 0:00:00.017095
    Time FA: 0:00:00.056470
15 components...
    Time PCA: 0:00:00.023689
    Time FA: 0:00:00.079793
20 components...
    Time PCA: 0:00:00.018067
    Time FA: 0:00:00.089906
25 components...
    Time PCA: 0:00:00.016740
    Time FA: 0:00:00.105046
30 components...
    Time PCA: 0:00:00.017797
    Time FA: 0:00:00.120986
35 components...
    Time PCA: 0:00:00.017244
    Time FA: 0:00:00.188839
40 components...
    Time PCA: 0:00:00.017586
    Time FA: 0:00:02.802308
45 components...
    Time PCA: 0:00:00.017656
    Time FA: 0:00:01.555045
Time PCA MLE: 0:00:00.000007

II - Heteroscedastic noise
0 components...
    Time PCA: 0:00:00.016803
    Time FA: 0:00:00.026970
5 components...
    Time PCA: 0:00:00.016859
    Time FA: 0:00:00.069176
10 components...
    Time PCA: 0:00:00.016116
    Time FA: 0:00:00.080635
15 components...
    Time PCA: 0:00:00.016484
    Time FA: 0:00:00.101170
20 components...
    Time PCA: 0:00:00.016745
    Time FA: 0:00:00.116705
25 components...
    Time PCA: 0:00:00.015989
    Time FA: 0:00:00.123466
30 components...
    Time PCA: 0:00:00.016480
    Time FA: 0:00:00.126200
35 components...
    Time PCA: 0:00:00.016477
    Time FA: 0:00:00.158986
40 components...
    Time PCA: 0:00:00.017423
    Time FA: 0:00:02.712774
45 components...
    Time PCA: 0:00:00.015865
    Time FA: 0:00:01.289101
Time PCA MLE: 0:00:00.000016
```

After applying the 2 modifications mentioned above, we achieve significant speed improvements:

```
I - Homoscedastic noise
0 components...
    Time PCA: 0:00:00.025566
    Time FA: 0:00:00.036936
5 components...
    Time PCA: 0:00:00.021508
    Time FA: 0:00:00.045077
10 components...
    Time PCA: 0:00:00.020512
    Time FA: 0:00:00.043922
15 components...
    Time PCA: 0:00:00.015972
    Time FA: 0:00:00.072845
20 components...
    Time PCA: 0:00:00.016356
    Time FA: 0:00:00.088447
25 components...
    Time PCA: 0:00:00.017344
    Time FA: 0:00:00.096822
30 components...
    Time PCA: 0:00:00.016814
    Time FA: 0:00:00.112733
35 components...
    Time PCA: 0:00:00.016290
    Time FA: 0:00:00.185826
40 components...
    Time PCA: 0:00:00.016499
    Time FA: 0:00:00.325999
Time PCA MLE: 0:00:00.000008

II - Heteroscedastic noise
0 components...
    Time PCA: 0:00:00.015870
    Time FA: 0:00:00.028239
5 components...
    Time PCA: 0:00:00.018323
    Time FA: 0:00:00.065301
10 components...
    Time PCA: 0:00:00.015181
    Time FA: 0:00:00.074517
15 components...
    Time PCA: 0:00:00.018003
    Time FA: 0:00:00.102947
20 components...
    Time PCA: 0:00:00.016931
    Time FA: 0:00:00.112086
25 components...
    Time PCA: 0:00:00.017937
    Time FA: 0:00:00.114989
30 components...
    Time PCA: 0:00:00.017200
    Time FA: 0:00:00.126424
35 components...
    Time PCA: 0:00:00.017375
    Time FA: 0:00:00.140677
40 components...
    Time PCA: 0:00:00.016992
    Time FA: 0:00:00.270111
Time PCA MLE: 0:00:00.000014
```

The printed chosen best numbers of components remain unchanged (I added the nature of the noise to improve its readability):

```
Homoscedastic Noise:
best n_components by PCA CV = 10
best n_components by FactorAnalysis CV = 10
best n_components by PCA MLE = 10
Heteroscedastic Noise:
best n_components by PCA CV = 35
best n_components by FactorAnalysis CV = 10
best n_components by PCA MLE = 38
```

However the 2 plots changed a bit:

**Homoscedastic Noise**

*Original plot*
![homo](https://user-images.githubusercontent.com/1850174/56464272-aaf52880-63b3-11e9-82e3-218d2111b609.png)

*Modified plot*
![homo_tol=1_loop_to_40](https://user-images.githubusercontent.com/1850174/56464292-edb70080-63b3-11e9-8fca-f3856fd1cfcf.png)

**Heteroscedastic Noise**

*Original plot*
![hetero](https://user-images.githubusercontent.com/1850174/56464300-07f0de80-63b4-11e9-970f-aa6ec765232d.png)

*Modified plot*
![hetero_tol=1_loop_to_40](https://user-images.githubusercontent.com/1850174/56464303-0c1cfc00-63b4-11e9-83fd-53aa1e83752d.png)

As you can see, estimated FA scores (average log-likelihood) are different for 40 components for both noises (and the points for 45 components are removed from the 2 plots). However I think that the plots are ok, because the interpretation is left unchanged as this part of the plot isn't important in the example.


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
942,2019-04-18T12:16:49Z,2020-03-02T19:00:02Z,,,"I want to resolve the [WIP] Fuzzy c-means clustering #9327 in a nice way with few commits.

I implement the Fuzzy c-means clustering in a concise way. And in order to make the algorithm continuable, I copy the frame of k_means_.py and implement FCM algorihtm based on it.

One more thing, it is the first time that I try to be a contributor of a public repository, so probably I have made a lot of mistakes. Hope you(someone may concern) can help me to point them out.

Thanks again and Enjoy the Code.

Good days.

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
943,2019-04-16T23:08:55Z,2020-02-14T17:10:29Z,,,"I found 5 ""todo"" comments in the Glossary, and added definitions for 4 out of 5 of them.
Partially fixes #13533.

The definitions I've provided come from me and my team and are for the following terms (previously left blank with ""todo"" comment):
1) Kernel
2) score_samples
3) density_estimator
4) Multilabel classification ",843222
944,2019-04-16T01:01:20Z,2019-08-29T12:18:30Z,,,"
I changed the eigen_solver to 'amg' in the function sklearn.cluster.SpectralClustering. The runtime on CircleCI is 4.4 sec after the change.

",843222
945,2019-04-15T22:29:01Z,2020-03-02T19:00:04Z,,,"Continuation of PR #7266 addressing issue #6656.

TODO: 
- [x] ~implement logic for new trees in #12807~ (already done by #15582)
- [x] discuss API (not sure about introducing 2 new kwargs `increasing` and `decreasing`) => let's go with the `montonic_cst` array
- [x] add tests
- [ ] improve tests as the current ones don't fail despite erroneous implementation",843222
946,2019-04-12T10:44:00Z,2020-03-02T19:00:05Z,,,"Fixes #4194

Checks the metric parameters of the given tree object and raise an error if the metric params of the given object and the self don't match.",843222
947,2019-04-07T02:02:07Z,2020-03-02T19:00:06Z,,,"…ctly

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes issue #13587.

#### What does this implement/fix? Explain your changes.
Fixes an issue where BaggingClassifier uses class labels as array indices when voting. This only occurs when _base_estimator_ does not implement _predict_proba_.

#### Any other comments?
This PR also provides an additional unit test that helps verify BaggingClassifier predicts valid results when voting.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
948,2019-03-28T16:17:23Z,2020-03-02T19:00:06Z,,,"1. The **default** `eps=0.5` parameter makes little sense for most data sets. A good value depends on the data set and distance function. 0.5 may work for a particularly dimensionality of normalized data with Euclidean and a typical toy example data set size, but usually will be a bad choice. Even most of the examples from sklearn use different values. If you have lots of data points, a smaller eps may be necessary, for example.
Hence, the user has to experiment with this parameter, the default usually won't give good results. By making it required instead of optional, we can prevent this mistake.

2. The **description** of the `eps` parameter is misleading. This was recently discussed on SO: https://stackoverflow.com/a/55388827/1939754 (""DBSCAN eps incorrect behaviour"")
As user Anonymousse stated, the old description is wrong (it's distance from some core point, not pairwise distances), but in particular it also is not a global diameter.",843222
949,2019-03-25T12:49:17Z,2020-03-02T19:00:07Z,,,"This PR fixes part of #11536 on windows targets.

On windows platforms, liblinear and libsvm have strong convergence issues because of the way random numbers are generated: max random number in Windows is 15 bits (even on 64 bit windows), which is 32767, while max random number in linux+GCC is 31 bits (resp. 63 bits in 64 bits systems I guess) so that's 2147483647 (resp 9223372036854775807). 

If I understand correctly, these random numbers are used in the coordinate gradient descent algorithms, to find the next coordinate to act upon. When the dimensionality (e.g. number of samples) is large, the random number generator on windows has a hard time to explore all dimensions.

This is a known bug documented in liblinear FAQ (strangely enough, not the libsvm FAQ) but the [proposed workaround](https://www.csie.ntu.edu.tw/~cjlin/liblinear/FAQ.html#windows_binary_files) was wrong.

I made a patch for this years ago, that was approved by several users yet never merged: https://github.com/cjlin1/liblinear/pull/28 .

Note that another user reported it on [libsvm](https://github.com/cjlin1/libsvm/issues/103), I proposed a similar PR there: https://github.com/cjlin1/libsvm/pull/140.

Since I realized that sklearn has it own internal copy of both libraries, this is the fix. ",843222
950,2019-03-24T10:05:43Z,2020-03-02T19:00:08Z,,,"
Fixes #13502 
Fixes #13503 

I think both of these problems I'm encountering are to do with changes which preserve datatypes. The first is a numerical issue and the second is a Cython error.

The changes I'm suggesting are pretty simple, though there may be better, more fundamental, fixes from those more familiar with the history.",843222
951,2019-03-22T05:38:42Z,2020-03-02T19:00:08Z,,,"#### Reference Issues/PRs
#7050 (PLSRegression VIP score calculation)

#### What does this implement/fix? Explain your changes.
Adds the Variable Importance in Projection (VIP) score to PLSRegression (as vip_ attribute)

",843222
952,2019-03-21T16:39:18Z,2020-03-02T19:00:09Z,,,"#### What does this implement/fix? Explain your changes.

This fixes the conversion of rgb to hexstrings in `sklearn.tree.export` and
adds the possibility for the user to supply his/her own colors.


```
from sklearn.tree.export import plot_tree
from sklearn.datasets import make_moons
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt

X, y = make_moons()
tree = DecisionTreeClassifier()
tree.fit(X, y)

plt.figure(dpi=300)
plot_tree(tree, fill_colors=['xkcd:red', 'xkcd:sky'], filled=True)
plt.savefig('tree.png')

```

![tree](https://user-images.githubusercontent.com/5488440/54769220-11dec080-4c01-11e9-9d6c-4ce172852460.png)
",843222
953,2019-03-21T11:48:42Z,2020-03-02T19:00:10Z,,,"#### What does this implement/fix?
This PR addresses the problem of computing the area under curves that its x changes direction as shown in the following figures.

In this case, we want to compute the area under the curve, without miscalculating the loop effect on the area.
![PR_thresholds](https://user-images.githubusercontent.com/42140441/55454161-bdccd680-5619-11e9-853e-7592679677d7.png)
![PR_parametric](https://user-images.githubusercontent.com/42140441/55454162-bdccd680-5619-11e9-9f46-ce96484d830b.png)

```Mathmatica
p[s_] := s (1 + Sin[s*2*Pi]/2)
r[s_] := 1 - s (1 - Sin[s*3*Pi]/2)
Plot[{p[s], r[s]}, {s, 0, 1}, AxesLabel -> {""s"", ""f(x)""}, 
 PlotLegends -> ""Expressions"", 
 PlotLabel -> ""Precision & Recall vs Threshold"", 
 BaseStyle -> {FontWeight -> ""Bold"", FontSize -> 16}]
ParametricPlot[{p[s], r[s]}, {s, 0, 1}, 
 AxesLabel -> {""r(s)"", ""p(s)""}, 
 PlotLabel -> ""Precision-Recall parametric curve"", 
 BaseStyle -> {FontWeight -> ""Bold"", FontSize -> 16}]
```

This fix relaxes the input requirements, as there is no need to limit the support to curves having their x values monotonically increasing or decreasing. The approach is to split the curve into segments that are monotonically increasing or decreasing, and compute the area under each individual segment separately.",843222
954,2019-03-18T20:40:45Z,2020-03-02T19:00:10Z,,,"#### Reference Issues/PRs

#10368

Also refer to old closed pull request #12578

#### What does this implement/fix? Explain your changes.

Adds three new RFE/RFECV parameters and associated functionality:

    tune_step_at : int or float or None, optional (default=None)
        Number of remaining features reached when ``tuning_step`` is used
        rather than ``step``. May be specified as an (integer) number of
        remaining features or, if within (0.0, 1.0), a percentage (rounded
        down) of the original number of features. If original number of
        features and parameter settings would result in stepping past
        ``tune_step_at``, then the number of features removed in the iteration
        prior to stepping over will adjust to arrive at this value.

    tuning_step : int or float, optional (default=1)
        Step to use starting at ``tune_step_at`` number of remaining features.
        If greater than or equal to 1, then ``tuning_step`` corresponds to the
        (integer) number of features to remove at each iteration. If within
        (0.0, 1.0), then ``tuning_step`` corresponds to the percentage (rounded
        down) of features to remove at each iteration.

    reducing_step : boolean, optional (default=False)
        If true and ``step`` or ``tuning_step`` is a float, the number of
        features removed is calculated as a fraction of the remaining features
        in that iteration. If false, the number of features removed is constant
        across iterations and a fraction of the original number of features for
        ``step`` or fraction of the ``tune_step_at`` number of remaining
        features for ``tuning_step``.

#### Comments

New options and their default settings do not alter existing default behavior.

",843222
955,2019-03-11T16:25:40Z,2020-03-02T19:00:11Z,,,"#### Reference Issues/PRs

Fixes #4632. See also #10806. 

#### What does this implement/fix? Explain your changes.

This PR adds `sample_weight` to metrics in cross validation routines by propagating the `sample_weight` parameter passed to `fit_params`.  It does not provide the ability to use different `sample_weight` values in validation.

#### Any other comments?

A [dask](http://dask.org) version of this PR has been created at https://github.com/dask/dask-ml/pull/481

For more detailed information on the motivation for this PR, see http://deaktator.github.io/2019/03/10/the-error-in-the-comparator/",843222
956,2019-03-04T15:25:12Z,2020-03-02T19:00:12Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
#13308 

#### What does this implement/fix? Explain your changes.
Modification of the NMF class to accept mini batches. For the moment, it works only with
`solver='mu'` and `beta_loss='kullback-leibler`.

I also added the benchmark code. 

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
957,2019-03-01T15:15:03Z,2020-03-02T19:00:12Z,,,"WeightVector is used in #13346 and has attributes that cannot be fused.
https://github.com/scikit-learn/scikit-learn/blob/984871b89baa183b1d0e284ac9bb22de06a59e8d/sklearn/utils/weight_vector.pxd#L12-L20

This PR uses Tempita to allow float32 float64.

cross ref: #11000
",843222
958,2019-03-01T13:06:45Z,2020-03-02T19:00:13Z,,,"If one of the features is Pandas Categorical it is supported by OneHotEncoder and the categories will be kept as set

TODO:
- [ ] Validate the dtype between fit and transform (for now only allow exactly the same dtype)
- [ ] Deprecation path:
  - [ ] By default still do it the old way, but provide a way to opt in to the new behaviour
- [ ] Test that the order of the categories is used",843222
959,2019-03-01T10:18:01Z,2020-03-02T19:00:14Z,,,"Works on #11000 
close #9084 (takes over)",843222
960,2019-02-28T14:48:42Z,2020-03-02T19:00:14Z,,,"
#### Reference Issues/PRs

Solve #13330

#### What does this implement/fix? Explain your changes.

By deactivating some undocumented debugging stats: improves perf gain with n_jobs > 1 for nearest neighbors based on tree algorithms (cf. benchmark in issue) and achieves almost linear perf increase with parallelism.

#### Any other comments?

I can notify the 2 repos that use those stats (https://github.com/fcrimins/py_idistance/blob/master/idist.py and https://github.com/wilseypa/dataAnalysis-scripts/blob/master/dataGeneration/GeneratorProject/ScikitSeqs.py) that those stats won't be computed anymore.",843222
961,2019-02-28T09:28:44Z,2020-03-02T19:00:15Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

Implementation of a mini-batch NMF with multiplicative updates.
#13308 
#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
962,2019-02-27T16:57:04Z,2020-03-02T19:00:16Z,,,"#### Reference Issues/PRs
Fixes #11536 

#### What does this implement/fix? Explain your changes.
This Pull request is meant to implement an automatic determination of the value of convergence parameters (max_iter and tol).

#### TODO

- [x] Initialize the structure of the code
- [x] Fill in the blank left in the code structure to implement the switch on the solver type to determine max_iter and tol
- [x] Set defaults corresponding to solver defaults
- [ ] Check if solver defaults generates convergence warnings
- [ ] Identify better defaults through benchmarks
- [ ] Investigate changing the type of tol of lbfgs to ftol rather than pgtol",843222
963,2019-02-27T13:07:07Z,2020-02-18T19:11:40Z,,,"Build on top of #12627 but provides users with a nicer interface:
Again using this example:
https://scikit-learn.org/dev/auto_examples/compose/plot_column_transformer_mixed_types.html

We now have:
```
clf.named_steps['classifier'].input_features_
```

or alternatively:
```
clf.get_feature_names(X.columns.map(lambda x: x.upper()))
clf.named_steps['classifier'].input_features_
```

Transformers not implementing get_feature_names after this PR are:
['AdditiveChi2Sampler', 'FunctionTransformer', 'Imputer' (deprecated one), 'IterativeImputer', 'KBinsDiscretizer', 'KernelCenterer', 'KernelPCA', 'MissingIndicator']

possibly others that I can't test, like TfidfTransformer.

One issue I haven't thought about enough is naming for transformers like pca.
Producing ""pca0, pca1"" etc is good in a pipeline but in a ColumnTransformer it will probably lead to redundant names.


Currently suggested names:
``feature_names_in_``, ``feature_names_out_``(?), ``update_feature_names(feature_names_in)`` (or should it be input_feature_names here? probably better be consistent, right?)",843222
964,2019-02-27T10:53:18Z,2020-03-02T19:00:18Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
this PR works on #11000 by preserving the dtype float32 in Factor Analysis.

cross reference: [#8769 (comment)](https://github.com/scikit-learn/scikit-learn/issues/8769#issuecomment-306725215)
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
965,2019-02-27T10:09:53Z,2019-02-28T09:00:14Z,,,"Here are several suggestions can be made more intuitive and easy to read.

In particular, ""iterative imputation"" to be plotted at the end may be a more accurate phrasing than ""multivariate imputation"" since the same supervised estimator - kNN - is simply used repeatedly while the number of input or output dimensions does not change.

cf. #12852",843222
966,2019-02-26T11:34:17Z,2020-03-02T19:00:19Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
this PR works on #11000 by preserving the dtype in Latent Dirichlet Allocation.

cross reference: [#8769 (comment)](https://github.com/scikit-learn/scikit-learn/issues/8769#issuecomment-306725215)
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
967,2019-02-26T09:14:49Z,2020-03-27T09:16:57Z,,,"#### Reference Issues/PRs
Close, #4143, #9630
Related SLEP: 
https://github.com/scikit-learn/enhancement_proposals/pull/15
#### What does this implement/fix? Explain your changes.
Implement estimators that can change the sample size at fit

#### Other comments
How could resamplers work with ColumnTransformer or FeatureUnion? 
Does the API allow the modification of `y` values? If so, should we reimplement TransformedTargetRegressor under this API? 

#### Plan
- [x] handle kwargs behavior
- [x] Rewrite all relevant common_tests involving fit to test fit_resample (see common test failures for FilterNaN)
- [x] FilterNaN Tests
- [ ] More estimator checks for resamplers
- [ ] More ResampledTrainer tests (including with FilterNaN and outlier rejectors)
- [ ] Docstrings for ResampledTrainer
- [ ] Port RandomOverSampler and RandomUnderSampler from imblearn 
- [ ] Sphinx Example(s)
",843222
968,2019-02-25T16:37:25Z,2020-03-02T19:00:20Z,,,"There are a lot of warnings generated during testing. Some of them might be ignored but some might be more important. Here I add ignore to the SparseEfficiencyWarning in test_validation

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
969,2019-02-25T13:38:39Z,2020-03-02T19:00:20Z,,,"Fixes #1572 
Follow-up of #7590 
TODO:
- [x] <strike>Try to be more robust by maybe testing only the score (if possible), with a certain tolerance, and for certain random seeds (or if testing the predict also maybe test that like 90% of predictions are the same ?)</strike> 
After discussion with @agramfort , it would be better to test on a realistic dataset which would avoid weird instable behaviours
- [x] address https://github.com/scikit-learn/scikit-learn/pull/13246#pullrequestreview-208581402
- [x] Remove the fix for the Affinity propagation and put it in another PR
- [ ] Wait for the tests to pass thanks to other PR's fix (see for instance #13336)",843222
970,2019-02-23T00:42:49Z,2020-03-02T19:00:21Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
 Fixes #12971 
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
1. code to raise an error if the number of groups is greater than 1
2. document the behaviour when there is a capturing group
#### Any other comments?
There is still need for deprecate support and backward compatibility.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
971,2019-02-22T17:25:02Z,2020-03-02T19:00:22Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #8607. Takes over #5181 and #8732.


#### What does this implement/fix? Explain your changes.
Actually I mainly used the @potash changes from #5181 and performed comparisons of variations of random forests on a standard benchmark that we have in `imbalanced-learn` . The Balanced Random Forest is triggered using the `balanced_bootstrap` in `class_weight`.

#### Any other comments?
Regarding the experiment. I build all forests using 100 trees and performed 5-fold cross-validation. The details of the datasets used can be found [here](https://imbalanced-learn.org/en/stable/generated/imblearn.datasets.fetch_datasets.html).

The following table contains the performance of the variations in different datasets in terms of roc auc. With `brf` is stated the new `balanced_boostrap` .


dataset_name | brf | rf | rf_balanced | rf_balanced_subsample
-- | -- | -- | -- | --
abalone | 0.855309 | 0.830593 | 0.834113 | 0.833721
abalone_19 | 0.812186 | 0.682914 | 0.694902 | 0.732027
arrhythmia | 0.906580 | 0.951122 | 0.974703 | 0.972892
car_eval_34 | 0.983932 | 0.935592 | 0.956367 | 0.953385
car_eval_4 | 0.958985 | 0.938677 | 0.963698 | 0.964530
coil_2000 | 0.742975 | 0.696293 | 0.699116 | 0.694468
ecoli | 0.905476 | 0.895000 | 0.910952 | 0.906905
isolet | 0.989580 | 0.992651 | 0.991919 | 0.991755
letter_img | 0.999549 | 0.999833 | 0.999830 | 0.999850
libras_move | 0.953613 | 0.932691 | 0.958367 | 0.956466
mammography | 0.956944 | 0.938190 | 0.909950 | 0.910677
oil | 0.873508 | 0.851528 | 0.879456 | 0.850017
optical_digits | 0.995662 | 0.997528 | 0.998007 | 0.997893
ozone_level | 0.880928 | 0.844524 | 0.859265 | 0.873991
pen_digits | 0.999836 | 0.999851 | 0.999851 | 0.999845
protein_homo | 0.984600 | 0.965014 | 0.970406 | 0.962937
satimage | 0.933214 | 0.936549 | 0.936868 | 0.933069
scene | 0.780679 | 0.722448 | 0.774284 | 0.773774
sick_euthyroid | 0.982234 | 0.975710 | 0.984795 | 0.983522
solar_flare_m0 | 0.748597 | 0.724919 | 0.695480 | 0.687116
spectrometer | 0.975155 | 0.973201 | 0.973088 | 0.984658
thyroid_sick | 0.993552 | 0.996121 | 0.996836 | 0.996300
us_crime | 0.919144 | 0.915725 | 0.911504 | 0.915458
webpage | 0.803485 | 0.901795 | 0.789156 | 0.788862
wine_quality | 0.835976 | 0.827878 | 0.805647 | 0.809426
yeast_me2 | 0.934917 | 0.929296 | 0.924718 | 0.913895
yeast_ml8 | 0.611755 | 0.586060 | 0.600982 | 0.606157

The average ranking across the datasets are shown in the following table.
As we can see all forests perform similarly. (The lower the better)

Forest Name | Average Rank
-- | --
brf |  2.148148
rf_balanced | 2.259259
rf_balanced_subsample | 2.703704
rf  | 2.888889

The average fit time of each forest for each datasets is presented in the following table.

dataset_name | brf | rf | rf_balanced | rf_balanced_subsample
-- | -- | -- | -- | --
abalone | 0.422506 | 1.042681 | 0.864763 | 1.263026
abalone_19 | 0.309891 | 0.659666 | 0.594561 | 1.007489
arrhythmia | 0.308522 | 0.574814 | 0.461610 | 0.612353
car_eval_34 | 0.247720 | 0.263163 | 0.262382 | 0.540598
car_eval_4 | 0.309501 | 0.366200 | 0.368351 | 0.578528
coil_2000 | 1.001036 | 3.873933 | 3.152484 | 6.266058
ecoli | 0.299333 | 0.288385 | 0.298355 | 0.323187
isolet | 4.061236 | 18.669528 | 22.482264 | 25.195425
letter_img | 0.761140 | 2.690679 | 2.405421 | 4.009034
libras_move | 0.343714 | 0.348212 | 0.415275 | 0.404716
mammography | 0.592800 | 1.921325 | 1.498232 | 2.192701
oil | 0.319863 | 0.580092 | 0.492893 | 0.652238
optical_digits | 0.783819 | 1.477507 | 1.128903 | 1.812815
ozone_level | 0.357791 | 1.447202 | 1.026845 | 1.307213
pen_digits | 0.959196 | 2.243534 | 2.317744 | 4.168145
protein_homo | 6.654938 | 288.988578 | 145.843798 | 162.089329
satimage | 0.888224 | 1.667351 | 2.026317 | 2.429273
scene | 0.726925 | 4.679455 | 3.539798 | 3.904238
sick_euthyroid | 0.320645 | 0.502668 | 0.484485 | 0.736504
solar_flare_m0 | 0.347235 | 0.446362 | 0.429545 | 0.671594
spectrometer | 0.241266 | 0.436779 | 0.355837 | 0.446164
thyroid_sick | 0.516746 | 0.962520 | 0.977183 | 1.214147
us_crime | 0.430915 | 1.081393 | 1.015309 | 1.199680
webpage | 2.543454 | 43.693878 | 25.273632 | 29.456088
wine_quality | 0.432087 | 1.762763 | 1.164291 | 1.587972
yeast_me2 | 0.306763 | 0.368546 | 0.358770 | 0.563083
yeast_ml8 | 0.616068 | 6.964830 | 2.893425 | 2.827342

The average rankings for the time across the datasets are shown in the following table.
We can observe that almost always the Balanced Random Forest is the fastest.

Forest Name | Average Rank
-- | --
brf                     | 1.074074
rf_balanced         |     2.296296
rf                      | 2.925926
rf_balanced_subsample    |3.703704

So, over all I think that it could be a nice addition

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
972,2019-02-20T23:39:53Z,2020-03-02T19:00:22Z,,,"#### Reference Issues/PRs
Related to #6322
Resolves #14376

#### What does this implement/fix? Explain your changes.
There are two parameters being added to `TimeSeriesSplit` which make it more flexible, particularly when working with financial data.

##### 1. `test_size` : int, optional
By default, `TimeSeriesSplit` divides the data into `n_splits + 1` folds, and the size of each split's test set is `n_samples / (n_splits + 1)`. However, it is sometimes useful to shift the balance between train and test data when doing cross validation relative to domain use cases.

Example use case: I have 4 years (4 * 252 trading days) of stock price data. I wish to run cross-validation with 4 splits. Each split I want to train on 2 years (504 samples) and test on 6 months (126 samples). Currently, using `TimeSeriesSplit(n_splits=4, max_train_size=504)` will only yield test sets of exactly 201 samples, and a train sets starting with 204 samples increasing until 504.
```
>>> x = np.arange(252 * 4)
>>> cv = TimeSeriesSplit(n_splits=4, max_train_size=504)
>>> for train_index, test_index in cv.split(x):
...     print(""Train Size:"", len(train_index), ""Test Size:"", len(test_index))
Train Size: 204 Test Size: 201
Train Size: 405 Test Size: 201
Train Size: 504 Test Size: 201
Train Size: 504 Test Size: 201
```
The optional `test_size` parameter allows me to control the number of samples used for each test split and use all remaining samples for training.
```
>>> x = np.arange(252 * 4)
>>> cv = TimeSeriesSplit3(n_splits=4, max_train_size=504, test_size=126)
>>> for train_index, test_index in cv.split(x):
....     print(""Train Size:"", len(train_index), ""Test Size:"", len(test_index))
Train Size: 504 Test Size: 126
Train Size: 504 Test Size: 126
Train Size: 504 Test Size: 126
Train Size: 504 Test Size: 126
```
See code docstring for a more visual example. `Test_size=None` by default preserves current functionality.

##### 2. `gap` : int, default=0
Currently each train/test set are adjacent to each other in sequence. However, for some use cases it may be important remove ""gap samples"" between each train set and test set when look-ahead bias can result from labeling the data.

Example use case: I am building a classifier to buy/sell stocks. I have some input features for each day, and I label my data by looking ahead 2 days and seeing if the stock price increases or decreases. Since I am looking ahead 2 days to create my data labels, I need to insert a 2 day ""gap"" between my train and test sets for cross validation to prevent look-ahead bias in my validation score.
```
>>> x = np.arange(15)
>>> cv = TimeSeriesSplit(n_splits=3)
>>> for train_index, test_index in cv.split(x):
...      print(""TRAIN:"", train_index, ""TEST:"", test_index)
TRAIN: [0 1 2 3 4 5] TEST: [6 7 8]
TRAIN: [0 1 2 3 4 5 6 7 8] TEST: [ 9 10 11]
TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11] TEST: [12 13 14]
```

If I label my data by looking ahead 2 days, then this is not representative of real-life training. I would not have those 2 extra days to label my data if training a model today. I need to remove the last two elements of each training data set to get an authentic cross validation score.
```
>>> x = np.arange(15)
>>> cv = TimeSeriesSplit(n_splits=3, gap=2)
>>> for train_index, test_index in cv.split(x):
...      print(""TRAIN:"", train_index, ""TEST:"", test_index)
TRAIN: [0 1 2 3] TEST: [6 7 8]
TRAIN: [0 1 2 3 4 5 6] TEST: [ 9 10 11]
TRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [12 13 14]
```
The default value of `gap=0` maintains current functionality.

#### Any other comments?


",843222
973,2019-02-11T14:58:39Z,2020-03-02T19:00:23Z,,,"#### Reference Issues/PRs
Fixes #13117 

#### What does this implement/fix? Explain your changes.
Fixes documentation of class `LinearModelCV()`, since it there is a check that
does not allow `y` to have more than 1 dimension (multiple outputs)

#### Any other comments?
I thinks this could be improved, since both `ElasticNet()` and `Lasso()` support multiple targets. I don't see a reason why `LinearModelCV()` could not accept multiple targets as well.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
974,2019-02-08T15:19:53Z,2020-03-02T19:00:24Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
See also #10972 #11818 #10733

#### What does this implement/fix? Explain your changes.
This replaces the functional `load_{datset name}` functions in the datasets base.py module. It improves the current implementation by standardizing the attributes of Bunch objects output by the API and adding more tests. My hope is to integrate (eventually) all datasets into a single API so integration with other sklearn modules and other packages can be enhanced by having a object like `Dataset` with helpful methods. This is inspired by the Dataset APIs found in common frameworks for deep learning (like PyTorch, etc.). 

#### Any other comments?
I really look forward to comments and code review. If you doubt the helpfulness of the current PR, you might be correct - but please look at as an incremental improvement which can be built upon by the community to make interacting with data in sklearn predictable and addition of more data (we love data!) easy.

If you would like to collaborate on this PR, or (hopefully) future improvements  on this PR, please let me know!

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
975,2019-02-08T05:54:38Z,2020-03-02T19:00:24Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs


#### What does this implement/fix? Explain your changes.
* **Precision and Recall Metrics are heavily influenced** by **class imbalance** in a binary classification. 
* By computing **minimum AUCPR, minimum Average Precision, minimum Precision** for a given class balance, users will be able to make better quantitative comparisons on how this **imbalance affects their precision-recall based metrics**, without changing anything about the learner. 
* To quote the paper in which this is from: ""For example, **changing the skew** of a data set from 0.01 to 0.5 **increases the minimum AUCPR** by approximately 0.3. This leads to an automatic jump of **0.3 in AUCPR simply by changing the data set** and with absolutely no change to the learning algorithm""

My code comes from translating Theorems 1 to 3 to code from Unachievable Region in Precision-Recall Space (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858955/)

* Added normalized_aucpr function
  * which computes the normalized average precision and AUCpr, with respect to the smallest average precision/AUCpr possible due to the skew.
  * There is also a random guess normalization strategy to compute how well a classifier does in Precision-Recall against a random guess prediction.
  * This statistic will help data scientists answer the question of how well their precision-recall ranking metrics will improve above the smallest average precision/AUCpr/random guess possible due to the class imbalance.

Willing to take any comments/feedback.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.
Thanks for contributing!
-->
",843222
976,2019-02-06T02:40:56Z,2020-03-22T10:23:03Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #13056 


#### What does this implement/fix? Explain your changes.
There was an error in the implementation of the FastICA algorithm since, when the `whiten` parameter was set to `True`, the variance of the result was not `1.0`. In this PR, this is corrected and also the default value `True` of the `whiten` parameter is changed to `'unit-variance'`.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
977,2019-01-28T18:29:52Z,2019-08-06T15:45:39Z,,,"`CalibratedClassifierCV` now handles the calibration process in such a way that probability estimates can be calibrated for multi-label targets. Also loosens input validation requirements to better interoperate with `Pipeline`.

#### Reference Issues/PRs
Fixes #8710.

#### What does this implement/fix? Explain your changes.
Changes include (roughly in source code order):
* Looser input validation on arguments passed to wrapped classifiers (fixes #8710)
* Target classes and type are determined before cross-validation, rather than on each fold individually
* Label predictions from `CalibratedClassifierCV.predict` are obtained using `LabelBinarizer.inverse_transform`, which supports multi-label predictions
* Specialized logic in `_CalibratedClassifier` for handling binary classification problems is tidied and more thoroughly commented
* Shape of uncalibrated estimates from wrapped classifier is checked against the expected shape in `_CalibratedClassifier`
* Simplification of logic in `_CalibratedClassifier.predict_proba` along with more comments explaining what's happening
* Tests for acceptance of 1D feature arrays as input and production of valid multi-label probability predictions

#### Any other comments?
Thanks for working on scikit-learn!",843222
978,2019-01-24T08:31:22Z,2020-03-02T19:00:26Z,,,"#### Reference Issues/PRs
Fixes #10168 #12285


#### What does this implement/fix? Explain your changes.
This is a follow-up on the stale PRs referenced above, the main diff is the fix for the previously failing unit test: 

https://travis-ci.org/scikit-learn/scikit-learn/jobs/437566342#L2818

```
        stress1 = mds.smacof(sim, normalize=True)[1]
        stress2 = mds.smacof(k * sim, normalize=True)[1]
    
        # Normed stress should be the same for
        # values multiplied by some factor ""k""
>       assert_allclose(stress1, stress2)
k          = 2
sim        = array([[0, 5, 3, 4],
       [5, 0, 2, 2],
       [3, 2, 0, 1],
       [4, 2, 1, 0]])
stress1    = 0.025998852705994606
stress2    = 0.033375197002665315
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/tests/test_mds.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python2.7/dist-packages/numpy/testing/utils.py:1183: in assert_allclose
    verbose=verbose, header=header)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
````

To my understanding, even using normalized stress, `smacof()` needs to be initialized at same configuration for the property `Normed stress should be the same for values multiplied by some factor ""k""`  to be true so I set `random_state` of `smacof()` to a fixed value. Dissimilarity matrix also needs to be large enough.

#### Any other comments?
The previous reviewer was @glemaitre . To my understanding review comments have been addressed but if something is missing, I'll do my best to fix it.



",843222
979,2019-01-22T23:43:11Z,2020-03-02T19:00:26Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
#12952

#### What does this implement/fix? Explain your changes.
Adds a partial_fit method for the NearestCentroid

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
980,2019-01-22T14:09:39Z,2020-03-02T19:00:27Z,,,"#### Reference Issues/PRs

Fixes  #11996

#### What does this implement/fix? Explain your changes.

I'll list the things I have implemented so far

- Draft implementation of nan compatible version of ```_encode``` named ```_nanencode```
- Improved implementation and added missing_value parameter to ```_nanencode```
- Added tests for ```_nanencode```

#### Any other comments?

Work in progress but feel free to comment/suggest improvements on the implementation
",843222
981,2019-01-21T19:11:39Z,2019-08-26T14:50:57Z,,,"Adding to #11977. This PR is a restart of #11370, which got messy.

Here is a quote from #11370 that explains what this PR does:

> This PR is an example that shows how to use IterativeImputer for Multiple Imputation.
> 
> As discussed in #11259, the defaults of IterativeImputer are such that single imputation is performed. Because the method is also quite powerful for Multiple Imputation, we agreed to make an example that shows the user how to use ImputerImputer to perform Multiple Imputation.
> 
> I made the document: examples/impute/plot_multiple_imputation.py and it shows 2 things:
> 
> Estimation of beta estimates and their standard error: compare IterativeImputer with using IterativeImputer as a MICE Imputer.
> How to use IterativeImputer as a MICE Imputer when making a prediction model (with train and test datasets).",843222
982,2019-01-21T17:52:31Z,2019-08-06T16:16:58Z,,,"#### Reference Issues/PRs
Fixes #13014

#### What does this implement/fix? Explain your changes.
Circle CI lint job will check for `[pypy]` marker in commit message and if available then it would trigger pypy test job.",843222
983,2019-01-21T16:26:37Z,2019-10-04T14:24:12Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
This is an alternative solution to issue #12712. This PR or PR https://github.com/scikit-learn/scikit-learn/pull/12968 can address the issue.

#### What does this implement/fix? Explain your changes.
This PR adds configuration options to change the suffix:
```python
 custom_autosummary_names_with_new_suffix = {
    'sklearn.cluster.dbscan',
    'sklearn.cluster.optics',
    'sklearn.covariance.oas',
    'sklearn.decomposition.fastica'
}
custom_autosummary_new_suffix = '-lowercase.rst'
custom_autosummary_generated_dirname = os.path.join('modules', 'generated')
```

This PR monkeypatches `os.path.join` used by [sphinx.ext.autosummary.generate](https://github.com/sphinx-doc/sphinx/blob/7ffd6ccee8b0c6316159c4295e2f44f8c57b90d6/sphinx/ext/autosummary/generate.py#L168) to return a filename with a new suffix.

#### Any other comments?
This PR contains less logic and gives the control to the user, thus a little simpler when compared to https://github.com/scikit-learn/scikit-learn/pull/12968.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
984,2019-01-17T12:26:31Z,2020-03-24T08:36:58Z,,,"This PR adds the **Tensor Sketch** [1] algorithm for polynomial kernel feature map approximation to the Kernel Approximation module. 

Tensor Sketch is a well established method for kernel feature map approximation, which has been broadly applied in the literature. For instance, it has recently gained a lot of popularity to accelerate certain bilinear models [2]. While the current kernel approximation module contains various kernel approximation methods, polynomial kernels are missing, so including TensorSketch completes the functionality of this module by providing an efficient and data-independent polynomial kernel approximation technique. 

The PR contains the implementation of the algorithm, the corresponding tests, an example script, and a description of the algorithm in the documentation page of the kernel approximation module. This implementation has been tested to produce the same results as the original matlab implementation provided by the author of the algorithm [1].

[1] [Pham, N., & Pagh, R. (2013, August). Fast and scalable polynomial kernels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 239-247). ACM.](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Fast+and+scalable+polynomial+kernels+via+explicit+feature+maps&btnG=)

[2] [Gao, Y., Beijbom, O., Zhang, N., & Darrell, T. (2016). Compact bilinear pooling. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 317-326).](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Compact+bilinear+pooling%2C+Y.+Gao&btnG=)

",843222
985,2019-01-14T09:11:47Z,2020-03-02T19:00:28Z,,,"Better error message when passing un-sortable data to the Encoders #12621

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
986,2019-01-09T17:21:43Z,2020-03-02T19:00:29Z,,,"
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
I did not create a separate issue since this is a small code change.

#### What does this implement/fix? Explain your changes.
This code adds a backward compatible flag `train_score_size` to the cross_validate function.
With this flag, we can request to subsample the training set for computing the train_scores
to save time. Currently, the documentation just warns users that setting `return_train_score` 
to True may be time-consuming and slow. This flag offers a way to improve this situation.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
987,2019-01-09T06:13:59Z,2020-03-02T19:00:30Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Resolves #12862 

#### What does this implement/fix? Explain your changes.

Implements the Good-Turing smoothing algorithm for Naive Bayes classifier and adds two other possible options.

#### Any other comments?

If the maintainers decide to add Jelinek-Mercer or Absolute Discounting, we need to use different default smoothing parameters depending on the selected algorithm. 

Since Good-Turing uses raw counts, the code won't work if the input is transformed using, for example, [tf-idf](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). Would throwing an error be a reasonable solution when the input is not raw counts?

At first, I implemented Simple Good-Turing operating on the entire matrix `self.feature_count_` but I the current solution is more readable. For more information on the notation see [Good-Turing Frequency Estimation Without Tears](https://www.grsampson.net/AGtf1.html).

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
988,2019-01-03T07:49:07Z,2020-03-02T19:00:30Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #12675 and will work on similar usages after having a check with the moderators.


#### What does this implement/fix? Explain your changes.
This changes use the `working_memory` parameter from config or the mentioned number of samples to be predicted per call to `predict_proba` in the `MLPClassifier`. In-code comments are present to make it more clear.

#### Any other comments?
Please let me know if its ok to make the same modification to other functions calling `_predict`, if yes, we can move the fix to the `_predict` function itself

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
989,2019-01-02T10:07:48Z,2020-03-02T19:00:31Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #12904 


#### What does this implement/fix? Explain your changes.

```
from sklearn.preprocessing import FunctionTransformer
import joblib

def f(x): return x*x
joblib.dump(FunctionTransformer(f), ""t.tmp"")

del f
unpickled = joblib.load(""t.tmp"") # Causes an exception
```


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
990,2019-01-01T13:40:30Z,2020-03-02T19:00:31Z,,,"Bugfix for not-yet-confirmed issue #12863: arpack returns singular values in ascending order, the opposite was supposed in sklearn

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes [MRG] not-yet-confirmed issue #12863 

<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

In the sklearn code it was assumed that the scipy wrapper for arpack singular value decomposition returns singular values (and corresponding vectors) in descending order. This is not the case. The ARPACK documentation (https://www.caam.rice.edu/software/ARPACK/UG/node136.html#SECTION001210000000000000000
) clearly states that the singular values are returned in ascending order.

The bug manifests in differing results depending on the solver:

```
import numpy as np
from sklearn import cluster

A= np.array([[-2, -4, 2], [-2, 1, 2], [4, 2, 5]])

sc= cluster.bicluster.SpectralCoclustering(n_clusters= 2, 
                                           svd_method='randomized')

sc.fit(A)
print(sc.column_labels_)
```
gives
```
[0 0 1]
```
, but
```
sc= cluster.bicluster.SpectralCoclustering(n_clusters= 2, 
                                           svd_method='arpack')
sc.fit(A)
print(sc.column_labels_)
```
gives
```
[0 0 0]
```

This bugfix makes the arpack call return the same result as the randomized sv based solution.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
991,2018-12-26T14:54:01Z,2020-03-02T19:00:32Z,,,"#### Referencing PR / Issue
This closes #6809 and closes #6833.
This also closes #12208 (by @nanshanli)

#### Note
This improves the error message to include NaN and inf values in metrics/classification.py

cc:  @tashay 
#wimlds 

",843222
992,2018-12-26T12:46:14Z,2020-03-26T10:19:59Z,,,"This PR continues the work of #4899. For now I've merged the master into the PR, made it compile and make the tests run. There are several issues which need to be fixed. The list will be updated as I encounter them. Also, not all of these items are necessarily open, I have only collected them from the comments on the original PR, and need to make sure they're either already addressed or address them.

- merge master into the PR (__done__)
- sparse tests pass (__done__)
    - The code is supposed to be the same as the status quo implementation if categories are not passed. But right now the tests related to sparse data fail.
    - __EDIT__: The tests pass if we compare floats with `almost_equal`
- LabelEncoder -> CategoricalEncoder (__done__)
    - Preprocessing is not a part of NOCATS anymore.
- Is maximum random generations 20 or 40 (__done__)
    - It's actually 60
- Don't quantize features automatically (__done__)
    - Doesn't happen anymore: https://github.com/scikit-learn/scikit-learn/pull/4899#issuecomment-271504258
- check the category count limits for given data. (__done__)
- add a benchmark
   - __done__. Results: https://github.com/scikit-learn/scikit-learn/pull/12866#issuecomment-453856876
- add tests (right now only invalid input are tested)
   - `tree/tests` __done__
   - `ensemble/tests` __done__
- benchmark against master
- add an example with plots
- check numpy upgrade related issues (we've upgraded our numpy requirement in the meantime)
- run some benchmarks with a simple integer coding of the features (with arbitrary ordering)
- add cat_split to NODE_DTYPE once joblib.hash can handle it (padded struct)
    - joblib issue: joblib/joblib#826

Closes #4899

__Future Work__: These are the possible future work we already know of (i.e. outside the scope of this PR):

- Heuristic methods to allow fast Breiman-like training for multi-class classification
- export to graphviz
- One-hot emulation using the NOCATS machinery
- support sparse input
- handle categories as their unique valies instead of `[0, max(feature)]`
  - This is to be consistent with our encoders' behavior
  - moved this to future work per https://github.com/scikit-learn/scikit-learn/pull/12866#issuecomment-455021204


P.S. I moved away from ""task list"" due to the extremely buggy interface when used in combination with editing the post, which I'm extensively doing to keep it easy for us to keep up with the status.",843222
993,2018-12-21T11:02:01Z,2020-02-14T16:37:00Z,,,"This PR changes the assertions made on the parametrization of the dual coefficients of SVM to check the equality of two models because the ordering of the support vectors is non-deterministic in case of duplicated samples in the training set (as is the case in the iris dataset and the subset of 20 newsgroups
 we use in some of the tests).

The problem was identified in #12738 when trying to replace the libsvm-base solver by the one from DAAL using the daal4py monkeypatch from https://github.com/IntelPython/daal4py/pull/15.

I have tried to run the tests with:

```
python -m daal4py -m pytest -vl  sklearn/svm/tests
```

and they pass on this branch (using the daal4py branch from  https://github.com/IntelPython/daal4py/pull/15).

However, if I decrease the regularization (larger C value) in `test_svc_iris` or in `test_sparse_20newsgroups_subset` they will fail. I am not sure whether this reveals a true discrepancy between the 2 solvers or  whether this is just that the optimization problem becomes too under determined to guarantee such a strict equivalence of the estimated parametrization of the decision function.

",843222
994,2018-12-20T12:46:40Z,2020-03-02T19:00:34Z,,,"#### Reference Issues/PRs
Following the discussion in #12794, I've changed the randomized SVD implementation so it now supports computing PCA as well, without ever having to explicitly centering X. This is especially useful for large sparse matrices, which can't be centered.


#### What does this implement/fix? Explain your changes.

1. Add degrees of freedom (ddof) parameters to sparse functions that compute variance. Error checking has been added for negative ddof, when `ddof >= n_samples` or when the data contain NaNs and `n_samples - ddof - num_nans <= 0`.

2. Implement the randomized PCA algorithm. I was unable to reuse the `randomized_svd` (although the algorithms are conceptually very similar, their code is slightly different).

    In the case where there are more features than samples, it is more efficient to transpose the matrix and work with that. However, simply transposing isn't ok, because we also need the means from the original columns. This would make `randomized_svd` quite complicated. Furthermore, `randomized_svd` is strongly tied to `randomized_range_finder`, and I would need to add similar changes to both functions, making them quite confusing. For these reasons, I strongly feel adding the similar `randomized_pca` function is justified.

#### Any other comments?

The previous behaviour raised a `TypeError` if a sparse matrix was passed to `PCA.fit`. The behaviour is now somewhat different. Sparse matrices are supported with `svd_solver='randomized'`, otherwise a `ValueError` is raised indicating the wrong solver. If the solver is set to `auto` then the `randomized` solver is always selected for sparse matrices.


",843222
995,2018-11-28T18:34:06Z,2020-03-02T19:00:35Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
None

#### What does this implement/fix? Explain your changes.

Add missing else clause that raises a TypeError to avoid the following UnboundLocalError error when passing a wrong type e.g. to train_test_split test_size:

File ""...sklearn/model_selection/_split.py"", line 1849, in _validate_shuffle_split
n_train = n_samples - n_test
UnboundLocalError: local variable 'n_test' referenced before assignment

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
996,2018-11-28T18:20:19Z,2019-08-06T15:53:31Z,,,"In this program, Sir Eustache Diemert used first 1000 samples to measure accuracy. 
I put my efforts to extend the same program, to separate Train & Test Datasets as per guideline mentioned in README.txt file listed in Reuters-21578 datasets as provided by the UCI ML repository. Test Datasets used to measure accuracy.

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
997,2018-11-28T15:37:37Z,2020-03-02T19:00:35Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #12623 


#### What does this implement/fix? Explain your changes.
The issue is that when trees are created by the base ensemble class, the parameters are not copied. This is problematic in the case of the criterion parameter if a Criterion object is passed in. When this happens, all the trees share the same object and mutate it. If n_jobs > 1, they are mutating the same object concurrently.  

#### Any other comments?

I couldn't think of a good test for this fix. Since the repro case causes a segfault it is hard to add a test case that doesn't cause grief.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
998,2018-11-26T13:15:22Z,2020-03-25T12:41:49Z,,,"This PR fix issue #12638 

Right now there is only the implementation on GaussianProcessRegressor, but if the idea of this PR is accepted I will also do it on GPClassifier and add some tests. 

I have used a tolerance of 1e-8 but I don't know if there is standard of float comparison in sklearn.",843222
999,2018-11-21T13:15:20Z,2020-03-02T19:00:37Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs

Fix #12628
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
I Add a method `set_diagonal_pairwise(distances, X, Y)` that will check if the conditions are met and then set diagonal to 0.0 using: `np.fill_diagonal` (https://github.com/scikit-learn/scikit-learn/issues/12628#issuecomment-440648630)

I decided create a method to don't add logical code to the methods.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1000,2018-11-20T19:42:19Z,2020-03-02T19:00:38Z,,,"#### Reference Issues/PRs
This is a draft implementation of #6424.
It doesn't really introduce anything new in the API, but I'm happy to move this to a SLEP.
Below is an initial description. Happy to include feedback in the SLEP.


#### What does this implement/fix? Explain your changes.
The main idea of this is to make compound scikit-learn estimators less opaque by providing ""feature names"" as strings.

#### Motivation
We've been making it easier to build complex workflows with the ColumnTransformer and I expect it will find wide adoption. However, using it results in very opaque models, even more so than before.
We have a great usage example in the gallery that applies a classifier to the titanic data set. To me this is a very simple standard usecase.

Markdown doesn't let me paste this as details, so just look here:
https://scikit-learn.org/dev/auto_examples/compose/plot_column_transformer_mixed_types.html

However, it's impossible to interpret or even sanity-check the ``LogisticRegression`` instance that's produced here, because the correspondence of the coefficients to the input features is basically impossible to figure out.
This PR enables using ``get_feature_names`` to obtain the semantics for the coefficients:
```python
preprocessor.get_feature_names(X.columns)
```
```
['num__age',
 'num__fare',
 'cat__embarked_C',
 'cat__embarked_Q',
 'cat__embarked_S',
 'cat__embarked_missing',
 'cat__sex_female',
 'cat__sex_male',
 'cat__pclass_1',
 'cat__pclass_2',
 'cat__pclass_3']
```

I think this is essential information in any machine learning workflow and it's imperative that we allow the user to get to this information in some way.

The proposed API will add a method ``get_feature_names`` to all supported (see below) transformers, with a (possibly optional, see below) parameter ""input features"" which is an array-like of strings.

#### Alternative Interfaces
To me there are four main options for interfaces to enable this:

1) Implement transformative ``get_feature_names`` as in this PR
2) Implement a more comprehensive feature description language (as done in ELI-5, I think)
3) Tie in more strongly with pandas and use dataframes  / column names
 a) to output feature semantics.
 b) to determine feature semantics
4) Leave it to the user.

While I think 2) and 3) a) is are valid option for the future, I think trying to implement this now will probably result in a gridlock and/or take too much time. I think we should iterate and provide something that solves the 80% use-case quickly. We can create a more elaborate solution later, in particular since this proposal/PR doesn't introduce any concepts that are not in sklearn already.
3 b) is discussed below.

I don't think 4) is a realistic option. I assume we can agree that the titanic example above is a valid use-case, and that getting the semantics of features is important. Below is the code that the user would have to write to do this themselves. This will become even harder in the future if the pipeline will do cloning.
<details>
I'm hardcoding that the second imputer uses ""constant"" and the first one doesn't otherwise this would get way too messy. This also hard-codes several other things, like the order of the transformers in the column transformer. It also already makes use of a transformative ``get_feature_names`` in the ``OneHotEncoder`` without which it would be completely impossible.

```python
numeric_fitted = preprocessor.named_transformers_.num

num_features_transformed = np.array(numeric_features)[np.logical_not(np.isnan(numeric_fitted.named_steps.imputer.statistics_))]
categorical_fitted = preprocessor.named_transformers_.cat
cat_features_transformed = categorical_fitted.named_steps.onehot.get_feature_names(categorical_features)

feature_names = np.hstack([num_features_transformed, cat_features_transformed])
```
</details>


#### Scope
I suggest we limit ``get_feature_names`` to transformers that either:
- leave columns unchanged
- Select a subset of columns
- create new columns where each column depends on at most one input column.
- PolynomialFeatures (or possibly algorithms that create combinations of O(1) features)

Also, I want the string to only convey presence or absence of features, or constant functions of the features. So scaling would not change a feature_name, while a log-transformation (or polynomial) might. This limits the complexity of the string (but also it's usefulness somewhat).

Together, these mean that there will be no support for multivariate transformations like PCA or NMF or KMeans.

#### Implementation
Given the above scope and API, and the current implementation of ``get_feature_names`` in ``ColumnTransformer`` there are two main mechanism that need to be implemented.

1. allow pipeline to pass around names
2. provide a mechanism for meta-estimators (ColumnTransformer and Pipeline and Feature Union) to ""know what to do"".

There are basically three cases the meta-estimators need to take care of:
a) The transformer does a non-trivial column transformation, like OneHotEncoder or feature selection
b) The transformer does ""nothing"" to the columns, like StandardScaler.
c) The transformer does a ""too complex"" operation to the columns, like PCA.

For a), only the estimator can handle this case, so the estimator needs to provide a function to do that - already implemented in several cases as a transformative ``get_feature_names``. For b) the meta-estimator can simply do a pass-through, so we need to ""flag"" these in some way. There is no way for the meta-estimator to really handle c) so if the estimator is not ""tagged"" as being trival and doesn't implement ``get_feature_names`` the meta-estimator needs to bail in some way.

I added a ""OneToOneMixin"" to tag the trivial transformations. It would be possible to just use this as a tag, and let the meta-estimators handle the pass-through. Given that we already have the mechanism to handle the pass-through, I thought it would be simpler to just implement a pass-through ``get_feature_names`` (another alternative would be to add an estimator tag, but that also seems less elegant).

Right now the bail in case c) is a TypeError.

#### Limitations
- The general API requires ""input features"". In ``PolynomialFeatures`` this was optional. Unfortunately we have no way to know the input dimensionality of a fitted transformer in general, so automatically generating ``x1``, ``x2``, etc is not possible. This could be fixed by adding a required ``n_features_`` to the API, which would probably be helpful but also would be a relatively heavy addition.

- Because we don't know the number of input features, there's no way to ensure the user passed the right length of ``input_features``

- The implementation of ``get_feature_names`` in Pipeline is a hack, because it includes or excludes the last step based on whether the last step has ``transform``. The reason for this is that given a trained pipeline with a classifier in the end, I want to be able to get the feature names, which would not include the last step. In preprocessing pipelines we always want to include all the steps, though.
The real solution to this in my opinion is always to include the last step, and allow slicing the pipeline (#2568) to get the feature names for a pipeline with a final supervised step.

- Bailing to a TypeError if any ""complex"" transformation happens is a bit of a bummer. We could try to generate names like ``pca_1``, ``pca_2``, ... but to do this automatically we would need to know the output dimensionality, which we don't (unless we add ``n_outputs_`` as required attribute to the API similar to ``n_features_`` above)

#### Open Questions

- Do we want to require ``get_feature_names`` to accept ``input_features``? Right now the vectorizers don't and it makes the code slightly more complex.

- How do we want to handle the hack in Pipeline.get_feature_names for the last step?

- Do we want to encode fixed univariate transformations (""scale"", ""log"", ""rank""?)

#### Possible Extensions

- don't require ``input_features`` and generate names
- generate names for ""complex transformations""
- Use pandas column names as ``input_features`` if available (3b above)

I already discussed the requirements for the first two extensions (adding ``n_features_`` and ``n_outputs_``).
The last one would require storing the input column names if the input is a pandas dataframe. It shouldn't be hard to do, and would also enable solving #7242 and I'd like to do that, but it's not required for this proposal to be useful.

#### Todo

- [ ] add to narrative docs
- [ ] add to remaining estimators (feature selection is the only left?)
- [ ] allow input_feature for all ``get_feature_names`` methods?",843222
1001,2018-11-17T01:48:40Z,2020-03-02T19:00:39Z,,,"Addresses #12505 (continued)

This PR addresses the unexpected behavior of `warm_start` in the multilayer perceptron. To recap, the current behavior is that warm_start breaks after a single iteration. I took the first steps of changing this by:
* Reworking the MLP classes to support the change
* Adding an option `warm_start='full'` to represent the future behavior
* Raising a FutureWarning for `warm_start=True` that it will be changed in 0.22

There are two FIXMEs in the file that indicate the very simple changes required to make the transition in 0.22 (just remove the warning and remove an if statement condition). I tested making those 0.22 changes and it works as intended. Lastly, I added 2 tests: one for the FutureWarning and one for the new `warm_start='full'` option.

Please let me know also if I got the futurewarning/deprecation stuff right.

@jnothman tagging you again

Edit: The test that's failing is because the FutureWarning I added is getting raised.
Edit 2: fixed tests",843222
1002,2018-11-15T22:16:14Z,2020-03-02T19:00:40Z,,,"Closes https://github.com/scikit-learn/scikit-learn/issues/12600

For `pairwise_distance(.., metric='sqeuclidean')`, this uses the fast `euclidean_distance(..., squared=True)` function to be consistent with `metric='euclidean'`, instead of the slower (but more accurate) `pdist` from scipy (cf parent issue for benchmarks).

`metric='sqeuclidean'` is not used in the code base, so it's not really critical, but it may make some users applications faster.",843222
1003,2018-10-31T22:43:20Z,2020-03-02T19:00:40Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

#12492 
#### What does this implement/fix? Explain your changes.
Added a subsampling hyperparameter and method in KBinsDiscretizer

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1004,2018-10-29T02:46:27Z,2020-03-02T19:00:41Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

#10971 

#### What does this implement/fix? Explain your changes.

Added calibration loss metric for classification 
#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1005,2018-10-29T00:37:24Z,2020-03-02T19:00:42Z,,,"Fixes #9313.

Supersedes #9384.

---- 

Instead of depending on the MNIST dataset in the original test, I've created a similar (random) dataset which also produced the same warnings.

*Sidenote:* I've had to disable `FutureWarning` in addition to `np.VisibleDeprecationWarning` in `assert_no_warnings` in `sklearn.utils.testing`. Perhaps this is overreaching a bit, so I could implement a new testing routine which only looks for absence of certain warnings in the output (as a separate PR?)",843222
1006,2018-10-28T22:07:56Z,2020-03-02T19:00:42Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

Converting early exaggeration iterations from a private constant to a public member variable of the TSNE class. The default value for this new argument is set at 250 such that code using previous versions of sklearn will still get identical results if no action is taken to account for the change.

Being able to set this variable is an important component of running t-SNE and it shouldn't be hidden. Other implementations ([e.g., LvdM's](https://github.com/lvdmaaten/bhtsne/blob/master/tsne.h#L44-L45)) expose it. In the LvdM case it's actually via two arguments but sklearn doesn't distinguish between momentum switch and stop lying. It doesn't seem necessary to treat the two arguments differently.

This change is motivated by [recent work](https://doi.org/10.1101/451690) that shows a high quality embedding can be achieved with much fewer than 250 early exaggeration iterations.

#### Any other comments?

Validated that results are identical before and after change for the same inputs.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1007,2018-10-26T13:14:08Z,2019-08-05T19:51:45Z,,,"This is WIP towards it. Extension of #12431 

Currently, when n_features changes between calls to partial_fit, for some estimators an error is raised with an informative error message, and for other estimators an error is raised but the message is not very informative (broadcasting error from numpy or bad shape from pairwise, ...).

In addition, for estimators which give an informative error message, the message differs between estimators.

Finally, there is a common test which checks that an estimator raises an error in that case. However, it's only done for classifiers, regressors and clusterers.

This PR fixes these 3 aspects:
- add a helper validation function to check if partial_fit is called on input data with appropriate number of features.
- modify the common tests to match the error message, ensuring consistency across estimators.
- modify the common tests to check partial_fit for all estimators.",843222
1008,2018-10-25T12:44:35Z,2020-03-02T19:00:43Z,,,"#### What does this implement?
GMMs can be used as conditional generative models by conditioning on a subset of the dimensions. This feature is missing at the moment i scikit-learn, which only supports sampling from the full (un-conditioned) distribution.

The added functionality allows to select a subset of the indices of the modeled variable, along with values of these dimensions, and sample from the remaining dimensions.",843222
1009,2018-10-22T13:14:59Z,2020-03-02T19:00:44Z,,,"Fixes #12430
It's all in the title. I don't think test is needed for that.",843222
1010,2018-10-19T04:01:51Z,2020-03-02T19:00:44Z,,,"
#### Reference Issues/PRs
Fixes #12401 

#### What does this implement/fix? Explain your changes.
Added a new parameter square_distance to TSNE class.
If 'legacy' (default) maintains the current behavior i.e. Squares distance only for 'euclidean' metric
If True, squares distances for every metric including 'precomputed'
If False, does not square distances even for 'euclidean' metric
If 'warn' (possible default), FutureWarning is metric is not 'euclidean'

#### Any other comments?",843222
1011,2018-10-15T17:44:58Z,2020-03-02T19:00:45Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #11793 


#### What does this implement/fix? Explain your changes.
it adds a dictionary parameter to TSNE constructor which represents the keyword arguments for distance metric to be used inside TSNE optimization

#### Any other comments?

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1012,2018-10-12T09:37:35Z,2020-03-02T19:00:45Z,,,"
#### Reference Issues/PRs
Fixes #12259 . See also  issues #2386 , #8443 and  #12188 

#### What does this implement/fix? Explain your changes.

Basically, Scikit-learn implements decision trees with 2 different types of splitters. A RandomSplitter and a BestSplitter. I focus on BestSplitter, which currently has two flaws:

1. It is random (provide different outputs on the default config). This happens when there are two or more 'best' features with equal criterion value.
2. It doesn't allow to give priors on the features, for example, when there is a Tie. It would be a nice-to-have if we could chose a features that we prefer. Either because it is easier to compute, or because it is less noisy or because it is easy to interpret. The ideal would be to encode it in the index of the column of the feature. Lower index means higher priority. 

We have a supplementary constraint which is the Shuffle constrain. Because currently there is a max_features parameter which allows us to do not test all the features at each node if there are many. So this is a nice feature and I believe we should always shuffle. The problem is that shuffling means non deterministic...


#### So we have:

- Sorted / Unsorted       -> stands for the fact of giving priority to lower index features if and only if there is a Tie.
- Deterministic / Non     -> stands for the fact of always obtaining the same result, at least for the default configuration.
- Shuffled / Looped / No  -> stands for the fact of shuffling the features when we try them. We can shuffle them randomly, we can loop through them or can test them in order (from 0 to N). This is important only if max_featues < nb_features  or if we do not sort the features.



So for me the ideal is to have a a prior on the features (Sorted, saying lower index means I prefer the feature). 
We need to do Shuffle or at least Loop, in order to support  max_features, which is often useful, and necessary for sparse cases.

Then it should be deterministic in the default configuration, so if we want this we have the following possibilities:
 - we can do looping instead of shuffling: this would make random_state useless here... so is not viable
 - we can do shuffle and sort the features to the best of our knowledge: So we do shuffling and if 2 features from the tested features are 'best' we keep the lower index. This will make the default config where `max_features=n_features` stable regardless of the `random_state` because we will test all the features.
 

So I implemented this BestSplitter2 that shuffles, and is deterministic when `max_features=n_features` and gives priority to the lower index features when there is a tie.


Here you can see an example of the previous and the new one:
Previous BestSplitter is on the left and New BestSplitter2 is on the right 

[![Screenshot-from-2018-10-12-11-31-55.png](https://i.postimg.cc/m2wbstBY/Screenshot-from-2018-10-12-11-31-55.png)](https://postimg.cc/ppmNQV9r)

We observe that BestSplitter2 give priority to lower index features.
And when we test stability 


```
import numpy as np
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
x,y  = iris.data, iris.target

dtc1 = DecisionTreeClassifier(random_state=1,splitter='best2')
dtc2 = DecisionTreeClassifier(random_state=2,splitter='best2')

rs = np.random.RandomState(1234)
itr = rs.rand(x.shape[0]) < 0.75

dtc1.fit(x[itr],y[itr])
dtc2.fit(x[itr],y[itr])

print(  (dtc1.predict(x[~itr]) != dtc2.predict(x[~itr])).sum() )
```

We obtain `0` as expected in #12259 


#### Any other comments?

I don't know what do you think, I think having many splitters is not a solution either.
Personally I would replace the current bestSplitter with the one described above, as they are essentially the same but with some improvements.

Thank you very much
",843222
1013,2018-10-08T13:05:28Z,2020-03-02T19:00:46Z,,,"Modified `feature_importances_` attribute in AdaBoost's base estimator to work with `coef_` attributes of given estimators if they don't provide `feature_importances_`  
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #12137 
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
`feature_importances_` attribute of `BaseWeightBoosting` class now converts `coef_` attribute of input classifiers to their respective `feature_importances_`  using existing `_get_feature_importances` method in `feature_selection/from_model.py`.

This method accepts an optional argument: `norm_order` which defaults to `2` (`L2 norm`).
```
norm_order : {non-zero int, inf, -inf, ""fro""}, optional
        Same as `numpy.linalg.norm`'s norm_order parameter
```   

#### Any other comments?
This method is now renamed to `get_feature_importances` and moved to `utils/__init__.py` as it is a shared resource.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1014,2018-10-07T15:50:13Z,2020-01-06T15:08:36Z,,,"#### Reference Issues/PRs
fixes #12079, fixes #12080

#### What does this implement/fix? Explain your changes.
#12079 adds LOBPCG as an SVD solver in PCA
#12080 adds LOBPCG solver to Truncated PCA

lobpcg_svd should also be useful in KernelPCA for faster partial decompositions, see #12068

This PR also includes multiple LOBPCG related bug fixes, including vendoring  sklearn/externals/_lobpcg.py from scipy 1.3.0

#### Any other comments?
@ogrisel Transferred from permanently closed PR #12291

Keep in mind for testing, that  lobpcg_svd falls back to dense eigensolver unless n_components < 3*matrix_size, where matrix_size = min (n_samples, n_features) 

Still to do, better in new focused PRs after this one is merged

1. example plot_faces_decomposition may include lobpcg_svd, just change 

    ('Eigenfaces - PCA using randomized SVD',
     decomposition.PCA(n_components=n_components, svd_solver='randomized',
                       whiten=True),
     True),

to 

    ('Eigenfaces - PCA using randomized SVD',
     decomposition.PCA(n_components=n_components, svd_solver='lobpcg',
                       whiten=True),
     True),

but lobpcg currently fails here for unclear numerical reasons. More testing may be needed for float32 data, like in this example. 

2. All four existing TruncatedSVD examples of scikit-learn in the examples/ folder do run with lobpcg, just by  adding the option "", algorithm='lobpcg' "" to TruncatedSVD function call. But none generates the matrix large enough to demonstrate the practical benefits of lobpcg_svd. ",843222
1015,2018-10-06T16:27:13Z,2020-03-02T19:00:47Z,,,"#### Reference Issues/PRs
closes #12164 

#### What does this implement/fix? Explain your changes.
#12164 adds clusterQR method to 'kmeans' and 'discretize' in spectral clustering

#### Any other comments?
The actual changes for #12164 are just a few lines in only 3 core codes, spectral.py, test_spectral.py, and plot_coin_segmentation.py",843222
1016,2018-10-04T15:42:50Z,2020-03-02T19:00:48Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
#10168 
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This is a follow-up on the stale PR referenced above, including some minor changes.


#### Any other comments?
It's my first PR in this repo. Please excuse any mistakes I might have made. The original PR reviewer @glemaitre requested a versionadded directive - I do not know how to do that and could not find it in the contributing guide.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1017,2018-10-03T19:23:37Z,2020-03-02T19:00:49Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes: #12153
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Currently, adds the option to add a frequency threshold to OneHot- and OrdinalEncoder.
All categories below this threshold are determined, sorted and mapped to the first category.

What needs to be done?
- [X] Adds `min_df`with implementation to Ordinal- and OneHotEncoder
- [ ] Example in `examples/ ` folder
- [ ] Documentation
- [ ] Probably add more tests and remove some tests
- [ ] add option to add a name of the `other` group -> What to do if not object/str? What happens if `other`already there?
- [ ] With a threshold, encoders are not ""really"" invertible anymore -> add at least documentation?
- [ ] Align if function names are appropriate


#### Any other comments?
Further points of extension:

* Instead of `min_freq` add `top_n` categories. Moreover, one could use integers instead of floats in `min_freq`. `top_n`and `min_freq`could interact
* Allow an array of frequencies for each feature
*  One could provide a mapping, to group certain values in a category together. It might be though, that a different Encoder would be more suitable

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
1018,2018-09-29T17:20:12Z,2020-03-02T19:00:49Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Addresses #6809, continuation of PR #6833 by hlin1117

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1019,2018-09-28T15:55:20Z,2020-03-02T19:00:50Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

See also #1188.

#### What does this implement/fix? Explain your changes.

Adds support for cosine distance in k-means including the minibatch implementation. We trust that users know what they're doing and allow the metric to be a callable with metric_kwargs. 

#### Any other comments?

I enabled the cleaning of headers, footers, and quotes from the 20newsgroups dataset documents for the example program plot_document_clustering.py. Now it clearly shows improvements in the metrics V-measure and ARI when using cosine distance for text document clustering. 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1020,2018-09-20T12:43:04Z,2020-03-02T19:00:51Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!-- Adding Fall-out, Miss rate, specificity as metrics
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
In this commit i  have added three metrics names miss_rate , specificity, fall_out in sklearn.metrics.
It gives user more flexibility to direct get the value for the given metrics..

#### Any other comments?
MY SYSTEM LIBRARY REQUIREMENTS ARE AS GIVE:
Linux-4.15.0-33-generic-x86_64-with-Ubuntu-16.04-xenial
Python 3.6.5 (default, May  3 2018, 10:08:28)
[GCC 5.4.0 20160609]
NumPy 1.15.0
SciPy 1.1.0


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1021,2018-09-13T22:18:35Z,2020-03-02T19:00:51Z,,,"#### Reference Issues/PRs
Will fix #7768

#### What does this implement/fix? Explain your changes.
- Adds test to ensure that meta-estimators faithfully pass on data to base estimator (wherever applicable)
- Will fix meta-estimators (which accept base estimators) to offload validation to the base estimator",843222
1022,2018-09-13T17:27:32Z,2020-03-12T08:37:24Z,,,Fixes #12068,843222
1023,2018-09-09T02:29:43Z,2020-03-02T19:00:53Z,,,"Fixes #11997. See also #10465.

* This implementation passes through values to transformer specified to be missing, and converts them to np.nan's by default.
* Still raises error if `transform()` sees an unknown value that hasn't been specified as missing
* Allows for one missing value or list of missing values
* User can specify transforming missing values as smallest (0), largest, or separate (-1) ordinal category.

Still to-do:
* Need to handle np.nan as missing (has issues with `_encode_python`)

Notes:
* This implementation can be moved out of `_BaseEncoder` to `OrdinalEncoder` if preferrable
",843222
1024,2018-09-06T10:16:22Z,2020-03-18T00:46:45Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes  #11996 
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Currently contains 3 edits:

1. Tests to check handle_missing and missing_values are passed and have correct values
2. An update to the OneHotEncoder docstring and the preprocessing module.
3. An initial implementation logic of the required features as stated by #11996 


",843222
1025,2018-09-01T18:17:42Z,2020-03-02T19:00:56Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
#6489 
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

Propagate the `eigen_tol` for the 'arpack', 'amg', and 'lobpcg' solvers. By default, we are using the scipy default ('arpack'=>0 otherwise None).


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1026,2018-08-28T00:36:42Z,2020-03-02T19:00:56Z,,,Explores #11924,843222
1027,2018-08-27T00:36:35Z,2020-03-02T19:00:57Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #11835 

#### What does this implement/fix? Explain your changes.
Add feature weight to isolation forest.

<!--
#### Any other comments?

Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1028,2018-08-23T04:22:07Z,2020-03-02T19:00:58Z,,,Makes some small tweaks to `enet_coordinate_descent` regarding conditions to avoid unnecessary computation and to simplify the code for readability.,843222
1029,2018-08-22T11:29:58Z,2020-03-02T19:00:59Z,,,"Treats #6821

Class GainRatio inherits from Entropy and only overwrites the impurity_improvements functions
of the Criterion super class

Adds gain_ratio as parameters to the respective test file.

TODO's

[X] Provide Code for GainRatio
[X] Extend criteria in tests_tree.py by gain_ratio
[ ] Documentation in _criterion.pyx, tree.py 
[ ] Provide example in examples folder
[ ] Add content to documentation 
[X] Provide a performance analysis, specifically since the criterion does not and can not use a real proxy_impurity_improvement function

Specifically I implemented balanced gain ratio. Code removes some edge cases and makes it to be coded easier
Sources:
http://hunch.net/~coms-4771/quinlan.pdf
https://arxiv.org/pdf/1801.08310.pdf


#### Reference Issues/PRs
Fixes #6821 


#### What does this implement/fix? Explain your changes.
Implements the (balanced) gain ratio criterion for decision trees

#### Any other comments?
It might be also possible, to modify the Criterion class and passing a ratio keyword. Then, one could would directly get a gini gain ratio. I did not create a class on the same level as Entropy to avoid duplicate code. Essentially, the impurity improvement is all that needs to be changed.
Any feedback is welcome",843222
1030,2018-08-21T06:05:31Z,2020-03-02T19:01:00Z,,,"Have noticed OpenBLAS's threadpool spin down and spin up in seconds or less when running `_update_dict` from `dict_learning`, which is mostly BLAS function calls. As creating and destorying threads is expensive, it's important to make sure that BLAS implementations (like OpenBLAS) maintain their threadpool between BLAS calls.

Thus this rewrites `_update_dict` in straight Cython (getting as close to C as possible). This should help make sure the glue code stays out of the way while BLAS does the heavy lifting.",843222
1031,2018-08-20T15:26:15Z,2020-03-02T19:01:00Z,,,"In most use cases, ICA is performed on arrays with `p << n` . In this case, using `linalg.svd` can be order of magnitude slower than just computing the eigen decompostion of `X.dot(X.T)`:


    In [1]: import numpy as np

    In [2]: X = np.random.randn(10, 1000)

    In [3]: %timeit np.linalg.svd(X)
    36.9 ms ± 1.39 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

    In [4]: %timeit np.linalg.eigh(X.dot(X.T))
    73.7 µs ± 4.25 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)


And then, SVD becomes the bottleneck of ICA!


Now, this PR breaks one test because of numerical issues (where the problem is degenerate), and I don't really know how to fix this. I do not think that this problem is likely to happen on any real case application though.



(Note that does not slow things down when `p` and `n` are similar, and is only slower when `p` is much larger than `n`. This hardly ever occurs in practice.)",843222
1032,2018-08-06T15:17:23Z,2019-09-08T17:14:12Z,,,"Fixes #11715 

I'll fix the other related classes once I know I'm doing it right.",843222
1033,2018-08-01T06:00:41Z,2020-03-02T19:01:03Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Add `min_tpr` parameter to `roc_auc_score` according to discussion in #11668 

<!--
#### What does this implement/fix? Explain your changes.


#### Any other comments?



Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1034,2018-07-31T19:28:09Z,2020-03-02T19:01:04Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #9113 
(should close stalled pull request #10588)

#### What does this implement/fix? Explain your changes.

- [x] Add sample_weight
  - [x] Add support to multiply sample weight sample wise on loss functions:
    - [x] log_loss
    - [x] binary_log_loss
    - [x] squared_loss
  - [x] Add sample_weight parameter to fit and partial_fit
    - [x] Propagate to relevant functions
    - [x] Validate sample_weight size on fit
  - [x] Add tests
    - [x] Test directly loss functions sample_weight invariance 
    - [x] Test fit validation of sample_weight size

- [x]  Add class_weight support
   - [x] Add support on fit and partial fit
   - [x] Add test for behaviour correctness 

- [ ] Test with `utils.estimator_checks.check_class_weight_classifiers`: Currently (Nov 18) have issues passing this test, however succeeded when using one neuron MLP
#### Any other comments?

I started by adding sample_weight parameter support, next should add class_weight.
Please share your comments

Thanks

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1035,2018-07-30T14:29:37Z,2020-03-02T19:01:05Z,,,"#### What does this implement/fix? Explain your changes.

(hopefully) clarifies how BaseSearchCV.fit handles fit_params. From the docs, it wasn't clear to me if / how these would be split.

IIUC, it comes down to the behavior of https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_validation.py#L919-L926. Parameters are split along with `X` and `y` when they are array-like and have the same number of samples.",843222
1036,2018-07-25T22:38:01Z,2020-03-02T19:01:06Z,,,"#### Reference Issues/PRs
Fixes #1243.

#### What does this implement/fix? Explain your changes.
Implements a meta classifier for semi-supervised learning based on the original Yarowsky self-training algorithm (refer to http://www.aclweb.org/anthology/P95-1026 for details).
Here is a comparison graph of our implemented version on the IRIS dataset. You can find the code under `examples/semi_supervised/plot_self_training_performance.py`.
![Comparison Graph](https://35903-843222-gh.circle-artifacts.com/0/doc/_images/sphx_glr_plot_self_training_performance_001.png)


#### Any other comments?
This PR was created in collaboration of @oliverrausch.
This PR is a work in progress and we'd continue working on it if you would be willing to merge once it is completed.",843222
1037,2018-07-24T19:31:50Z,2020-03-02T19:01:07Z,,,"
This pull request implements the feature in issue #11566 

Also, I added a test for this feature.

",843222
1038,2018-07-21T07:10:34Z,2020-03-02T19:01:08Z,,,"`K` is not being updated properly in certain situations where we have non-uniform sample weights.  This occurs during a removal/pop or push onto the `WeightedMedianCalculator`.  The proposed fix solves this problem by identifying the exact index a new / old sample is added / removed and applying additional logic to update K correctly.

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #10725 (BUG Median not always being calculated correctly for DecisionTrees in the WeightedMedianCalculator)

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1039,2018-07-20T02:49:03Z,2020-03-02T19:01:09Z,,,"#### Reference Issues/PRs
Fixes #11463

#### What does this implement/fix? Explain your changes.
1. Running `inverse_transform` with overlap or `drop` will raise a `ValueError`
2. `_calculate_inverse_indices` is used to connect indices from the output space back to the input space.",843222
1040,2018-07-18T20:10:55Z,2020-03-02T19:01:10Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

I want to implement MCA into sklearn. 

#### Any other comments?

I'm more than willing to work further and support this code so please do let me know what ever I need to do to get this PR approved! Very much looking forward to working with sklearn!

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1041,2018-07-17T17:21:07Z,2019-08-06T15:15:22Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Contains the basis for fixing #9698.

#### What does this implement/fix? Explain your changes.

Utility functions that provide safe and friendlier versions of str() and repr() were added to `utils`: `safe_str()` and `safe_repr()`.

These functions _should never crash_ and should _always return a string_.

They are intended to be used in `raise` statements and warning calls. I created a **list that should contain most (if not all) such places**: https://gist.github.com/lebigot/8526af2425355b5cf837068c4182fc2d.

I have thus started to go through this list (in the listed order): this pull requests contains the first changes.

#### Any other comments?

I considered that there is no need to protect the representation of something like `arr.shape` when `arr` is supposed to be a NumPy array or equivalent (and has likely be used as such before the error message).

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
1042,2018-07-17T12:30:57Z,2020-03-02T19:01:11Z,,,"This PR addresses #8438 (stalled PR which aimed at fixing #4225).

The changes in this PR aim at allowing `GridSearchCV` and `cross_val_score` to accept sparse `y`. This can be particularly useful in some extreme learning situations where one has a lot of samples and many labels. This is only useful as long as `GridSearchCV` is passed an estimator which also accepts a sparse `y`. Future PRs such as #4354 are needed to add support for sparse y in Scikit-Learn estimators.

Details:

- in `model_selection/_search.py` and `model_selection/tests/test_search.py`: allow GridSearchCV to be passed a sparse y. The `fit` method of the `BaseSearchCV` class calls `check_cv`, in which `type_of_target` needed to support sparse input. 
- in `model_selection/_split.py` and `model_selection/tests/test_split.py`: The `split` method of CV classes (`KFold`, `StratifiedKFold`,...) needed to accept a sparse `y`. The typical use case of sparse `y` is multi-label. Therefore, `StratifiedKFold` and `StratifiedShuffleSplit` should not be used, unless `y` has shape `(n_samples,)` or `(n_samples, 1)`. In this case, the sparse `y` is converted to a dense array.
- in `utils/multiclass.py` and `utils/test/test_multiclass.py`: allow `type_of_target` and `unique_labels` to accept a sparse matrix.
- in `/utils/sparsefuncs.py` and `/utils/tests/test_sparsefuncs.py`: added a function to obtain the unique elements in a sparse array.
- in `model_selection/_validation.py` and `model_selection/tests/test_validation.py`: allow `cross_val_score` to accept a sparse `y`.

@GaelVaroquaux, @jnothman  
",843222
1043,2018-07-16T10:54:26Z,2020-03-02T19:01:12Z,,,"#### Reference Issues/PRs

Closes #5602

#### What does this implement/fix? Explain your changes.

use parallel algo from https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm

I still new to check if same problem occurs on sparse matrices and for variance... which I suspect
",843222
1044,2018-07-15T22:27:14Z,2020-03-02T19:01:13Z,,,"Address Issue #597, which requests a refactoring of the existing fibonacci tree used for Dijkstra's algorithm in graph_nearest_neighbor.py in order to accelerate the Ball Tree in sklearn.neighbors.

This pull request is currently a work in progress. The fibonacci tree has been extracted, but the ball tree code still needs to be modified.",843222
1045,2018-07-15T17:35:09Z,2020-03-02T19:01:14Z,,,"closes #9729 Change KMeans n_init default value to 1.

#### What does this implement/fix? Explain your changes.
Creates a warning message when the user does not specify a value for n_init. The warning message tells the user that the default value will change from 10 to 1 in 0.22 

#### Any other comments?
Thanks for helping us contribute at SciPy 2018!",843222
1046,2018-07-15T15:19:27Z,2020-03-02T19:01:15Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

Adding explained variances for sparse PCA during a sprint.

I am unfamiliar with Sparse PCA. Based on conversation with @amueller 

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Issue #11512 

The variance computed from sparse PCA should match regular PCA. Printed variances [here](
https://gist.github.com/jrmlhermitte/ea7bcf5058358af209ac380e025162e8) should match.



#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1047,2018-07-15T10:18:26Z,2019-12-06T05:24:18Z,,,"#### Reference Issues/PRs
Fixes #5545

#### What does this implement/fix? Explain your changes.
This PR creates a function sklearn.utils.init_arpack_v0(size, random_state) which goal is to be used each time eigs, eigsh or svds from scipy.sparse.linalg is called. It initializes the v0 parameter correctly with value sampled from the uniform distribution in [-1, 1] (like in ARPACK) to avoid convergence issues with another initialization. The v0 parameter is mandatory as it is the only way to render linalg functions behaviour deterministic.

#### Any other comments?
I put the function in __init__py as I have seen that some general utils functions are there but I'm not convinced by this choice. Maybe a utils.utils could be created to contain one shot functions which don't belong to a group ?
For now I just replaced places where randomization was correctly set using v0 parameter.
**TODO** :
- [x] svds calls are not correctly seeded with v0 --> change that
- [x] git grep v0 to check that everything is correct
- [x] Create unit test for init_arpack_v0

@amueller @rth : Should I change some tests or define new ones for the functions and classes that have been changed (most notably those calling svds without random_state defined) ? It seems that random_state params are not often checked by test so maybe leave it like that ?

I opened an issue for scipy to add a seed parameter, I'm probably going to take care of it after this PR. Maybe there are some impacts on this work but I don't think so.

There are still **some stuffs bothering me** but I can't find a better solution : 
- the way I'm forced (at least to my understanding) to initialize multiple times v0 in bicluster.py
- the fact that  _init_arpack_v0 is fed with A.shape[0] for eigsh and min(A.shape) for svds (this one causes the first annoyance)",843222
1048,2018-07-07T12:31:23Z,2020-03-02T19:01:16Z,,,"#### Reference Issues/PRs
Fixes #11432 

#### What does this implement/fix? Explain your changes.
Adds the `_pairwise` attribute to `cluster/dbscan_.py`, returning `True` if a `precomputed` distance metric is indicated.

Also adds a common heuristic test for checking if an estimator should have the `_pairwise` property, but isn't. The test is based on the idea that if an estimator had a `metric`, `affinity` or `kernel` parameter which supports 'precomputed', and if the estimator was then able to fit on pairwise data, but raised an error on non-square training data, a `_pairwise` attribute should exist.",843222
1049,2018-07-05T22:44:18Z,2020-03-02T19:01:17Z,,,"Related: #8960 
Explanation: https://github.com/scikit-learn/scikit-learn/pull/8960#issuecomment-402837043

Basically `Pipeline` and `FeatureUnion` classes are really useful for combining models and making the code more extensible may provide a nice starting point for other features. My personal use case is building a stacked generalization framework of of it.

## Summary of changes

- Extracted code to stack estimators' outputs in a private method (`_stack_results`);
- Extracted snippet responsible for applying weights to results (_apply_weights`);
- Added functions that were extracted for parallel computation as attributes of `FeatureUnion` and `Pipeline` so they can be overridden when needed.",843222
1050,2018-07-04T22:26:30Z,2019-08-05T19:28:39Z,,,"Solves issue #10584.
Adds the new chapter ""3.3.2. Which scoring function should I use?"" to the user guide.",843222
1051,2018-07-04T14:51:28Z,2020-03-02T19:01:18Z,,,"Code for **Score Function** has been added in sklearn/metrics/cluster/unsupervised.py. Score Function is a **Cluster validation index** and was introduced in this [paper](https://pdfs.semanticscholar.org/9701/405b0d601e169636a2541940a070087acd5b.pdf).

This Pull Request Fixes #11351 


",843222
1052,2018-06-27T10:19:54Z,2020-03-02T19:01:19Z,,,"This is WIP to add the sampling imputation strategy as discussed in #11209

I'm not sure if we want to make it a new strategy for the ``SimpleImputer``, or a new class: ``SamplingImputer``, only dedicated to this transformation.

I made a commit for both so you can see and tell me what you think is best.
To me, just make it new strategy for the ``SimpleImputer`` is fine, since it's a simple strategy and it doesn't change the code at all (except adding the strategy).

There are still TODOs in there:

- [x] support sparse
- [x] make tests
- [x] update doc",843222
1053,2018-06-24T20:07:08Z,2020-03-02T19:01:20Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #11339 
simply changed `force_all_finite` to `'False'`

**Update:**
turn off all finiteness checks
#### What does this implement/fix? Explain your changes.
Allow target values to have missing values in `TransformedTargetRegressor`


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1054,2018-06-21T13:55:54Z,2020-03-02T19:01:23Z,,,"#### Reference Issues/PRs
Closes [#11267](https://github.com/scikit-learn/scikit-learn/issues/11267)

#### What does this implement/fix? 
Adds a `n_features` parameter and a `partial_fit() `method to `DictVectorizer`. Useful when we are learning a `vocab` on the fly and still need a fixed size output.

TODO:

- [x] Ask for feedback on implementation decision 

- [ ] Decide how/if `fit_transform()` will deal without materializing X in memory

- [ ] The code follow the code guidelines

- [ ] Check for common programming errors

- [ ] The new implementation doesn't break current `DictVectorizer` tests

- [ ] The new implementation doesn't break all library tests

- [ ] Create new test cases for `n_features != None`

- [ ] Add a script to `/examples`

- [ ] Update documentation

",843222
1055,2018-06-20T07:40:43Z,2019-08-12T14:45:51Z,,,"This is an initial implementation of what I suggested in https://github.com/scikit-learn/scikit-learn/pull/8022#discussion_r196659219 (ping @amueller).

The idea is to put the test configuration for an estimator class on the class. This provides:
* advantage: can instantiate meta-estimators which have required args
* advantage: can see the test parametrisation clearly on the class
* disadvantage: code limiting `max_iter`, etc., is repetitive (this PR adds lines and decentralises functionality)
* disadvantage: harder for reviewers to point out to a contributor that these changes are needed

The key changes are to [`base.py`](https://github.com/scikit-learn/scikit-learn/pull/11324/files#diff-f392a01b355b04b8c90c84175d118fdc), [`test_common.py`](https://github.com/scikit-learn/scikit-learn/pull/11324/files#diff-b6e2c29089c86706dc868c7bfd58c1d6) and [`estimator_checks.py`](https://github.com/scikit-learn/scikit-learn/pull/11324/files#diff-a95fe0e40350c536a5e303e87ac979c4)

TODO:
* [ ] documentation
* [ ] use `_generate_test_params` when calling check_estimator on a class
* [ ] replace `check_parameters_default_constructible`
* [x] work out how to still do `check_no_attributes_set_in_init`
* [ ] test exception when running check_estimator on class requiring parameters",843222
1056,2018-06-17T14:02:38Z,2019-08-06T19:20:50Z,,,"This PR aims to implement bits of #10453 as part of the python-sprints London doc sprint.

More details such as which bits will be worked on and by whom will follow as part of the sprint.",843222
1057,2018-06-15T17:36:47Z,2020-03-02T19:01:24Z,,,"#### Reference Issues/PRs
Fixes #10372

#### What does this implement/fix? Explain your changes.
We should accept the recommended idea.

When there are a small number of unique values along a particular dimension, we could build a smaller grid by just picking those unique values.
This PR can make this grid slightly more efficient by just considering the unique values inside the specified percentile range.",843222
1058,2018-06-15T16:55:41Z,2019-08-05T19:15:03Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Fixes #3130.

#### What does this implement/fix? Explain your changes.

This implements a winsorizer transformer for clipping feature values.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1059,2018-06-14T19:35:55Z,2020-03-02T19:01:25Z,,,"#### Reference Issues/PRs
Addresses (but doesn't fix) #1626: ""API Proposal: Genearlized Cross-Validation and Early Stopping"". This PR only address early stopping on models that have `partial_fit` (https://github.com/scikit-learn/scikit-learn/issues/1626#issuecomment-12739541), not addressing ""when more computation doesn't improve the result."" (https://github.com/scikit-learn/scikit-learn/issues/1626#issue-10319901).

I will use this in https://github.com/dask/dask-searchcv/pull/72. Briefly, I want to repeatedly use `cross_validate` with a specified number of `partial_fit` calls.

#### What does this implement/fix? Explain your changes.
This PR implements early stopping for cross validation when `partial_fit` is present. This requires that

- `_fit_and_score` have a `partial_fit` keyword argument that can either be a bool or an int that reflects the number of times `est.partial_fit` is called.
  - I also add a `partial_fit` keyword arg to `cross_validate`, which relies on `_fit_and_score`.
- the specification of a different train/validation set if `_fit_and_score` is called repeatedly.
  - yes, it's possible to make sure train/validation are always separated on repeated calls but not really user-friendly and doesn't have the cleanest documentation.

#### TODO
-  ~~make work with pipelines~~
- [x] allow for specification of a separate train and validation set as mentioned in https://github.com/scikit-learn/scikit-learn/issues/1626#issue-10319901
- [x] make sure will as expected when called repeatedly, which comes up in https://github.com/dask/dask-searchcv/pull/72.
-  ~~integrate with GridSearchCV and RandomizedSearchCV~~",843222
1060,2018-05-31T13:23:52Z,2019-08-05T18:49:38Z,,,"## Enhances: CalibratedClassifierCV
This pull request implements out-of-bag (oob) support for `CalibratedClassifierCV` when using a base_classifier that uses out-of-bag (e.g. `RandomForestClassifier`).

If the base_classifier has the attribute `oob_decision_function_` after training (which means that the base classifier was trained with `oob=True`) the calibrated classifier uses the fitted calibration to transform the out-of-bag predictions using that fit. 

Importantly, since the oob prediction is calculated for each fold, the model holds a matrix of the oob predictions for all samples in each fold and embeds the predictions for the fold (leaving nan values for the left-out fold). After the cross-validation the matrix is averaged (ignoring nan values) to obtain an oob prediction for all the samples.

Finally, the `oob` attribute is set to True

###note:
I could not find a specific issue that this resolves.",843222
1061,2018-05-31T01:02:57Z,2020-03-02T19:01:27Z,,,"This PR adds control over how many examples `partial_fit` sees. That is, how
many epochs (/passes over the training set) should `partial_fit` observe before
returning? I tried to add these to all classifiers listed in [User guide:
Scaling strategies][1], but some of them already had support for this.

This PR adds a `max_iter` keyword arg to `PassiveAgressiveClassifier.partial_fit`
and `BaseSGDClassifier.partial_fit`.

This change amounts to moving a hard coded constraint to a user-facing keyword arg.

TODO: test.

[1]:http://scikit-learn.org/stable/modules/scaling_strategies.html
",843222
1062,2018-05-22T03:44:29Z,2020-03-02T19:01:28Z,,,"
Corrected gradient of the RBF kernel

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes https://github.com/scikit-learn/scikit-learn/issues/11113
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Use the correct formula for the gradient of the RBF kernel: 
gradient = k(x_i, x_j) * ( ||x_i - x_j||^2 / length_scale^3)




<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1063,2018-05-17T09:03:47Z,2020-03-18T15:27:52Z,,,"#### Reference Issues/PRs
Fixes #7687.

#### What does this implement/fix? Explain your changes.
The original selection algorithm used by Scikit-learn's Kd-tree may suffer from time complexity degeneracy on some special cases. Two obvious cases are sorted inputs and duplicate inputs. Fortunately, there is a efficient and robost intro-select implementation in C++ STL which can handle our problem perfectly. So I replaced the original implementation with a wrapped call to `std::nth_element`.
",843222
1064,2018-05-16T16:53:29Z,2020-03-02T19:01:29Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #10850 
See also #10741 
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

Added a third option for initialisation `'rand_data'` which samples points from the data set for initialisation. It does this by assigning zero to all responsibilities except for the sampled points which are assigned responsibility of 1 to a given component. 

When the `init_params` are calculated when calling the `gmm`, this resp will produce inital means at the sampled points.

#### Any other comments?

I think this has added the desired function from the original PR #10741 but I'm not sure if it is a useful addition so I did a bit of extra looking. I've added gmm_test2.py (which I would not include in an eventual merge) to show how I produced the following.

Here (seed 1234) the sampling works fine for all three methods. The original data is 4 sets of Gaussian data. Orange crosses are the initial_mean values and the colouring is the labelling of the data by the gmm.

![figure_1234](https://user-images.githubusercontent.com/16683894/40130947-6afabf1c-5930-11e8-82b8-9ea4936b5c15.png)

But here (seed 10)

![figure_10](https://user-images.githubusercontent.com/16683894/40130984-7969b5da-5930-11e8-97b6-283206eaca35.png)

I think the fact that two of the sampled points are close together gives a poor fit for rand_data. I've labelled this as WIP to see if you think this is worth investigating further as a feature? I know that things like documentation (and proper testing) would need to be updated in addition to this.

n.b Also worth pointing out is that the current `'random'` does take data magnitude into account in some respect as it will produce inital centres very close to the mean of the data set (along all dimensions) as seen here.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1065,2018-05-15T14:07:07Z,2020-03-25T10:56:06Z,,,"See discussion on issue #10883.

This PR implements calibration losses for binary classifiers.

It also updates the doc about calibration, especially inaccurate references to the Brier score.",843222
1066,2018-05-08T00:25:36Z,2020-03-02T19:01:31Z,,,"

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Fixes #8191. Supersedes and resolved #8428 


#### What does this implement/fix? Explain your changes.

Similar to PR #8428, but adds parameters (`positive` and `max_iter`) to
`LinearRegression` consistent with `Lasso` and `ElasticNet`. Uses
`scipy.optimize.nnls` under the hood.

#### Any other comments?

**note** `scipy.optimize.nnls` cannot accept a sparse matrix for `X` or
`y`. Passing sparse `X` throws error through `check_X_y`.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1067,2018-05-05T10:36:34Z,2020-03-02T19:01:32Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #10993 

#### What does this implement/fix? Explain your changes.
It fixes a small bug in `spectral_embedding` while finding the neighbors when the input array is a `np.matrix`. 

#### Any other comments?
 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1068,2018-05-03T13:30:29Z,2020-03-10T06:00:44Z,,,"#### Reference Issues/PRs
closes #2688

#### What does this implement/fix? Explain your changes.
As discussed in #2688, sklearn's Factor Analysis does not support rotations. This makes it essentially useless for e.g. psychologists.

This PR implements the varimax and quartimax rotations.",843222
1069,2018-05-02T22:44:21Z,2020-03-02T19:01:33Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Fixes #10684

#### What does this implement/fix? Explain your changes.
Added support of stratified splits of training data to learning_curve method by using StratifiedShuffleSplit.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1070,2018-05-01T11:01:15Z,2020-03-02T19:01:34Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Background
The Mean Shift algorithm in scikit-learn is not implemented as described in the reference cited in the docs (Comaniciu and Meer 2002). The issue is the labelling of the data points, i.e. cluster assignment: in scikit-learn data points are assigned to the nearest centroid. It is evident from Fig 2. in (Comaniciu and Meer 2002) that this is not what is done there. Moreover, the paper states that

”The delineation of the clusters is a natural outcome of the mode seeking process. After convergence, the basin of attraction of a mode, i.e., the data points visited by all the mean shift procedures converging to that mode, automatically delineates a cluster of arbitrary shape.” [p.  608, Section 3]

This implies that a data point should be assigned to its attractor in the mode-seeking process, i.e. to the point it converges to.

+ Dorin Comaniciu and Peter Meer, “Mean Shift: A robust approach toward feature space analysis”. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619

#### What does this implement/fix? Explain your changes.
I have added an option `cluster_assignment` to the MeanShift and mean_shift functions that can be set to `nearest_centroid` (default) to get the current scikit-learn cluster assignment and to `attractor` to get assignment as described above. When bin seeding is used together with `attractor` cluster assignment, a data point is assigned to the same mode as the seed closest to it.

The advantage of using `attractor` cluster assignment is that the cluster shapes adapts to the density of the data. I have chosen a dataset for the example in examples/cluster/plot_mean_shift.py that showcases how the clustering can be different with the two `cluster_assignment` options. When an outlier is chosen as cluster center, with `nearest_centroid` also other points can be assigned to this cluster, something that is avoided with the `attractor` option. An outlier does not attract any data points in the mode-seeking process and will be the single data point in the cluster.

#### Any other comments?
I have noted that with coarse bin seeding and the `attractor` option, the clusters shapes will be awkward - a collection of rectangles pasted together. To remedy this I put the default bin size for the `attractor` option to 0.2 times the bandwidth; the default for `nearest_centroid` equals the bandwidth as before.  This affects the complexity of the algorithm and a paragraph addressing this has been added to the documentation of the MeanShift class, under Notes/Scalability:
```
With cluster_assignment = 'nearest_centroid', the default bin size equals
the bandwidth, whereas for cluster_assignment = 'nearest_centroid' the
default bin size is 0.2 times the bandwidth. This setting gives
'attractor' approximately 5^n_features times as many seeds.
```



<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
1071,2018-04-24T20:08:34Z,2020-03-02T19:01:35Z,,,Fixes #10985 ,843222
1072,2018-04-22T10:38:19Z,2019-08-05T18:37:33Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

closes #10257 

#### What does this implement/fix? Explain your changes.

Change the example according to the discussion in #10257.
A scatter plot is used instead of the histogram to show the impact of both transformers.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1073,2018-04-21T14:40:49Z,2019-08-06T15:40:15Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
related to #11000 


#### What does this implement/fix? Explain your changes.
Avoid conversion float 64 in PLS

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1074,2018-04-15T16:38:53Z,2019-08-05T18:35:06Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Fix #10903 
#### What does this implement/fix? Explain your changes.
y_min set to 0 y_max set to 1

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1075,2018-04-13T08:32:35Z,2020-03-02T19:01:36Z,,,"#### Reference Issues/PRs
Fixes #10733 

#### What does this implement/fix? Explain your changes.
This PR proposes a way to return datasets as data frames that incorporate information about the feature names and classes potentially. It follows a suggestion made by @jnothman but can go any direction if the community feels this is a valuable feature.
",843222
1076,2018-04-11T14:38:30Z,2020-03-02T19:01:37Z,,,"Fixes #10944. 
Replace all occurrences of WH with HW in sklearn/decomposition/nmf.py
change WH in doc/modules/decomposition.rst with HW",843222
1077,2018-04-06T17:02:19Z,2020-03-02T19:01:37Z,,,"#### Reference Issues/PRs
Fixes #10905 

#### What does this implement/fix? Explain your changes.
Improves the docstring of `permutation_test_score` and modifies change log section in `whats_new` module
",843222
1078,2018-03-28T21:51:22Z,2020-03-02T19:01:38Z,,,"#### Reference Issues/PRs
Fixes  #10742

#### What does this implement/fix? Explain your changes.
When the feature names contain whitespaces, like the column `""A B""` in the example below  the output of the get_feature_names method is ambiguous, e.g `['1', 'A B', 'C', 'D', 'A B^2', 'A B C', 'A B D', 'C^2', 'C D', 'D^2']`, i.e we can't differentiate between whitespaces used to denote multiplication and those within the feature name. 

Here's the full example: 

```
>>> import pandas as pd 
>>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=[""A B"", ""C"", ""D""])
>>> df
   A B  C  D
0    1  2  3
1    4  5  6
2    7  8  9
>>> from sklearn.preprocessing import PolynomialFeatures
>>> poly = PolynomialFeatures(2)
>>> poly.fit(df)
PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)
>>> poly.transform(df)
array([[  1.,   1.,   2.,   3.,   1.,   2.,   3.,   4.,   6.,   9.],
       [  1.,   4.,   5.,   6.,  16.,  20.,  24.,  25.,  30.,  36.],
       [  1.,   7.,   8.,   9.,  49.,  56.,  63.,  64.,  72.,  81.]])
>>> poly.get_feature_names() 
['1', 'x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2', 'x2^2']
>>> poly.get_feature_names(input_features=df.columns.tolist()) 
['1', 'A B', 'C', 'D', 'A B^2', 'A B C', 'A B D', 'C^2', 'C D', 'D^2']

```

This PR changes the separator to `""*""` instead of a whitespace, thus removing the ambiguity: 
```
>>> poly.get_feature_names(input_features=df.columns.tolist())
['1', 'A B', 'C', 'D', 'A B^2', 'A B*C', 'A B*D', 'C^2', 'C*D', 'D^2']
```",843222
1079,2018-03-26T18:41:43Z,2020-03-02T19:01:38Z,,,"#### Reference Issues/PRs
Fixes #10864

#### What does this implement/fix? Explain your changes.
Enables using 2SD scaling in StandardScaler.

References:
Gelman, A. (2008). Scaling regression inputs by dividing by two standard deviations.
Statistics in medicine, 27(15), 2865-2873.
",843222
1080,2018-03-13T13:03:32Z,2020-03-02T19:01:39Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #4632. Highly relevant to #4497. 


#### What does this implement/fix? Explain your changes.
Add `test_score_weight` option so that user can choose to pass in a sample weight string that is used in `fit_params` dictionary if user intends to get weighted cv test scores, or do nothing if user wants un-weighted test scores. This is very important to certain fields of research, e.g. finance. For now, it is silently default to un-weighted, which is a little undesirable.

#### Any other comments?
This supersedes #10800, which is closed. ",843222
1081,2018-03-13T10:46:22Z,2020-03-02T19:01:40Z,,,"#### What does this implement/fix? Explain your changes.
As per the t-sne implementation, any perplexity value larger than the number of samples is mathematically incorrect and should result in an error. Current behavior when perplexity is larger than n_sample results in a seemingly structured, but in reality broken, output. 

See SO link for examples, discussion and references: https://stats.stackexchange.com/questions/332370/why-do-i-get-weird-results-when-using-high-perpexity-in-t-sne

#### Any other comments?
extended description of perplexity parameter as well.",843222
1082,2018-03-12T13:59:49Z,2020-03-02T19:01:40Z,,,"#### Reference Issues https://github.com/scikit-learn/scikit-learn/issues/10778

#### What does this implement/fix?
Added a parameter log_base to toggle between calculation of information in bits vs nats.
Currently, the calculation of information is only for nats, which is not conventional and confusing. Log_base defaults to 'e' such that existing code will not break. Providing log_base = 2 will give the results in bits units.

#### Any other comments?
There is an SO thread about this [here](https://stackoverflow.com/questions/24686374/pythons-implementation-of-mutual-information)",843222
1083,2018-03-08T03:50:34Z,2019-08-06T20:43:41Z,,,"#### Reference Issues/PRs
Documentation for BernoulliNB and MultinomialNB documentation for alpha=0 
Partially fixes issue #10772

#### What does this implement/fix? Explain your changes.
The MultinomialNB cannot have an alpha = 0, it originally set alpha>=0 and i changed it to alpha>0

#### Any other comments?
",843222
1084,2018-03-02T08:13:23Z,2020-03-02T19:01:43Z,,,"#### Overview
This PR refactors `cd_fast.pyx` module (`linear_module`) which implements coordinate-descent, and make it more transparent and extensible.

The resulting implementation, is [a single function](https://github.com/dohmatob/scikit-learn/blob/refactor-cd/sklearn/linear_model/coordescendant.pyx) which can solve general penalized / constrained multi-task least-squares. regression problems. Out-of-the-box, it supports
  - Gram mode (crucial when p << n) and non-Gram mode (crucial when p >> n)
  - a variety of penalties and constraints, including:
    * L11 penalty (sum of L1 norms), as in multi-task Lasso;
    * L21 penalty (sum of L2 norms), as in G-Lasso;
    * L2INF constraint (L2 constraint on each block), as in vanilla dictionary-learning;
    * L1INF constraint (L1 constraint on each block), e.g as in sparse PCA

The API remains consistent with the legacy implementation via [cd_fast2](https://github.com/dohmatob/scikit-learn/blob/refactor-cd/sklearn/linear_model/cd_fast2.pyx).

Ping @agramfort 

**Other benefits**
* Things like screening rules for LASSO-type problems (inexact Stanford, GapSafe, etc.) and be implemented in a global way (by just modifying the single central cd solver), reducing maintenance complexity
 * The use of cBLAS has been factored out into a [transparent API](https://github.com/dohmatob/scikit-learn/blob/refactor-cd/sklearn/linear_model/blas_api.pxd) which is easier to understand, modify or replace (say, with the modern scipy's [cython_blas]([url](https://docs.scipy.org/doc/scipy/reference/linalg.cython_blas.html)) way)
* Factored the implementation of the dual gap computation into [a single transparent function](https://github.com/dohmatob/scikit-learn/blob/refactor-cd/sklearn/linear_model/utils.pyx#L69).

**Also**
 *  It also supports user-defined penalties (via their prox operators). For example, with this technology, it is straightforward to implement otherwise non-trivial models like:
  - BCD dictionary update in online structured dictionary-learning with general (e.g see Dohmatob el al. ""Learning brain regions via large-scale online structured sparse dictionary-learning"", In NIPS 2016"")

* The same code can be used in implement dictionary updates (once again with both standard and exotic penalties / constraints), via a single appropriate call (another upcoming PR)

**Some very preliminary benchmarks**
![bench](https://user-images.githubusercontent.com/634068/36889289-aaea2166-1df9-11e8-9252-91904e4561bb.png)

* Script: https://github.com/dohmatob/scikit-learn/blob/refactor-cd/benchmarks/bench_lasso_cd_fast2.py

",843222
1085,2018-03-01T12:03:23Z,2020-03-02T19:01:44Z,,,"#### Reference Issues/PRs
Fixes #10736


#### What does this implement/fix? Explain your changes.
This PR improve the spectral clustering implementation.

The first eigenvector is now dropt in spectral clustering. And the eigenvectors is weithted by their related eigenvalues for the embedding.

#### Any other comments?
Givng a simple affinity matrix like 
[ (1, 2, 100), (1, 3, 100), (2, 3, 100), (3, 4, 1), (4, 5, 100), (4, 6, 100), (5, 6, 100) ]

If the cluster number is set 2, now spectral clustering can stably resulting in clusters (1,2,3) and (4,5,6)

",843222
1086,2018-02-28T09:23:53Z,2020-03-02T19:01:44Z,,,"Fixes #10715

This patch change spectral embedding eigen_solver variable from amg to arpack when number of nodes is low. 

The original code use arpack to avoid the bug of amg, but not change this variable.  Then the resulting embedding would not be used, and a new embedding would be computed still using amg solver.

Since the laplacian has been transformed in the arpack part, the new embedding I think is incorrect.

Please note that I haven't make a test because this patch is simple and I have no idea about the standard result of  spectral embedding.

",843222
1087,2018-02-22T14:31:43Z,2020-03-02T19:01:46Z,,,"#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
\-

#### What does this implement/fix? Explain your changes.

This PR improves different data handling related sections. In most cases the `fit` method will be called once and the `predict` method multiple times I guess. So I improved the prediction part by removing redundant code and adding a lightweight validation. I hope that such kind of improvements are welcome.

The original `_fit` method:

```python
def _fit(self, X, y, incremental=False):
    X, y = self._validate_input(X, y, incremental)

    # Make sure self.hidden_layer_sizes is a list
    hidden_layer_sizes = self.hidden_layer_sizes       # <--- `hidden_layer_sizes` is a hyperparameter
    if not hasattr(hidden_layer_sizes, ""__iter__""):
        hidden_layer_sizes = [hidden_layer_sizes]
    hidden_layer_sizes = list(hidden_layer_sizes)

    # Validate input parameters.
    self._validate_hyperparameters()                   # <--- run other hyperparameter validations
    if np.any(np.array(hidden_layer_sizes) <= 0):      # <--- check the `hidden_layer_sizes` after all
        raise ValueError(""hidden_layer_sizes must be > 0, got %s."" %
                         hidden_layer_sizes)
    # ...
```

The improved version run all validations in the method `_validate_hyperparameters`: [multilayer_perceptron.py#L385-L391](https://github.com/nok/scikit-learn/blob/c7acb5b0446af723617950160a76a17e8ea14ea9/sklearn/neural_network/multilayer_perceptron.py#L385-L391).

Furthermore I changed it, because the `_predict` method does exactly the same, because the variable `layer_units` is required for the initialization of weights: [multilayer_perceptron.py#L679-L681](https://github.com/scikit-learn/scikit-learn/blob/d840ad3dec539f1b6314efb7d9299da18d34ac1e/sklearn/neural_network/multilayer_perceptron.py#L679-L681).

```python
def _predict(self, X):
    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])

    # Make sure self.hidden_layer_sizes is a list
    hidden_layer_sizes = self.hidden_layer_sizes       # <--- redundant code and validation
    if not hasattr(hidden_layer_sizes, ""__iter__""):
        hidden_layer_sizes = [hidden_layer_sizes]
    hidden_layer_sizes = list(hidden_layer_sizes)

    layer_units = [X.shape[1]] + hidden_layer_sizes + \ 
        [self.n_outputs_]                              # <--- b/c layer_units is required

    # Initialize layers
    activations = [X]

    for i in range(self.n_layers_ - 1):
        activations.append(np.empty((X.shape[0],
                                     layer_units[i + 1])))  # <--- usage of layer_units

    # ...
```

The improved method uses the new attribute `layers_`:

```python
def _predict(self, X):
    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])

    if X.shape[1] != self.n_inputs_:
        raise ValueError(""The size of the passed feature vector does not ""
                         ""match the number of neurons of the input layer, ""
                         ""got %s instead of %s."" % (X.shape[1],
                                                    self.n_inputs_))

    # Initialize layers:
    activations = [X]
    for i in range(self.n_layers_ - 1):
        activations.append(np.empty((X.shape[0], self.layers_[i + 1])))
    # ...
```

The new attributes `n_inputs_` and `layers_` are useful like `n_outputs_`:

```python
# Extract meta data:
n_samples, n_features = X.shape
self.n_inputs_ = n_features
self.n_outputs_ = y.shape[1]
self.layers_ = [self.n_inputs_] + layers + [self.n_outputs_]

is_initialized = hasattr(self, 'coefs_')
if not is_initialized or (not self.warm_start and not incremental):
    # First time training the model:
    self._initialize(y, self.layers_)                  # <--- e.g. usage of `self.layers_`
```

The new tests:

- Test for the new attributes: [test_mlp.py#L401-L410](https://github.com/nok/scikit-learn/blob/c7acb5b0446af723617950160a76a17e8ea14ea9/sklearn/neural_network/tests/test_mlp.py#L401-L410).
- Test for the new lightweight validation: [test_mlp.py#L442-L447](https://github.com/nok/scikit-learn/blob/c7acb5b0446af723617950160a76a17e8ea14ea9/sklearn/neural_network/tests/test_mlp.py#L442-L447).

#### Any other comments?

\-",843222
1088,2018-02-20T21:12:28Z,2020-03-02T19:01:46Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

Implementation of the leaky ReLU activation function and its derivative, to use as activation in multilayer_perceptron neurons.

This is done as a possible solution to the  **dying ReLU** problem, a situation in which the ReLU function always outputs 0 for any given input.

The leaky ReLU function allows a small gradient when the unit is not active.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1089,2018-02-14T08:18:43Z,2020-03-02T19:01:47Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

Kernel Ridge Regression (the only Kernel Ridge estimator is right now implemented in master) has a real simple mathematical formulation to adjust a regression problem using squared error loss combined with l2 regularization. It is similar in SVM is done, but with equal constrictions instead of inequal ones as in SVM.

![elm_min](https://user-images.githubusercontent.com/17336458/35975459-276dd2ec-0cdd-11e8-9256-af6d894dce7c.png)

Using a vector encoding, this regressor can be adapted as a multilabel classifier while keep all the advantages over SVM. And this is my proposal. I followed the mathematical explaining from section 3 in [Venkatesh's paper from 2007](http://dro.deakin.edu.au/eserv/DU:30044591/venkatesh-facerecognition-2007.pdf). Basically, it transforms the target with *r* labels into a vector with *r* components.

#### Any other comments?

This pull request comes after a controversial one about Kernel Extreme Learning Machine ( https://github.com/scikit-learn/scikit-learn/pull/10602 ). As @amueller pointed, ELM is so similar to Kernel Ridge that it is possibly a scam because it [Huang presents in 2012](http://www.neuromorphs.net/nm/raw-attachment/wiki/2015/scc15/ELM-Unified-Learning.pdf) ELM as a new algorithm. But Kernel Ridge, as it is presented in `scikit-learn`, just allows regression. This is a modification, using `sklearn.kernel_ridge.KernelRidge` as base, to allow Kernel Ridge multilabel Classification. Name, tests, code and implementation is open to discussion.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1090,2018-02-12T08:05:25Z,2020-03-02T19:01:48Z,,,"Updated the code, sorry for putting in lackluster pull requests. Will put in more attention to detail, something silly like putting in commands after the function ended was easy to prevent.

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

Resolves #10598 

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1091,2018-02-09T15:56:46Z,2020-03-02T19:01:48Z,,,"#### Reference Issues/PRs
None

#### What does this implement/fix? Explain your changes.
I added support for not normalizing the predict_proba result, and returning it as it is, even if the data is not multilabel. This is useful for implementing a rejection threshold.

#### Any other comments?
I can also add thresholding in ovrclassifier or somewhere else to reject classifications results, but I didn't see anything else like that in scikit, so I don't know if it's a wanted feature.
",843222
1092,2018-02-07T21:53:01Z,2020-03-02T19:01:49Z,,,"Since I needed to calculate accuracy from a clustering result in my research, I think it could be really useful to have it on the library depending on the task.
Specially useful when the task being implemented involves different sets of classes for each new dataset.
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

#### What does this implement/fix? Explain your changes.
The class_cluster_match(y_true, y_pred) translates clusters found through clustering methods, generally given in numbers to the corresponding best possible match of true class labels, generally strings.

#### Any other comments?
The max_main_diagonal was implemented by user Paul Panzer at https://stackoverflow.com/questions/48511584/how-to-efficiently-make-class-to-cluster-matching-in-order-to-calculate-resultin.
<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1093,2018-02-06T23:22:35Z,2020-03-02T19:01:50Z,,,"#### Reference Issues/PRs

Fixes #8359
Closes  #8373  

#### What does this implement/fix? Explain your changes.

The Gaussian Process code is still missing a basic implementation of a linear kernel. This project was started by @dalmia, but it seems some of the reviewer comments were never addressed. I've made an attempt at addressing them and have been using it for my own purposes. It would be nice to make this standard functionality.

#### Any other comments?

- [x] Remove the varaiance parameters 
- [x] Replace `_check_offset` with `_check_length_scale`
- [x] Make sure tests pass

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1094,2018-02-05T18:10:45Z,2020-03-02T19:01:50Z,,,"#### Reference Issues/PRs

Continuation of the awesome work in PR #4980 and PR #8724 respectively.

- Resumes and closes #4980, closes #8724


#### What does this implement/fix? Explain your changes.

Scope of this PR (and the PRs it is based on) is to implement the functionality
to compute Detection Error Tradeoff (DET) curves from within scikit-learn.
DET-curves can used as a classification evaluation metric in scenarios where
an explicit comparison of different errors types is required.


#### Any other comments?

Credit goes to the authors @jkarnows @jucor of the aforementioned PRs likewise.
For now I have essentially fixed some basic inconsistencies which made test fail
and prevented the doc from being compiled.

DET curves are a niche classification evaluation metric.
Moving forward I would like to know whether they have a place in Scikit and if contribution like this is appreciated before putting additional effort into it.

If so, I am happy to continue to work on this PR as per ToDo list below.

#### ToDo list and next steps (without particular ordering)

- [x] Fix broken/non-existent links in doc
- [x] Fix failing tests
- [x] Add tests for DET curves
- [x] Expand Usage notes with more background information about DET curves. Explain how DET curves can be used and be interpreted.
- [x] (Optional) Add example plot
- [x] Add automated invariance tests


#### Open questions

Additionally I compiled a list of question about the status-quo so far

- Do we need references in the module description also?
I compared to similar modules like ROC and Precision-Recall curves but the use of references is somewhat inconsistent. I personally would prefer to not put references in multiple places and opted to put them into the documentation.
- I could not find any documentation whether or not to add contributors to the list of authors. I removed the references for now, but am happy to add them back if desired.
- Any further comments of the original authors @jkarnows and @jucor ?
- Any further suggestion for the ToDo list?",843222
1095,2018-01-29T08:26:04Z,2020-03-02T19:01:51Z,,,"#### Reference Issues/PRs
Fixes #10529 

#### What does this implement/fix? Explain your changes.
A warning is displayed when non-finite train or test scores (inf or -inf) are generated from cross-validation. These infinite values cause the `mean` to become infinite and hence are problematic. 



<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1096,2018-01-21T20:30:55Z,2020-03-02T19:01:51Z,,,"#### Reference Issues/PRs
Fixes  #10233
Continue the work of #10249

#### What does this implement/fix? Explain your changes.
add epsilon to favour the true distribution in case when class_weight 'balanced' is used and float imprecision does not allow the sum of weights to be equal (float vs rational).

#### Any other comments?
Not sure if this is necessary.",843222
1097,2018-01-17T14:38:18Z,2020-03-02T19:01:52Z,,,"
**Reference Issues**
Fixes #10144
Fixes #8234

**What does this implement/fix? Explain your changes.**

This implements a top-k accuracy classification metric, for use with probabilistic class predictions in multiclass classification settings. A prediction is considered top-k accurate if the correct class is one of the k classes with the highest predicted probabilities. Probability maybe is too strong a term here, as all that is really required is a ranking of the predicted classes.",843222
1098,2018-01-16T11:43:53Z,2020-03-02T19:01:53Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #10470 
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
- [x] Make FeatureHasher provide get_feature_names
- [x] Test changes
#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1099,2018-01-13T21:27:10Z,2020-03-02T19:01:53Z,,,"This addresses  #10467, Bandwidth estimation for kernel density estimates.

String defining the bandwidth selection type is overwritten with fit.

This is a work in progress.

The cross-validation method to select the bandwidth should be added too.",843222
1100,2017-12-23T12:12:02Z,2019-08-05T19:29:35Z,,,"### Reference Issues/PRs
Fixes #10289 

### What does this implement/fix? Explain your changes.
This PR implements a generic benchmarking tool `sklearn.benchmark.benchmark_estimator_cost` .
The tool allows users to estimate runtime or space complexity of an estimator by fitting in on varied subsets of a particular input data, and then using a GaussianProcessRegressor to model the time used, peak memory and model_memory(The memory in use at the end of fitting, minus that at the beginning)

The PR is still a work in progress

### TO-DO
- [ ] Add code to stop a fit if time runs out
- [x] Work on benchmarking memory
- [x] Add code to log fit exceptions in errors dict
- [ ] Add an example
- [ ] Add tests",843222
1101,2017-12-20T15:31:23Z,2020-03-02T19:01:55Z,,,"Fixes #10337 
- added a test for all classifiers with sample_weight in the fit arguments where only one class remains after trimming.
 - make SVC and linearSVC fail.
- bug correction on GaussianNB and ComplementNB.",843222
1102,2017-12-14T18:28:14Z,2020-03-02T19:01:56Z,,,"
Fixes #9388 

Added a function to check for consistency between docstring of objects.

In this approach there is a python dictionary for each of Parameters, Attributes and Returns. Indexed by the name of the parameter/attribute/return with the value being a Python dictionary containing its type definition and description. The function checks for each object if the docstring is identical (excluding whitespaces) for a given parameter/attribute/return in the main dictionary. If a new parameter/attribute/return are found they are added in the main dictionary.",843222
1103,2017-12-09T06:10:01Z,2020-03-02T19:01:56Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Fixes #10252 


#### What does this implement/fix? Explain your changes.

Add `fit_params` to `validation_curve` and [WIP] `learning_curve` (have to fix some errors)


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1104,2017-12-08T04:25:13Z,2019-08-06T08:49:20Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

In response to #10064


#### What does this implement/fix? Explain your changes.

This PR will add a `requirements/` directory with various requirements files (e.g. `docs.txt`, `test.txt`, etc.)



<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1105,2017-12-07T18:23:54Z,2020-03-02T19:01:57Z,,,"[https://en.wikipedia.org/wiki/Sammon_mapping]

This is my first time contributing to `sklearn`. Please be patient with me. ",843222
1106,2017-11-28T12:56:16Z,2020-03-02T19:01:58Z,,,"This problem was spotted in the context of https://github.com/erikbern/ann-benchmarks. In particular look at http://www.itu.dk/people/pagh/SSS/ann-benchmarks/rand-euclidean-data_10_-1_rand-euclidean-query_euclidean.html where bruteforce (NearestNeighbors with `algorithm='brute'`) is a lot slower than bruteforce-blas (similar to `fast_euclidean_neighbors` in the snippet below):

This is a simple snippet showing a 3x speed-up in `NearestNeighbors.kneighbors`. The cost is to store the precomputed norms of `X` at fit time.

```py
import numpy as np

from sklearn.utils.extmath import row_norms
from sklearn.neighbors import NearestNeighbors

n_samples = int(1e6)
n_features = 1000
n_neighbors = 10
n_queries = 1
metric = 'euclidean'

rng = np.random.RandomState(42)
X = rng.randn(n_samples, n_features)
X_norms = row_norms(X, squared=True)

def fast_euclidean_neighbors(X_queries, n_neighbors):
    """"""Quick function extracting bits of pieces from the NearestNeighbors code""""""
    norms_squared = np.dot(X_queries, X.T)
    norms_squared *= -2
    norms_squared += row_norms(X_queries, squared=True)[:, np.newaxis]
    norms_squared += X_norms
    indices = np.argpartition(norms_squared, n_neighbors - 1, axis=1)
    indices = indices[:, :n_neighbors]

    neighbors_range = np.arange(n_queries)[:, np.newaxis]
    norms_squared = norms_squared[neighbors_range, indices]
    arg_ind = np.argsort(norms_squared, axis=1)
    norms = np.sqrt(norms_squared[neighbors_range, arg_ind])
    indices = indices[neighbors_range, arg_ind]
    return norms, indices

X_queries = rng.randn(n_queries, n_features)

res_fast = fast_euclidean_neighbors(X_queries, n_neighbors)

nn = NearestNeighbors(algorithm='brute', metric=metric)
nn.fit(X)
res_kneighbors = nn.kneighbors(X_queries, n_neighbors=n_neighbors)

np.testing.assert_allclose(res_fast, res_kneighbors)

print('fast_euclidean_neighbors')
%timeit fast_euclidean_neighbors(X_queries, n_neighbors)
print('NearestNeighbors.kneighbors')
%timeit nn.kneighbors(X_queries, n_neighbors=n_neighbors)
```

scikit-learn master:
```
fast_euclidean_neighbors
430 ms ± 7.94 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
NearestNeighbors.kneighbors
1.44 s ± 19.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

This PR:
```
fast_euclidean_neighbors
424 ms ± 9.23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
NearestNeighbors.kneighbors
421 ms ± 6.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

I'd like to do more extensive benchmarks, suggestions about which parameters to vary and parameters range more than welcome.",843222
1107,2017-11-22T00:33:36Z,2020-03-02T19:01:58Z,,,"#### Reference Issues/PRs
None

#### What does this implement/fix? Explain your changes.
Multidimensional scaling (MDS) fails for masked arrays, producing a false embedding.  This was due to the absence of any mask handling.  This code adds support for masked arrays for MDS, so MDS can be done with distance matrices that contain masked, i.e. unknown, elements.  ",843222
1108,2017-11-19T01:14:44Z,2020-03-02T19:01:59Z,,,"Change introduces additional parameter to _sklearn.manifold.MDS_, namely _normalize_ (default _False_), that can be used to return and use _Stress-1_ instead of raw Stress value. Already implemented _stress\__ attribute contains raw stress defined as:

<img width=""324"" alt=""zrzut ekranu 2017-11-19 o 02 00 40"" src=""https://user-images.githubusercontent.com/12532185/32986219-b04f752a-cccd-11e7-89ac-d0651db72796.png"">

The raw Stress value itself is not very informative, and its high value does not necessarily indicate bad fit. A better way of communicating reliability is to calculate a normed stress, eg. with Stress-1  implemented in current PR:

<img width=""349"" alt=""zrzut ekranu 2017-11-19 o 02 00 48"" src=""https://user-images.githubusercontent.com/12532185/32986216-a49f3a30-cccd-11e7-8092-a9869f648d3a.png"">

According to Kruskal (1964, p. 3): value 0 indicates ""perfect"" fit, 0.025 excellent, 0.05 good, 0.1 fair, and 0.2 poor.

For more information cf. Kruskal (1964, p. 8-9) and Borg (2005, p. 41-43).

",843222
1109,2017-11-17T13:08:26Z,2020-03-02T19:02:00Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


Fix #7755.

Add top level methods for `predict` and `predict_proba`.

Have not been able to test it though (tests pass).
Would appreciate some help with testing.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1110,2017-11-14T22:11:29Z,2020-03-02T19:02:00Z,,,"I made some changes in the function to implement gauss_adjust following the request in the thread [Feature Request: Gaussian adjust option in RobustScaler #10139](https://github.com/scikit-learn/scikit-learn/issues/10139).

There is a new optional input parameter for RobustScaler called gauss_adjust. If gauss_adjust is set to True, the scale will be adjusted so the distribution of the output feature has std = 1. By default, gauss_adjust is set to False.

Also built the package and run the tests. 

I'm new in open source projects, so let me know if there are any more steps I should follow.
",843222
1111,2017-11-13T03:05:24Z,2020-03-02T19:02:03Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs

#### What does this implement/fix? Explain your changes.
This branch implements LoOP (Local Outlier Probabilities), an unsupervised outlier detection approach that's similar to LOF (Local Outlier Factor) but that normalizes scores in the range [0, 1]. 

#### Any other comments?
I'm looking for contributing authors / developers interested in contributing and adding their names to the list of authors by completing this feature. Due to time constraints, I am unable to perform any rigorous testing and am looking for collaborators to add any missing / additional features and ensure proper functionality. 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1112,2017-11-12T16:23:17Z,2020-03-02T19:02:04Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes #8614 

#### What does this implement/fix? Explain your changes.
This PR adds a decision threshold calibration wrapper for binary classifiers that calibrates the decision threshold based on three objectives:

* optimise sum of true positive and true negative rate
* optimise true positive rate while keeping a minimum of true negative rate
* optimise true negative rate while keeping a minimum for the true positive rate

The classifier gives the option to either receive a pre-trained base estimator or train one and do the calibration using cross validation loops.

#### Any other comments?

There are two available examples to illustrate the first two points


#### the comments bellow are no longer valid but I will leave them for the completeness of the conversation bellow

1) Since (as discussed in #8614 ) this wrapper focuses on Binary Classification I have made the assumption that labels will always be `0` or `1`. Is this a correct assumption or can it be that labels are completely arbitrary? Can they also be more than two as long as one is considered to be the positive the rest negative? 

2) The current implementation gives the option for choosing the positive label, assuming that the indexes of the classes coincide with the labels. If labels will always be in `[0, 1]` should we fix the positive label to `1`? I though that it makes some sense for to be able to choose which class it will consider sensitive.

3) In the case of `cv!=""prefit""` after calibrating the threshold by averaging across different folds that have been trained with different data than those used for calibration the base estimator is trained using all the data. Do you see anything wrong with that?

<!-- 
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1113,2017-11-09T18:12:13Z,2020-03-02T19:02:05Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Based on our conversation with @ogrisel.
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
multilayer_perceptron.py did not accept multiple activation functions previously. Now, users can enter a list of activation functions for each hidden layer. If a single activation function is entered, it will be used for all hidden layers as previously.

#### Any other comments?
Initialization of coefficients are done based on the activation functions. However, it is not clear how to do initialization for the output layer where the activation function is ``softmax`` or ``logistic``. For such cases, I did the initialization based on the activation function in the previous layer. 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1114,2017-11-09T14:53:23Z,2020-03-02T19:02:05Z,,,"Fixes #10077

Error fixed in adaboost classifier.At some iterations weighted error was underflowing, due to high learning rate.,Making error as NaN and hence making subsequent iterations useless.
This update halts the iterations when such warning is encountered and prints a warning message to inform about the underflow.

This is my first PR. I am open to all sorts of criticism.",843222
1115,2017-11-06T22:03:46Z,2020-03-02T19:02:06Z,,,"Fixes #10055 

#### Changes
Changed MultinomialDeviance from total logloss to average logloss.
",843222
1116,2017-10-30T11:47:22Z,2020-03-02T19:02:07Z,,,"#### Reference Issues/PRs
Fixes #9933

#### What does this implement/fix? Explain your changes.
It adds the possibility to pick odd-based voting rule to the `VotingClassifier`. WIP

#### Any other comments?
See [this paper](https://arxiv.org/pdf/1302.0540.pdf) for why this might be a good voting rule (based on a game-theoretical aproach)
",843222
1117,2017-10-27T16:28:17Z,2020-03-14T14:21:29Z,,,"#### What does this implement/fix? Explain your changes.
This implements an online linear version of the One-Class SVM based on the SGD implementation. This implementation thus scales linearly with the number of samples and has a `partial_fit` method. Combining this implementation with kernel approximation techniques, we can approximate the solution of a kernelized `OneClassSVM` and still benefit from the training time complexity improvement (see example and benchmark below).

The optimization problem of the One-Class SVM can be written as an optimization problem that is very close to the ones solved by the SGD implementation (see the [doc for details](https://14573-843222-gh.circle-artifacts.com/0/home/ubuntu/scikit-learn/doc/_build/html/stable/modules/sgd.html#sgd-online-one-class-svm)). This implementation thus requires very few changes in the SGD cython code.

Benchmark comparing `OneClassSVM` and `SGDOneClassSVM` in terms of training time and AUC (n: number of training samples, d: number of features). The training size has been reduced for some of the datasets for LibSVM to finish in a decent time.

![train_time](https://user-images.githubusercontent.com/15966638/32114362-9ce36406-bb43-11e7-8ada-8f0a304ad334.png)

![auc](https://user-images.githubusercontent.com/15966638/32114366-a2ac1522-bb43-11e7-8ca3-e0206c82f9ef.png)

Toy example
![toy](https://user-images.githubusercontent.com/15966638/32114391-b086bd64-bb43-11e7-9da1-6de1540cc41d.png)

#### Any other comments?
This is still WIP because the tests can be refactored. Any comment is more than welcome.
cc @agramfort

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1118,2017-10-22T20:50:37Z,2020-03-02T19:02:09Z,,,"This PR fixes issue #3148

This new feature implements quantile regression - an algorithm that directly minimizes mean absolute error of a linear regression model. 

The work is still in progress, but I do want to receive some feedback.",843222
1119,2017-10-20T07:22:48Z,2020-03-02T19:02:09Z,,,"This commit implements gradient boosted regression trees for the Tobit model (gradient_boosting.py changed).

See https://arxiv.org/abs/1711.08695 for more background information on gradient boosting for the Tobit model.

It is shown in an example file (plot_gradient_boosting_tobit.py) how the model is applied.

The Tobit model is a widely used model in statistics and econometrics. See https://en.wikipedia.org/wiki/Tobit_model for more information. 

<del>A reference to an article with more background information on gradient boosting for the Tobit model will soon be available.</del>
",843222
1120,2017-10-09T18:06:26Z,2020-03-02T19:02:10Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->

We don't have an issue for that but discussed this via the mailing list (https://mail.python.org/pipermail/scikit-learn/2017-October/001960.html)


#### What does this implement/fix? Explain your changes.

Adds a `prefit` parameter to the `VotingClassifier` so that it accepts prefitted estimators; currently, it's only possible to use the `VotingClassifier` upon refitting clones of the original estimators by running `fit`.


#### Any other comments?

This is an early PR for feedback. 

Below are todos to get to when the implementation details are sorted out.


- [ ] Add note to ""What's New"" docs
- [ ] Add appropriate unit tests for `predict`, `predict_proba`, and `transform` when `prefit=True`

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
1121,2017-10-08T16:36:00Z,2020-03-02T19:02:11Z,,,"#### Reference Issue
Fixes #9868

#### What does this implement/fix? Explain your changes.
The LocallyLinearEmbedding class uses the NearestNeighbors class to calculate the K nearest neighbors yet only allowed the use of the euclidean distance metric to calculate those. I added the arguments needed to the class (and all functions used by it) in order to allow any valid metric for nearest neighbors to be used. Also added a test to ensure that another metric can be used.

#### Any other comments?
I'm not entirely sure of the utility of changing the metric from the euclidean to another one since the error function is calculated based on the euclidean metric. Maybe I also need to change the error calculation function to be dependent on the metric? Either way it is at least possible to choose the neighbors for the embeding based the user choice of metric.
Apreciate any feedback since its my first contribution",843222
1122,2017-09-27T17:41:44Z,2020-03-02T19:02:11Z,,,"This PR is a speedup to the confusion_matrix function in the case where labels is specified and already in index form. I added check to bypass expensive label to index conversion, which results in a nice 16x speedup. 

I've labeled this as WIP because I've only tested the case where `len(labels) = 12` and `len(y_true) = len(y_pred) = 172800`.  I need to do is more comprehensive testing to ensure that I didn't slow down other cases for the sake of one particular case. I believe this will be an overall improvement, but I want to make sure. 

The one benchmark I've done so far (on the aforementioned data) resulted in a reduction of compute time from `90.0ms` to `6.0ms`. This is a 15x increase. When computing several hundred confusion matrices, this becomes quite significant. 

The running time can be further improved to a `36x` increase if we added an extra flag (called `enable_checks=True`) to allow the user to disable the `_check_targets` and `check_consistent_length` call when appropriate.  I didn't add this to the PR by default because I thought there might be some pushback on adding a argument to a function signature. However, if a reviewer thinks this is ok, let me know and I'll add it to get some extra speed. 


TODO 
- [x] baseline proof-of-concept
- [x] is adding an extra flag ok for an extra x2 speedup? (lets just keep this PR simple)
- [x] benchmark: test speed differences on arrays that satisfy the new check condition with different numbers of labels / items (with different data types). Ensure there is now significant slowdown, and find the point where the speedup becomes significant.
- [x] benchmark: test speed differences on arrays that do not satisfy the new check condition with different numbers of labels / items (with different data types). Ensure there is not a significant slowdown.
- [x] associated what's new / documentation changes if necessary",843222
1123,2017-09-26T21:35:48Z,2020-03-02T19:02:12Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Continuation of #6222.

#### What does this implement/fix? Explain your changes.
Extended MDS object to include a transform method for out-of-sample points as described here:
http://papers.nips.cc/paper/2461-out-of-sample-extensions-for-lle-isomap-mds-eigenmaps-and-spectral-clustering.pdf

SMACOF algorithm is used in the non-extendible (default) case but not for the extendible case, which requires eigen-decomposition of (dis)similarity matrix.

Originally implemented by @webdrone in #6222.   It has since been modified to alter the `fit`/`transform` API and speed up subsequent `MDS.transform()` calls

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1124,2017-09-22T20:02:06Z,2019-08-05T19:30:56Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Fixes #9739

#### What does this implement/fix? Explain your changes.
Transposes the comparison of clustering and classification algorithms.

#### Any other comments?
Couldn't see any clever way to invert the for loops in the clustering example to do a trivial transpose, so I used a known conversion between matrix indexes to 1-dim array.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1125,2017-09-13T23:33:14Z,2020-03-02T19:02:13Z,,,"Ping @tommoral, @ogrisel ",843222
1126,2017-09-12T17:55:59Z,2020-03-02T19:02:14Z,,,"Fixes #9443.

After #9716 I think it's time to add these checks.",843222
1127,2017-09-06T10:38:33Z,2019-08-05T17:55:28Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
closes #5563 

#### What does this implement/fix? Explain your changes.
Do not trigger a deep copy of the parameters when cloning estimators.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1128,2017-08-23T23:32:04Z,2020-03-02T19:02:15Z,,,"Continuing work from #4237 and #2387. 

Rather than minimizing the funcSVD-type lost functions specified in the previous attempts, this PR follows advice from @amueller and @GaelVaroquaux , and imports SoftImpute from [FancyImpute](https://github.com/hammerlab/fancyimpute) instead. 

Here is the new cost function being minimized:

![image](https://user-images.githubusercontent.com/1597013/29642894-342457c8-8820-11e7-9acc-93c1e9e27f40.png)


#### What this implements/fixes

A new transformer `sklearn.preprocessing.FactorizationImputer`, that uses tests/structure originally created by the previous author as part of #4237

#### What I'd like to hear back

1. Any general feedback on how to improve this.
2. How I can add the fancyimpute dependency to scikit-learn

#### Other comments

Thanks to @sergeyf and iskandr, the authors who created this initial implementation of SoftImpute in python.

Here is [the original paper](http://web.stanford.edu/~hastie/Papers/mazumder10a.pdf) where the SoftImpute algorithm was introduced. Here is [another helpful introduction](https://web.stanford.edu/~hastie/swData/softImpute/vignette.html) to how the algorithm works.
",843222
1129,2017-08-23T20:00:10Z,2020-03-02T19:02:16Z,,,"#### Reference Issue

https://github.com/scikit-learn/scikit-learn/issues/5853
#### What does this implement/fix? Explain your changes.

It adds the CountFeaturizer transformation class, which can help with getting better accuracy because it will use how often a particular data row occurs as a feature 
#### Any other comments?

Currently work in progress, please let me know if there is something that I should add or if there is anything I can do in a better or faster way!

Also a continuation of 
https://github.com/scikit-learn/scikit-learn/pull/7803
https://github.com/scikit-learn/scikit-learn/pull/8144/",843222
1130,2017-08-22T18:26:52Z,2019-11-02T17:10:37Z,,,"#### Reference

Tutorial for #9601 

#### What does this implement/fix?

This pull request builds a tutorial for writing a custom kernel for the gaussian process module. Writing a custom kernel is currently error-prone, because there is no clear documentation outlining how to write a kernel that will correctly conform to scikit-learn requirements.

#### Any other comments?

This is my first pull request to scikit-learn. It only begins to solve the problem; it would be great if another contributor picked this up from here.",843222
1131,2017-08-18T21:21:10Z,2020-03-02T19:02:17Z,,,"Scorers pass classes from estimators into the labels argument of the metrics.
allow_subset_labels added to all the metrics allowing scoring on a subset of labels.

Issues: #6231 #8100 ",843222
1132,2017-08-18T04:52:37Z,2020-03-02T19:02:18Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Fixes: [9268](https://github.com/scikit-learn/scikit-learn/issues/9268)
However, please note that the implementation initially proposed in  [9268](https://github.com/scikit-learn/scikit-learn/issues/9268) has been replaced with the implementation as proposed in the primary reference mentioned below.

#### What does this implement/fix? Explain your changes.
This PR will implement a Gaussian Mixture Model with support for missing (NaN) data. The support will be available both for direct estimation of parameters in the presence of NaN or to impute missing values to be used with other estimators. The primary reference for this implementation will be [Ghahramani, Zoubin, and Michael I. Jordan. ""Supervised learning from incomplete data via an EM approach."" Advances in neural information processing systems. 1994](http://papers.nips.cc/paper/767-supervised-learning-from-incomplete-data-via-an-em-approach.pdf). The implementation will also draw heavily from the book [Schafer, Joseph L. Analysis of incomplete multivariate data. CRC press, 1997.](https://books.google.com/books?id=3TFWRjn1f-oC&dq=schafer+incomplete+multivariate+data&lr=&source=gbs_navlinks_s), especially in terms of implementing the EM algorithm in the presence of missing data. A further reference is Little, R. J. A., and D. B. Rubin. ""Statistical Analysis with Missing Data."" (2002).

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1133,2017-08-16T05:34:05Z,2020-03-02T19:02:19Z,,,"## Reference issues

In an ideal world:

* Closes #4497 (Gaël's overarching issue)
* Closes #4696 (Andy's sample properties PR)
* Closes #1574 (Noel's sample_weight support everywhere PR)
* Closes #3524 (Vlad's extension of Noel's work)
* Closes #4632 (Andy's issue about weighted scoring in CV)
* Closes #8158 (issue about parameters to CV scoring)
* Closes #2630 (issue about putting a Pipeline in AdaBoost)
* Closes #7646 (issue about nested grouped CV)
* Closes #8950 (inability to use grouped CV splitters in LogisticRegressionCV)
* Closes #8127 (inability to use grouped CV splitters in permutation_test_score)
* Helps towards #6322 (time series props)

## Functionality

As derived from my ranting at Thierry's https://github.com/scikit-learn/enhancement_proposals/pull/6, this:
* maintains backwards compatibility with the routing of kwargs to `Pipeline.fit`
* maintains backwards compatibility with the routing of kwargs and `groups` to `*SearchCV.fit`
* allows the user to specify a custom routing scheme with a fairly straightforward notation ([see `check_routing` docs](https://12858-843222-gh.circle-artifacts.com/0/home/ubuntu/scikit-learn/doc/_build/html/stable/modules/generated/sklearn.utils.metaestimators.check_routing.html)), in which the `*SearchCV` default policy, but not the Pipeline default policy, can be expressed
    * each destination name takes one of: all props, all but specified props (blacklist), only specified props (whitelist) optionally renamed, no props
* a meta-estimator has a `prop_routing` parameter to facilitate this
* a meta-estimator defines:
    * a set of destinations (e.g. steps' fit methods in Pipeline; perhaps also steps' transform methods in Pipeline; {cv, estimator, scoring} in *SearchCV)
    * a set of aliases for each destination (e.g. in Pipeline the step name to route to each fit method; ""*"" to route to all steps' fit methods)
    * a default routing policy (a specialised function in Pipeline; {'estimator': '-groups', 'cv': 'groups'} in *SearchCV)

## Example?

Ideally we should be able to get nested CV with groups and weighted scoring working:
```python
cross_val_score(GridSearchCV(..., cv=GroupKFold(),
                             prop_routing={'estimator': '-groups',
                                           'cv': 'groups',
                                           'scoring': 'sample_weight'}),
                X, y, groups,
                fit_params={'sample_weight': sample_weight},
                cv=GroupKFold(),
                prop_routing={'estimator': '*',
                              'cv': 'groups',
                              'scoring': 'sample_weight'})
```

It's pretty intricate, but I'm not sure we're going to get better than this!

## TODO (and help wanted!)

* [ ] ensure `check_routing` API is as we want (please comment!)
* [x] documentation for `check_routing`
* [ ] documentation for `Pipeline` and `*SearchCV`
* [ ] test ""*"" destination alias in Pipeline.
* [ ] test prop_routing in *SearchCV, particularly to metrics
* [ ] tests for error cases in Router and check_routing
* [ ] more tests for valid cases in Router and check_routing
* [ ] make prop_routing available in all CV routines
* [ ] make prop_routing available in all metaestimators
* [ ] make prop_routing available in gaussian processes (for kernels)
* [ ] get rid of `has_fit_parameter` where possible

The last steps there are the most arduous, and if we decide this is the way forward, I would welcome collaboration when we get to that stage.",843222
1134,2017-08-15T07:19:12Z,2020-03-25T20:09:25Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Fixes #5884

#### What does this implement/fix? Explain your changes.
Implements the Gower similarity in the sklearn.metrics.pairwse

#### Any other comments?
Unit tests are on the way, but please review and advise this piece of code while this.
This code cares about NaN propagation and non square matrix for parallel processing.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1135,2017-08-11T22:24:14Z,2019-08-05T17:52:58Z,,,"A WIP PR, ref: #8100 and #9290 

Added classes to following classifiers:

- GaussianNB
- BernoulliNB
- MultinomialNB
- DecisionTreeClassifier
- ExtraTreeClassifier
- RandomForestClassifier
- ExtraTreesClassifier
- GradientBoostingClassifier [WIP]
- LinearSVC
- SVC (on hold)
- LogisticRegression
- LogisticRegressionCV

To be done:
- Others classifiers
- sparse input support (svm, gbm, etc)",843222
1136,2017-08-01T19:47:36Z,2020-03-02T19:02:21Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Fixes #9305 


#### What does this implement/fix? Explain your changes.
This adds an attribute to the `AdaBoostClassifier` and `AdaBoostRegressor` estimators called `sample_weights_`. It also adds a new hyper-parameter to the respective `__init__` constructors (`'retain_sample_weights'`). If `'retain_sample_weights'` is True, at each iteration in the fit procedure, the adjusted `sample_weight` will be stored in an array.

#### Any other comments?
The `sample_weights_` matrix can become quite large, so the default value for `'retain_sample_weights'` is False. The user has to consciously want to analyze the sample weights (in the case of #9305 it was a researcher needing access to the weights for a paper).

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1137,2017-08-01T14:44:48Z,2020-02-05T16:15:51Z,,,"#### Description
The `predict` method of  `MLPRegressor` (for example) only returns the activation of the last layer (consistently with the rest of sklearn). It'd be nice to be able to get the activation of all the other (non-input) layers too ---via a new method named something like `_predict_all_activations`-- since these are computed anyways in the `_predict` method invoked by `predict`. This PR implements such a method.

This can be useful for understanding what's happening deep in the network by inspecting the features extracted by the different layers.",843222
1138,2017-07-23T17:28:23Z,2020-03-02T19:02:22Z,,,"This is a very simple way of making the `knn` kernel matrix symmetric. However, this slightly changes the meaning of `n_neighbors`: the _actual_ number of neighbors may vary from `n_neighbors` to `2 * n_neighbors` in the worst case.

 - [ ] Update tests after #5893 is merged.

Fixes #8008.",843222
1139,2017-07-19T16:15:24Z,2020-03-02T19:02:23Z,,,"Supersedes  #5396

Adds an option ""method"" to GroupKFold to change the way groups are distributed over folds. Current default is to balance the sizes of the folds. This adds the alternative of stratifying on the y variable, or shuffling the groups to randomize the folds they end up in.",843222
1140,2017-07-18T20:05:43Z,2020-03-04T14:25:09Z,,,"This PR adds Generalized Linear Models (full Tweedie family) with elastic net penalty comparable to R's glmnet, see issue #5975. Futher distributions of the exponential dispersion family can be added easily.
It also solves #11566.

Sorry, for not posting this earlier.

I'm excited about your feedback.

**TODO**

- [x] loss function (deviance) for Tweedie family (Normal, Poisson, Gamma, ...)
- [x] link functions
- [x] estimator class `GeneralizedLinearRegressor`
- [x] elastic net penalties
- [x] solver: coordinate descent, irls
- [x] tests
- [x] documentation
- [x] example",843222
1141,2017-07-18T12:11:34Z,2019-08-05T18:55:32Z,,,"#### Reference Issue
Fixes #9385

#### What does this implement/fix? Explain your changes.
Clarifications on splitters.

#### Any other comments?
No.",843222
1142,2017-07-16T20:14:43Z,2020-03-02T19:02:26Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
Fixes #9313 


#### What does this implement/fix? Explain your changes.
This pull request fixes issues of underflow and invalid value errors thrown from LabelPropagation, which are expected in certain data topologies. Error settings are temporarily changed to ignore `invalid` and `under` errors and restored at the end of the function.

#### Any other comments?
It appears that the issue of underflow for RBF and Laplacian kernels may occur for other models as well. I will try to go through SVMs and other models using kernel methods and see if this error appears.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1143,2017-07-14T08:38:51Z,2020-03-02T19:02:26Z,,,"#### What does this implement/fix? Explain your changes.
Exposing the metric param to allow custom distance function to be used in Meanshift

#### Any other comments?",843222
1144,2017-07-12T05:59:11Z,2020-03-02T19:02:27Z,,,"#### Reference Issue
Implements #8714 Online Inference for BayesianGaussianMixture

#### What does this implement/fix? Explain your changes.
Implements a partial_fit() method for BayesianGaussianMixture models, which allows for online or minibatch inference.  It simply sets the posterior from the previous fit as the new prior,  a method known as [Streaming Variational Inference](http://proceedings.mlr.press/v45/Huynh15.pdf).  Under very mild conditions, the expectation of the posterior on each batch forms an unbiased estimate of the posterior on the whole data, so after going through all batches the models will be very similar.  Due to the stochastic nature of the fitting process, it often reaches a lower Evidence Lower Bound than fitting the model to the whole dataset, because it is less likely to get stuck in local minima during optimization.

I included an example identical to the one comparing Bayesian and non-Bayesian mixture models, but with many more samples to take advantage of the speed increase.

#### Any other comments?
Was not sure what tests were required/useful, since it just sets a few parameters and then makes use of the existing fit() function which has its own tests.  I also wanted add the image from the example to the mixture docs but couldn't figure out how auto_examples worked.

- [x] main implementation
- [x] example
- [x] documentation
- [x] tests",843222
1145,2017-07-11T15:56:50Z,2020-03-02T19:02:28Z,,,"## _c_-means clustering

_c_-means clustering is a generalisation of _k_-means clustering that solves the problem of ambiguous data points. Rather than being assigned complete membership to a cluster, each data point has a membership to *all* clusters that varies between 0 and 1. The exact behaviour of this is algorithm-dependent. Although this changes the update rules for calculation of the centres, the basic algorithm (update centres, update labels, repeat) is the same as _k_-means [1].

![comparison](https://user-images.githubusercontent.com/15631502/28077191-f49ec946-6658-11e7-971a-af9f82226b9a.png)

#### To Do:
- [x] Implement probabilistic algorithm
- [x] Implement possibilistic algorithm
- [x] Take advantage of k++ initialization
- [x] Include the Gustafson-Kessel extension for non-circular clusters [2]
- [ ] Examples
- [ ] Descriptive documentation
- [ ] Full test coverage
- [ ] Optimisation using Cython (help would be appreciated on this as I've got very limited experience with Cython)

#### References
[1] Bezdek, James C., Robert Ehrlich, and William Full. ""FCM: The fuzzy c-means clustering algorithm."" Computers & Geosciences 10.2-3 (1984): 191-203.
[2] Gustafson, Donald E., and William C. Kessel. ""Fuzzy clustering with a fuzzy covariance matrix."" Decision and Control including the 17th Symposium on Adaptive Processes, 1978 IEEE Conference on. IEEE, 1979.",843222
1146,2017-07-10T14:15:54Z,2019-08-05T17:46:21Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
Fixes #9247 


#### What does this implement/fix? Explain your changes.
ClassifierChain uses the prediction from the classifier for each label as a feature for the next label's classifier. This change will enable users to select a method to chain on, one of { predict, predict_proba, decision_function }. The default being categorical prediction.

#### Any other comments?
No.",843222
1147,2017-07-06T16:17:14Z,2019-08-05T17:45:53Z,,,"Fixing issue #8100 

This PR involves adding a classes argument to the initialization of the gaussian naive bayes estimator. This would make the classes argument of partial_fit redundant as we'll no longer need to specify it every time.",843222
1148,2017-06-29T00:52:51Z,2020-03-02T19:02:29Z,,,"

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->


#### What does this implement/fix? Explain your changes.

On the current MeanShift website (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html), in the ""Parameters"" section, it states that ”Bandwidth used in the RBF kernel”.  However, the RBF kernel is not implemented in the MeanShift package.  The current MeanShift package uses, and only uses the linear kernel.

I introduced one more parameter of this package that the user can choose to use linear or RBF kernel, and also implement the RBF kernal's way to calculate the weighted mean.

I also removed the description ”Bandwidth used in the RBF kernel” from the website.


#### Any other comments?
I tested the code using ./build_tools/travis/flake8_diff.sh, pyflakes, and pep8.  Did not see error messages.  But the tool ""make"" says ""make: *** [test-code] Killed: 9"".  I don't know if it means 9 tests have failed.  But my change should not affect the results of the calculations, because if the user does not specify the value of the new parameter ""kernel"", it uses the default value, linear kernel, which is how the current code implemented.   

Please advise how I can past all the tests using ""make""

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1149,2017-06-20T15:13:21Z,2020-03-02T19:02:30Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

Finishes up and closes https://github.com/scikit-learn/scikit-learn/pull/7070

Fixes #6868

#### What does this implement/fix? Explain your changes.

I just made some minor cosmetic changes to an example.

#### Any other comments?

The original PR already had two +1's by me and by @jmetzen . This should be good to merge once tests pass.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1150,2017-06-20T06:54:31Z,2019-08-05T18:02:49Z,,,"Continues PR [8082](https://github.com/scikit-learn/scikit-learn/pull/8082)

Changes made:

- Added plot functions - plot_confusion_matrix, plot_gridsearch_results
- Updated examples - plot_rbf_parameters.py, plot_confusion_matrix.py
- added unit tests for new plot modules",843222
1151,2017-06-19T21:40:24Z,2020-03-02T19:02:31Z,,,"Another fix from the estimator tags branch. If ``y.shape == (n_samples, 1)`` master crashes.",843222
1152,2017-06-09T16:35:38Z,2020-03-02T19:02:32Z,,,"Continuation of #9017 (itself a continuation of #7356).

Adds an eleven-point interpolation method for average precision, as described in the IR book.
",843222
1153,2017-06-09T14:49:50Z,2020-03-02T19:02:32Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
Works on #11000 

#### What does this implement/fix? Explain your changes.
Avoids Bayesian Regression to aggressively cast the data to np.float64 when np.float32 is supplied.

#### Any other comments?

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1154,2017-06-09T14:18:17Z,2020-03-02T19:02:33Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

Works on #8769 for Stochastic Gradient Descent.

#### What does this implement/fix? Explain your changes.
The goal is to avoid aggressively casting input data to `np.float64` when `np.float32` is supplied to the `fit()` method of `SGDClassifier`. A unit test is added to `DenseSGDClassifierTestCase` to check whether the dtype of the output of `fit()` is consistent with the input.

#### Any other comments?
",843222
1155,2017-06-08T12:51:49Z,2020-03-02T19:02:34Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->

Fixes #8057


#### What does this implement/fix? Explain your changes.

Added support for sparse multilabel y for nearest neighbor classifiers. Firstly, it checks if the input to fit is sparse + multilabel and converts it to a dense one for storing. Also, it stores another parameter indicating whether the original input was sparse + multilabel or not. Now, in predict, if this stored value is true, then it converts y_pred to sparse CSC.
Also, added tests for the same.

#### Any other comments?

This PR wraps up @dalmia 's work in #8096 ",843222
1156,2017-06-07T11:20:55Z,2020-03-02T19:02:35Z,,,"Addresses issue #9009.

#### Proposal
Have a single function `dict_learning_online(batch_size=some_default)` which reproduces full-batch mode (the current  `dict_learning` function)  when `batch_size == n_samples`.",843222
1157,2017-06-06T15:30:04Z,2020-03-02T19:02:35Z,,,"#### Reference Issue
Closes #7549

#### What does this implement/fix? Explain your changes.
This PR implements a partial_fit method for TfidfTransformer.
As discussed in the thread https://github.com/scikit-learn/scikit-learn/issues/7549#issuecomment-250955363 , the number of features should not change after the partial_fit call, so I only update the DF. In order to do that, I slightly changed the logic of the TfidfTransformer: it maintains the vector of document frequencies (df) and the number of documents n_sample, not the actual idf vector. Instead, the idf vector is calculated when needed.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1158,2017-05-09T13:05:28Z,2019-08-05T17:44:31Z,,,"#### Reference Issue
This is just a first pass to rearrange the model evaluation documentation as discussed in #8646.

#### What does this implement/fix? Explain your changes.

- The scoring parameters section has a page of it's own
- Each metric function now has a page of it's own
- Referenced the scoring paramenters and metrics from the main model evaluation page

#### Any other comments?

This is still a work in progress, and any feedback would be valuable as I'm not sure if this split is convincing enough.

There is still work left to do to bring together the clustering and biclustering metrics in to the newly separated clustering page, but I'll wait for feedback on this change and then continue further. ",843222
1159,2017-05-07T17:34:49Z,2020-03-02T19:02:36Z,,,"#### Reference Issue
Fixes #8834 


#### What does this implement/fix? Explain your changes.

This adds a new parameter `copy` which when set to False, modifies the original matrix to make the Laplacian matrix without creating additional memory.

#### Any other comments?

I am stuck in making tests. Need advice from the community on how to go about it.
",843222
1160,2017-04-14T20:23:01Z,2020-03-02T19:02:37Z,,,"1. Finish the writeup about average score descriptions
2. Update a more obvious example for understanding and calculation

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1161,2017-04-09T16:05:41Z,2020-03-02T19:02:38Z,,,"#### Reference Issue
Resumes #4980 started by 	@jkarnows , which was left almost entirely finished.

#### What does this implement/fix? Explain your changes.
The great PR #4980 was left somewhat unfinished, and due to changes to the main trunk could not be merged any more.

The present PR rebases on master, and offers to finish this DET curve that I need for my own work.

#### Any other comments?

First time I contribute to scikit-learn, I do not mean to steal @jkarnows work, but it'd be a waste not to build on his great contribution.",843222
1162,2017-04-01T06:19:23Z,2020-03-02T19:02:39Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->

#6545

#### What does this implement/fix? Explain your changes.

Implements a new class for sequential feature selection

#### Any other comments?

This is a first PR to pick-up the discussion from #6545 and to get some feedback for improvements. 
Also, there are some more todos I have in mind when we get further down this PR:

Todos:
- [ ] Add documentation & examples
- [ ] Add more unit tests (e.g., GridSearchCV)


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
1163,2017-03-29T07:09:05Z,2019-08-06T16:01:57Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->


#### What does this implement/fix? Explain your changes.

This pull request aims to add the `sample_weight` parameter to the `calibration_curve` method in order for it to behave similarly to `cross_val_score`, `classification_report`, `roc_auc_score`, `roc_curve`, `average_precision_score`, `precision_recall_curve`, `brier_score_loss`, `precision_score`, `recall_score`, and `f1_score`.

Additionally, this implementation checks to make sure that there is a positive weighted number of samples. I am uncertain of whether this last requirement is strictly necessary.

```python
if sample_weight.sum() <= 0:
```

#### Any other comments?

In particle physics, it is common for Monte Carlo simulations to incorrectly generate the number of background and signal events by a small amount when producing a sample. As a means to remedy the composition of the sample a signal event may receive, for example, a weight of 1.3, while a background event may be given a weight of 0.8. Because the `calibration_curve` method is performing a particular frequency count on the prediction values of the sample it is important that the sample composition be corrected on an event-by-event basis. This is the physics motivation behind my PR. Thank you for your considerations. 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1164,2017-03-27T04:02:51Z,2020-03-02T19:02:40Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->

Resolves #8628 
Resolves #3336

#### What does this implement/fix? Explain your changes.
This implements OrdinalEncoder, which is a more informative encoding than one-hot for ordinal features. Implemented a new class OrdinalEncoder whose interface is same as OneHotEncoder class.

#### Any other comments?
Logic: For k values 0, ..., k - 1 of the ordinal feature x, this creates k - 1 binary features such that the ith is active if x > i (for i = 0, ... k - 1)

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->

**Working Example**
```py
>>> from sklearn.preprocessing import OrdinalEncoder
>>> enc = OrdinalEncoder()
>>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  
OrdinalEncoder(dtype=<type 'numpy.float64'>, handle_unknown='error',
        n_values='auto', ordinal_features='all', sparse=True)
>>> 
>>> enc.n_values_
array([2, 3, 4])
>>> 
>>> enc.feature_indices_
array([0, 1, 3, 6])
>>> 
>>> enc.active_features_
array([0, 1, 2, 3, 4, 5])
>>>
>>> enc.transform([[0, 1, 1]]).toarray()
array([[ 0.,  1.,  0.,  1.,  0.,  0.]])
```
**Another One**
```
>>> OrdinalEncoder(3).fit_transform([[0], [1], [2]]).toarray()
array([[ 0.,  0.],
       [ 1.,  0.],
       [ 1.,  1.]])
```
**A to-do list**
- [x] Converge on a final name for this feature. Current suggestions: `UnaryEncoder[+2]`, `OrdinalEncoder[0]`
- [x] Fix the nosetests errors
- [x] Write examples
- [x] Write unit-test cases
- [x] Any documentation work
",843222
1165,2017-03-23T03:45:05Z,2020-03-02T19:02:41Z,,,"#### Reference Issue
#8635

#### What does this implement/fix? Explain your changes.
I add the script print_changed_docfiles.sh in build_tools/circle/

It reproduces the _changed.html file locally in doc/_build/stable
The changes are checked between the actual commit and master. 
Of course master should be up-to-date.

```bash
source build_tools/circle/print_changed_docfiles.sh
```

#### Any other comments?
Just an example of utilization. Let me know if it is working.
If it looks useful, we can discuss in #8635 how to add it in a better way.",843222
1166,2017-03-19T16:50:25Z,2020-03-02T19:02:41Z,,,"The preprocessing.imputer module removes attributes that are completely empty. While this makes sense in general, when used in a pipeline this is undesirable (see issue #8539)

In consultation with @amueller I wrote an extension that (if desired) replaces these attributes with a constant. This way, in the pipeline we can always rely on a constant feature ordering (and if needed, remove the constant features afterwards)

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1167,2017-03-17T16:50:34Z,2020-03-02T19:02:42Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
Fixes #8566 


#### What does this implement/fix? Explain your changes.
When sample_weights are such that some classes do not have any assigned weight, libsvm results exclude the missing classes. This causes BaseSVC.classes_ to be out of sync with libsvm results. This commit fixes the issue by limiting BaseSVC.classes_ to the ones that have a sample_weight.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1168,2017-03-17T00:34:46Z,2020-03-02T19:02:43Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->


#### What does this implement/fix? Explain your changes.
Hi! This is an implementation of the Large Margin Nearest Neighbor algorithm that follows the original implementation of the author's (K.Weinberger) Matlab code (see also my docstrings). As far as I know there is no prior implementation of this version of the algorithm in python and no version at all in scikit-learn. The implementation lies in a single file (sklearn/neighbors/lmnn.py). Maybe it should go directly in the sklearn/neighbors/classification.py but I thought it would make things too cluttered. It is basically a subclass of the `KNeighborsClassifier` with a lot of added functionality. I put also an example in examples/neighbors/plot_lmnn_classification.py that is basically a copy of the simple nearest neighbor example in examples/neighbors/plot_classification.py. 

#### Any other comments?
I run all scikit-learn tests I could find and it now passes all of them. There is still a TODO in the code which I think is a minor issue (the algorithm works anyway). Somehow during fitting, I need to pass a possibly changed attribute (n_neighbors) to the superclass and call the fit method of the superclass. I would appreciate any comments and/or help to finalize this. Thanks!

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1169,2017-03-16T18:11:17Z,2020-03-02T19:02:44Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Fixes #8589 

#### What does this implement/fix? Explain your changes.
It adds a test to check the invariance of metrics against permutations of labels for multilabel and multiclass classifiers. There are four new tests in total, one for each of the following: multiclass, multilabel, thresholded multiclass, thresholded multilabel.

#### Any other comments?
I had to make an exception for confusion_matrix, as is expected after permutating labels these are not equal anymore.

One of the tests was based on a test in PR https://github.com/scikit-learn/scikit-learn/issues/7663, since it was mentioned in the issue.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1170,2017-03-09T13:20:10Z,2019-08-05T18:50:55Z,,,"
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
Fix #8194 

#### What does this implement/fix? Explain your changes.
Pass random_state in all test cases where it was missing in the estimator's invocation. 

#### Any other comments?
Added only in a few tests yet, nosetests ran OK. If it is correct and there are no further problems, I will add all the rest. Also, should I add in the contributing docs that when possible all estimators in new tests must be invoked with random_state set?



<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1171,2017-03-06T22:29:02Z,2020-03-02T19:02:46Z,,,"Closes #8993, #3449, #4848. This minimalistic patch adds multi-output support to BaggingRegressor.

",843222
1172,2017-03-04T21:04:22Z,2020-03-02T19:02:46Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Related to issue #3270... 

#### What does this implement/fix? Explain your changes.
In researching the background for the Jake's question ""One thing I'd thought of here: the default parameter value asks for exact results, which is basically the slowest possible algorithm. Most users will not likely dig into the doc strings to figure this out... perhaps we should change it to use some reasonable error threshold as the default?"" .... I discovered a typo in the docs. The default value for rtol was stated as a very small number (1E-8). This is true in the underlying code, but is overwritten when called with rtol=0 as default in the source code, as shown at the top of the docstring page in the example default function call. Basically this is a clarifying commit.  

#### Any other comments?
I think Jake makes a good point though that the default shouldn't be the slowest option. The question becomes then, what is reasonable? 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1173,2017-03-04T20:40:49Z,2020-03-02T19:02:47Z,,,"Fixes #8368
Addresses nan output from fit_transform() function of sklearn/decomposition/kernel_pca.py
Handles nan output when taking square root of zero with nan_to_num function. Suppresses warnings. 

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1174,2017-03-02T01:34:16Z,2019-07-12T20:00:46Z,,,"#### What does this implement/fix? Explain your changes.

This reorganizes the `plot_kernel_approximation.py` to achieve several objectives.

1. DRYer code, specifically as it relates to doing timing, and plotting.
1. More extensible code, by removing variables with `<variable>_<label>` and wrapping them into dictionaries, so that additions can be made that will be automatically reflected in the output.
1. More clearly separating modeling, figure layout and plotting concerns within the example code.

#### Any other comments?

I believe this is important (and I'm committed to working on more examples in this spirit) because these examples serve as important starting points for beginners who may not be able to overcome some of the minor issues present in these examples.",843222
1175,2017-03-01T08:00:40Z,2020-03-02T19:02:48Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Fixes #8466 

#### What does this implement/fix? Explain your changes.
This PR adds the Gabor kernel to sklearn.gaussian_process.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1176,2017-02-28T15:55:21Z,2020-03-02T19:02:49Z,,,"#8447 

Here I update NMF's multiplicative update solver to handle missing values (`np.nan`).
The solver simply doesn't take into account the loss at missing values, and optimizes the factorization on the rest of the data. This can be useful for imputation, recommendation (a bit out of scope for sklearn), or cross-validation.

**Not sure this is in scikit-learn scope though.**
___
The solver update is simple. Except for beta = 1, both denominator and numerator (in the multiplicative update) have either `X` or `W * H`, so we can mask them and sum the denominator and the numerator over valid elements. This is referred in the literature as ""weighted NMF"".

For the coordinate descent solver, the method uses the following factorization, to avoid recomputing `W * H` for each coordinate update of W: `(X - W * H) * H.T = X * H.T - W * (H * H.T)`
During the entire W-update, `X * H.T` and `H * H.T` do not change, so the optimization is fast.
But if we want to handle missing value, `H * H.T` changes for each line of W, which increases heavily the computations.
So I did not update the coordinate descent solver to handle missing values.

For the multiplicative update solver, the approximation `W * H` is used for the update of all coordinates, so we can compute it only once for each update of `W`. This is still more costly than `W * (H * H.T)` as used with beta = 2 without missing values, but this is already what we do for the other beta losses (!= 2).

I skipped the sparse case for now.
___
TODO:
- [ ] add more tests?
- [ ] add some documentation and an example
- [ ] do speed benchmark",843222
1177,2017-02-28T13:20:23Z,2020-03-02T19:02:50Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
<!-- Example: Fixes #1234 -->

#### What does this implement/fix? Explain your changes.
This PR follows #7853 to add screening to precompute ElasticNet and MultiTaskElasticNet.

#### Any other comments?
Needs #7853 to be merged :).


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1178,2017-02-22T19:24:14Z,2019-08-06T16:57:13Z,,,"#### Reference Issue
Fixes #4225


#### What does this implement/fix? Explain your changes.
This PR attempts at the following:

- [x] Add proper error message for sparse `y` in `GridSearchCV`
- [x] Add tests for the same
- [ ]  Examine difference in behavior between sparse 2-class `y` and sparse multilabel `y`
- [ ] Add support for sparse `y` in all estimators or support for sparse multilabel `y` in estimators support multilabel classification.
- [ ] Make `GridSearchCV` support sparse `y`
- [ ] Add tests

#### Any other comments?
Related PR #7996.
I am thinking of starting my work from the above linked PR as it is a subset of what I intend to achieve.


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1179,2017-02-21T22:04:50Z,2020-03-02T19:02:50Z,,,"This pull request adds a non-negative least squares to the linear model submodule.
The implementation is based on scipy's optimize module, which has a specific function for NNLS.

**Todo**
- [ ] User documentation.
- [ ] Possibly a better test :)

ref: #8191",843222
1180,2017-02-20T19:19:33Z,2020-03-02T19:02:51Z,,,"#### Reference Issue
Fixes #8393


#### What does this implement/fix? Explain your changes.
This corrects the docstrings of the `GaussianProcess` kernels to indicate that when `eval_gradient` is `True`, the matrix returned is the gradient of the log-transformed hyperparameters of the kernel and not that of the hyperparameters. Also, I've added comments in sections of the code indicating how the gradient has been calculated.
",843222
1181,2017-02-16T14:45:19Z,2020-03-02T20:11:05Z,,,"#### Reference Issue
Fixes #8359


#### What does this implement/fix? Explain your changes.
This adds `LinearKernel` to GaussianProcesses as per the link mentioned in the issue thread.

#### Any other comments?

- [x] Implementation of `sigma_v` and `sigma_b`
- [x] Add `diag(X)`
- [x] Add documentation
- [x] Implementation of `c`",843222
1182,2017-02-13T19:33:40Z,2020-03-02T20:11:05Z,,,"#### Reference Issue
Fixes #7195


#### What does this implement/fix? Explain your changes.
refactored as described in the issue: 
+ the `f_regression` is split into two functions : `f_regression` per se and `r_regression` which precomputes Pearson R
+ `abs_r_regression` wrapper around  `r_regression` added for use with `SelectKBest`
",843222
1183,2017-02-13T17:11:11Z,2020-03-02T20:11:06Z,,,"#### Reference Issue

#8157 

#### What does this implement/fix? Explain your changes.

Modify the Pipeline API such that `steps` given by the user at initialization is not modified.

#### Any other comments?

* `steps` is replaced by `steps_` at fit time to follow scikit-learn API.
* `named_steps` has the same behaviour as before with a `FutureWarning`. It will return unfitted estimators from `steps`.
* `named_steps_` returns fitted estimators from `steps_` .
* `steps` becomes `_steps` in order to create property and setter `steps`.",843222
1184,2017-02-11T00:14:26Z,2020-03-02T20:11:07Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
#6346

#### What does this implement/fix? Explain your changes.
Changed default param for percentile to a more reasonable default value.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1185,2017-02-08T13:41:13Z,2020-03-02T20:11:08Z,,,"This is a proof of concept for having an arbitrary callback function in `GridSearchCV` (and other CVs in the future) to return non-standard metrics, classification reports, coefficients, etc.; or to dump the model to disk, for instance. This gets around some of the limitations, and the complex engineering, of multiple metric support as in #7388; see https://github.com/scikit-learn/scikit-learn/issues/1837#issuecomment-277912593

Note that the modifications to the example here are currently rubbish just to be played with, not a true proposed change to that example.",843222
1186,2017-02-06T15:51:59Z,2018-02-06T23:33:43Z,,,"#### Reference Issue
Fixes #7980 


#### What does this implement/fix? Explain your changes.
This adds `parse_version` to `sklearn.utils` which is to replace `sklearn.utils.fixes._parse_version` and `LooseVersion` in scikit-learn

#### Any other comments?
I have just followed what @lesteve guided me in the issue thread. Please let me know if my approach is correct and if we can start implementing it throughout scikit-learn.",843222
1187,2017-02-06T08:01:41Z,2019-08-05T17:34:11Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Addresses #2890 

#### What does this implement/fix? Explain your changes.
Add gaussian 

#### Any other comments?
Add in plot example of a more scattered plot (using gaussian distribution) to model a more real world example. 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1188,2017-02-05T21:54:29Z,2018-02-06T23:33:49Z,,,"#### Reference Issue
Fixes #8266 

#### What does this implement/fix? Explain your changes.
Links the related projects from the wiki page to `related_projects.rst`

#### Any other comments
Am linking everything that is missing as of now. However, in the issue thread it was mentioned that we need include only the important. Please provide feedback as to what all should be included.
",843222
1189,2017-02-03T04:22:39Z,2020-03-02T20:11:09Z,,,"I've made a provision to show split progress when `verbose>2` and candidate progress when `verbose>9`

#### So in this branch
#### `verbose=2`

```py
In [1]: from sklearn import svm, datasets
   ...: from sklearn.model_selection import GridSearchCV
   ...: iris = datasets.load_iris()
   ...: parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
   ...: svr = svm.SVC()
   ...: clf = GridSearchCV(svr, parameters, n_jobs=8, verbose=2)
   ...: clf.fit(iris.data, iris.target)
   ...: 
Fitting 3 folds for each of 4 candidates, totalling 12 fits
[CV] START C=1, kernel='linear'.................................................
[CV] START C=10, kernel='rbf'...................................................
[CV] START C=1, kernel='rbf'....................................................
[CV] START C=10, kernel='rbf'...................................................
[CV] START C=10, kernel='linear'................................................
[CV] END .................................C=10, kernel='rbf'; total time=   0.0s
[CV] END .................................C=10, kernel='rbf'; total time=   0.0s
[CV] END ..................................C=1, kernel='rbf'; total time=   0.0s
[CV] START C=1, kernel='linear'.................................................
[CV] START C=1, kernel='rbf'....................................................
[CV] END ..............................C=10, kernel='linear'; total time=   0.0s
[CV] START C=10, kernel='linear'................................................
[CV] START C=1, kernel='linear'.................................................
[CV] END ..............................C=10, kernel='linear'; total time=   0.0s
[CV] END ..................................C=1, kernel='rbf'; total time=   0.0s
[CV] START C=10, kernel='rbf'...................................................
[CV] END ...............................C=1, kernel='linear'; total time=   0.0s
[CV] END .................................C=10, kernel='rbf'; total time=   0.0s
[CV] END ...............................C=1, kernel='linear'; total time=   0.0s
[CV] END ...............................C=1, kernel='linear'; total time=   0.0s
[CV] START C=10, kernel='linear'................................................
[CV] START C=1, kernel='rbf'....................................................
[CV] END ..................................C=1, kernel='rbf'; total time=   0.0s
[CV] END ..............................C=10, kernel='linear'; total time=   0.0s
```

#### This branch `verbose=3`

```py
Fitting 3 folds for each of 4 candidates, totalling 12 fits
[CV split 0 of 3] START C=10, kernel='rbf'......................................
[CV split 1 of 3] START C=1, kernel='linear'....................................
[CV split 1 of 3] START C=1, kernel='rbf'.......................................
[CV split 1 of 3] START C=10, kernel='linear'...................................
[CV split 1 of 3] START C=10, kernel='rbf'......................................
[CV split 1 of 3] END .C=10, kernel='linear'; score=0.921569, total time=   0.0s
[CV split 1 of 3] END ..C=1, kernel='linear'; score=0.960784, total time=   0.0s
[CV split 0 of 3] END ....C=10, kernel='rbf'; score=0.980392, total time=   0.0s
[CV split 2 of 3] START C=1, kernel='linear'....................................
[CV split 2 of 3] START C=1, kernel='rbf'.......................................
[CV split 1 of 3] END .....C=1, kernel='rbf'; score=0.960784, total time=   0.0s
[CV split 2 of 3] END ..C=1, kernel='linear'; score=0.979167, total time=   0.0s
[CV split 1 of 3] END ....C=10, kernel='rbf'; score=0.960784, total time=   0.0s
[CV split 2 of 3] START C=10, kernel='linear'...................................
[CV split 2 of 3] START C=10, kernel='rbf'......................................
[CV split 2 of 3] END .C=10, kernel='linear'; score=1.000000, total time=   0.0s
[CV split 0 of 3] START C=1, kernel='linear'....................................
[CV split 0 of 3] START C=10, kernel='linear'...................................
[CV split 0 of 3] START C=1, kernel='rbf'.......................................
[CV split 2 of 3] END ....C=10, kernel='rbf'; score=1.000000, total time=   0.0s
[CV split 2 of 3] END .....C=1, kernel='rbf'; score=0.979167, total time=   0.0s
[CV split 0 of 3] END .....C=1, kernel='rbf'; score=0.980392, total time=   0.0s
[CV split 0 of 3] END .C=10, kernel='linear'; score=1.000000, total time=   0.0s
[CV split 0 of 3] END ..C=1, kernel='linear'; score=1.000000, total time=   0.0s
```

#### This branch `verbose=10`

```py

Fitting 3 folds for each of 4 candidates, totalling 12 fits
[CV split 0 of 3; candidate 0 of 4] START C=1, kernel='linear'..................
[CV split 0 of 3; candidate 0 of 4] END C=1, kernel='linear'; score=1.000000, total time=   0.0s
[CV split 1 of 3; candidate 3 of 4] START C=10, kernel='rbf'....................
[CV split 2 of 3; candidate 0 of 4] START C=1, kernel='linear'..................
[CV split 2 of 3; candidate 0 of 4] END C=1, kernel='linear'; score=0.979167, total time=   0.0s
[CV split 1 of 3; candidate 3 of 4] END C=10, kernel='rbf'; score=0.960784, total time=   0.0s
[CV split 2 of 3; candidate 1 of 4] START C=1, kernel='rbf'.....................
[CV split 2 of 3; candidate 2 of 4] START C=10, kernel='linear'.................
[CV split 2 of 3; candidate 2 of 4] END C=10, kernel='linear'; score=1.000000, total time=   0.0s
[CV split 2 of 3; candidate 1 of 4] END C=1, kernel='rbf'; score=0.979167, total time=   0.0s
[CV split 2 of 3; candidate 3 of 4] START C=10, kernel='rbf'....................
[CV split 2 of 3; candidate 3 of 4] END C=10, kernel='rbf'; score=1.000000, total time=   0.0s
[CV split 0 of 3; candidate 2 of 4] START C=10, kernel='linear'.................
[CV split 1 of 3; candidate 2 of 4] START C=10, kernel='linear'.................
[CV split 1 of 3; candidate 0 of 4] START C=1, kernel='linear'..................
[CV split 1 of 3; candidate 0 of 4] END C=1, kernel='linear'; score=0.960784, total time=   0.0s
[CV split 0 of 3; candidate 2 of 4] END C=10, kernel='linear'; score=1.000000, total time=   0.0s
[CV split 1 of 3; candidate 2 of 4] END C=10, kernel='linear'; score=0.921569, total time=   0.0s
[CV split 0 of 3; candidate 3 of 4] START C=10, kernel='rbf'....................
[CV split 0 of 3; candidate 1 of 4] START C=1, kernel='rbf'.....................
[CV split 1 of 3; candidate 1 of 4] START C=1, kernel='rbf'.....................
[CV split 1 of 3; candidate 1 of 4] END C=1, kernel='rbf'; score=0.960784, total time=   0.0s
[CV split 0 of 3; candidate 3 of 4] END C=10, kernel='rbf'; score=0.980392, total time=   0.0s
[CV split 0 of 3; candidate 1 of 4] END C=1, kernel='rbf'; score=0.980392, total time=   0.0s
```

#### At master it was

(I find this difficult to debug especially when `n_jobs > 1`)

```py
Fitting 3 folds for each of 4 candidates, totalling 12 fits
[CV] kernel=linear, C=1 ..............................................
[CV] kernel=rbf, C=10 ................................................
[CV] ............... kernel=linear, C=1, score=1.000000, total=   0.0s
[CV] kernel=linear, C=1 ..............................................
[CV] ................. kernel=rbf, C=10, score=0.960784, total=   0.0s
[CV] ............... kernel=linear, C=1, score=0.979167, total=   0.0s
[CV] kernel=rbf, C=1 .................................................
[CV] kernel=linear, C=10 .............................................
[CV] .............. kernel=linear, C=10, score=1.000000, total=   0.0s
[CV] .................. kernel=rbf, C=1, score=0.979167, total=   0.0s
[CV] kernel=rbf, C=10 ................................................
[CV] ................. kernel=rbf, C=10, score=1.000000, total=   0.0s
[CV] kernel=rbf, C=1 .................................................
[CV] kernel=rbf, C=10 ................................................
[CV] kernel=linear, C=10 .............................................
[CV] kernel=linear, C=10 .............................................
[CV] .................. kernel=rbf, C=1, score=0.980392, total=   0.0s
[CV] .............. kernel=linear, C=10, score=0.921569, total=   0.0s
[CV] .............. kernel=linear, C=10, score=1.000000, total=   0.0s
[CV] ................. kernel=rbf, C=10, score=0.980392, total=   0.0s
[CV] kernel=rbf, C=1 .................................................
[CV] kernel=linear, C=1 ..............................................
[CV] ............... kernel=linear, C=1, score=0.960784, total=   0.0s
[CV] .................. kernel=rbf, C=1, score=0.960784, total=   0.0s
[Parallel(n_jobs=8)]: Done   2 out of  12 | elapsed:    0.0s remaining:    0.1s
[Parallel(n_jobs=8)]: Done   7 out of  12 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=8)]: Done  12 out of  12 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=8)]: Done  12 out of  12 | elapsed:    0.0s finished
```

@jnothman @lesteve Thoughts?",843222
1190,2017-01-24T12:15:58Z,2020-03-02T20:11:10Z,,,"Alternative to #8226, provides a generic CV optimisation making use of `warm_start`.

The example, modified from #8226, shows the benefit of using this for optimising GBRT `n_estimators`.

![times](https://cloud.githubusercontent.com/assets/78827/22247073/0cc06fe4-e28c-11e6-9369-0547d083e969.png)

* [x] Basic implementation version 2 (`use_warm_start` takes a string/list) as at 1-Feb-2017
* [x] Parameter docstring
* [x] Tests
* [x] Example
* [x] Narrative docs",843222
1191,2017-01-20T20:01:23Z,2020-03-02T20:11:10Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->


#### What does this implement/fix? Explain your changes.
Regarding regularization parameters balancing the tradeoff between
the fitting goodness and the constraints of NMF, it is better to have an option:beta as well as alpha.
e.g equation 13
http://oa.ee.tsinghua.edu.cn/%7Ezhangyujin/Download-Paper/E224%3DTKDE-13.pdf


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1192,2017-01-15T17:45:19Z,2020-03-02T20:11:11Z,,,"### Description
Prediction strength is a metric that measures the stability of a clustering algorithm and can be used to determine an optimal number of clusters without knowing the true cluster assignments. It has been introduced by [Tibshirani and Guenther (2005)](http://doi.org/10.1198/106186005X59243).

First, one splits the data into two parts (A) and (B). One obtains two cluster assignments, the first one using the centroids derived from the subset (A), and the second one using the centroids from the subset (B). Prediction strength measures the proportion of observation pairs that are assigned to the same clusters  according to both clusterings. The overall prediction strength is the minimum of this quantity over all predicted clusters.

The definition of prediction strength makes it difficult to integrate it into the existing GridSearchCV and metrics API, because a clusterer needs to trained twice, each time on a different subset of the data, and evaluated on a single subset.

### Implementation
Given two lists of cluster assignments, the function `prediction_strength_score` computes the actual score.
The class `PredictionStrengthGridSearchCV`extends `GridSearchCV` and modifies the way `_fit_and_score` operates. Instead of fitting an estimator on a training set and evaluating it on a test set (`_default_fit_and_score` function),  we need to fit and predict twice and compute the prediction score based on the two predictions (`_prediction_strength_fit_and_score` function). In addition, `PredictionStrengthGridSearchCV` uses the cross-validation generator twice, where the roles of training and test set are reversed the second time around. The final prediction strength for a given *k* and fold are the averages from both runs.

Moreover, `PredictionStrengthGridSearchCV` overrides how the best estimator is selected. The optimal number of clusters *k* is the largest *k* such that the corresponding prediction strength is above some threshold. Tibshirani and Guenther suggest a threshold in the range 0.8 to 0.9 for well separated clusters. If no configuration exceeds the threshold, a warning is displayed and the configuration with the highest prediction strength is selected.

`PredictionStrengthGridSearchCV` assumes that the `param_grid` parameter contains a key ""n_clusters"". 

### Open questions
1. The user could define additional parameters beside ""n_clusters"" in the parameter grid. For a given number of clusters, one would first need to choose the configuration that maximizes the prediction strength, and then choose the maximum *k* such that the maximum prediction strength is above a threshold.

2. Alternatively, one could remove ""param_grid"" and only offer the option to define a list of parameters for ""n_clusters"".

3. Finally, I had to add a flag `use_prediction_strength` to `_fit_and_score` to allow deviating from the common case: fit once and predict once. I welcome any suggestions on how this could be improved without duplicating too much code.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1193,2016-12-23T07:34:16Z,2020-03-02T20:11:12Z,,,"[WIP] Just started work on #8103. Some functions in supervised clustering metrics now allow to pass
contingency matrix for more efficient calculations. I'll add this functionality to all the functions.",843222
1194,2016-12-22T18:46:37Z,2020-03-02T20:11:13Z,,,"#### Reference Issue
This is start of work to address the issues in #78 #7574

#### What does this implement/fix? Explain your changes.
This adds an initial draft of the ProgIter object. This object 
wraps around long-running loops and reports progress in a simple but customization way. 
The purpose is meant to simplify writing progress messages and create a construct that can simply be dropped into existing code.

I've added in usage of this object into kmeans++ and MiniBatchKmeans to address my original use case. The case with MiniBatchKmeans requires a bit more customization because other useful messages in the loop. However, the case with kmeans++ where the change is extremely minimal and really demonstrates how this can just be dropped into existing code. 


#### Any other comments?

While this feature does work end-to-end it is not complete. 
Because this is a work in progress I have left some debugging code, references to my utility library (utool), and documentation constructs that I use in my coding workflow to quickly test things. 

I don't see myself able to work much on this feature in the near future, but I did want to push what I had so far because (a) just to put it out there, (b) so I don't lose it, and (c) in case it is useful to someone else. 


* **Basic Usage**
Basic usage of the object looks like this: 

```python
    >>> from sklearn.externals.progiter import ProgIter
    >>> def is_prime(n):
    ...     return n >= 2 and not any(n % i == 0 for i in range(2, n))
    >>> for n in ProgIter(range(10000)):
    >>>     # do some work
    >>>     is_prime(n)
    10000/10000... rate=13294.94 Hz, eta=0:00:00, total=0:00:00, wall=13:34 EST 
```
By default the output is continually updated by clearing the previous line, so at the end only the last progress message shows.

* **Demo Function**
The progiter module itself contains a small demo function to demonstrate how it works. Running the demo results in this output: 

```
-----
Demo #0: progress can be disabled and incur essentially 0 overhead
However, the overhead of enabled progress is minimal and typically insignificant
this is verbosity mode verbose=0

tic(u'demo0')
...toc(u'demo0')=0.6284s

-----
Demo #1: progress is shown by default in the same line
this is verbosity mode verbose=1

tic(u'demo1')
demo1 1000/1000... rate=1584.12 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
...toc(u'demo1')=0.6314s

-----
Demo #2: clearline=False prints multiple lines.
Progress is only printed as needed
Notice the adjustment behavior of the print frequency
this is verbosity mode verbose=2

tic(u'demo2')
demo2    0/1000... rate=0.00 Hz, eta=?, total=0:00:00, wall=13:20 EST 
demo2    1/1000... rate=2702.52 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo2  257/1000... rate=1522.38 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo2  642/1000... rate=1560.16 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo2 1000/1000... rate=1573.36 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
...toc(u'demo2')=0.6357s

-----
Demo #3: Adjustments can be turned off to give constant feedback
this is verbosity mode verbose=3

tic(u'demo3')
demo3    0/1000... rate=0.00 Hz, eta=?, total=0:00:00, wall=13:20 EST 
demo3  100/1000... rate=1476.12 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo3  200/1000... rate=1427.07 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo3  300/1000... rate=1479.27 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo3  400/1000... rate=1505.12 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo3  500/1000... rate=1491.30 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo3  600/1000... rate=1515.48 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo3  700/1000... rate=1512.45 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo3  800/1000... rate=1514.54 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo3  900/1000... rate=1515.45 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
demo3 1000/1000... rate=1516.72 Hz, eta=0:00:00, total=0:00:00, wall=13:20 EST 
...toc(u'demo3')=0.6594s
```

* **Usage in MiniBatchKMeans**
An example showing how this works in the context of MiniBatchKMeans is here: 

```python
        >>> from sklearn.cluster import MiniBatchKMeans
        >>> from sklearn.datasets.samples_generator import make_blobs
        >>> import numpy as np
        >>> n_clusters = 4000
        >>> X, true_labels = make_blobs(n_samples=int(1E5), centers=n_clusters,
        ...                             cluster_std=1., random_state=42)
        >>> mbkm = MiniBatchKMeans(n_clusters=n_clusters,
        ...                        init_size=3 * n_clusters, n_init=2,
        ...                        random_state=0, verbose=2).fit(X)
        >>> print('mbkm.labels_ = %r' % (mbkm.labels_,))
```

First with verbose=2, which means the ProgIter object will not clear progress lines and always just append the next line to stdout.

```
Init cluster centers with k-means++
kmeans++    0/4000... rate=0.00 Hz, eta=?, total=0:00:00, wall=13:24 EST 
kmeans++    1/4000... rate=192.34 Hz, eta=0:00:20, total=0:00:00, wall=13:24 EST 
kmeans++  192/4000... rate=931.83 Hz, eta=0:00:04, total=0:00:00, wall=13:24 EST 
kmeans++  480/4000... rate=1034.86 Hz, eta=0:00:03, total=0:00:00, wall=13:24 EST 
kmeans++ 1117/4000... rate=1084.50 Hz, eta=0:00:02, total=0:00:01, wall=13:24 EST 
kmeans++ 1125/4000... rate=1084.81 Hz, eta=0:00:02, total=0:00:01, wall=13:24 EST 
kmeans++ 2250/4000... rate=1107.09 Hz, eta=0:00:01, total=0:00:02, wall=13:24 EST 
kmeans++ 2260/4000... rate=1107.16 Hz, eta=0:00:01, total=0:00:02, wall=13:24 EST 
kmeans++ 3390/4000... rate=1112.93 Hz, eta=0:00:00, total=0:00:03, wall=13:24 EST 
kmeans++ 4000/4000... rate=1114.58 Hz, eta=0:00:00, total=0:00:03, wall=13:24 EST 
Inertia for init 1/2: 169.317605
Init cluster centers with k-means++
kmeans++    0/4000... rate=0.00 Hz, eta=?, total=0:00:00, wall=13:24 EST 
kmeans++    1/4000... rate=3412.78 Hz, eta=0:00:01, total=0:00:00, wall=13:24 EST 
kmeans++  257/4000... rate=975.97 Hz, eta=0:00:03, total=0:00:00, wall=13:24 EST 
kmeans++  642/4000... rate=1059.49 Hz, eta=0:00:03, total=0:00:00, wall=13:24 EST 
kmeans++ 1123/4000... rate=1044.57 Hz, eta=0:00:02, total=0:00:01, wall=13:24 EST 
kmeans++ 2050/4000... rate=1060.59 Hz, eta=0:00:01, total=0:00:01, wall=13:24 EST 
kmeans++ 2160/4000... rate=1063.67 Hz, eta=0:00:01, total=0:00:02, wall=13:24 EST 
kmeans++ 3240/4000... rate=1074.09 Hz, eta=0:00:00, total=0:00:03, wall=13:24 EST 
kmeans++ 3285/4000... rate=1074.54 Hz, eta=0:00:00, total=0:00:03, wall=13:24 EST 
kmeans++ 4000/4000... rate=1082.62 Hz, eta=0:00:00, total=0:00:03, wall=13:24 EST 
Inertia for init 2/2: 169.461913
Begining mini-batch iterations
minibatch      0/100000... rate=0.00 Hz, eta=?, total=0:00:00, wall=13:24 EST 
minibatch      1/100000... rate=24.79 Hz, eta=1:07:13, total=0:00:00, wall=13:24 EST inertias: batch=0.024289, ewa=0.024289 
[MiniBatchKMeans] Reassigning 50 cluster centers.
minibatch     12/100000... rate=37.27 Hz, eta=0:00:00, total=0:00:00, wall=13:24 EST inertias: batch=0.022055, ewa=0.024292 
Converged (lack of improvement in inertia) at iteration 12/100000
Computing label assignment and total inertia
labels inertia    0/1000... rate=0.00 Hz, eta=?, total=0:00:00, wall=13:24 EST 
labels inertia    1/1000... rate=348.57 Hz, eta=0:00:02, total=0:00:00, wall=13:24 EST 
labels inertia  257/1000... rate=382.73 Hz, eta=0:00:01, total=0:00:00, wall=13:24 EST 
labels inertia  382/1000... rate=382.87 Hz, eta=0:00:01, total=0:00:00, wall=13:24 EST 
labels inertia  764/1000... rate=389.35 Hz, eta=0:00:00, total=0:00:01, wall=13:24 EST 
labels inertia  792/1000... rate=389.13 Hz, eta=0:00:00, total=0:00:02, wall=13:24 EST 
labels inertia 1000/1000... rate=390.99 Hz, eta=0:00:00, total=0:00:02, wall=13:24 EST 
mbkm.labels_ = array([ 972, 1783,  797, ..., 3203, 3363, 2020], dtype=int32)

```

Verbosity 2 can be a bit much, so when verbosity=1 a clearline sequence will tell the terminal to delete the previous verbosity line before it prints the next one. This has a nice where you see a single line in the terminal update every once in awhile. Obviously I can't show updates in static text, but at the end of the script it looks like this: 

```
Init cluster centers with k-means++
kmeans++ 4000/4000... rate=1260.10 Hz, eta=0:00:00, total=0:00:03, wall=13:28 EST 
Inertia for init 1/2: 169.317605
Init cluster centers with k-means++
kmeans++ 4000/4000... rate=1243.40 Hz, eta=0:00:00, total=0:00:03, wall=13:28 EST 
Inertia for init 2/2: 169.461913
Begining mini-batch iterations
minibatch      1/100000... rate=24.85 Hz, eta=1:07:03, total=0:00:00, wall=13:28 EST inertias: batch=0.024289, ewa=0.024289 
[MiniBatchKMeans] Reassigning 50 cluster centers.
minibatch     12/100000... rate=36.89 Hz, eta=0:00:00, total=0:00:00, wall=13:28 EST inertias: batch=0.022055, ewa=0.024292 
Converged (lack of improvement in inertia) at iteration 12/100000
Computing label assignment and total inertia
labels inertia 1000/1000... rate=451.42 Hz, eta=0:00:00, total=0:00:02, wall=13:28 EST 
mbkm.labels_ = array([ 972, 1783,  797, ..., 3203, 3363, 2020], dtype=int32)
```

",843222
1195,2016-12-09T22:36:33Z,2020-03-02T20:11:13Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->

Add SmartSplitter: use local response proportion for categorical feature during decision tree node split process.

TODO:
- Add tests
- Run tests

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->",843222
1196,2016-12-08T21:31:43Z,2019-11-01T12:00:30Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
#8025

#### What does this implement/fix? Explain your changes.
This PR adds samplers for intersection and Jensen-Shannon kernels (in addition to additive chi2 kernel sampler).

#### Any other comments?
Most part of the AdditiveChi2Sampler class code has been moved to a new class BaseAdditiveHomogenousKernelSampler which is made a base class for AdditiveChi2Sampler, IntersectionSampler and JensenShannonSampler. Those three classes only override __init__ method which sets class-specific attributes.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1197,2016-12-07T03:21:44Z,2020-03-02T20:11:15Z,,,"#### Reference Issue
Fixes #7886 


#### What does this implement/fix? Explain your changes.
This adds the test that we don't support sparse `y` as of yet for multilabel estimators.

#### Any other comments?
Since `RandomTreesEmbedding` doesn't make use of `y`, so I was not really clear as to how should it be included in the tests and hence, have left it out currently. Also, I did cross the maximum characters limit for the sake of clarity by a very small margin and noticed that there are other such lines (bigger)  present, hence, chose to stick to it.

",843222
1198,2016-12-05T22:31:27Z,2020-03-02T20:11:15Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
<!-- Example: Fixes #1234 -->
Fixes #7975 
Fixes #7821 
(Duplicate issues)

#### What does this implement/fix? Explain your changes.
If different samples have different noise-levels(Should be weighted differently), these noise levels should be added in the fit() method. Not in constructor, as it is currently.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->

-Alpha passed to constructor of GaussianProcessRegressor now must be scalar.
-In addition to alpha, sample_alpha argument in fit() is added  to the kernel matrix diagonal.  None(default),float or array, shape(num_samples,)

Quick test:
![figure_2](https://cloud.githubusercontent.com/assets/24376889/20905019/693da956-bb42-11e6-97e2-56d85c0479b5.png)
![figure_1](https://cloud.githubusercontent.com/assets/24376889/20905025/6b5a05ae-bb42-11e6-8a2a-ec63a34d93b1.png)
[sample_alpha.zip](https://github.com/scikit-learn/scikit-learn/files/632457/sample_alpha.zip)",843222
1199,2016-11-20T16:08:37Z,2020-03-02T20:11:16Z,,,"It would be nice to have this feature so I decided to finish #2680.
To-do:
- [X] Use BaseRBM for both BernoulliRBM and GaussianBernoulliRBM
- [ ] Test Gaussian Bernoulli RBM
- [ ] Illustrative example",843222
1200,2016-11-19T17:02:42Z,2020-03-12T20:17:08Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue
This PR covers previous issues concerning SVDD:
* the original unfinished PR #3201 which is now very outdated;
* Issue #2807 requesting SVDD to be added to Scikit;
* PR #5899 which attempted to revive PR #3201 but was closed;
* Unfinished PR #3013 which tried to modify the libsvm part only.

#### What does this implement/fix? Explain your changes.
This PR offers the following:
* SVDD-L1 implementation based on this [libsvm implementation](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#libsvm_for_svdd_and_finding_the_smallest_sphere_containing_all_data). The model was extended to the case of a penalty cost vector (in line with other implemented support vector models in Scikit);
* documentation, outlining the SVDD-L1 model;
* an example showing a difference between the One-Class SVM and SVDD models.

#### Any other comments?
The original model was proposed by Tax and Duin (2004), and later reformulated by Chang, Lee, Lin (2013). This PR implements the latter reformulation and extends it to the case of different weights of the observations. I can provide proof of the key theorems if necessary.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1201,2016-11-10T17:03:04Z,2020-03-02T20:11:18Z,,,"This is a revival of #5075, I've done a rebase, some corrections and fix some bugs.

If you don't remember :
This PR implements the screening rules for the Lasso and ElasticNet presented in
http://jmlr.org/proceedings/papers/v37/fercoq15.html

From the experiment the screening offers a systematic speed gain on all dataset we tested.

@agramfort @josephsalmon @ofercoq",843222
1202,2016-11-07T20:17:21Z,2020-03-02T20:11:20Z,,,"I added an option to CountVectorizer and TfidfVectorizer to use a floating point value for the max_features parameter to express a percentage of the features instead of an absolute value.

I also changed the default from None to 1.0 and added tests.

This is my first contribution to this project so let me know if I did anything wrong :).",843222
1203,2016-11-06T02:52:38Z,2020-03-02T20:11:20Z,,,"Many algorithms, such as word2vec result in nearest neighbor computations based on cosine similarity. Unfortunately, since cosine (dis)similarity is not a metric it can't be used with kd-trees and ball-trees. This means that algorithms that make use of these structures (e.g. DBSCAN clustering, fast t-SNE, etc.) can't operate with regard to the ""appropriate"" (dis)similarity measure. Here we add angular (or arccos) distance which is the natural metric analogue of cosine dissimilarity to the valid metrics used for kd-trees and ball-trees. Credit for this work belongs to @brunoalano who submitted a similar change to hdbscan.

",843222
1204,2016-10-27T22:26:21Z,2020-03-02T20:11:21Z,,,"#### Reference Issue #6191
#### What does this implement/fix? Explain your changes.

changes in function label_binarize(...):
- extended doc string
- change pos and neg labels just to be non equal
- added param to force return matrix even for binary case
- minor code cleaning (rename Y -> y_out according pep8)

## Checklist
- [x] Clean style in [the spirit of PEP8](https://www.python.org/dev/peps/pep-0008/)
- [x] [Docstrings for all functions](https://github.com/numpy/numpy/blob/master/doc/example.py)
- [x] Unit tests",843222
1205,2016-10-26T17:26:17Z,2020-03-02T20:11:22Z,,,"#### Reference Issue

 #7736
#### What does this implement/fix? Explain your changes.

I have replaced `n_iter` by `max_iter` and added the `deprecated` decorators in the methods discussed in #7736.
#### Any other comments?

The tests are not passing yet and I have a hard time to debug without test isolation. How can I isolate the file I want to test?
",843222
1206,2016-10-20T23:19:54Z,2020-03-02T20:11:23Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

<!-- Example: Fixes #1234 -->

Fixes #6437 
#### What does this implement/fix? Explain your changes.

By default, RANSACRegressor uses a sample size based on the number of features in the input, which is appropriate for linear regression (which is used by default) but not for other models. This patch raises a ValueError if a model is used other than linear regression and no sample size is provided.
#### Any other comments?

Based on the discussion in the issue thread, there seemed to be no default option that could possibly be appropriate for an arbitrary model because the appropriate sample size is different for each problem. This raises an error early instead of going ahead with hyperparameters that will probably cause the regression to fail.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1207,2016-10-13T15:56:58Z,2020-03-02T20:11:23Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

<!-- Example: Fixes #1234 -->

Addresses #6189.
#### What does this implement/fix? Explain your changes.

It follows [the Weighted Percentile method](https://en.wikipedia.org/wiki/Percentile#The_Weighted_Percentile_method) to implement interpolation in `_weighted_percentile`.
#### Any other comments?

~~Changing and adding test cases currently.~~

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->

An example that this changes is

```
y_true = [0, 1]
weights = [1, 1]
_weighted_percentile(y_true, weights, 50)
# before: output ==> 0
# after: output ==> 0.5
```
",843222
1208,2016-09-13T04:31:23Z,2020-03-02T20:11:24Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

<!-- Example: Fixes #1234 -->
#6175
#### What does this implement/fix? Explain your changes.

Add dropout for MLP and new gradient test with dropout

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1209,2016-09-10T05:02:48Z,2020-03-02T20:11:25Z,,,"#### Reference Issue

Fixes what @amueller thought was the issue in https://github.com/scikit-learn/scikit-learn/issues/5664,
though that's not really the issue [edited by @amueller so that the issue remains open after this fix is merged]
#### What does this implement/fix? Explain your changes.

The previous code had got around the case where the distance equals zero by setting the gradient to zero after finding where it becomes infinite that raises a `ZeroDivisionWarning`. This is another solution using masked arrays in NumPy. I am unable to reproduce any other warning on master with the latest versions of NumPy and SciPy

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/scikit-learn/scikit-learn/7385)

<!-- Reviewable:end -->
",843222
1210,2016-09-09T21:11:38Z,2020-03-02T20:11:26Z,,,"The following change significantly speeds up the kmeans++ initialization used
in MiniBatchKmeans.

The Euclidean distance computation is the bottleneck in kmeans++. However, on
every call to Euclidean distance there is also a call to check_pairwise_arrays. 
In in kmeans++, the same Y arrays are being checked every time. One of the checks 
in here turns out to cause a speed issue. Specifically the finite check. This patch adds a flag to disable this check.

I'm not sure if this is the desired way to go about this change, but I do think something needs to be 
done about this functions efficiency. 

Here is some data that shows the speed increase:
For these tests I'm clustering with  n_clusters=1000. The feature dimension is 128 and 
the number of data points is 10*n_clusters. I then profiled different versions of the code.
First here is the slow version where I force it to perform the finite check every time.

``` python
Total time: 5.67384 s
File: /home/joncrall/code/scikit-learn/sklearn/metrics/pairwise.py
Function: euclidean_distances at line 173

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   173                                           @ut.profile
   174                                           def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False,
   175                                                                   X_norm_squared=None, force_all_finite=True):
   237      1000      1402395   1402.4     24.7      X, Y = check_pairwise_arrays(X, Y, force_all_finite=True)
   239                                           
   240      1000         1000      1.0      0.0      if X_norm_squared is not None:
   241                                                   XX = check_array(X_norm_squared)
   242                                                   if XX.shape == (1, X.shape[0]):
   243                                                       XX = XX.T
   244                                                   elif XX.shape != (X.shape[0], 1):
   245                                                       raise ValueError(
   246                                                           ""Incompatible dimensions for X and X_norm_squared"")
   247                                               else:
   248      1000        36429     36.4      0.6          XX = row_norms(X, squared=True)[:, np.newaxis]
   249                                           
   250      1000         1251      1.3      0.0      if X is Y:  # shortcut in the common case euclidean_distances(X, X)
   251                                                   YY = XX.T
   252      1000          976      1.0      0.0      elif Y_norm_squared is not None:
   253      1000        15173     15.2      0.3          YY = np.atleast_2d(Y_norm_squared)
   254                                           
   255      1000         2450      2.5      0.0          if YY.shape != (1, Y.shape[0]):
   256                                                       raise ValueError(
   257                                                           ""Incompatible dimensions for Y and Y_norm_squared"")
   258                                               else:
   259                                                   YY = row_norms(Y, squared=True)[np.newaxis, :]
   260                                           
   261      1000      3846672   3846.7     67.8      distances = safe_sparse_dot(X, Y.T, dense_output=True)
   262      1000       108011    108.0      1.9      distances *= -2
   263      1000        64575     64.6      1.1      distances += XX
   264      1000        64196     64.2      1.1      distances += YY
   265      1000       128121    128.1      2.3      np.maximum(distances, 0, out=distances)
   266                                           
   267      1000         1648      1.6      0.0      if X is Y:
   268                                                   # Ensure that distances between vectors and themselves are set to 0.0.
   269                                                   # This may not be the case due to floating point rounding errors.
   270                                                   distances.flat[::distances.shape[0] + 1] = 0.0
   271                                           
   272      1000          941      0.9      0.0      return distances if squared else np.sqrt(distances, out=distances)
```

Digging a little deeper shows the timings int his function in this function.
As you can see most of this functions time is spent in `check_array`.

``` python
Total time: 1.40017 s
File: /home/joncrall/code/scikit-learn/sklearn/metrics/pairwise.py
Function: check_pairwise_arrays at line 58

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    58                                           @ut.profile
    59                                           def check_pairwise_arrays(X, Y, precomputed=False, dtype=None,
    60                                                                     force_all_finite=True):
   104      1000         8652      8.7      0.6      X, Y, dtype_float = _return_float_dtype(X, Y)
   105                                           
   106      1000          659      0.7      0.0      warn_on_dtype = dtype is not None
   107      1000          460      0.5      0.0      estimator = 'check_pairwise_arrays'
   108      1000          507      0.5      0.0      if dtype is None:
   109      1000          524      0.5      0.0          dtype = dtype_float
   110                                           
   111      1000          581      0.6      0.0      if Y is X or Y is None:
   112                                                   X = Y = check_array(X, accept_sparse='csr', dtype=dtype,
   113                                                                       warn_on_dtype=warn_on_dtype, estimator=estimator,
   114                                                                       force_all_finite=force_all_finite)
   115                                               else:
   116      1000          761      0.8      0.1          X = check_array(X, accept_sparse='csr', dtype=dtype,
   117      1000          481      0.5      0.0                          warn_on_dtype=warn_on_dtype, estimator=estimator,
   118      1000       153559    153.6     11.0                          force_all_finite=force_all_finite)
   119      1000          633      0.6      0.0          Y = check_array(Y, accept_sparse='csr', dtype=dtype,
   120      1000          514      0.5      0.0                          warn_on_dtype=warn_on_dtype, estimator=estimator,
   121      1000      1230357   1230.4     87.9                          force_all_finite=force_all_finite)
   122                                           
   123      1000          559      0.6      0.0      if precomputed:
   124                                                   if X.shape[1] != Y.shape[0]:
   125                                                       raise ValueError(""Precomputed metric requires shape ""
   126                                                                        ""(n_queries, n_indexed). Got (%d, %d) ""
   127                                                                        ""for %d indexed."" %
   128                                                                        (X.shape[0], X.shape[1], Y.shape[0]))
   129      1000         1287      1.3      0.1      elif X.shape[1] != Y.shape[1]:
   130                                                   raise ValueError(""Incompatible dimension for X and Y matrices: ""
   131                                                                    ""X.shape[1] == %d while Y.shape[1] == %d"" % (
   132                                                                        X.shape[1], Y.shape[1]))
   133                                           
   134      1000          638      0.6      0.0      return X, Y
```

Looking at `check_array` we see the offending function call to `_assert_all_finite`

``` python
Total time: 1.28438 s
File: /home/joncrall/code/scikit-learn/sklearn/utils/validation.py
Function: check_array at line 271

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   271                                           @ut.profile
   272                                           def check_array(array, accept_sparse=None, dtype=""numeric"", order=None,
   273                                                           copy=False, force_all_finite=True, ensure_2d=True,
   274                                                           allow_nd=False, ensure_min_samples=1, ensure_min_features=1,
   275                                                           warn_on_dtype=False, estimator=None):

   342      2001         2498      1.2      0.2      if isinstance(accept_sparse, str):
   343      2000         2531      1.3      0.2          accept_sparse = [accept_sparse]
   344                                           
   345                                               # store whether originally we wanted numeric dtype
   346      2001         4113      2.1      0.3      dtype_numeric = dtype == ""numeric""
   347                                           
   348      2001         3419      1.7      0.3      dtype_orig = getattr(array, ""dtype"", None)
   349      2001         4128      2.1      0.3      if not hasattr(dtype_orig, 'kind'):
   350                                                   # not a data type (e.g. a column named dtype in a pandas DataFrame)
   351                                                   dtype_orig = None
   352                                           
   353      2001         1811      0.9      0.1      if dtype_numeric:
   354         1            1      1.0      0.0          if dtype_orig is not None and dtype_orig.kind == ""O"":
   355                                                       # if input is object, convert to float.
   356                                                       dtype = np.float64
   357                                                   else:
   358         1            1      1.0      0.0              dtype = None
   359                                           
   360      2001         4210      2.1      0.3      if isinstance(dtype, (list, tuple)):
   361                                                   if dtype_orig is not None and dtype_orig in dtype:
   362                                                       # no dtype conversion required
   363                                                       dtype = None
   364                                                   else:
   365                                                       # dtype conversion required. Let's select the first element of the
   366                                                       # list of accepted types.
   367                                                       dtype = dtype[0]
   368                                           
   369      2001         1944      1.0      0.2      if estimator is not None:
   370      2000         3129      1.6      0.2          if isinstance(estimator, six.string_types):
   371      2000         1908      1.0      0.1              estimator_name = estimator
   372                                                   else:
   373                                                       estimator_name = estimator.__class__.__name__
   374                                               else:
   375         1            1      1.0      0.0          estimator_name = ""Estimator""
   376      2001         3926      2.0      0.3      context = "" by %s"" % estimator_name if estimator is not None else """"
   377                                           
   378      2001         3753      1.9      0.3      if sp.issparse(array):
   379                                                   array = _ensure_sparse_format(array, accept_sparse, dtype, copy,
   380                                                                                 force_all_finite)
   381                                               else:
   382      2001         8476      4.2      0.7          array = np.array(array, dtype=dtype, order=order, copy=copy)
   383                                           
   384      2001         2034      1.0      0.2          if ensure_2d:
   385      2001         2534      1.3      0.2              if array.ndim == 1:
   386                                                           if ensure_min_samples >= 2:
   387                                                               raise ValueError(""%s expects at least 2 samples provided ""
   388                                                                                ""in a 2 dimensional array-like input""
   389                                                                                % estimator_name)
   390                                                           warnings.warn(
   391                                                               ""Passing 1d arrays as data is deprecated in 0.17 and will ""
   392                                                               ""raise ValueError in 0.19. Reshape your data either using ""
   393                                                               ""X.reshape(-1, 1) if your data has a single feature or ""
   394                                                               ""X.reshape(1, -1) if it contains a single sample."",
   395                                                               DeprecationWarning)
   396      2001        16453      8.2      1.3              array = np.atleast_2d(array)
   397                                                       # To ensure that array flags are maintained
   398      2001         3891      1.9      0.3              array = np.array(array, dtype=dtype, order=order, copy=copy)
   399                                           
   400                                                   # make sure we actually converted to numeric:
   401      2001         2012      1.0      0.2          if dtype_numeric and array.dtype.kind == ""O"":
   402                                                       array = array.astype(np.float64)
   403      2001         2282      1.1      0.2          if not allow_nd and array.ndim >= 3:
   404                                                       raise ValueError(""Found array with dim %d. %s expected <= 2.""
   405                                                                        % (array.ndim, estimator_name))
   406      2001         1898      0.9      0.1          if force_all_finite:
   407      2001      1114783    557.1     86.8              _assert_all_finite(array)
   408                                           
   409      2001        61351     30.7      4.8      shape_repr = _shape_repr(array.shape)
   410      2001         2295      1.1      0.2      if ensure_min_samples > 0:
   411      2001        16251      8.1      1.3          n_samples = _num_samples(array)
   412      2001         2250      1.1      0.2          if n_samples < ensure_min_samples:
   413                                                       raise ValueError(""Found array with %d sample(s) (shape=%s) while a""
   414                                                                        "" minimum of %d is required%s.""
   415                                                                        % (n_samples, shape_repr, ensure_min_samples,
   416                                                                           context))
   417                                           
   418      2001         2557      1.3      0.2      if ensure_min_features > 0 and array.ndim == 2:
   419      2001         2229      1.1      0.2          n_features = array.shape[1]
   420      2001         1982      1.0      0.2          if n_features < ensure_min_features:
   421                                                       raise ValueError(""Found array with %d feature(s) (shape=%s) while""
   422                                                                        "" a minimum of %d is required%s.""
   423                                                                        % (n_features, shape_repr, ensure_min_features,
   424                                                                           context))
   425                                           
   426      2001         1958      1.0      0.2      if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:
   427                                                   msg = (""Data with input dtype %s was converted to %s%s.""
   428                                                          % (dtype_orig, array.dtype, context))
   429                                                   warnings.warn(msg, _DataConversionWarning)
   430      2001         1776      0.9      0.1      return array
```

Disabling this check after it runs the first time gives a better profile.

We could probably even scrape a bit more performance by checking everything at
the start of kmeans++ and then disabling all subsequent checks. This should be
ok because the new centroids are always just previously existing ones. 

```
Total time: 3.82659 s
File: /home/joncrall/code/scikit-learn/sklearn/metrics/pairwise.py
Function: euclidean_distances at line 173

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   173                                           @ut.profile
   174                                           def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False,
   175                                                                   X_norm_squared=None, force_all_finite=True):
   238      1000       288126    288.1      7.5      X, Y = check_pairwise_arrays(X, Y, force_all_finite=force_all_finite)
   239                                           
   240      1000          718      0.7      0.0      if X_norm_squared is not None:
   241                                                   XX = check_array(X_norm_squared)
   242                                                   if XX.shape == (1, X.shape[0]):
   243                                                       XX = XX.T
   244                                                   elif XX.shape != (X.shape[0], 1):
   245                                                       raise ValueError(
   246                                                           ""Incompatible dimensions for X and X_norm_squared"")
   247                                               else:
   248      1000        21165     21.2      0.6          XX = row_norms(X, squared=True)[:, np.newaxis]
   249                                           
   250      1000          927      0.9      0.0      if X is Y:  # shortcut in the common case euclidean_distances(X, X)
   251                                                   YY = XX.T
   252      1000          699      0.7      0.0      elif Y_norm_squared is not None:
   253      1000         8768      8.8      0.2          YY = np.atleast_2d(Y_norm_squared)
   254                                           
   255      1000         1956      2.0      0.1          if YY.shape != (1, Y.shape[0]):
   256                                                       raise ValueError(
   257                                                           ""Incompatible dimensions for Y and Y_norm_squared"")
   258                                               else:
   259                                                   YY = row_norms(Y, squared=True)[np.newaxis, :]
   260                                           
   261      1000      3174245   3174.2     83.0      distances = safe_sparse_dot(X, Y.T, dense_output=True)
   262      1000        93573     93.6      2.4      distances *= -2
   263      1000        69584     69.6      1.8      distances += XX
   264      1000        72021     72.0      1.9      distances += YY
   265      1000        92431     92.4      2.4      np.maximum(distances, 0, out=distances)
   266                                           
   267      1000         1575      1.6      0.0      if X is Y:
   268                                                   # Ensure that distances between vectors and themselves are set to 0.0.
   269                                                   # This may not be the case due to floating point rounding errors.
   270                                                   distances.flat[::distances.shape[0] + 1] = 0.0
   271                                           
   272      1000          804      0.8      0.0      return distances if squared else np.sqrt(distances, out=distances)
```

Disabling the profiler and using a coarser function timer we get the following timings:

Without Checks: 3.9655s
With Checks: 4.9669s

This is a 20% decrease in the amount of time taken (1 second total).

To ensure that this speedup was not just for parameters resembling my problem I
did a gridsearch on various parameter values and looked at the percent change.
For larget datasets the change is consistently positive. There are a few
negative changes for small datasets, but this is likely because of random
fluctuations. For datasets with at least a .1 second speed increase, there is a
15% average improvement with the improvement increasing for larger datasets.

```
    n_clusters  n_features  per_cluster  new_speed  old_speed  percent_change  absolute_change
0         2000         512          200  45.520926  56.996406        0.201337        11.475480
1         2000         512          100  46.009816  57.047427        0.193481        11.037611
2         2000         512           10  46.023527  56.965505        0.192081        10.941978
3         2000         512            1  46.036122  57.028242        0.192749        10.992120
4         2000         128          200  13.359815  16.861238        0.207661         3.501423
5         2000         128          100  13.594423  16.169258        0.159243         2.574835
6         2000         128           10  13.041987  16.357325        0.202682         3.315338
7         2000         128            1  13.517229  15.883567        0.148980         2.366338
8         2000          32          200   4.691806   5.384530        0.128651         0.692724
9         2000          32          100   4.785984   5.434597        0.119349         0.648613
10        2000          32           10   4.816490   5.309634        0.092877         0.493144
11        2000          32            1   4.678081   5.419021        0.136729         0.740940
12        2000           4          200   2.194186   2.311234        0.050643         0.117048
13        2000           4          100   2.195776   2.310168        0.049517         0.114392
14        2000           4           10   2.197284   2.313143        0.050087         0.115859
15        2000           4            1   2.245128   2.298406        0.023181         0.053278
16        1000         512          200  10.325005  13.263920        0.221572         2.938915
17        1000         512          100  10.372535  13.245843        0.216922         2.873308
18        1000         512           10  10.340922  13.195515        0.216331         2.854593
19        1000         512            1  10.377185  13.245806        0.216568         2.868621
20        1000         128          200   2.862865   3.559318        0.195670         0.696453
21        1000         128          100   2.814686   3.682527        0.235664         0.867841
22        1000         128           10   2.866434   3.692132        0.223637         0.825698
23        1000         128            1   3.000796   3.592812        0.164778         0.592016
24        1000          32          200   1.063762   1.172979        0.093111         0.109217
25        1000          32          100   1.053794   1.172330        0.101111         0.118536
26        1000          32           10   1.042551   1.164388        0.104636         0.121837
27        1000          32            1   1.045254   1.165895        0.103475         0.120641
28        1000           4          200   0.576731   0.612182        0.057909         0.035451
29        1000           4          100   0.576767   0.609157        0.053172         0.032390
30        1000           4           10   0.598036   0.607962        0.016327         0.009926
31        1000           4            1   0.578046   0.610441        0.053068         0.032395
32         100         512          200   0.124780   0.144742        0.137913         0.019962
33         100         512          100   0.122643   0.144813        0.153094         0.022170
34         100         512           10   0.123119   0.142057        0.133312         0.018938
35         100         512            1   0.122787   0.144781        0.151913         0.021994
36         100         128          200   0.049460   0.058768        0.158387         0.009308
37         100         128          100   0.060528   0.072691        0.167326         0.012163
38         100         128           10   0.049410   0.061536        0.197052         0.012126
39         100         128            1   0.052959   0.061433        0.137941         0.008474
40         100          32          200   0.029961   0.035051        0.145223         0.005090
41         100          32          100   0.028987   0.033165        0.125970         0.004178
42         100          32           10   0.030262   0.039366        0.231266         0.009104
43         100          32            1   0.028978   0.033200        0.127173         0.004222
44         100           4          200   0.022328   0.027136        0.177188         0.004808
45         100           4          100   0.015451   0.017483        0.116230         0.002032
46         100           4           10   0.014638   0.016714        0.124215         0.002076
47         100           4            1   0.014893   0.018615        0.199944         0.003722
48          10         512          200   0.003733   0.002868       -0.301604        -0.000865
49          10         512          100   0.002347   0.002843        0.174507         0.000496
50          10         512           10   0.002344   0.002907        0.193718         0.000563
51          10         512            1   0.002358   0.006877        0.657086         0.004519
52          10         128          200   0.001717   0.002016        0.148196         0.000299
53          10         128          100   0.007036   0.002073       -2.394020        -0.004963
54          10         128           10   0.001862   0.002053        0.093021         0.000191
55          10         128            1   0.001747   0.002061        0.152244         0.000314
56          10          32          200   0.001596   0.001831        0.128385         0.000235
57          10          32          100   0.001543   0.001787        0.136606         0.000244
58          10          32           10   0.001550   0.001786        0.132159         0.000236
59          10          32            1   0.001551   0.001857        0.164719         0.000306
60          10           4          200   0.001525   0.001715        0.110663         0.000190
61          10           4          100   0.007363   0.001704       -3.320974        -0.005659
62          10           4           10   0.001505   0.001696        0.112470         0.000191
63          10           4            1   0.001494   0.009352        0.840255         0.007858
```

The script I used to generate these numbers is:

``` python
""""""
python speedup_kmeans.py --profile
python speedup_kmeans.py

For 1000 clusters and 10000 datapoints:
    5 seconds with checks
    3 seconds without checks
    3 seconds without checks and with out safe_dot
""""""
from __future__ import absolute_import, division, print_function, unicode_literals
import utool as ut
import sklearn  # NOQA
from sklearn.datasets.samples_generator import make_blobs
from sklearn.utils.extmath import row_norms, squared_norm  # NOQA
import sklearn.cluster
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances  # NOQA
(print, rrr, profile) = ut.inject2(__name__, '[tester]')


#@profile
def test_kmeans_plus_plus_speed(n_clusters=1000, n_features=128, per_cluster=10, fix=True):
    # Make random cluster centers on a ball
    rng = np.random.RandomState(42)
    centers = rng.rand(n_clusters, n_features)
    centers /= np.linalg.norm(centers, axis=0)[None, :]
    (centers * 512).astype(np.int) / 512
    centers /= np.linalg.norm(centers, axis=0)[None, :]

    n_samples = n_clusters * 10
    n_clusters, n_features = centers.shape
    X, true_labels = make_blobs(n_samples=n_samples, centers=centers,
                                cluster_std=1., random_state=42)

    x_squared_norms = row_norms(X, squared=True)

    _k_init = sklearn.cluster.k_means_._k_init
    random_state = rng
    n_local_trials = None  # NOQA

    #print('Testing kmeans init')
    with ut.Timer('testing kmeans init') as t:
        centers = _k_init(X, n_clusters, random_state=random_state, x_squared_norms=x_squared_norms, fix=fix)
    #print('Done testing kmeans init')
    return t.ellapsed


def main():
    basis = {
        'n_clusters': [10, 100, 1000, 2000][::-1],
        'n_features': [4, 32, 128, 512][::-1],
        'per_cluster': [1, 10, 100, 200][::-1],
    }
    vals = []
    for kw in ut.ProgIter(ut.all_dict_combinations(basis), lbl='gridsearch',
                          bs=False, adjust=False, freq=1):
        print(kw)
        new_speed = test_kmeans_plus_plus_speed(fix=True, **kw)
        old_speed = test_kmeans_plus_plus_speed(fix=False, **kw)
        kw['new_speed'] = new_speed
        kw['old_speed'] = old_speed
        vals.append(kw)

    import pandas as pd
    pd.options.display.max_rows = 64
    pd.options.display.width = 100
    pd.options.display.max_ = 64
    df = pd.DataFrame.from_dict(vals)
    df['percent_change'] = (df['old_speed'] - df['new_speed']) / df['old_speed']
    df = df.reindex_axis(['n_clusters', 'n_features', 'per_cluster', 'new_speed', 'old_speed', 'percent_change'], axis=1)
    df['absolute_change'] = (df['old_speed'] - df['new_speed'])
    print(df)

    print(df['percent_change'][df['absolute_change'] > .1].mean())
    #print(df.loc[df['percent_change'].argsort()[::-1]])

    #try:
    #    profile.dump_stats('out.lprof')
    #    profile.print_stats(stripzeros=True)
    #except Exception:
    #    pass

#if __name__ == '__main__':
#    main()
```

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/scikit-learn/scikit-learn/7383)

<!-- Reviewable:end -->
",843222
1211,2016-09-03T17:02:12Z,2020-03-02T20:11:26Z,,,"… and GradientBoosting

This fixes an error in feature importances computation of forests (i.e. RandomForest, ExtraTrees and GradientBoosting). As it is now, the feature importances are computed as the mean over the trees of the _normalized_ feature importances of each tree.

This is wrong especially for GradientBoosting as it means that the feature importances of the lastest trees will be weighted the same as the ones of the first trees although the lastest splits contribute much less to the global reduction of impurity. This is why the normalization must be done at the end.
(although it is not exactly the same computation, this is what is done in J. Friedman, “Greedy Function Approximation: A Gradient Boosting Machine”, The Annals of Statistics, Vol. 29, No. 5, 2001. in equation 44 and 45)

Thus, I have added a method to `BaseDecisionTree` called `_compute_feature_importances` that does the same as the attribute `feature_importances_` except that it accepts a facultative `normalize` argument (`feature_importances_` has been kept). `BaseForest` and `BaseGradientBoosting` now use that method to compute the non-normalized feature importances of each tree, sum them and normalize them before returning them.
",843222
1212,2016-09-02T15:23:21Z,2020-03-02T20:11:27Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

<!-- Example: Fixes #1234 -->

Fixes  #7308
#### What does this implement/fix? Explain your changes.

Adds support for passing sample weights into `RFE.fit()` and having them used by the estimator's `fit` method.

Adds a test for this with the iris dataset, where sample weights are used to give one class double its normal weight and compare that to doubling the samples in that class. The test passes if both these approaches produce the same feature ranking. This feature ranking is different from the one which arises when all samples have the same weight.
#### Any other comments?

I will look in to adding the same support for `RFECV`, and to `RFE.score()`.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1213,2016-08-29T13:47:30Z,2019-08-05T17:28:34Z,,,"This PR adds the doc concerning the derivation formula of the `BayesianGaussianMixture` class.

I've done a separate PR because this is just formula and I thought it wil be easier to review if it was extracted on another PR.

Of course, this can't be merge before #6651.
",843222
1214,2016-08-27T21:54:24Z,2020-03-02T20:11:28Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

Fixes #7139 
#### What does this implement/fix? Explain your changes.

Fit two instances of the same estimator on a toy problem. Use both to predict on an unseen subset of data and compare predictions. If an estimator has a `random_state` argument provide it, if not then not.
#### Any other comments?

There are a few estimators that I am skipping at the moment because they fail the test. Most blatantly are not deterministic but two or so have a different error.

Unsure about the location, should this be a check in `utils/estimator_checks.py` instead?
- [x] move to `check_deterministic` in`utils/estimator_checks.py`
- [ ] transformers
- [ ] unsupervised algorithms
- [ ] why do `HuberRegressor`, `LogisticRegressionCV`, `LinearRegression`, `RANSACRegressor` fail?
- [ ] why does `RadiusNeighborsClassifier` fail? Failure mode is different to the above
",843222
1215,2016-08-27T15:59:19Z,2020-03-02T20:11:29Z,,,"#### Reference Issue

https://github.com/scikit-learn/scikit-learn/issues/6656
#### What does this implement/fix?

Support for monotonically increasing or decreasing features in decision trees, random forests and gradient boosted trees.
#### Any other comments?
#### Tasks
- [x] DecisionTreeRegressor
- [x] DecisionTreeClassifier
- [x] ExtraTreeClassifier
- [x] ExtraTreeRegressor
- [x] RandomForestClassifier
- [x] RandomForestRegressor
- [x] ExtraTreesClassifier
- [x] ExtraTreesRegressor
- [x] RandomTreesEmbedding
- [x] GradientBoostingClassifier
- [x] GradientBoostingRegressor
- [ ] Test runtime when no monotonicity specified and consider a separate loop
- [x] Refactor monotone check into a function
- [ ] Finalize public API and argument error handling
- [ ] Decide if `splitter=""random""`should honour monotonic constraints and verify behaviour
- [x] Check if the divisor `criterion.weighted_n_<left/right>` can ever be 0.0 and handle it. Answer: it's OK.
- [ ] Finalize docstrings 
- [ ] Add tests
- [ ] Satisfy coding guidelines
",843222
1216,2016-08-26T08:33:35Z,2020-03-02T20:11:29Z,,,"fix #7233 
",843222
1217,2016-08-11T11:52:01Z,2020-03-02T20:11:30Z,,,"For non-thresholded classification metrics, this adds tests for:
- column vectors as lists of lists
- multiclass input

and ensures their support. This effectively fixes #7081.

Column vectors are analogous to multioutput classification targets with a single target.

We did not previously support nested lists (unlike arrays) as a target format because these were confusable with the now-unsupported-for-some-time list of lists multilabel format. I'm not _entirely_ sure they should be supported now, but it seems consistent with our support of ""array-likes"" elsewhere.
",843222
1218,2016-08-07T01:45:35Z,2019-08-16T19:33:20Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

<!-- Example: Fixes #1234 -->

This PR is a response to Scikit-learn issue #7142 which is a documentation addition for ""rolling your own estimator""
#### What does this implement/fix? Explain your changes.

This PR adds two additional paragraphs detailing conventions for additional parameters to pass to the fit, predict, and transform functions of the custom estimator. 
#### Any other comments?

If you'd like me to change the wording or add/remove sections, please feel free to reach out. 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->

… passing rules

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/scikit-learn/scikit-learn/7156)

<!-- Reviewable:end -->
",843222
1219,2016-08-05T20:47:00Z,2020-03-02T20:11:31Z,,,"Ball Tree and KD Tree currently only support `np.float64`, which is a waste of memory as same quality results should be obtainable with `np.float32`.

**Note**: `self.new_data` and `self.new_data_arr` are just for development usage, I try to first use them to repeat what `self.data` and `self.data_arr` do. With this manner, I can still build scikit-learn successfully and I'll change `self.data` and `self.data_arr` in the end.
",843222
1220,2016-07-28T19:53:58Z,2020-03-02T20:11:31Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

<!-- Example: Fixes #1234 -->
#### What does this implement/fix? Explain your changes.
#### Any other comments?

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->

When max_features are specified, CountVectorizer will first count the word occurrence. It will take the the top occurring words up to max_features.  It will only make a 2-gram when its 1 gram count is apart of max_features, make a 3-gram when its 2-gram is apart of the max features and so on. This faster and more memory efficient on n-grams when the data sets are large.
",843222
1221,2016-07-25T18:45:30Z,2020-03-02T20:11:32Z,,,"#### Reference Issue
#6887
#### What does this implement/fix? Explain your changes.

Adds support for a callable computing covariance matrix.
#### Any other comments?
",843222
1222,2016-07-24T13:15:38Z,2020-03-02T20:11:33Z,,,"#### Reference Issue

issue #6868.
#### What does this implement/fix? Explain your changes.

Implements new kernel which is only applied on specific dimensions of input. 
#### Any other comments?
",843222
1223,2016-07-15T07:55:48Z,2020-03-02T20:11:34Z,,,"This pull request proposes 3 changes:
1. In the same way as the [original Tenenbaum's implementation and paper of isomap](http://isomap.stanford.edu/), I added the option to choose the type of neighborhood graph (kneighbor or radius), which can be really useful.
2. Removing the redundant use of NearestNeighbors object and function kneighbors_graph, the latter already existing as a method in NearestNeighbors
3. Structuring+commenting the code to easily get the main steps of isomap
",843222
1224,2016-06-29T13:26:36Z,2020-03-02T20:11:34Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

<!-- Example: Fixes #1234 -->

I propose to integrate the algorithms which compute automatically the `n_clusters` values proposed in #4301 by @afouchet.

This PR is the the second step of the work initiated by #6823.
#### What does this implement/fix? Explain your changes.

I propose several algorithms that analyses the best value for 'n_clusters' :
- max value for some unsupervised metrics (silhouette, calinski and harabraz)
- stability criterium (using folwkes and mallows metric) 
- max value of the distortion jump
- gap criterium
- pham criterium
#### Any other comments?

TODO:
- [X] Use parallel to accelerate the computation
- [X] Not range parameters for `n_clusters`
- [X] Using ParameterGrid class allowing to see the evolution of other parameters
- [X] Adapted to have a similar use than GridSearch
- [x] The results can be analysed by pandas
- [ ] docstring
- [ ] docs
- [ ] tests
- [ ] examples

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1225,2016-06-23T06:21:48Z,2020-03-02T20:11:35Z,,,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests
-->
#### Reference Issue

<!-- Example: Fixes #1234 -->

Fixes #6929
#### What does this implement/fix? Explain your changes.

Replaces 'print' with 'logging.info' so the library will no longer print to stdout.
#### Any other comments?

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",843222
1226,2016-06-22T21:23:12Z,2020-03-02T20:11:36Z,,,"#### What does this implement/fix?

It adds graphical examples directly to some of the data generators
#### Comments

Currently, `make` does not work. But it seems not to be related to my changes:

```
Doctest: unsupervised_learning.rst ... ok
Doctest: working_with_text_data.rst ... /home/moose/GitHub/scikit-learn/doc/tutorial/text_analytics/working_with_text_data.rst:1: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future
  .. _text_data_tutorial:
FAIL

======================================================================
FAIL: Doctest: working_with_text_data.rst
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python2.7/doctest.py"", line 2226, in runTest
    raise self.failureException(self.format_failure(new.getvalue()))
AssertionError: Failed doctest test for working_with_text_data.rst
  File ""/home/moose/GitHub/scikit-learn/doc/tutorial/text_analytics/working_with_text_data.rst"", line 0

----------------------------------------------------------------------
File ""/home/moose/GitHub/scikit-learn/doc/tutorial/text_analytics/working_with_text_data.rst"", line 99, in working_with_text_data.rst
Failed example:
    twenty_train = fetch_20newsgroups(subset='train',
        categories=categories, shuffle=True, random_state=42)
Expected nothing
Got:
    Downloading 20news dataset. This may take a few minutes.
----------------------------------------------------------------------
File ""/home/moose/GitHub/scikit-learn/doc/tutorial/text_analytics/working_with_text_data.rst"", line 452, in working_with_text_data.rst
Failed example:
    gs_clf.best_score_
Expected:
    0.900...
Got:
    0.90000000000000002

>>  raise self.failureException(self.format_failure(<StringIO.StringIO instance at 0x7fb149ea1d88>.getvalue()))

-------------------- >> begin captured logging << --------------------
sklearn.datasets.twenty_newsgroups: WARNING: Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)
sklearn.datasets.twenty_newsgroups: INFO: Decompressing /home/moose/scikit_learn_data/20news_home/20news-bydate.tar.gz
--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
Ran 38 tests in 43.950s

FAILED (SKIP=3, failures=1)
Makefile:40: recipe for target 'test-doc' failed

```

Also, the printed plots open during the test. I'm not sure if this might be a problem.
",843222
1227,2016-06-14T14:07:50Z,2020-03-02T20:11:36Z,,,"Current implementation of `SGDClassifier` and `SGDRegressor` doesn't allow the user to specify the `dtype` they want and may cause a `MemoryError` when the user wants to train the model since it will try to copy input data into `np.float64`. (See #5776).

To address the problem, this PR wants to make SGD related algorithms in scikit-learn supports Cython fused types.
",843222
1228,2016-05-27T14:57:19Z,2020-03-02T20:11:37Z,,,"Pull request linked to the issue #6828 
",843222
1229,2016-05-27T03:21:28Z,2020-03-02T20:11:38Z,,,"Addresses #6809.

If we move forward with this PR, I'll be adding the appropriate for-loop to make sure none of the metrics have `nan` as input.

Off the top of my head, I can't think of a metric which would accept `nan` as input. Please let me know if you know of the exceptions.
",843222
1230,2016-05-27T02:53:41Z,2020-03-02T20:11:38Z,,,"# Roadmap
- [x] Added documentation for `sklearn.metrics.get_scorer`
- [ ] Input validation check
  - [x] `scoring` parameter should be a callable
  - [ ] `scoring` should only receive 3 parameters
  - [ ] Add test cases for input validation
## Original Post

Added documentation for `sklearn.metrics.get_scorer`, which (until now) seemed blank. (Link [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.get_scorer.html#sklearn.metrics.get_scorer).)

To be honest, I can't seem to find a user case where this function would be useful. For example, if we were to use `cross_val_score`, we can state `cross_val_score(..., scoring=""accuracy"")` instead of `cross_val_score(..., scoring=get_scorer(""accuracy"")` to evaluate using the accuracy score. On the other hand, if a user generated his or her own evaluation metric, it would make more sense to wrap it with `make_scorer`. 

@GaelVaroquaux, do you have any ideas? (My idea is to deprecate the function altogether.)

If we choose to move forward with documenting this function, we probably want to make sure that if `scoring` is not a string, that it is a callable that takes in three arguments.

Motivation for PR: #6697.
",843222
1231,2016-05-20T23:30:43Z,2020-03-02T20:11:39Z,,,"Addressed #5851 
This is an implementation of the Robust PCA algorithm as described in 
http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf

Thanks to @amueller for his patience and helping me out with this. I would appreciate any suggestions on a possible example. Can we write a function to download the escalator dataset and run the algorithm on that as shown in the paper ?

Also, I am not sure if my implementation of `transform` and `inverse_transform` is correct. I am doing something similar to what `PCA` is doing with the exception of adjusting for mean.  @amueller Am I missing something ? Ideally Line 26 in `test_rpca.py` should not fail, isn't it ?
",843222
1232,2016-05-02T15:52:54Z,2020-03-02T20:11:41Z,,,"#### Reference Issue

Implements #6630 

This is by no means optimal so it would be great if somebody could have a look and discuss, if interested.

NOTE: I've also fixed some pep8 warnings (lines too long) in the same file but unrelated to the change.
#### What does this implement/fix? Explain your changes.

Added a parameter to LinearModelCV and subclasses that allows one to select a different alpha selection strategy than ""take the minimum"". I implement a ""upper"", pretty much equal to glmnet.1se (ie, take the greatest alpha within 1 standard error of the optimal alpha) as well as a ""lower"", on the other direction.

This allows a bit more control on how many variables will be selected.

Also, this is actually more complete than glmnet's in another regard: it nicely fits into the l1_ratio optimisation loop, not only the alpha.

Before, we selected the ""globally minimum"" alpha across l1_ratios. Now for each l1_ratio, we select either the minimum alpha or one of the other two ""tails"". Then we still minimise by MSE across the l1_ratios.

It can be done in various ways of course; this seemed the least disruptive to me. I couldn't find a ""consensus approach"" in the literature, so it's just a personal take.
#### Any other comments?

I ran a ""make"" and made sure the tests passed. I haven't written specific unit tests for this.
",843222
1233,2016-04-14T17:07:22Z,2020-03-02T20:11:42Z,,,"Together these two classes can be used in a pipeline
to change the classification threshold from the default
of 0.5 to any value

This is a start on #4813

Is `pipeline.py` the right home for this?
- [ ] documentation
- [ ] example
- [ ] tests
- [ ] checks and robustness in the estimators

edit: from [comment](https://github.com/scikit-learn/scikit-learn/pull/6663#issuecomment-252781270) below:
> Err also is `PredictionTransformer` the same as `VotingClassifier` by any chance? ;)

Yes it is, should ditch `PredictionTransformer` from this PR.

---
/cc @joshlk
",843222
1234,2016-04-08T04:28:52Z,2020-03-02T20:11:42Z,,,"Parallelized the function ""_update_cdnmf_fast"" by using prange/openmp, so that the process of nmf will be faster. The environment variable OMP_NUM_THREADS can be used to the max thread used.
",843222
1235,2016-04-05T14:09:24Z,2020-03-02T20:11:43Z,,,"Ever since #2068 was merged, the `Ridge` class can take multiple alpha values: namely one for each target. This PR updates `RidgeCV` to match. It adds an `alpha_per_target` flag which instructs `RidgeCV` to estimate separate alpha values for each target.

This can be done nearly for free when using GCV (by passing `cv=None`). **edit:** When using a custom CV object (so not using GCV), this would be slow as molasses since it needs to grid search over a (#alphas x #targets) space. Therefore, `alpha_per_target = True` is only supported when using GCV and throws an error otherwise.
",843222
1236,2016-03-30T15:19:30Z,2020-03-02T20:11:43Z,,,"I am pretty sure I saw someone on scikit-learn github considering adding MAPE as loss function and I wanted to have it anyway, so there it is (although I can't google it back)

If that is useful, I should probably add some tests and/or documentation. Let me know!
",843222
1237,2016-03-30T15:02:17Z,2020-03-02T20:11:44Z,,,"The following enhancements have been made:
- `X` accepts sparse matrixes
- `X` can now directly represent an adjacency matrix. This is useful when X represents a graph and therefore no kernel is required (`kernel=None`)
- Currently if a sample has been predicted with 0 or NaN probability the argmax will select the first column and therefore classify the sample with the class with an index of 0. e.g. `argmax([0,0,0]) = 0` This is obviously undesirable. Therefore now such samples will remain unclassified with a class of -1
- A `transduction_prob_` attribute has been added after fitting which contains the probability of label assigned to each sample via the transduction

Note:
- I don't know how to implement `predict_proba` and `predict` when `kernel=None`. Currently a `NotImplementedError` is raised. Any suggestions on how to do it is much welcomed
",843222
1238,2016-03-27T17:47:27Z,2020-03-02T20:11:45Z,,,"this PR addresses the issue #4757

tests attached (fixed) 
",843222
1239,2016-03-17T12:04:48Z,2020-03-02T20:11:45Z,,,"As described in #6553 it would be great to have such ability. This pull request adds such ability + some tests for export graphviz.
",843222
1240,2016-03-14T14:13:41Z,2020-03-02T20:11:46Z,,,"This PR is an implementation of ensemble selection which is discussed in #6329 , 
however, the API of this module is not yet determined.

**Check List:**
- [x] Implement main algorithm
- [x] Add example code
- [ ] Determine API
- [ ] Test
- [ ] Detail documentation

**Reference:**
[Ensemble Selection from Libraries of Models](http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf)  R. Caruana et.al. ICML 2004
",843222
1241,2016-03-13T22:34:03Z,2020-03-02T20:11:47Z,,,"The commit implements Information Gain [1] and Information Gain Ratio functions used for feature selection. The functions are commonly used in the filtering approach to feature selection in tasks such as text classification ([2] and [3]). IG is implemented in WEKA package.

The input parameters and output values as well as tests of the functions follow the example for the chi-square function.

The coverage of sklearn.feature_selection.univariate_selection is 98%.

PEP8 and PyFlakes pass.

```
References:
-----------
.. [1] J.R. Quinlan. 1993. C4.5: Programs for Machine Learning. San Mateo,
CA: Morgan Kaufmann.

.. [2] Y. Yang and J.O. Pedersen. 1997. A comparative study on feature
selection in text categorization. Proceedings of ICML'97, pp. 412-420.
http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.9956

.. [3] F. Sebastiani. 2002. Machine Learning in Automatic Text
Categorization. ACM Computing Surveys (CSUR).
http://nmis.isti.cnr.it/sebastiani/Publications/ACMCS02.pdf
```
",843222
1242,2016-03-04T19:18:46Z,2020-03-02T20:11:47Z,,,,843222
1243,2016-02-27T07:27:18Z,2020-03-02T20:11:48Z,,,"Add the RMSE([Root Mean Squared Error](https://www.kaggle.com/wiki/RootMeanSquaredError)) option to the [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html).

Many Kaggle competitions are selecting RMSE as their official evaluation score. ([Home Depot Product Search Relevance](https://www.kaggle.com/c/home-depot-product-search-relevance/details/evaluation), [Restaurant Revenue Prediction](https://www.kaggle.com/c/restaurant-revenue-prediction/details/evaluation), [Facial Keypoints Detection](https://www.kaggle.com/c/facial-keypoints-detection/details/evaluation), etc)

Usually, Kagglers directly implement this use of [mean_squared_error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html). However, I think this is a waste of time, so I decided to implement RMSE and thereby add it to the cross_val_score function.

Hope this helps Kagglers :D
",843222
1244,2016-02-25T03:32:46Z,2020-03-02T20:11:49Z,,,"This branch subsumes the changes made to support larger feature sparse arrays. I don't know of a clean, simple way of separating the two without creating a new fork.
I suspect that if this change is of interest, the maintainers may want to change how the functionality is interfaced in the API, which may require a new branch anyway.
",843222
1245,2016-02-24T14:50:16Z,2020-03-02T20:11:50Z,,,"Attempt to add get_feature_names to PCA as suggested in #6425 .
The present idea is to take the input feature with the most variance along the component. But it doesn't consider the importance of the component. It definitely seems that the present approach can be modified to accommodate the significance of the components.
Please let me know about the best way to move forward here. Thanks!
",843222
1246,2016-02-23T14:34:46Z,2020-03-02T20:11:50Z,,,"This is a PR for #6425 .
I've added `get_feature_names` for scalers, normalizers and imputers.

Is the purpose of this function to maintain compatibility when `get_feature_names()` is implemented in Pipeline?

Can @jnothman please have a look?
",843222
1247,2016-02-17T17:42:55Z,2020-03-02T20:11:51Z,,,"Allows the threshold for when to generate a new atom for the dictionary to be set externally. Still needs tests.
",843222
1248,2016-02-16T06:44:43Z,2020-03-02T20:11:52Z,,,"…rizer in sklearn.feature_extraction. See https://github.com/scikit-learn/scikit-learn/issues/6368
",843222
1249,2016-02-12T10:10:57Z,2020-03-02T20:11:52Z,,,"I tried to run the LDA model on a fairly big dataset, on multiple processors. Sadly, this doesn't work very well at all -- the processes require a lot of memory, and especially, their interprocess communication consumes all of the space on `/run/shm`, which is the default file-based interchange for joblib. This lets the user specify where to store these temporary files, mitigating at least one of the issues.
",843222
1250,2016-02-12T00:27:29Z,2020-03-02T20:11:53Z,,,"It's been a while when @amueller asked me about moving this utility function ""upstream,"" but I finally got around it now (https://github.com/rasbt/mlxtend/issues/8) ...

Although such a function probably less useful in real-world applications (since we typically have more than 1 or 2 features in our dataset), but I think it would be a nice utility for replacing the many repeated lines of codes in the tutorials and examples on the scikit-learn website.

So, this is a simple matplotlib-wrapping convenience function to plot a decision surface of a classifier; it looks like this:

``` python
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# Loading some example data
iris = datasets.load_iris()
X = iris.data[:, [0,2]]
y = iris.target

# Training a classifier
svm = SVC(C=0.5, kernel='linear')
svm.fit(X,y)

# Plotting decision regions
plot_decision_regions(X, y, clf=svm, res=0.02, legend=2)

# Adding axes annotations
plt.xlabel('sepal length [cm]')
plt.ylabel('petal length [cm]')
plt.title('SVM on Iris')
plt.show()
```

![plot_decision_regions_9_0](https://cloud.githubusercontent.com/assets/5618407/12995332/9d62876c-d0f4-11e5-9a7e-73f27f275a79.png)

It also supports 1D decision regions (if the input array has only one feature column) and you can highlight the test datapoint, which can be quite useful for teaching (tutorial) purposes.
![plot_decision_regions_11_0](https://cloud.githubusercontent.com/assets/5618407/12995381/e6c34f0e-d0f4-11e5-9403-db80f6176482.png)
![plot_decision_regions_14_0](https://cloud.githubusercontent.com/assets/5618407/12995384/ec6d6e4e-d0f4-11e5-99f4-d212228bede7.png)
![plot_decision_regions_16_0](https://cloud.githubusercontent.com/assets/5618407/12995390/f5260c76-d0f4-11e5-92d2-4f50a6d7d9a4.png)

<br><br>

<hr>

I am looking forward to feedback! Also, I am wondering how (or if) we implement tests for such a function?
",843222
1251,2016-02-07T17:43:08Z,2020-03-02T20:11:54Z,,,"See #4752 .

There is a problem right now with the reordering of the children (the large commented out block). This is necessary because without a priority queue, we don't remove clusters that are the absolute lowest distance from each other. Reordering the children is necessary if the unstructured call must match the structured one. It takes a very large amount of time (even longer than the clustering loop), so I am working on making it faster.

@GaelVaroquaux Here are some results on the nilearn dataset:

master (cache cleared)

Ward agglomeration 1000 clusters: 14.12s
Ward agglomeration 2000 clusters: 7.65s

ward-nn-chaining (cache not cleared from master run):

Ward agglomeration 1000 clusters: 36.22s
Ward agglomeration 2000 clusters: 32.11s

ward-nn-chaining (cache now cleared):

Ward agglomeration 1000 clusters: 23.23s
Ward agglomeration 2000 clusters: 24.21s

I'm hoping the low speedup from caching is due to the fact that the children are not ordered, but I don't know enough about Memory.cache() right now to investigate this further.
",843222
1252,2016-01-26T14:02:34Z,2019-08-05T19:59:36Z,,,"this example shows us is the behavior “rich getting richer” of agglomerative clustering that tends to create uneven cluster sizes. Graphs help to compare us where the behavior is more pronounced between average, ward and complete linkage strategy. Using random data helps to strengthen this notion.
",843222
1253,2016-01-26T13:10:24Z,2020-03-02T20:11:54Z,,,"In sklearn.cluster.hierarchical.py clustering based on cophenetic
distance was added to Agglomerative Clustering as defined in issue #6197
",843222
1254,2016-01-23T16:51:55Z,2020-03-02T20:11:55Z,,,"Enable support for `sample_weight` in `median_absolute_error` as suggested in #3450. Also make `_weighted_percentile` strong as discussed in #6189. The idea of the midpoint of weights was originally given in [here](http://in.mathworks.com/matlabcentral/newsreader/view_thread/97571). Please let me know if something else is to be done and if there is a need to add new tests. Thanks.

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/scikit-learn/scikit-learn/6217)

<!-- Reviewable:end -->
",843222
1255,2016-01-23T14:18:01Z,2020-03-02T20:11:56Z,,,"Extension of work on #5498 
",843222
1256,2016-01-13T16:41:33Z,2020-03-02T20:11:57Z,,,"This generalises `BaseSearchCV` as `BaseSearch` so that it can be used on unsupervised learning where cross-validation does not make sense. The approach is to use a pair of mixins, `SearchClusterMixin` and `SearchCVMixin` to add `_fit` methods appropriate to each type of search.

The existing `GridSearchCV` and `RandomizedSearchCV` now inherit `BaseSearch` and `SearchCVMixin` while the new `GridSearchCluster` and `RandomizedSearchCluster` inherit `BaseSearch` and `SearchClusterMixin`.

See #6154 for a little more background.
",843222
1257,2015-12-24T20:57:39Z,2020-03-02T20:11:57Z,,,"Initial implementation of randomised block krylov svd from the paper:

http://www.cameronmusco.com/personal_site/pdfs/blockKrylov.pdf

Also see this talk for more information:

http://research.microsoft.com/apps/video/default.aspx?id=259582&l=i
",843222
1258,2015-12-21T20:31:26Z,2020-03-02T20:11:58Z,,,"1.  In many instances of the variable `scale` setting was ignored.  Data was always centered, and always scaled.  The data is now scaled appropriately when `scale=True` is set, and not scaled when it's `False`
2. The return from the `predict()` function was always de-centered, but never de-scaled, so that we got:
   `Ypred = Ypred + self.y_mean_`  for all cases
   instead of:
   `Ypred = (Ypred * self.y_std_) + self.y_mean_` when `scale=True`
3. In the nipals algorithm, either C and Q matrices are used for deflation modes (regression or canonical).  This is now implemented.  The #FIXME note is now not necessary, as `coef_` is calculated differently depending on those two cases.  The calculated `coef_` was also scaled unnecesarily by standard deviations of X and Y: this is now commented out.  Only matrices W and C are scaled by their norms in the inner loop.  The outer loop scales P, Q, and C (canonical and regression) matrices by `T.T*T` or `U.T*U`
4. Some documentation was changed to reflect code modifications.  Also, instructions to use `sklearn.preprocessing.scale` are included, as that could be a useful step for most users.
5. It should be noted that the example included in the comment sections now gives better predictions of Y.
6. if X and Y are scaled using `sklearn.preprocessing.scale` (or column mean centered and column variance scaled), the pls_ functions all return the SAME results when `scale=True` and  `scale=False` this was not the case in the previous version of the code
",843222
1259,2015-12-07T17:46:18Z,2020-03-02T20:11:59Z,,,"Fixes #5870 (Adds support to tree based classifiers, excluding ensemble methods)

<hr> 

For current status, notes, references - https://github.com/raghavrv/sklearn_dev_sandbox/tree/master/tree_methods_missing_val_support

<hr>

**TODO:**
- [x] Support missing_values in `*Tree(s)Classifier` / `*ForestClassifier`
  - [x] Train with missing values
    - [x] `DepthFirstTreeBuilder`
    - [x] `BestFirstTreeBuilder`
    - [x] `ClassificationCriterion`
    - [x] `BestSplitter`
    - [x] `RandomSplitter`
    - [x] `BestSparseSplitter`
    - [x] `RandomSparseSplitter`
  - [x] Predict with missing values
    - [x] Send randomly, if missing direction is undefined
    - [x] `apply_dense`
    - [x] `apply_sparse_csc`
- [x] Make this work with all the ensemble methods
- [ ] Add `drop_values` function to generate missing values. - https://github.com/scikit-learn/scikit-learn/pull/7084
- [ ] Add Example 1 illustrating the method in comparison with imputer
- [ ] Add Example 2 comparing MCAR and MNAR.
- [ ] Add a section for narrative docs

<hr>

**NOTE:**
- The 2 other promising alternative methods are
  - [Use surrogates](http://www.salford-systems.com/videos/tutorials/tips-and-tricks/using-surrogates-to-improve-datasets-with-missing-values) to handle missing values as done in [`rpart`](https://cran.r-project.org/web/packages/rpart/rpart.pdf) - Seems promising with respect to the relative accuracy scores as reported by [Ding and Simonoff's paper](http://people.stern.nyu.edu/jsimonof/jmlr10.pdf) - Needs some refactoring to our API for this to work - Widely used - importantly this will work even if the training data had no missing values.
  - Probabilistic split - This basically sends the the missing-valued samples to both right and left but sets the weights of the samples (in the child) to the number of non-missing samples in the right (or left) split, such that the total weight at the parent node is 1. This seems to be the most widely used method apart from imputation. Gilles feels this cannot be easily accomplished with our current API.

<hr>

CC: @agramfort @glouppe @jmschrei @arjoly @tguillemot

Thanks a lot to @glouppe, @agramfort, @TomDLT & @vighneshbirodkar for all the patience and help (in and out of github)!
",843222
1260,2015-12-07T08:13:39Z,2020-03-02T20:11:59Z,,,"This is the PR reg: Stratifying Across Classes in ShuffleSplit #5965 , in which I have done the following:
Introduced a new optional flag stratify_across_classes to StratifiedShuffleSplit.
Programmed the inner-working to achieve the expected CV splits.
Enhanced the validation of the specification for train_size and test_size (sum=1).

**Also**, I feel there might be a potential bug in the ShuffleSplit class, which primarily anchors on test_size, and doesn't handle the specification of train_size (and infer the test_size appropriately). It seems like not much testing has been performed in cases when the user only specifies train_size.
### Example outputs from running the committed code:

Class sizes:  [30, 80]
test_size:  0.3
------- specifying test_size --------
With the proposed stratification:
  Training:  21 21   Testing:  9 59
Without 
  Training:  21 56   Testing:  9 24

Class sizes:  [70, 50]
test_size:  0.3
------- specifying test_size --------
With the proposed stratification:
  Training:  35 35   Testing:  35 15
Without 
  Training:  49 35   Testing:  21 15

Class sizes:  [20, 90]
test_size:  0.1
------- specifying test_size --------
With the proposed stratification:
  Training:  18 18   Testing:  2 72
Without 
  Training:  18 81   Testing:  2 9

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/scikit-learn/scikit-learn/5972)

<!-- Reviewable:end -->
",843222
1261,2015-11-23T21:46:52Z,2020-03-02T20:12:00Z,,,"This pull request addresses issue #5585.

I realise the function should have a proper documentation as well as some tests. I will add these as soon as / if the code is positively received. Please review.
",843222
1262,2015-11-16T12:52:11Z,2020-03-02T20:12:03Z,,,"Hello,

I am proposing a new estimator, named Rakel\* and already discussed in the mailing list. The pdf description of the estimator can be found here: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.97.5044&rep=rep1&type=pdf

It implements Rakel lazily and only supports examples with 0 and 1 in the target values.
I tested it with both Python 2 and 3 and also tested the coverage (was 100% on the additions for Python 2 without compatibility, but should have been lowered for compatibility with Python 3).

Being not totally sure about where to put the various files I used for it, i put them at the same level as multiclass.py for now, but any advice on where to move them is welcome.

For the doc, I have trouble generating the html and thus cannot check that it is absolutely correct.

In general, advices are welcome on how I should modify it to be more friendly.

Best regards.

(*)

```
    Tsoumakas, G., Vlahavas, I.:

    Random k-labelsets: An ensemble method for multilabel
    classification.

    In: Proceedings of the 18th European Conference
    on Machine Learning (ECML 2007),

    Warsaw, Poland (2007) 406-417
```
",843222
1263,2015-11-06T00:21:46Z,2020-03-02T20:12:04Z,,,,843222
1264,2015-10-26T04:52:36Z,2020-03-02T20:12:05Z,,,"Use `estimate_bandwidth` from MeanShift to provide an optional `threshold='auto'` parameter for Birch.

Meanwhile, it would be good if we are able to set a heuristic for `n_samples` so that we avoid calculating the nearest neighbors for all samples.
",843222
1265,2015-10-23T14:56:22Z,2020-03-02T20:12:05Z,,,"I have added a first version of the grid search default parameter in `grid_search_default.py` as suggested in issue #5004 .

I'm not sure how you guys would like to use it and for now I have added a dummy `AutomaticGridSearchCV` class to the `grid_search.py` file. That broke many tests though, for a reason I  am still investigating.

Should the dict be used directly with `GridSearchCV` instead?
",843222
1266,2015-10-23T11:43:30Z,2019-08-05T17:04:53Z,,,"Colorblind compatibility as discussed in #5435.

plot_adjusted_for_chance_measures.py
![figure_1](https://cloud.githubusercontent.com/assets/1568249/10691613/b3f34ff0-798a-11e5-8772-95a33b4a6fea.png)
![figure_2](https://cloud.githubusercontent.com/assets/1568249/10691612/b3f33290-798a-11e5-91cb-f8f1bcd152bd.png)

plot_affinity_propagation.py
![figure_1](https://cloud.githubusercontent.com/assets/1568249/10691560/435c73ac-798a-11e5-9871-ae64178b6255.png)

plot_agglomerative_clustering_metrics.py
![figure_1](https://cloud.githubusercontent.com/assets/1568249/10691648/09e1fe8e-798b-11e5-9c77-0ed46a172649.png)
![figure_5](https://cloud.githubusercontent.com/assets/1568249/10691649/09e30252-798b-11e5-82b9-eb43d9aae864.png)
![figure_6](https://cloud.githubusercontent.com/assets/1568249/10691647/09e07b68-798b-11e5-9618-b451668ab49d.png)
![figure_7](https://cloud.githubusercontent.com/assets/1568249/10691646/09e0701e-798b-11e5-8ab8-f64087b80242.png)

plot_dbscan.py
![figure_1](https://cloud.githubusercontent.com/assets/1568249/10691715/a333a2fe-798b-11e5-97b3-ba75e574d94f.png)

plot_kmeans_stability_low_dim_dense.py
![figure_1](https://cloud.githubusercontent.com/assets/1568249/10691733/c1d20fe8-798b-11e5-87cf-7ae776f04b93.png)

plot_mean_shift.py
![figure_1](https://cloud.githubusercontent.com/assets/1568249/10691754/e2e9edfe-798b-11e5-83fd-a32dbc80297a.png)
",843222
1267,2015-10-22T16:19:52Z,2017-12-19T16:38:06Z,,,"Reviewed and edited calibration for colorblind compatibility as discussed in #5435. 
",843222
1268,2015-10-22T13:21:56Z,2020-03-02T20:12:07Z,,,"This supersedes #3645.
- [x] Implement http://arxiv.org/pdf/1311.4555.pdf for confidence intervals in RFs
- [ ] Update GBRT's API for quantile regression
- [x] Update and polish example
- [ ] Add tests
- [ ] Add warnings in GPs if fixed parameters and n_restarts > 0
",843222
1269,2015-10-21T15:03:28Z,2020-02-05T21:19:31Z,,,"Supersedes #5461 

Added a 0/1 sample weight test. Failing estimators are

```
AdaBoostRegressor, BaggingClassifier, BaggingRegressor, CalibratedClassifierCV, LogisticRegressionCV, Perceptron, RandomForestClassifier, RandomForestRegressor, RidgeCV, RidgeClassifierCV, SGDClassifier, SGDRegressor
```

For the previous test, 

```
AdaBoostRegressor, BaggingRegressor, DecisionTreeRegressor, ExtraTreesRegressor, GradientBoostingRegressor, LogisticRegressionCV, Perceptron, RandomForestRegressor, 
RidgeCV, RidgeClassifierCV, SGDClassifier, SGDRegressor
```

The difference between the 2 are:

```
BaggingClassifier, CalibratedClassifierCV, DecisionTreeRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestClassifier
```

How do I proceed? Do I create an exclusion list or an inclusion list (p.ex. linear models) for the previous test?

ping @eickenberg, @amueller, @glouppe, @arjoly, @agramfort, @GaelVaroquaux 
",843222
1270,2015-10-21T08:01:38Z,2020-03-02T20:12:08Z,,,"Fixes #3379 

Still to do:
- Examples
- Documentation
",843222
1271,2015-10-16T17:20:42Z,2020-03-02T20:12:08Z,,,"Commit 1. Changes defaults to address issues in #5340
Commit 2. Issue warning if affinity propagation doesn't converge; error if fit is called after.
",843222
1272,2015-10-10T19:17:58Z,2020-03-02T20:12:09Z,,,,843222
1273,2015-09-26T20:46:39Z,2020-03-02T20:12:10Z,,,"Addresses #5318 

Comments:
- The two datasets that I added consistency to were iris and linnerud. Instead of list of strings now, they have arrays of strings. I added a few tests to enforce the consistency.
- The other datasets either did not have strings `feature_names` attribute and / or `target_names` attribute (or these fields were empty), but I added a small test case for load_digits which enforced that `digits.target_names` was a array.
",843222
1274,2015-09-25T20:09:32Z,2020-03-02T20:12:10Z,,,"Is it acceptable to expect y also to be copied when `copy` is set to True.

This makes sure that y is not modified when y is centered inplace. I hit this bug here, https://github.com/scikit-learn/scikit-learn/pull/5291/files#diff-7416ccedd45a5840c67ff7877d24e1ceR52

I also added a test case that fails in master.
",843222
1275,2015-09-22T17:06:03Z,2020-03-02T20:12:11Z,,,"(I separated this modification from https://github.com/scikit-learn/scikit-learn/pull/4852#issuecomment-132255591 for further discussion about it.)
In NMF, I scaled here the regularization parameter `alpha` with `n_samples` and `n_features`.

---

 Indeed, without scaling, two problems appear:
- The regularization penalizes the sum of coefficients in W and H. If the number of element in W and H is not the same (i.e. `n_samples != n_features`), the constraint is unbalanced. One of H or W goes to zero, and the other one, which is less penalized, increases to compensate.
- The value of alpha that makes the coefficients of W and H collapse is proportional with `sqrt(n_features * n_samples)`. It makes the scaling of alpha depends on the size of the data.

**Test to prove the point:** I tested several sizes for the input `X`, and plotted how the coefficients in W and H collapse with respect to the `alpha` parameter.

without scaling alpha:
![noregul](https://cloud.githubusercontent.com/assets/11065596/10025176/31ef9e1a-615b-11e5-86b5-70f42ec45442.png)
with properly scaling alpha:
![regul](https://cloud.githubusercontent.com/assets/11065596/10025177/31f14080-615b-11e5-90b3-a26a751ce7a6.png)

**Scaling used:** I used `alpha_W = alpha * n_features` and `alpha_H = alpha * n_samples`.

**Conclusion:** the effect of the `alpha` parameter is much more consistent if we scale it.

---

As L1 and L2 regularizations in NMF is fresh new (#4852), it would not really break any code (before 0.17 at least).
But do we want to add this? 
Is it consistent with other estimators in scikit-learn?
What Do You Think?

@vene @mblondel 
",843222
1276,2015-09-17T20:50:02Z,2020-03-02T20:12:12Z,,,"This PR adds the FABIA biclustering algorithm to sklearn. Fabia is a general biclustering algorithm based on matrix factorization.

I had discussed this addition with Kemal Eren (who implemented the rest of the biclustering package during the GSoC 2013) way back in 2013. As far as I know, his mentors (IIRC @GaelVaroquaux )  welcomed the idea back then. I've had the code lying around in #2476 , but never got around to polish it until now. If there is still interest in the addition, here it is.

Notes: I also have a Cython version that is ~20-200% faster (the larger the amount of biclusters to detect, the smaller the speedup. In realistic settings it will be close to 20%). However that implementation is based around the `cython_lapack` module that comes with scipy 0.16. Without that, it would require pulling a large-ish amount of lapack into sklearn. I didn't think that was worth it.
",843222
1277,2015-09-13T15:19:59Z,2020-03-02T20:12:13Z,,,"Adds methods to compute the gradient and hessian of GMM and uses them to get a list of the posterior modes as suggested in issue #4688
",843222
1278,2015-09-10T22:02:52Z,2020-03-03T08:13:07Z,,,"I noticed some strange predicted probabilities while working on an unbalanced classification problem and using `LogisticRegression` with the new `class_weight='balanced'` option. The first pass solution for this problem appears to be (from [King & Zeng](http://gking.harvard.edu/files/0s.pdf)) to simply correct the intercept.

Here's an example of what `intercept_correction` does to the probabilities on a generated dataset:
![demo-probability-calibration](https://cloud.githubusercontent.com/assets/704919/9802215/76f0edda-57cd-11e5-83d4-6a9111bd4bb1.png)
https://gist.github.com/adamgreenhall/266e9027d50637af0fef

@amueller - can you have a look or point me to the right person to ask? Also, this is more of a conversation starter than a finished PR. Is this the way you would start to handle it?
",843222
1279,2015-09-10T14:35:50Z,2019-08-05T16:40:01Z,,,"Shouldn't matter for randomly distributed data, but for time series that isn't heteroskedastic, it does.
",843222
1280,2015-09-09T11:25:44Z,2018-09-28T11:49:07Z,,,"This extends #5195 to update documentation on building documentation, and to make `optipng` execution more economical from within `doc/Makefile`.
",843222
1281,2015-09-07T12:19:41Z,2020-03-02T20:12:15Z,,,"In #2691, it was brought up that passing an estimator as a base class to Gradient Boosting Classifiers or Regressors would cause it to crash due to various shape issues on y. The main issue being that it was expected that the predictions of the base estimator should be of shape (n_samples, n_classes) for multinomial classification, or (n_samples, 1) for binary classification and regression. 

This PR solves this issue by handling each case separately. If the initialization estimator is one of those already in `gradient_boosting.py`, it uses those predictions as normal. However, if a classifier was passed in, it will check to see if it has a `predict_proba` method and use that if possible (collapsing into the log odds if only two classes). If the classifier does not have a `predict_proba` method, it will use the `predict` method and hot encode that into a matrix. If this needs to be collapsed because there are only two classes, it adds a small epsilon to the matrix before calculating the log odds. If a regressor is passed in, then the predictions are just reshaped to make sense.

I also reordered the code a little bit for it to be more organized, and added two unit tests to make sure that it works. It now works with arbitrary estimators, as long as they take sample weights into their fit method, and the unit test includes tests on Support Vector Machines and ridge regression initializations.

ping @ogrisel @agramfort @glouppe @pprett 
",843222
1282,2015-08-28T20:03:26Z,2020-03-02T20:12:16Z,,,"I have implemented balanced random forest as described in Chen, C., Liaw, A., Breiman, L. (2004) [""Using Random Forest to Learn Imbalanced Data""](http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf), Tech. Rep. 666, 2004. It is enabled using the balanced=True parameter to RandomForestClassifier.

This is related to the class_weight='subsample' feature already available but instead of down-weighting majority class(es) it undersamples them. According to the referenced paper (and personal experience) balanced random forests perform well for very imbalanced data.

In order to do the balanced sampling we need some class summary data (distribution of classes, etc.). For efficiency, this is precomputed in fit() by the _get_balance_class_data() function and then passed to _parallel_build_trees() which, when specified, calls generate_balanced_sample_indices() instead of the default _generate_sample_indices().

If there is interest in this feature, I'd be happy to write some tests for it and discuss code style, etc. Thanks!
",843222
1283,2015-08-27T11:25:59Z,2020-03-02T20:12:16Z,,,"In the case:
make_pipeline(a,b,CountVectorizer()) you can now call get_feature_names() and get the result from the last step in the pipeline's get_feature_names function.
",843222
1284,2015-08-15T12:09:40Z,2020-03-02T20:12:17Z,,,"When `n_features` gets big, `feature_count_` has a large memory footprint. When using MultinomialNB with sparse features (e.g. a HashingVectorizer over short sentences), this is appears to be an unnecessary overhead. This commit adds a optional sparse argument to the constructor of MultinomialNB and the necessary adjustments to `(partial_)fit()`. A new nosetest is added, `make` runs succesfully.

`bench_sparse_mnnb.py` contains an experiment comparing the sparse v/s non-sparse implementation. Memory_profile result indicates that 300mb less memory is used, with `n_features = 2 ** 20`. Yappi indicates a 6x speed increase. 

This pull request is a work in progress, please note the #TODO's. Feedback is welcome :).

Relevant profiling logs:

memory_profile:

```
Line #    Mem usage    Increment   Line Contents
================================================
    29  349.762 MiB    0.000 MiB   @profile
    30                             def fit_set_sp(X, clf, y): #Sparse 
    31                                 clf.fit(X, y)
    32                                 y_pred = clf.predict(X)
    33  369.879 MiB   20.117 MiB       return y_pred

Line #    Mem usage    Increment   Line Contents
================================================
    22   29.664 MiB    0.000 MiB   @profile
    23                             def fit_set(X, clf, y): #Not sparse
    24                                 clf.fit(X, y)
    25                                 y_pred = clf.predict(X)
    26  349.762 MiB  320.098 MiB       return y_pred
```

Yappi pstat log:

```
Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.022    0.022    1.362    1.362 bench_sparse_mnnb.py:1(<module>)
...
        1    0.000    0.000    0.388    0.388 bench_sparse_mnnb.py:22(fit_set)
...
        1    0.000    0.000    0.064    0.064 bench_sparse_mnnb.py:29(fit_set_sp) # Sparse
...
```
",843222
1285,2015-07-31T17:24:10Z,2020-03-02T20:12:18Z,,,"A simple utility function to plot decision regions to avoid implementing the code over and over (e.g., in the scikit-learn documentation examples; I feel like this could make the code leaner and easier to read).

I am wondering though how to implement unittests for this. Any ideas?

Other ""to dos"" may be:
- In addition to `True` and `False` let the user provide a custom marker list via `cycle` marker, e.g., in the format, e.g., as string 'sxo^v' (squares, crosses, circles, upper triangles, lower triangles)
- maybe create and return a `figure` object?

Here are some examples how it currently looks like:
# Simple 2D Plot

```
from sklearn.utils import plot_decision_regions
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Loading some example data
iris = datasets.load_iris()
X = iris.data[:, [0,2]]
sc = StandardScaler()
X = sc.fit_transform(X)

y = iris.target

# Training a classifier
svm = SVC(C=0.5, kernel='linear')
svm.fit(X, y)

##########################################################
# Plotting decision regions

plot_decision_regions(X, y, clf=svm, res=0.02, legend=2)
##########################################################

# Adding axes annotations
plt.xlabel('sepal length [standardized]')
plt.ylabel('petal length [standardized]')
plt.title('SVM on Iris')
plt.show()
```

![unknown](https://cloud.githubusercontent.com/assets/5618407/9012966/f5f32f74-3786-11e5-9147-8341bd7cd016.png)
# Highlighting test data points

```
from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0) 

# Training a classifier
svm = SVC(C=0.5, kernel='linear')
svm.fit(X_train, y_train)

##########################################################
# Plotting decision regions

plot_decision_regions(X, y, clf=svm, 
                      X_highlight=X_test, 
                      res=0.02, legend=2)
##########################################################

# Adding axes annotations
plt.xlabel('sepal length [standardized]')
plt.ylabel('petal length [standardized]')
plt.title('SVM on Iris')
plt.show()
```

![unknown-1](https://cloud.githubusercontent.com/assets/5618407/9013004/35e4b396-3787-11e5-8c9e-76a6d2e31326.png)
# 1D example

```
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC

# Loading some example data
iris = datasets.load_iris()
X = iris.data[:, 2]
X = X[:, None]
y = iris.target

# Training a classifier
svm = SVC(C=0.5, kernel='linear')
svm.fit(X,y)

# Plotting decision regions
plot_decision_regions(X, y, clf=svm, res=0.02, legend=2)

# Adding axes annotations
plt.xlabel('sepal length [cm]')
plt.ylabel('petal length [cm]')
plt.title('SVM on Iris')
plt.show()
```

![unknown-2](https://cloud.githubusercontent.com/assets/5618407/9013023/4f92afd2-3787-11e5-8fff-a6343a1b696b.png)

Let me know what you think @amueller 
",843222
1286,2015-07-15T16:14:20Z,2020-03-02T20:12:18Z,,,,843222
1287,2015-07-14T15:24:16Z,2020-03-02T20:12:19Z,,,"This pull request implement precision at k score for multilabel classification.
",843222
1288,2015-07-13T20:07:01Z,2020-03-02T20:12:20Z,,,"Added support for sample_weight option, refactoring some code along the way. See commit message for more details.

I have tested that results do not change when no sample_weight option is specified, and when sample_weight = ones(len(X)). Also, in my weighted real-world use case, results look reasonable and are an improvement over using no weights.

Deepak
",843222
1289,2015-07-11T22:05:34Z,2020-03-02T20:12:21Z,,,"As per discussion in issue #4892 , add a ""per_feature"" option to the Scaler classes. When False (defaults to the previous behavior of True), this modifies behavior such that scaling is based on a consideration of the entire data array at once, instead of one feature at a time. Also allow ""axis=None"" in addition to axis=0 or axis=1 in the standalone scaling functions.

This PR includes tweaks to functions in the ""sparsefuncs"" module where it makes axis=None behavior easier to code.

TODO:
- [x] Add ""per_feature"" option to RobustScaler
- [x] Add ""axis=None"" option to `preprocessing.data.scale`
- [x] Add ""axis=None"" option to `preprocessing.data.maxabs_scale`
- [x] Add ""axis=None"" option to `preprocessing.data.robust_scale`
",843222
1290,2015-07-11T00:13:25Z,2020-03-02T20:12:22Z,,,"This PR adds a new optional parameter ""monotonicity"" to the ""fit"" method of Gradient Boosting models and of Decision Tree models, allowing the caller to pass in an array of length [n_features] specifying the desired monotonicity of the predicted output with respect to each input feature. When a monotonicity is specified, the algorithm only constructs decision trees that obey the desired monotonicity -- possible values are -1 (output must be decreasing), 1 (output must be increasing), and 0 (no constraint for this feature).

This functionality is modeled after the package ""GBM"" in R, which allows for a monotonicity parameter, and is implemented similarly.
",843222
1291,2015-07-08T05:36:30Z,2020-03-02T20:12:23Z,,,"fixes #4577 Added boolean parameter 'interpolated' to sklearn.metrics.precision_recall_curve(). Returns an interpolated, de-noised precision score, if True
",843222
1292,2015-06-11T12:16:19Z,2020-03-02T20:12:25Z,,,"This pull request brings multi-output support (#3449) to the bagging meta estimators.

It's different of https://github.com/scikit-learn/scikit-learn/issues/3449 since the implementation to make the averaging is shared for single-output and multi-output data.

I haven't implemented multi-output decision function as no base estimator currently support this.
",843222
1293,2015-06-10T12:27:55Z,2020-03-02T20:12:25Z,,,"Split from #3907 and #4162
- New helpers for model comparison - `assert_same_model`, `assert_not_same_model`, `assert_fitted_attributes_equal` and `assert_safe_sparse_allclose`.
- Check if all estimators reset upon fit

<hr>

**TODO**
- [ ] `assert_safe_sparse_allcose` - To support sparse/dense matrices. (naming inspired from `safe_sparse_dot`)
- [ ] `assert_same_model` / `assert_not_same_model` / `assert_fitted_attributes_equal`
- [ ] Unit tests for the assert helpers.
- [ ] Check to make sure estimator resets when `fit`.

<hr>

Partially fixes : #406 
",843222
1294,2015-06-05T09:34:41Z,2020-03-02T20:12:26Z,,,"Implementation of the ROCCH calibration method, as described in http://www.jmlr.org/papers/volume13/hernandez-orallo12a/hernandez-orallo12a.pdf and http://link.springer.com/article/10.1007/s10994-007-5011-0

It is implemented as an other method within the CalibratedClassifierCV class. Moreover, the documentation is expanded to include the results.

The method performs similarly to Isotonic calibration when measured by Brier loss, F1 Score or AUC. But  with an speedup of up to 1.3X

Here an example

```
In[2]: %paste

import pandas as pd
from sklearn import datasets
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn.cross_validation import train_test_split
from sklearn.metrics import brier_score_loss, f1_score, roc_auc_score

X, y = datasets.make_classification(n_samples=100000, n_features=20,
                                    n_informative=2, n_redundant=10,
                                    random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,
                                                    random_state=42)

def test_times(method, est):
    clf =  CalibratedClassifierCV(est, cv=5, method=method)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    y_prob = clf.predict_proba(X_test)[:,1]
    brier = brier_score_loss(y_test, y_prob, pos_label=y.max())
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    return brier, f1, auc

res = pd.DataFrame(columns=['brier_loss','f1_score','auc_score'])
## -- End pasted text --
Backend TkAgg is interactive backend. Turning interactive mode on.
In[3]: %timeit res.loc['GNB+isotonic'] = test_times(method='isotonic', est=GaussianNB())
1 loops, best of 3: 823 ms per loop
In[4]: %timeit res.loc['GNB+sigmoid'] = test_times(method='sigmoid', est=GaussianNB())
1 loops, best of 3: 776 ms per loop
In[5]: %timeit res.loc['GNB+rocch'] = test_times(method='rocch', est=GaussianNB())
1 loops, best of 3: 752 ms per loop
In[6]: %timeit res.loc['LSVC+isotonic'] = test_times(method='isotonic', est=LinearSVC())
1 loops, best of 3: 416 ms per loop
In[7]: %timeit res.loc['LSVC+sigmoid'] = test_times(method='sigmoid', est=LinearSVC())
1 loops, best of 3: 367 ms per loop
In[8]: %timeit res.loc['LSVC+rocch'] = test_times(method='rocch', est=LinearSVC())
1 loops, best of 3: 324 ms per loop
In[9]: res['time'] = [823, 776, 752, 416, 367, 324]
In[10]: print res
```

| brier_loss | f1_score | auc_score | time |
| --- | --- | --- | --- |
| GNB+isotonic | 0.098487 | 0.854058 | 0.939305 |
| GNB+sigmoid | 0.108937 | 0.866259 | 0.937025 |
| GNB+rocch | 0.098752 | 0.851963 | 0.939027 |
| LSVC+isotonic | 0.099401 | 0.864936 | 0.936852 |
| LSVC+sigmoid | 0.098603 | 0.862041 | 0.937657 |
| LSVC+rocch | 0.099584 | 0.864046 | 0.936733 |
",843222
1295,2015-05-29T04:17:21Z,2020-03-02T20:12:27Z,,,"I took a stab at implementing the optimization described [here](http://faculty.washington.edu/dwitten/Papers/jcgs.2011.pdf): the block diagonal structure of the graphical lasso solution can be identified by thresholding the sample covariance, and the exact solution is found by solving the graphical lasso for each block separately. The authors find that there is a huge speedup when the solution is very sparse; when the solution is mostly dense, the results are basically the same (or very slightly slower due to the extra thresholding step). This modification was made in the `glasso` R package some time ago; timing results are given in the paper above, but I also ran a test of my implementation with p=1000, n=100 and a block diagonal population covariance matrix.
<img src=""https://cloud.githubusercontent.com/assets/903655/7876255/b1e0e08c-057d-11e5-8d90-5b52f531e04b.png"" width=""400px"" height=""400px"">

Couple of questions:
1. the `glasso` R package was changed to only use this algorithm, so I followed the same convention and did not allow the user to choose whether to perform the block diagonal screening procedure. It would be very easy to add this, I'm just not sure if there's a case where it would ever be desired.
2. There's a bug in our connected_components function that was fixed a while ago in scipy (https://github.com/scipy/scipy/pull/3819). I included this in my commit, but maybe it should be a separate pull request?
3. Does the overall logic make sense here? I only added a couple of comments but if it's not clear what's going on then I can try to clarify.
",843222
1296,2015-05-11T13:50:40Z,2020-03-02T20:12:27Z,,,"Tasks
- [x] Add doc in code
- [x] Add more weight initialization methods
- [x] Add another example

This is meant to be the first stage of the pipeline for the random neural network algorithm [1]

It fits on the input data by considering the number of features and then randomly generates an `n_features x n_activated` coefficient matrix where `n_activated` is the number of the ""hidden layer"" features defined by the user.

The coefficient matrix can be used to transform the input data to a different space.

[1] http://homepage.tudelft.nl/a9p19/papers/icpr_92_random.pdf
",843222
1297,2015-05-08T13:26:05Z,2020-03-02T20:12:28Z,,,"The dictionary learning algorithm was assuming that the norm of the filters was equal to one. By using a heuristic to control for the norm of the filters, we allow for a more equilibrated learning. The implementation is a simplification of the one used in the original paper from Olshausen.

The dictionary learning is tested in:
https://laurentperrinet.github.io/sciblog/posts/2015-05-05-reproducing-olshausens-classical-sparsenet.html
and this PR is tested in:
https://laurentperrinet.github.io/sciblog/posts/2015-05-06-reproducing-olshausens-classical-sparsenet-part-2.html
",843222
1298,2015-04-05T21:34:59Z,2017-12-19T16:37:43Z,,,"#4521
",843222
1299,2015-04-05T20:22:32Z,2020-03-02T20:12:29Z,,,"There have recently popped up some issues about support for different metrics. See #4520 and #4452 for example.
I am trying to add more tests, but I am not sure I am familiar enough with the metrics / neighbors modules.
@jakevdp your help would be much appreciated.
Currently travis fails because the Jaccard distances in the trees seem to be very different from the scipy ones. I think this is because our trees cast everything to bool, while scipy uses floats.
This seems to be a pretty big issues, as the algorithm that is used might change automatically depending on the dataset!!!
",843222
1300,2015-04-04T17:29:00Z,2020-03-02T20:12:30Z,,,"There is a [PR](https://github.com/scikit-learn/scikit-learn/pull/3026) that adds new kernels for mean shift clustering. However, the owner (@PeterJacob) of the PR seems to be inactive for a while. This PR includes the changes from the previous [PR](https://github.com/scikit-learn/scikit-learn/pull/3026).

In addition,

According to [the discussion here](https://github.com/scikit-learn/scikit-learn/pull/3026#issuecomment-73734480), I updated the RBF kernel.

I also refactored the control flow of _kernel_update function. There is now one return statement and therefore it is a bit cleaner.
",843222
1301,2015-04-02T15:42:12Z,2020-03-02T20:12:30Z,,,"It is now possible to define a distance_threshold that will be used as an early break criterion in Hierarchical Clustering. It is used in both ward_tree() and linkage_tree().
An error is now raised if n_cluster and distance_threshold are not set OR are both set
(I don't know yet what should be the behavior if the number of clusters AND the distance threshold
are set together).
",843222
1302,2015-04-02T12:57:47Z,2020-03-02T20:12:31Z,,,"#4475 : Add a safe_pairwise_distances function, dealing with zero variance samples when using correlation metric.

The best fix would be to have the metric not returning NaN values, but as the correlation metric is actually computed by scipy, we can't modify it directly.
So, when metric=='correlation', we replace rows and cols corresponding to zero variance samples by the maximum distance (here 1.0).

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/scikit-learn/scikit-learn/4495)

<!-- Reviewable:end -->
",843222
1303,2015-04-02T09:21:39Z,2020-03-02T20:12:32Z,,,"This is a follow up on PR #3141.

I've fixed most of @ogrisel's comment.
",843222
1304,2015-02-27T16:20:09Z,2020-03-02T20:12:33Z,,,"This module implements 7 algorithms to find ""optimal"" number of clusters (stabiliity, gap statistic, distortion jump, silhouette, Calinsky and Harabasz index, and 2 ""elbow"" methods)

If a metric is needed (for example, to compute distortion, the mean distance of a point to its cluster's center), all distance of scipy.spatial.distance can be used.
",843222
1305,2015-02-24T14:41:16Z,2020-03-02T20:12:33Z,,,"...e Cauchy Schwartz divergence combined with the Parzen window density estimator for continuous variable.

Work in progress for Parallelization Code with joblib and improve the function for MI of multiple features
",843222
1306,2015-02-24T13:21:34Z,2020-03-02T20:12:34Z,,,"fixes #4283
",843222
1307,2015-02-18T01:21:41Z,2020-03-02T20:12:35Z,,,"This PR adds `metric` and `metric_params` options to `Isomap` which allows users to specify a metric, other than the standard Euclidean metric, to use with Isomap. I opt'd not to carry over the `p` parameter---it seems a bit awkward, especially since it is only useful when the metric is `'minkowski'`. Besides, users can still pass it in via `metric_params={'p': 1}`.

@jakevdp Is this all that needs to be done to make Isomap ""metric aware""? I wasn't quite sure how to test this either.  I thought of adapting the first test `test_isomap_simple_grid`, but I don't think we can expect that result for non-Euclidean metrics.  
",843222
1308,2015-02-10T23:13:49Z,2019-08-05T16:41:51Z,,,"This is update to PR https://github.com/scikit-learn/scikit-learn/pull/2387. Unfortunately, original author dropped development, so I picked up his code.

It's still under development, but I'd like to hear feedback as I go, since I have no previous experience with sklearn's codebase. Also, my Cythonic implementation might be non-optimal.

**PR description**:

This PR introduces Matrix Factorization support. The problem is to factorize a matrix with missing values into product of 2 matrices of lower rank in order to restore missing values (perform imputation).

Optimization problem is to minimize the following loss:

![codecogseqn 1](https://cloud.githubusercontent.com/assets/434122/6128957/8201ae36-b14b-11e4-93b4-fab67d1782ed.png)

For now I have 4 (or, to be honest, 2 algorithms, each comes in 2 variants) algorithms:
- SGD — currently, only fixed learning rate is supported. I plan, though, add support for other schemes, like in `SGDClassifier`.
- SGD with AdaGrad
- ALS (Alternating Least Squares) — for reference see paper about ALS1
- ALS1 — based on [Fast als-based matrix factorization for explicit and implicit feedback datasets](http://dl.acm.org/citation.cfm?id=1864726)

**That this PR introduces**:
1. 2 new transformers `sklearn.decomposition.MatrixFactorization` and `sklearn.preprocessing.FactorizationImputer`.
2. MovieLens dataset fetcher. This one is mostly unchanged from the original PR. I contacted one of its author for fetcher permission (since dataset's license prohibits redistribution), but haven't got any feedback.
3. A couple of tests. Not all of possible usages are tested by now, I'm going to add more testcases.

**What I'd like to hear back**
0. Any issues are welcome. Again, I'm new to this.
1. Cython implementation. Should I use numpy's vectorized operations or unroll them into loops?
2. Transformers. Since factorization is data-dependent, we can either transform only rows we've already seen, or, run factorization again, reusing results of previous factorization (that's how it's done now). `FactorizationImputer` is implemented similar to `Imputer` in case of row-statistics, but I feel that `decomposition.MatrixFactorization` is somewhat clumsy.
3. Should I throw away some of implemented algorithms?
",843222
1309,2015-02-08T14:16:59Z,2020-03-02T20:12:36Z,,,"I started working on a warm_start option for Ridge (only used for solver=""sparse_cg""). However, there is an issue in the n_samples < n_features case. We need to warm-start with the dual coefficients but they are currently not returned by ridge_regression and thus not stored in the estimator. One way would be to transform the primal coefficients to dual coefficients but I couldn't find an efficient way.

See issue #769
",843222
1310,2015-02-07T12:16:58Z,2020-03-02T20:12:37Z,,,"Big refactor of the nearest neighbors/space partitioning code.

Currently, KD-tree and ball tree include (textually) the source of their base class (`binary_tree.pxi`) so that the code in that module gets compiled twice. The code in this PR merges the modules and compiles the base class once.

Also: removal of workaround for old Cython and NumPy versions, tiny optimizations (less Python C-API calling).

Preliminary to optimizing radius neighbor queries as promised in #4157.
",843222
1311,2015-02-07T02:57:58Z,2020-03-02T20:12:37Z,,,"Refactoring #4114 on top of #4190 and removing the GBM coding style changes from the scope of this PR for ease of review. Basically I'm adding `class_weights` to the remaining ensemble classes as was done in #3961 for forests and trees.

@amueller @pprett @glouppe you have all at least glanced at #4114 so perhaps you'll have a chance to review the new refactored version here.
",843222
1312,2015-02-01T05:39:00Z,2020-03-02T20:12:38Z,,,"The square root of [Jensen-Shannon divergence](http://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) is a commonly used distance metric that is appropriate probability distributions. This PR adds support for it in `sklearn.neighbors`.
",843222
1313,2015-01-19T22:12:55Z,2020-03-02T20:12:39Z,,,"Fixes #3452 

~~Based on #3614~~ (This is rewritten to conform to the referenced implementations). Thanks @Magellanea!
- [x] `binarized_multilabel_confusion_matrix` -> `multilabel_confusion_matrix`
- [x] Return MCM of shape `(n_labels, 4)` instead of numpy struct array.
- [x] Add `multilabel_confusion_matrix` entry to `test_common.py`
- [x] Add `multilabel_confusion_matrix` tests to `test_classification.py`
- [x] Add what's new entry
- [x] Add to metrics doc.

Rewrite TODO
- [ ] Rewrite `multilabel_confusion_matrix` to conform to more standard approaches.

Ref - http://www.clips.ua.ac.be/~vincent/scripts/confusionmatrix.py
Ref2 - http://www.clips.ua.ac.be/~vincent/pdf/microaverage.pdf
Ref3 - http://metaoptimize.com/qa/questions/8964/confusion-matrix-for-multiclass-multilabel-classification
",843222
1314,2015-01-12T13:35:12Z,2020-03-02T20:12:39Z,,,"I sought `sample_weight` in `silhouette_score`, to account for multiple points that are merged into one when calculating average distances. Hacking it into the current implementation resulted in a very slow solution. Thus this PR also rewrites the implementation, yielding something that's a bit slower than the solution at master, but supports `sample_weight`.

I've also added tests for correctness which I haven't otherwise found in the code.
",843222
1315,2014-12-30T17:58:58Z,2020-03-02T20:12:40Z,,,"Hi,

I added an implementation of the H-Measure, an alternative performance metric to the AUC for binary classifiers.

The AUC uses different misclassification costs (false positive - false negative) for different classifiers, which means it effectively compares different things when used on two different classifiers even on the same problem/dataset (see the following paper for proof: http://engr.case.edu/ray_soumya/mlrg/measuring_performance_hand.mlj09.pdf). The H-Measure solves this issue by explicitly specifying the relative costs of false positives and false negatives as a probability distribution.

This new method uses a beta distribution with parameters alpha and beta for the relative misclassification costs, and its implementation follows the paper mentioned above. If the parameters alpha and beta are not provided then the default distribution will be the one explained in the following paper:
http://arxiv.org/pdf/1202.2564v2.pdf

This new method was added to the metrics module in ranking.py. More information on the H-Measure can be found at the following website: http://www.hmeasure.net/

Kind regards,

 Borja
",843222
1316,2014-12-08T17:00:07Z,2020-03-02T20:12:41Z,,,"Hello! 

I have kernel CCA implemented, hope this can be used by others as well. The ''make'' doesn't complain any errors or warnings. Please have a look. 
",843222
1317,2014-12-01T16:25:30Z,2020-03-02T20:12:42Z,,,"In sklearn.decomposition, TruncatedSVD, PCA, RandomizedPCA, and KernelPCA all have an inverse_transform() method, but not SparsePCA.
",843222
1318,2014-11-29T17:50:48Z,2019-08-06T16:59:12Z,,,"Fixes #3896 

**New `partial_fit` tests:**
The below tests are based off @arjoly's [suggestion](https://github.com/scikit-learn/scikit-learn/issues/3896#issuecomment-64862823).
- [x] 1. General tests
  - Assert that `partial_fit` returns `self`. - Thanks @jnothman for [the suggestion.](https://github.com/scikit-learn/scikit-learn/issues/3896#issue-50330161)
  - Clone test... `assert clone(est).partial_fit == est.partial_fit`
- [x] 2. Check that an error is raised if the number of features changes.
- [x] 3. Estimator reset test
  - Check that doing `fit` after a set of `fit` / `partial_fit` restarts the estimator.
  - Assert that `partial_fit` and then `fit` with a different number of features works without raising any Exceptions... - Thanks @amueller for [the suggestion](https://github.com/scikit-learn/scikit-learn/pull/3907#issuecomment-68411916)
- [x] 4. Check if `partial_fit` does not overwrite the previous model.
  - Based on @jnothman [suggestion](https://github.com/scikit-learn/scikit-learn/pull/3907#issuecomment-68478630)
- [x] 5. Check that classifier handles correctly the classes argument in the `partial_fit`.
  - a. Check if mismatch between `classes` argument and `np.unique(y_i)` raises `ValueError`.
  - b. Check that error is raised if `classes` is not specified during first `partial_fit` call.
  - c. Check error is not raised if `classes` is not specified during subsequent calls.
- [x] Helper functions
  - `_validate_y_against_classes` - Check label mismatch between `y` and `classes` arg
  - `assert_same_model(X, est1, est2)`
  - `assert_not_same_model(X, est1, est2)`
    - Use `predict`, `transform`, `decision_function` and `predict_proba` to check equality of models.
  - In `sklearn.utils.estimator_checks` `_partial_fit` and `_fit` to use the appropriate parameters.
  - `assert_attributes_equal(est1, est2)`
  - `assert_attributes_not_equal(est1, est2)`
  - `assert_array_not_equal`

**Refs**
Also see #406 - This PR fixes 1st under not so easy for pfit-able estimators, via 3a ( of this PR )

<!-- Reviewable:start -->

---

This change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/scikit-learn/scikit-learn/3907)

<!-- Reviewable:end -->
",843222
1319,2014-11-25T19:18:52Z,2020-03-02T20:12:43Z,,,"Add Matern kernel to the kernels in pairwise.py.

The Matern kernel (https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function and Rasmussen and Williams 2006, pp84) is a generalization of the RBF and the absolute exponential kernel with an additional hyperparemeter nu, which allows interpolating between these two (RBF: nu=inf, absolute exponential: nu=0.5). In contrast to the RBF kernel, it makes less strict assumptions on the smoothness of the function to be learned. This is shown in an example for a step-function for different values of nu:

![figure_0](https://cloud.githubusercontent.com/assets/1116263/5189587/64c46cb2-74df-11e4-9b94-684e9751bc78.png)

TODOs:
- [x] support for arbitrary values of nu (coef0)
- [x] tests
",843222
1320,2014-10-23T23:26:51Z,2020-03-02T20:12:43Z,,,"Also edited the tests to evaluate the multioutput changes; currently passes all tests, but let me know if I missed an obvious case!
",843222
1321,2014-10-15T22:19:03Z,2020-03-02T20:12:45Z,,,"added features to:
- allow inverse onehot names to original names.
- allow to update current dict when using fit_transform multiple times and stay compatible with previous encoding.
",843222
1322,2014-10-14T14:18:43Z,2020-03-02T20:12:46Z,,,"This adds the bottou learning rate described here: http://leon.bottou.org/projects/sgd#stochastic_gradient_svm. This pr is to assist in demonstrating the performance of asgd for #3480
",843222
1323,2014-10-13T20:37:41Z,2019-08-05T16:50:50Z,,,"- Change the process of generating output code for OutputCodeClassifier. The process is to draw subsets of the exhaustive code book, see [1], multiple times and pick the one that give largest hamming distances between classes.
- Change the default value of code_size from 1.5 to 1. 1.5 is problematic. For example when n_classes = 3, the size of exhaustive code book is 3, so 1.5 for code_size is not possible.
- Add test case.
- Update the document.

[1] Thomas G. Dietterich, Ghulum Bakiri. Solving Multiclass Learning Problems via Error-Correcting Output Codes
",843222
1324,2014-10-06T09:19:46Z,2020-03-02T20:12:46Z,,,"Implemented a K-SVD algorithm as a free function.

Notes/questions:
- I haven't done any optimizations yet, but I left TODO-notes in places where it can be done. I am going to profile the code later and see where/if the optimizations are necessary.
- The algorithm currently uses OMP as a sparse coder, and it is possible to make it customizable. However this will make the function interface more complicated (as different sparse coders have different parameters). Should I do it, or should I leave it like this?
- There are now no unit-tests for the implementation, but I am a bit confused on how to create them for such functions. Should I just check the API and simple functions like `_worst_represented_example`?
",843222
1325,2014-09-30T05:53:36Z,2020-03-02T20:12:47Z,,,"Enhances the oob_score parameter of forest related predictors (e.g. RandomForestClassifier) to accept a string representing a scoring method or a callable scorer.

The existing out of bag scoring implementations use custom prediction logic in order to predict only out of bag input rows for each estimator. Additionally, per existing functionality the out of bag predictions are saved in documented fields of the predictor (oob_decision_function_, oob_prediction_). To simplify integration of this functionality with the existing scorer interface, the out of bag predictions are passed to a _DummyPredictor, which in turn makes them available to the scorer.

For backwards compatibility, the oob_score argument is not renamed, and it continues to accept True / False arguments (default still False). Passed True, the oob_score calculation is unchanged in the single output case (r2 for regression, accuracy for classification).

In the multi output case, the score calculations have changed. For regression, formerly individual r-squared metrics for each field were averaged across all output fields. Now a single r-squared metric is calculated across all outputs using the r2_score implementation. For classification, formerly the individual accuracy metrics were averaged across all output fields. Now a single accuracy metric is calculated based on complete accuracy of the set of outputs for a given input row, using the accuracy_score implementation.

This is a WIP, some known TODOS:
- Is the proposed implementation using DummyPredictor reasonable?
- Are the proposed scoring changes in the multi output case acceptable?
- Ensure that the accuracy scorer handles combined multi class + multi output cases
- Implement this for BaggingClassifier and BaggingRegressor as well?
- Tests, example etc
",843222
1326,2014-09-20T09:12:53Z,2020-03-02T20:12:48Z,,,"Fix for #3453 
Ping @arjoly . Added support for `zero_one_loss` and `accuracy_score`
",843222
1327,2014-08-13T13:42:15Z,2020-03-02T20:12:48Z,,,"This is a checkpoint to solve #3481 
",843222
1328,2014-08-04T17:48:13Z,2020-03-02T20:12:49Z,,,"This PR addresses the issues raised in #3218. There are two changes that might affect users:
1. A new kwarg is added to `LabelPropagation`/`LabelSpreading` initializers, which lets the user set default labels for instances that aren't labeled by the `fit()` call. This defaults to `None`, which means that the presence of any unlabeled instances will trigger an exception. If not `None`, the `transduction_` array will be given default values for unlabeled instances.
2. The termination condition in `fit()` had an off-by-one error, where setting `max_iter=1` was iterating zero times. This has been fixed.

I've also supplied a test case that exercises this new behavior.
",843222
1329,2014-08-01T14:24:42Z,2020-03-02T20:12:49Z,,,"This is me picking up the remaining commits from #1574, rebasing on master, and adding a different API:

Since I want to write a `sample_group`-aware scorer, I needed an API to pass arbitrary params to scorers in cross-validation in the same way as `fit_params`. This supersedes the need for an explicit sample_weights parameter, at the cost of some duplication in the function call (`fit_params=dict(sample_weight=sw), scorer_params=dict(sample_weight=sw)`). Internally this saves us the need for special treatment, though, and allows scorers to be more powerful.
- [ ] Consider API change: `fit_params` and `scorer_params` starting with `sample_` will be indexed by train-test splits.
- [ ] support `fit_params` and `scorer_params` in learning_curve and RFECV
- [ ] test that `scorer_params` are getting appropriately indexed.
- [ ] proof of concept learning to rank grid search using this API
",843222
1330,2014-07-17T17:05:04Z,2020-03-02T20:12:50Z,,,"I started working on returning prediction variances in Ridge (for the Cholesky solver only). The n_samples > n_features (primal) case works. The n_features > n_samples (dual) case is a work in progress.
",843222
1331,2014-07-14T17:24:45Z,2020-03-02T20:12:50Z,,,"Addresses #3379

Some problem with matrix transposes (failing tests). Once solved it should be clean.

cc @GaelVaroquaux @ogrisel @agramfort
",843222
1332,2014-07-14T16:58:01Z,2020-03-02T20:12:51Z,,,"Adressing https://github.com/scikit-learn/scikit-learn/issues/3373

A first implementation of a ridge path using `'eigen'` solver. This solver is now available in `ridge_regression`, which calls upon a reduced functionality of `ridge_path`.

Next steps:
- The same path approach can be straightforwardly used with SVD as well: Write svd path and replace in ridge_regression
- Attack `RidgeCV`: Replace the `GridSearchCV` by `ridge_path` if `eigen` or `svd` are selected.
- Incorporate `ridge_path_looe` from the gist https://gist.github.com/eickenberg/e9eb106f12e40d017fd0
- Adjust `RidgeCV` to use the looe functionalities
",843222
1333,2014-07-03T22:36:31Z,2020-03-02T20:12:52Z,,,"The output of MinMaxScaler doesn't always lie within the passed feature range, if data that you transform() has values outside the range of the values that you fit() on. If you're using it just to make the scale of the data nicer, this probably doesn't matter, but if your algorithm actually relies on the data lying in a certain range ([example](https://github.com/dougalsutherland/skl-groups/blob/2c7874e824b6f4c78d15c5e1c3f963b444a98b6e/skl_groups/summaries/l2_density.py)) this is no good.

So, this PR adds optional support for truncation, so that values that would be transformed outside of feature_range are clipped to the ends of it. It also adds a fit_feature_range to make truncation less likely (e.g. if you need your data to lie in [0, 1], you can make your training data like in [.1, .9] and then test values have more of a range to avoid clipping).

Incidentally, I also add assert_array_{less_equal,greater,greater_equal} because my tests wanted them and it's silly that numpy only provides assert_array_less.
",843222
1334,2014-05-26T12:53:49Z,2020-03-02T20:12:52Z,,,"Added realization of support vector data description from http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/
",843222
1335,2014-05-02T00:27:44Z,2019-08-05T16:43:57Z,,,"Because I wanted to try K-means clustering as the basis for Nystroem approximation and it appeared as though pull request #2591 might be stalled I created a slightly modified version. I also tried to address @amueller comment about the effectiveness of the method by including it in the plot_kernel_approximation example and @dougalsutherland comment concerning the possible singularity of the sub-sampled kernel matrix using the same approach as scipy does in pinv2.

Since it is my first commit to the project (hopefully the first of many) any feedback or suggestions you have would be appreciated.
",843222
1336,2014-04-28T16:50:12Z,2020-03-02T20:12:53Z,,,"This patch enables the use of kernel functions for neighbors weighting.

It adds the following keywords for `weights` argument: `tophat`, `gaussian`, `epanechnikov`, `exponential`, `linear`, `cosine`, i.e. all kernels presented in `KernelDensity` class.

For `KNeighborsClassifier` and `KNeighborsRegressor` the kernel bandwidth is equal to the distance to the k+1 nearest neighbor (i. e. it depends on a query point).

For `RadiusNeighborsClassifier` and `RadiusNeighborsRegressor` the kernel bandwidth is equal to the radius parameter of the classifier (i. e. it is constant).

Please, take a look.
",843222
1337,2014-04-01T17:01:31Z,2020-03-02T20:12:54Z,,,"Discussion started 2 years ago on implementing other kernels besides the flat kernel, see thread here: https://github.com/scikit-learn/scikit-learn/issues/442

The advantage of the rbf/Gaussian kernel over the flat kernel is shown here: http://sociograph.blogspot.nl/2011/11/accessible-introduction-to-mean-shift.html

I implemented the code from the issue thread and added 2 more kernel types. The new function _kernel_update has a docstring. The main function docstring is updated. Pyflakes runs without errors. Pep8 runs with 1 minor error about indenting. The example for MeanShift clustering still runs without errors.
",843222
1338,2014-03-28T15:38:24Z,2020-03-02T20:12:54Z,,,"Support Vector Data Description (SVDD) could be a nice enhancement to OneClassSVM implementation.

A technical implementation is described in this paper:
http://www.csie.ntu.edu.tw/~cjlin/papers/svdd.pdf

Source code compatible with libsvm is available here:
http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#libsvm_for_svdd_and_finding_the_smallest_sphere_containing_all_data

I have modified the sklearn/svm/src/libsvm/svm.h and svm.cpp files to add additional functionality of SVM.

-s 5 SVDD
-s 6 R^2 L1SVM
-s 7 R^2 L2SVM
",843222
1339,2014-02-11T17:11:09Z,2020-03-02T20:12:55Z,,,"Neither one of these added metrics has been added to the mini batch kmeans at this point.
",843222
1340,2014-01-31T10:52:54Z,2020-03-02T20:12:55Z,,,"An early PR to make reading the diff easier. Not ready for detailed comments but high-level comments welcome :)
",843222
1341,2014-01-14T01:07:47Z,2020-03-02T20:12:56Z,,,"Hi all,

I've had a crack at implementing a function for out-of-core SVD on large memory-mapped arrays (as per issue #2661). It's based on the algorithm in [Halko, et al., 2011](http://arxiv.org/pdf/1007.5510), which is essentially identical to the one currently used by `randomized_svd` (originally from [Halko, et al., 2009](http://arxiv.org/abs/arXiv:0909.4061)). The main difference is that I've implemented a function for computing dot products on pre-cached blocks of memory-mapped arrays. Doing manual buffering in this way is many times faster than calling `np.dot` directly on an `np.memmap` array (see my [SO question/answer here](http://stackoverflow.com/q/20983882/1461210)).

For the time being I've just added another user option to `randomized_svd` to force buffering, with an additional keyword argument to control the size of the buffer in bytes. If people think this would be a potentially useful avenue to go down, I could have a look at integrating it into the `RandomizedPCA` class. However, there may be issues arising from the fact that the PCA classes force a copy of the input array (in order to do centering etc), which would be problematic for very large memory-mapped arrays.

Let me know what you think.
",843222
1342,2014-01-10T16:23:28Z,2020-03-02T20:12:57Z,,,"It seems that the current eigendecomposition-based method for orthogonalizing the mixing matrix in `sklearn.decomp.fastica` is less numerically stable and produces a less orthogonal output than using a different SVD-based strategy. In some cases, the original method yields NaNs in the result, resulting in a `ValueError` exception that does not occur with the new SVD-based method. However, there seems to be a trade-off between speed and quality, as the new method is not quite as fast as the original - see the extended discussion in #2735.

I've added another boolean keyword argument to `fastica` and to the `FastICA` class to enable the use of the SVD-based method rather than the original eigendecomposition-based method.
## Quality

![ortho_closeness](https://f.cloud.github.com/assets/2284074/1894446/c927898c-7ae9-11e3-8f5f-790f6c2f9e5a.png)
## Speed

![ortho_bench](https://f.cloud.github.com/assets/2284074/1894443/9c1fc6d4-7ae9-11e3-9a7a-d116f0d8c030.png)
",843222
1343,2013-12-02T20:12:45Z,2020-03-02T20:12:57Z,,,"This pull request makes the following changes to the Pipeline class:
1. Allow passing arguments to methods of steps, not just their constructors.  Necessary so that AdaBoost can pass the sample_weight argument.
2. If an argument is not of the form step__argname it will be passed to all steps.  Necessary again so that AdaBoost can pass the sample_weight argument.
3. Forward attribute access to steps if the attribute is not found in the Pipeline object itself.  Steps are searched backward starting with the final step.  Allows AdaBoostClassifier to access the classes_ attribute of the final classifier in the Pipeline.  

My objective was to make changes that would generalize well to other potential uses of the Pipeline.  For example, it is now possible to pass an argument to the fit method of a particular step by passing a parameter named step__argname instead of just argname.  It is also possible to access step attributes other than the classes_ attribute of the final step.  Neither of these features is strictly necessary to get AdaBoost compatibility, but seemed like reasonable generalizations of the necessary functionality.  I think sklearn devs should pay particular attention to change 3 above and make sure they're comfortable with it.  There are definitely other generalizations that might be better, such as only allowing access to the attributes of the final step or even only forwarding the classes_ attribute.

A gist showing the intended usage of this enhancement is located here:

https://gist.github.com/jcrudy/7756798

The gist uses py-earth because I was not familiar with a scikit-learn transformer that takes sample_weight (I don't know them all, though).  If the EarthRegressor (https://github.com/scikit-learn/scikit-learn/pull/2285) pull request is merged then there will be at least one such transformer.
",843222
1344,2013-11-14T17:02:08Z,2019-08-05T16:43:50Z,,,"Hi, 
My name is Patrick and I have been using Scikit learn for a while but it's first time for me to contribute. I have added k-means sampling to Nystroem method, which is listed in the issues. Thanks!

Best,
Patrick  
",843222
1345,2013-11-09T08:35:21Z,2020-03-02T20:12:58Z,,,"This PR is an attempt to implement LambdaMART [1]. I imagine the biggest hurdle will be coming to some conclusion over the correct API since we need to include the queries somehow. In my implementation I use an extra keyword argument, I don't know if this causes problems elsewhere. My hope is that this PR can serve as a catalyst to resolve that, and then I can finish up the PR.

TODO
- [ ] tests
- [ ] docs
- [ ] gbm comparison
- [ ] [Ranklib](http://sourceforge.net/p/lemur/code/HEAD/tree/RankLib/trunk/) comparison
- [x] support an optional cutoff parameter for NDCG, usually denoted as NDCG@k
- [ ] rewrite the example to use the yandex 2009 dataset instead of MQ2007/8 that would require the `unrar` command to execute
- [x] add a `max_rank=10` cutoff parameter for NDCG and LambdaMART and use it to compute the lambdas too
- [ ] factorize a public function `ndcg_score` in `sklearn.metrics` in pure Python first (see [this comment](https://github.com/scikit-learn/scikit-learn/pull/2580#issuecomment-33670859))
- [x] [benchmark](https://github.com/scikit-learn/scikit-learn/pull/2580#issuecomment-34137058) python and cython DCG
- [x] implement pessimistic tie break for `ndcg_score` and bench it against no-tie break (default implementation sort order) and random tie break
- [x] clarify if the pessimistic tie break is only need for the stability `ndcg_score` score or for its derivatives in the LambdaMART optim loop as well (see [this comment](https://github.com/scikit-learn/scikit-learn/pull/2580#issuecomment-33677056))
- [ ] investigate if pre-computing (caching) the ideal DCG for each query on the training set as done by Ranklib can speed up learning

Possibly for another PR:
- [ ] abstract code to support measures other than NDCG

There was also a brief discussion on the mailing list [2]. Pinging @mblondel @ogrisel @pprett from that discussion.

[1] https://research.microsoft.com/en-us/um/people/cburges/tech_reports/msr-tr-2010-82.pdf
[2] http://sourceforge.net/mailarchive/forum.php?thread_name=CAP%2B3rpGVbSux5u4dZiatV3p1f1zUvndHXoi-oh4CjMRtSpjFsw%40mail.gmail.com&forum_name=scikit-learn-general
",843222
1346,2013-09-21T15:46:54Z,2020-03-02T20:12:59Z,,,"This PR adds a simple meta-estimator which accepts any generative model (normal approximation, `GMM`, `KernelDensity`, etc.) and uses it to construct a generative Bayesian classifier.

Todo:
- [x] code documentation
- [x] narrative docs
- [x] testing
- [x] examples
- [ ] allow class-wise cross validation for the density model?
",843222
1347,2013-09-20T14:28:35Z,2020-03-02T20:12:59Z,,,"Add one of the simplest and common multi-label classification strategy which use 
a multi-class classifier as a base estimator.

The core code is functional, but there is still things to do:
- [x] Add some word about binary relevance in ovr narrative doc
- [x] Write narrative doc about LP
- [x] Add some references  
- [x] Add some regression tests
- [x] ""making your remark about overfitting a bit more explicit maybe?""
",843222
1348,2013-09-17T18:51:08Z,2020-03-02T20:13:00Z,,,"Improvements and additions to the bicluster metrics module.

TODO:
- [ ] refactor hungarian matching code (Let's lave this for later -- Vlad)
- [x] tests: full code coverage
- [x] implement gene match score
- [x] update documentation to explain new functionality
- [x] implement size bias correction
- [x] implement other similarity metrics (Dice and goodness measure)
",843222
1349,2013-08-24T08:27:45Z,2019-07-14T22:50:17Z,,,"The algorithm as proposed in:

O. Pujol, P. Radeva, , and J. Vitria`. ""Discriminant ECOC: A heuristic method for application dependent design of error correcting output codes""
",843222
1350,2013-08-23T09:12:38Z,2020-03-02T20:13:01Z,,,"This is still a work in progress. I still have to test a few things:
- [ ] Check that it works well with different datatypes (the cython code assumes that it's receiving floats)
- [ ] Check if it could be a good idea to raise a warning when the factorization isn't working (the gradient is added to multiple items in the factor, thereby they will diverge if the learning rate is too high. But it may no be a good idea to simply reject some learning rate values, because they could work with another initial values of the factors)
- [ ] Print warnings if rows/columns only contains missing values
- [ ] Add a better way to test the score in the imputer
- [ ] Replace the parameters init_\* in the constructor by a different one that could be used with grid_search
- [ ] Finish docstrings
- [ ] Run pep8, pyflakes, ...
- [ ] Fix the build (I have an error with another module, probably a rookie mistake with the imports)
",843222
1351,2013-08-22T04:52:28Z,2020-03-02T20:13:02Z,,,"Implementing the BiMax biclustering algorithm.

TODO:
- [ ] test
- [ ] profile and optimize. currently extremely slow.
- [ ] documentation
",843222
1352,2013-04-03T00:16:27Z,2020-03-02T20:13:02Z,,,"Evidence accumulation clustering: EAC, an ensemble based clustering framework:
Fred, Ana LN, and Anil K. Jain. ""Data clustering using evidence
    accumulation."" Pattern Recognition, 2002. Proceedings. 16th International
    Conference on. Vol. 4. IEEE, 2002.

Basic overview of algorithm:
1. Cluster the data many times using a clustering algorithm with randomly (within reason) selected parameters.
2. Create a co-association matrix, which records the number of times each pair of instances were clustered together.
3. Cluster this matrix.

This seems to work really well, like a kernel method, making the clustering ""easier"" that it was for the original dataset.

The default of the algorithm are setup to follow the defaults used by Fred and Jain (2002), whereby the clustering in step 1 is k-means with k selected randomly from 10 and 30. The clustering in step 3 is the MST algorithm, which I have yet to implement (will do in this PR).

**After initial feedback,** I think people are happy with the API.

TODO:
- [x] MST algorithm from the paper, which was used as the final clusterer. Completed in PR #1991 
- [ ] ~~There is an improvement to the speed of the algorithm (don't have the paper on hand) that has been published, that should be incorporated~~ (will be done in a later PR)
- [x] Examples/Usage
- [x] Narrative documentation
- [x] Revert test_clustering, line 508, to only check for SpectralClustering
- [x] Use a sparse matrix for the co-association matrix
",843222
1353,2012-07-19T19:43:38Z,2020-03-02T20:13:03Z,,,"The k-Nearest-Neighbors and Radius Neighbors classifiers now make explicit use of the Bayesian prior probabilities of the classes (`self.class_prior_`). By default, each of these prior probabilities (for a given class) is set to the proportion of sample points which are in the given class; but they can also be set to other values by the class user.

(This is my first submission so I'd be grateful for any and all advices. Thank you!)

Fixes #399
  ",843222
1354,2020-03-27T07:29:23Z,2020-03-27T08:06:23Z,,,"Bumps [jupyter-client](https://github.com/jupyter/jupyter_client) from 6.0.0 to 6.1.2.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/jupyter/jupyter_client/blob/master/docs/changelog.rst"">jupyter-client's changelog</a>.</em></p>
<blockquote>
<h1>6.1.2</h1>
<ul>
<li>Fixed a bug causing clients to sometimes hang after a stop call was made (536)</li>
</ul>
<h1>6.1.1</h1>
<ul>
<li>Subprocess kill action fix for async execution (535)</li>
<li>Doc fix for xeus kernel list (534)</li>
</ul>
<h1>6.1.0</h1>
<p>This release includes support for asyncio patterns! Downstream tools should soon have releases to additionally support async patterns.</p>
<ul>
<li>AsyncKernelManager and AsyncMultiKernelManager are now available for async jupyter_client interactions (528, 529)</li>
<li>Removed unused sphinx dependency (518, 518).</li>
<li>Added install instructions for pip to documentation (521)</li>
<li>Improved docs around version protocol and messaging (522, 526)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/9bdbe25461a5657ab8cfff82b2a9dc1930d5370a""><code>9bdbe25</code></a> Bumped version for release</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/ee9351619a9972509e3698eedf5979490a09af04""><code>ee93516</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/536"">#536</a> from kevin-bates/retain-instance-on-stop</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/ddb7e0213e929e41ef2e9e7f84bce39af45abdc3""><code>ddb7e02</code></a> Retain restarter instance on stop</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/492a764b54dd9a3f4efb7f789ab4564376b2d32b""><code>492a764</code></a> next dev version</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/bba9403cd4422eddb3800c1684892fa2381a494e""><code>bba9403</code></a> Release 6.1.1</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/3ab5e5afc551b6eee4c59dc3f253fcf739a239c7""><code>3ab5e5a</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/535"">#535</a> from davidbrochart/fix_kill</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/4430e735bae8ae1628e68193923686123f38d83c""><code>4430e73</code></a> Subprocess kill is not async</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/28bf7209c510125d29d3dc1687b3617dc82205c5""><code>28bf720</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/jupyter/jupyter_client/issues/524"">#524</a> from minrk/docs-kernels</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/bbfe1e7c3605f18b4cf36b0609b75dc6982d19b0""><code>bbfe1e7</code></a> Added dev back to version</li>
<li><a href=""https://github.com/jupyter/jupyter_client/commit/8a55070d3f9c4376ea2927494aa3d51264d82fdf""><code>8a55070</code></a> Making 6.1.0 release</li>
<li>Additional commits viewable in <a href=""https://github.com/jupyter/jupyter_client/compare/6.0.0...6.1.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=jupyter-client&package-manager=pip&previous-version=6.0.0&new-version=6.1.2)](https://dependabot.com/compatibility-score/?dependency-name=jupyter-client&package-manager=pip&previous-version=6.0.0&new-version=6.1.2)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",49346299
1355,2020-03-19T07:27:07Z,2020-03-26T19:45:56Z,,,"Bumps [pandas](https://github.com/pandas-dev/pandas) from 0.25.3 to 1.0.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/pandas-dev/pandas/releases"">pandas's releases</a>.</em></p>
<blockquote>
<h2>Pandas 1.0.3</h2>
<p>This is a minor bug-fix release in the 1.0.x series and includes some regression fixes
and bug fixes. We recommend that all users upgrade to this version.</p>
<p>See the <a href=""https://pandas.pydata.org/docs/whatsnew/v1.0.3.html"">full whatsnew</a> for a list of all the changes.</p>
<p>The release will be available on the defaults and conda-forge channels:</p>
<pre><code>conda install pandas
</code></pre>
<p>Or via PyPI:</p>
<pre><code>python3 -m pip install --upgrade pandas
</code></pre>
<p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>
<h2>Pandas 1.0.2</h2>
<p>This is a minor bug-fix release in the 1.0.x series and includes some regression fixes
and bug fixes. We recommend that all users upgrade to this version.</p>
<p>See the <a href=""https://pandas.pydata.org/docs/whatsnew/v1.0.2.html"">full whatsnew</a> for a list of all the changes.</p>
<p>The release will be available on the defaults and conda-forge channels:</p>
<pre><code>conda install pandas
</code></pre>
<p>Or via PyPI:</p>
<pre><code>python3 -m pip install --upgrade pandas
</code></pre>
<p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>
<h2>Pandas 1.0.1</h2>
<p>This is a minor bug-fix release in the 1.0.x series and includes some regression fixes
and bug fixes. We recommend that all users upgrade to this version.</p>
<p>See the <a href=""https://pandas.pydata.org/docs/whatsnew/v1.0.1.html"">full whatsnew</a> for a list of all the changes.</p>
<p>The release will be available on the defaults and conda-forge channels:</p>
<pre><code>conda install pandas
</code></pre>
<p>Or via PyPI:</p>
<pre><code>python3 -m pip install --upgrade pandas
</code></pre>
<p>Please report any issues with the release on the <a href=""https://github.com/pandas-dev/pandas/issues"">pandas issue tracker</a>.</p>
<h2>Pandas 1.0.0</h2>
<p>This is a major release from 0.25.3, and includes a number of API changes, new</p>
</tr></table> ... (truncated)
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/pandas-dev/pandas/commit/3adf3340453d6704d4a2cb47058214cc697a7d29""><code>3adf334</code></a> RLS: 1.0.3</li>
<li><a href=""https://github.com/pandas-dev/pandas/commit/156141c878965fec57c2dc1e0b6074fe3ff1d790""><code>156141c</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32783"">#32783</a>: DOC: 1.0.3 release date (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32784"">#32784</a>)</li>
<li><a href=""https://github.com/pandas-dev/pandas/commit/cfccb68cb5532e4400a9f2bfef38228a6c2e5880""><code>cfccb68</code></a> BUG: arithmetic with reindex pow (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32734"">#32734</a>) (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32777"">#32777</a>)</li>
<li><a href=""https://github.com/pandas-dev/pandas/commit/d8a9cb7da9aca7808ac8bd7f6c40fcef0fb5ece2""><code>d8a9cb7</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32708"">#32708</a>: skip 32 bit linux (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32771"">#32771</a>)</li>
<li><a href=""https://github.com/pandas-dev/pandas/commit/ef2ebc3202766a59c5c0c09847619d240272e3eb""><code>ef2ebc3</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32758"">#32758</a>: BUG: resample.agg with read-only data (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32765"">#32765</a>)</li>
<li><a href=""https://github.com/pandas-dev/pandas/commit/efa701d4f375d2ecd643d639c8768f290ca0edea""><code>efa701d</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32746"">#32746</a>: DOC: start 1.0.3 (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32750"">#32750</a>)</li>
<li><a href=""https://github.com/pandas-dev/pandas/commit/7485dbe6fcdab3fe2e5a23534ba00767d50374d8""><code>7485dbe</code></a> RLS: 1.0.2</li>
<li><a href=""https://github.com/pandas-dev/pandas/commit/017022290824e081e740b84e84db5f7c94b59155""><code>0170222</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32658"">#32658</a>: DOC: Organize regressions (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32661"">#32661</a>)</li>
<li><a href=""https://github.com/pandas-dev/pandas/commit/bcc27588f0030c79fc14710ee4c3b219cf99539c""><code>bcc2758</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32490"">#32490</a>: BUG: Fix bug, where BooleanDtype columns are converted to...</li>
<li><a href=""https://github.com/pandas-dev/pandas/commit/e09abde6c004f66df8db880fd5e360975c6e8dfb""><code>e09abde</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32656"">#32656</a>: DOC: fix announce formtting (<a href=""https://github-redirect.dependabot.com/pandas-dev/pandas/issues/32659"">#32659</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/pandas-dev/pandas/compare/v0.25.3...v1.0.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=pandas&package-manager=pip&previous-version=0.25.3&new-version=1.0.3)](https://dependabot.com/compatibility-score/?dependency-name=pandas&package-manager=pip&previous-version=0.25.3&new-version=1.0.3)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",49346299
1356,2020-03-19T07:26:46Z,2020-03-19T08:01:49Z,,,"Bumps [matplotlib](https://github.com/matplotlib/matplotlib) from 3.2.0 to 3.2.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/matplotlib/matplotlib/releases"">matplotlib's releases</a>.</em></p>
<blockquote>
<h2>REL: v3.2.1</h2>
<p>This is the first bugfix release of the 3.2.x series.</p>
<p>This release contains several critical bug-fixes:</p>
<ul>
<li>fix <code>Quiver.set_UVC</code> calls with scalar inputs</li>
<li>fix <code>bezier.get_parallels</code> failure from floating point rounding errors</li>
<li>fix markers specified as tuples (polygons, stars, or asterisks)</li>
<li>fix saving PNGs to file objects in some places</li>
<li>fix saving figures using the nbAgg/notebook backend</li>
<li>fix saving with tight layout using the PGF backend</li>
<li>fix setting custom datapath in rcParams (note: it is still deprecated)</li>
<li>fix various issues running setup.py in non-CI environments</li>
<li>fix xpdf distiller</li>
<li>various minor bug and documentation fixes</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/ca3d653536dec38a0c1ac3b80413961ca1bcdda6""><code>ca3d653</code></a> REL: v3.2.1</li>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/098d12ec37c6edacd0718f9a002e8031ccfa5f78""><code>098d12e</code></a> Remove GitHub stats that aren't on this milestone.</li>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/9aaca298186d67a2f8e21289b6eb3413a61e0f95""><code>9aaca29</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/matplotlib/matplotlib/issues/16813"">#16813</a> from QuLogic/v3.2.x</li>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/13e21ad690cd8d66b520efb3869d49c1b57e3bee""><code>13e21ad</code></a> DOC: Fix broken links.</li>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/320defeedae0c223ba3f0b241f3bf4ce5c516493""><code>320defe</code></a> Add GitHub stats for 3.2-doc changes.</li>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/232e702ce903c8acb8744b54fe6a72d7ff605c7e""><code>232e702</code></a> Update GitHub stats for 3.2.1.</li>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/5b4c190320545f77f549d38832251523c4d704f5""><code>5b4c190</code></a> DOC: Cleanup trailing whitespace.</li>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/56666b851addc5c037ac7924329955277d1a5a04""><code>56666b8</code></a> Merge branch 'v3.2.0-doc' into v3.2.x</li>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/a40622f87c3d3e76a35d9db88d97aa8b6ec4e3b0""><code>a40622f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/matplotlib/matplotlib/issues/16809"">#16809</a> from meeseeksmachine/auto-backport-of-pr-16779-on-v...</li>
<li><a href=""https://github.com/matplotlib/matplotlib/commit/5ef8a1292b12fc0ecd5cd099e16b9396322aaaa6""><code>5ef8a12</code></a> Backport PR <a href=""https://github-redirect.dependabot.com/matplotlib/matplotlib/issues/16779"">#16779</a>: Documentation: make instructions for documentation contri...</li>
<li>Additional commits viewable in <a href=""https://github.com/matplotlib/matplotlib/compare/v3.2.0...v3.2.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=matplotlib&package-manager=pip&previous-version=3.2.0&new-version=3.2.1)](https://dependabot.com/compatibility-score/?dependency-name=matplotlib&package-manager=pip&previous-version=3.2.0&new-version=3.2.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",49346299
1357,2020-03-11T07:24:43Z,2020-03-11T07:24:44Z,,,"Bumps [visions](https://github.com/dylan-profiler/visions) from 0.2.3 to 0.3.0.
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/dylan-profiler/visions/commits"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=visions&package-manager=pip&previous-version=0.2.3&new-version=0.3.0)](https://dependabot.com/compatibility-score/?dependency-name=visions&package-manager=pip&previous-version=0.2.3&new-version=0.3.0)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",49346299
1358,2020-03-10T13:50:40Z,2020-03-10T14:25:21Z,,,#214,49346299
1359,2020-03-05T17:11:13Z,2020-03-05T17:12:37Z,,,"As I said in #279, this is my work so far. I changed the name of the lib so I could have both packages installed.

As you can see, it's far from a finished work for:

1. It's from an old version
1. I replaced the pandas functions, therefore it is concurrent to the original
1. Many features were lost in the process

But I'm willing to put the effort into it, so I created this mostly to discuss what would be a great approach for adding support for dask dataframes.",49346299
1360,2020-03-03T09:55:59Z,2020-03-03T12:58:43Z,,,"Hey guys I was reading through the code and came across this:
```
count = series.count()
n_inifinte = count - series.count()
```

So I am pretty sure n_infinite is always 0. I believe the following expression is correct:

```
leng = len(series)
count = series.count()
n_inifinite = leng - count
```

Hope this helps",49346299
1361,2020-03-10T09:17:16Z,2020-03-26T10:27:26Z,,,This shall fix issue #635 ,71996613
1362,2020-02-10T13:34:51Z,2020-03-01T14:21:47Z,,,"from distributed.security import Security
sec = Security(tls_ca_file='./dask-ca.pem', tls_client_cert='./dask-cert.pem', tls_client_key='./dask-key.pem', require_encryption=True)
Distributor = ClusterDaskDistributor(address=""tls://ip_adress_or_url:8786"", security=sec)",71996613
1363,2020-02-04T07:49:27Z,2020-02-04T08:22:24Z,,,"Improves performance of #7 by using the much faster `np.argsort` instead of `sorted`.

### All Submissions Basics:

* [x] Have you followed the guidelines in our Contributing document?
* [x] Have you checked to ensure there aren't other open [Pull Requests](../../pulls) for the same update/change?
* [x] Have you checked all [Issues](../../issues) to tie the PR to a specific one?

### All Submissions Cores:

* [x] Have you added an explanation of what your changes do and why you'd like us to include them?
* [ ] ~~Have you written new tests for your core changes, as applicable?~~
* [x] Have you successfully ran tests with your changes locally?
* [x] Does your submission pass tests, including CircleCI, Travis CI, and AppVeyor?
* [x] Does your submission have appropriate code coverage? The cutoff threshold is 95% by Coversall.",105699750
1364,2019-11-19T12:28:55Z,2019-12-18T21:45:49Z,,,"### All Submissions Basics:

* [x] Have you followed the guidelines in our Contributing document?
* [x] Have you checked to ensure there aren't other open [Pull Requests](../../pulls) for the same update/change?
* [x] Have you checked all [Issues](../../issues) to tie the PR to a specific one?

### All Submissions Cores:

* [x] Have you added an explanation of what your changes do and why you'd like us to include them?

According to the https://keras.io/ I think, that that's good idea to switch from pure keras to keras loaded from tensorflow:

### Multi-backend Keras and tf.keras:
At this time, we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to tf.keras in TensorFlow 2.0. tf.keras is better maintained and has better integration with TensorFlow features (eager execution, distribution support and other).

Keras 2.2.5 was the last release of Keras implementing the 2.2.* API. It was the last release to only support TensorFlow 1 (as well as Theano and CNTK).

The current release is Keras 2.3.0, which makes significant API changes and add support for TensorFlow 2.0. The 2.3.0 release will be the last major release of multi-backend Keras. Multi-backend Keras is superseded by tf.keras.

Bugs present in multi-backend Keras will only be fixed until April 2020 (as part of minor releases).

For more information about the future of Keras, see the Keras meeting notes
",105699750
1365,2019-11-12T15:38:11Z,2019-11-12T15:38:11Z,,,,105699750
1366,2019-05-22T19:31:06Z,2019-05-23T21:11:09Z,,,"### All Submissions Basics:
Closes #101 
* [x] Have you followed the guidelines in our Contributing document?
* [x] Have you checked to ensure there aren't other open [Pull Requests](../../pulls) for the same update/change?
* [x] Have you checked all [Issues](../../issues) to tie the PR to a specific one?

### All Submissions Cores:

* [x] Have you added an explanation of what your changes do and why you'd like us to include them?
* [x] Have you written new tests for your core changes, as applicable?
* [x] Have you successfully ran tests with your changes locally?
* [x] Does your submission pass tests, including CircleCI, Travis CI, and AppVeyor?
* [x] Does your submission have appropriate code coverage? The cutoff threshold is 95% by Coversall.

<!-- You can erase any parts of this template not applicable to your Pull Request. -->

### New Model Submissions:
* [x] Have you created a <NewModel>_example.py in ~/examples/?
* [x] Have you lint your code locally prior to submission?

----------------------

### Description

The idea of the algorithm is to create a pool of features based on the number of features passed by user. 
This pool will be the base of generating all categorical data.
Also, the user can specify the number of categories  in the normal points and in the outliers.
Added to that, the user can specify the number of informative features in the outlier points in which the higher the easier to detect and classify, whereas the lower the more redundant (non-informative and insignificant) the features in the outlier points which makes it more difficult to detect.

Required tests and example have been added.",105699750
1367,2019-05-14T14:45:23Z,2019-07-04T13:21:17Z,,,"### All Submissions Basics:
Closes #21 
* [x] Have you followed the guidelines in our Contributing document?
* [x] Have you checked to ensure there aren't other open [Pull Requests](../../pulls) for the same update/change?
* [x] Have you checked all [Issues](../../issues) to tie the PR to a specific one?

### All Submissions Cores:

* [x] Have you added an explanation of what your changes do and why you'd like us to include them?
* [x] Does your submission pass tests, including CircleCI, Travis CI, and AppVeyor?

### New Model Submissions:
* [x] Have you created a <NewModel>_example.py in ~/examples/?
* [x] Have you lint your code locally prior to submission?

------------------------------
## Description
There are several ways to convert *categorical* values to *numerical* ones in a given dataset, so `HBOS` can work with it.

I implemented 3 ways, left the option to the user to specify which by changing parameter `category` that has been added to `HBOS` Class.
Methods are:
 1. One Hot Encoding.
 2. Label Encoding.
 3. Frequency Ratio Encoding.

Since, as far as I am ware of, **PyOD** does not provide synthesized categorical data (*can be added on the list for future work* ;-) ) , I tested the implementation on 3 different real-world categorical datasets, namely: Breast Cancer, Car Evaluation, Tic Tac Toe. Which can be found in `HBOS_categorical_example.py` file. 

",105699750
1368,2020-03-26T22:21:04Z,2020-03-26T22:22:21Z,,,"Fixes #2748 
fixes #1689
relates #226

Do a quick substitution of `!` when creating the wrapped model.
This should clear all problems.

```r
  if (grepl(""!"", learner$package)) {
    learner$package = gsub(""!"", """", learner$package)
  }
```

# Reprex

``` r
library(mlr)
#> Loading required package: ParamHelpers
#> 'mlr' is in maintenance mode since July 2019. Future development
#> efforts will go into its successor 'mlr3' (<https://mlr3.mlr-org.com>).

learner <- makeLearner(""classif.kknn"",
  predict.type = ""response"")
#> Loading required package: kknn

model <- train(learner, iris.task)
model$learner$package
#> [1] ""kknn""
```

<sup>Created on 2020-03-26 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>",12465340
1369,2020-01-13T13:18:48Z,2020-02-12T10:55:00Z,,,"Here I added the fregre.glm learner from package fda.usc and the corresponding test script.


",12465340
1370,2020-03-20T18:06:26Z,2020-03-25T16:20:59Z,,,"Thanks for contributing.

- [x] Tick to sign-off your agreement to the [Developer Certificate of Origin (DCO) 1.1](https://developercertificate.org)

## Description

Current behavior of tablesaw behaves like a right outer join or full outer join should behave if the left table is empty. Based on review of the source code, this appears to be the case because the normal termination for left outer join and inner join is within a loop over the rows of the first table. This commit handles the special case of an empty left table for left outer join and inner join, correctly returning an empty table with the appropriate columns.

## Testing

Although it should be fairly simple to test this, I did not take the time to understand your testing framework and add a unit test.",48880766
1371,2020-03-16T07:01:39Z,2020-03-25T16:20:59Z,,,expose more configurable options for bubble plot.,48880766
1372,2020-03-03T16:44:47Z,2020-03-25T16:20:59Z,,,"Thanks for contributing.

- [X] Tick to sign-off your agreement to the [Developer Certificate of Origin (DCO) 1.1](https://developercertificate.org)

## Description

Closes https://github.com/jtablesaw/tablesaw/issues/771

Made `CsvReadOptions` and `CsvWriteOptions` use the same defaults. The options were nullable in `CsvReadOptions` and fell back to the univocity defaults whereas `CsvWriteOptions` was non-nullable and specified its own non-standard defaults. I changed `CsvWriteOptions` to be nullable and fallback to the univocity defaults.

## Testing

Added a unit test",48880766
1373,2020-01-30T22:22:01Z,2020-03-25T16:20:59Z,,,"- [x] Tick to sign-off your agreement to the [Developer Certificate of Origin (DCO) 1.1](https://developercertificate.org)

## Description

Introduces new option `ignoreZeroDecimal` to `ReadOptions`. This option controls whether a numeric value ending with "".0"" may be considered an integer (or short, long). Default value is `true` and retains the current behavior. If set to false, values ending with "".0"" will be not be considered an integer and instead be considered a floating point.

Since different software/systems have different opinions on this matter, an option to control this behavior could be justified. Will be useful when using TableSaw together with other systems.

The logic could be centralized in `AbstractColumnParser` if that is preferred. Open for other variable naming suggestions.

Relates to issue #747.

## Testing

Tests added to `CsvReaderTest` and `StringUtilsTest`.
",48880766
1374,2020-01-12T19:42:45Z,2020-03-25T16:21:00Z,,,"Thanks for contributing.

- [x] Tick to sign-off your agreement to the [Developer Certificate of Origin (DCO) 1.1](https://developercertificate.org)

## Description

Implemented two new features:
- Reading files via memory mapping
- Single-pass row sampling while reading CSVs

I ended-up implementing these features for a project where I needed to read large files. Please let me know if you are interested in merging any of these features and if you'd like to see any other changes before merging.

## Testing

Did you add a unit test?

Yes. And all existing tests are still passing after the changes.",48880766
1375,2019-11-22T10:22:27Z,2020-03-25T16:21:00Z,,,"Thanks for contributing.

- [x] Tick to sign-off your agreement to the [Developer Certificate of Origin (DCO) 1.1](https://developercertificate.org)

## Description

1. Added support for reading and writing Apache ORC file format 
 Fixes #620 

## Testing

Unit Test cases added
",48880766
1376,2019-11-04T12:33:26Z,2020-03-25T16:21:00Z,,,"Thanks for contributing.

- [x] Tick to sign-off your agreement to the [Developer Certificate of Origin (DCO) 1.1](https://developercertificate.org)

## Description

Adds `MarkerBuilder.addColorScale(double value, int r, int g, int b)` and related JS generation code which allows custom (non-palette) color scales.

## Testing

Yes.
",48880766
1377,2019-10-30T06:44:50Z,2020-03-25T16:21:01Z,,,"Thanks for contributing.

- [x] Tick to sign-off your agreement to the [Developer Certificate of Origin (DCO) 1.1](https://developercertificate.org)

## Description

Adds flag to parse percentage strings (10%) as doubles (0.1).

## Testing

Yes.
",48880766
1378,2019-06-13T04:25:52Z,2019-06-13T04:34:35Z,,,,65155092
1379,2018-11-01T10:17:40Z,2018-11-05T12:45:26Z,,,"So `evolutionary_search`  library did not pass their test cases. 

On `cv.EvolutionaryAlgorithmSearchCV` class, it extends `sklearn.model_selection._search.BaseSearchCV` which requires `_run_search` method to be implemented. This method is not implemented on the library and therefore, it threw the error 

```
TypeError: Can't instantiate abstract class EvolutionaryAlgorithmSearchCV with abstract methods _run_search
```

According to the implementation here at [sklearn_deep](https://github.com/rsteca/sklearn-deap), the method was ignored.

Therefore, I created a class that inherits `EvolutionaryAlgorithmSearchCV` and ignored that method. ",65155092
1380,2018-07-17T13:53:56Z,2018-07-17T14:05:06Z,,,"#407 

Doc fix for issue #407, `market` --> `model`",65155092
1381,2018-06-25T21:35:07Z,2018-06-25T21:46:08Z,,,Change multi-class scoring metric to f1_score and improve scoring return to only return negatives for metrics that you want higher values,65155092
1382,2018-06-25T20:10:32Z,2018-06-25T20:21:35Z,,,"auto_ml would crash when using python3 due to changed class names. This resolves issue #393 using the six package. Dependencies were updated to reflect that.
",65155092
1383,2018-04-25T11:22:36Z,2018-04-25T11:49:42Z,,,When loading a deep learning keras model form file without a previous training the keras_load_model function will not be imported. ,65155092
1384,2018-03-26T11:13:02Z,2018-03-28T07:02:24Z,,,fix _pickle_method for predictor.py,65155092
1385,2020-03-26T15:59:36Z,2020-03-27T10:13:59Z,,,,135673451
1386,2020-03-25T07:19:33Z,2020-03-27T08:16:54Z,,,"The tests cover:

* Search space generator for classic NAS
* DARTS
* ENAS
* Naive
* P-DARTS

I'm not gonna finish the tests for other examples in this release, because

* I need to adjust the distributed scripts to make CDARTS work on pipeline.
* I need to prepare ImageNet on pipeline machine to test SPOS and ProxylessNAS.
* I need to prepare glove and SST dataset on machine for TextNAS.
* Distributed NAS (NAS with nnictl) needs extra engineering effort on pipeline code.

The tests take ~25min on our pipeline, lengthening our pipeline to ~80min. I tried to reduce this time with minimum configurations (minimum epochs, minimum channels). Perhaps it can be further optimized without intrusive modification to example code, but it's not trivial to me.
",135673451
1387,2020-03-23T02:50:43Z,2020-03-26T16:52:17Z,,,,135673451
1388,2020-03-22T12:10:56Z,2020-03-25T03:01:39Z,,,"Currently, the reproduction couldn't be guaranteed, will keep polishing this PR.

Since the dataloader changed, we need re-do the hyper-parameter search. The performance of current setting: sst-2: 90.12, sst-5: 53.3",135673451
1389,2020-03-09T11:36:42Z,2020-03-27T10:50:25Z,,,"The implementation of the paper ""Population Based Training of Neural Networks"" on NNI",135673451
1390,2020-03-03T22:33:31Z,2020-03-23T09:13:08Z,,,"I will re-generate the commit history to make this patch clear. Expected commit order is shown as follow.
The items in _itatic_ font are not yet ready.
Some features may be moved to future PRs.

+ [x] mutable interface
+ [x] mutator interface
+ [x] ENAS
+ [ ] ENAS-specific static graph
+ [ ] DARTS
+ [ ] DARTS-specific static graph
+ [ ] docstring
+ [ ] _Proxyless NAS_ (under evaluation)
+ [ ] _General static graph interface_
+ [ ] _Port static graph interface to model compression_
+ [ ] _Other NAS algorithms_",135673451
1391,2020-02-18T03:07:59Z,2020-03-27T00:35:31Z,,,,135673451
1392,2020-02-10T10:16:51Z,2020-02-10T10:21:21Z,,,"
In the latest version, PyTorch or upper 1.0  _DataLoaderIter is no more exist. 

so `ImportError: cannot import name '_DataLoaderIter'` raise some Import error.(in gradient_selector/fginitialize.py)

I do some exception handling code the head of code. and replace `_DataLoaderIter` to `_BaseDataLoaderIter`.

Thanks for your works.",135673451
1393,2019-09-27T13:48:44Z,2020-02-07T07:31:49Z,,,"when doing setup for remote nni, counting .git folder may make total number of folder larger than 1,000(nni file transfer threshold). generally we don't need to include .git folder for remote execution for code. so I excluded .git folder when counting and zipping files. ",135673451
1394,2020-03-25T06:43:36Z,2020-03-25T07:12:40Z,,,"Addresses #852 

Changes:
 - Add ``Constant`` kernel
 - Add tests for ``Constant`` kernel",108053674
1395,2020-03-15T17:09:04Z,2020-03-18T07:55:03Z,,,"Formula from the
Handbook of Differential Entropy, J.V. Michalowicsz, J.M. Nichols, F. Bucholtz
p. 113, 114",108053674
1396,2020-03-14T16:23:14Z,2020-03-22T12:32:58Z,,,"Replace deprecated `tfb.AffineScalar` with `tfb.Shift(shift=...)(tfb.Scale(scale=...))` for `tfb.MaskedAutoregressiveFlow` as suggested in #448 . 
Pytest is failing but I can't figure out why.

Sorry for the previous PR, had an issue with mismatching users w/ google CLA...",108053674
1397,2020-02-27T16:37:06Z,2020-02-27T16:37:13Z,,,"When I first read this notebook, the statement `model._to_track = self` gave me an impression that `_to_track` must be a private property. However, on further reading and understanding of how tf.Module works (i.e. it does reflection on properties attached to it and if the property is of type tf.Module then it includes its variables in the graph) cleared the picture.

That said, I think usage of '_to_track' could mislead others (as it did to me) because of two reasons -
* First it gives an impression that a private property is being used because of underscore in front of it
* and then usage of 'track' in the name further solidifies ones reasoning that it must be a private property meant for tracking. Reason it could become problem is when some one wants to attach two tf.Modules to JointDistributionSequential, then they would start to wonder if they should use list etc. I hope you see how naming of a variable opens up confusion for someone who may not be an expert in the underlying mechanism. 

This revision simply add more comments on this aspect and even clarification on the name of the property being used to attach the tf.Module.  

> A side comment that has nothing to do with this revision - IMHO, overall I believe the design of tf.Module to allow someone to do this (i.e reliance on attaching properties to object) is not so great even though python (the dynamic language) allows it. It may be a good idea for TFP team to expose a proper construct/api to add submodules to help mitigate the impact of the design choice made by tf.Module. ",108053674
1398,2020-02-20T00:04:28Z,2020-03-09T23:06:22Z,,,Addresses #678 ,108053674
1399,2020-02-15T17:32:22Z,2020-02-15T18:10:03Z,,,"Following #791, this implements the `FillDiagonal` and `FillScaleDiagonal` bijectors.
I also added some tests, similar to those for `FillTriangular` and `FillScaleTriL`.",108053674
1400,2019-09-12T03:09:09Z,2020-01-27T21:28:35Z,,,"The line here is wrong:   `loc = k(t, x) @ inv(k(x, x) + v * I) @ (y - loc)` - `loc` is on both the right and left-hand side, and `y` is not defined in the context of the comment block.

I believe this term should be `f(x)` looking at formula 2.23 of Rasmussen and Williams, but worth reviewing.",108053674
1401,2019-09-10T18:55:45Z,2019-09-10T18:55:49Z,,,"One file changed: `tensorflow_probability/python/distributions/poisson.py`

Updated the datatype for poisson distribution, default is now `int32`.  Updated documentation to include `dtype` option with description.  Description is copied from the `dtype` description for the Bernoulli distribution. There does not appear to be any other dependency on Poisson `dtype`.",108053674
1402,2019-06-23T02:34:15Z,2019-08-08T19:20:11Z,,,"This problem fixes [issue 461](https://github.com/tensorflow/probability/issues/461). It might not be the best solution. 

Do you have any other ideas @saxena-ashish-g?",108053674
1403,2019-06-03T03:10:37Z,2019-09-19T17:40:39Z,,,"This pull request adds a new distribution, the generalized normal distribution variant 2, as defined on Wikipedia: https://en.wikipedia.org/wiki/Generalized_normal_distribution

This variant generalizes the Gaussian with a new parameter, the peak or shape, which controls skew and with mean and variance define an upper bound on the distribution. The classic Gaussian is the special case where this parameter is set to 0.",108053674
1404,2019-02-25T08:33:33Z,2019-04-03T15:14:13Z,,,"Added example on how to set up the CAPM model for stocks, and how this compares to the simpler GLM interface for Tensorflow Probability.

This one was NOT based on an example from the edward1 library.",108053674
1405,2019-02-25T08:19:22Z,2019-02-25T08:23:49Z,,,"Added JuPyter notebook detiling how to implement a Stochastic Block model using Tensorflow Probability. This was based on an example from the edward1 library.

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/stochastic_block_model.py), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/).

At the moment, this example runs into problems with Convergence. It appears that there were attempts to remedy this with Edward2 in a [previous PR](https://github.com/tensorflow/probability/pull/84), though there were also discussions of using TFP without Edward2. At the moment when the example is run, the following outputs are produced:
```
Result (label flip can happen):
Predicted
[0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0]

True
[0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1]

Adjusted Rand Index = -0.016866688290626087
```
Similar results happend with label switching. When looking back at the previous attempt, I found this:

> Also, I found [this issue](https://github.com/blei-lab/edward/pull/715) describing a similar problem. It made me think of a [paper](https://arxiv.org/pdf/1008.3926.pdf) where Newman (re?)introduces degree-corrected stochastic blockmodels. If you look at Figure 1, you see that the standard SBM performs poorly because two individuals have a disproportionately large degree. On the other hand, the degree-corrected model gives results that make complete sense.

So there seems to be disagreement about whether the original is the best model for this problem. However, I'd be happy to implement the degree-corrected version of this (since the author of the previous PR has gone silent). My main pressing question is whether or not Edward2 should be used in this example, of if we should continue to try with just the rest of TFP.
",108053674
1406,2019-02-25T08:09:13Z,2019-02-25T08:21:58Z,,,"Added JuPyter notebook detailing how to implement Latent Space Models with Tensorflow probability. This was based on an example from the edward1 library.

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/notebooks/latent_space_models.ipynb) and [this tutorial](http://edwardlib.org/tutorials/latent-space-models), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/). The initial version of this tutorial was written by Maja Rudolph. [David Ha](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/) also wrote a post on it.",108053674
1407,2019-02-25T08:03:27Z,2019-02-25T08:23:27Z,,,"Added Jupyter notebook detailing how to implement a Rasch Model [(Rasch, 1960)](https://en.wikipedia.org/wiki/Rasch_model) in Tensorflow Probability. This is based off an example from the edward1 library.

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/rasch_model.py), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/)",108053674
1408,2019-02-25T07:58:34Z,2019-02-25T08:23:17Z,,,"Added example JuPyter notebook detailing how to implement a Sigmoid Belief network in Tensorflow probability (based on an example from the edward1 library).

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/sigmoid_belief_network.py), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/)",108053674
1409,2019-02-25T07:50:41Z,2019-02-25T08:23:07Z,,,"Added Jupyter notebook demonstrating how to implement an LSTM in Tensorflow Probability (complete with custom diagrams). This was based on an example from th edward1 library.

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/lstm.py), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/).",108053674
1410,2019-02-25T07:48:31Z,2019-02-25T08:22:56Z,,,"Added Jupyter notebook detailing how to implement a Mixture Density Network in Tensorflow Probability. This is based on an example from the edward1 library.

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/notebooks/mixture_density_network.ipynb) and [this tutorial](http://edwardlib.org/tutorials/mixture-density-network), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/). Initial version of this tutorial written by Christopher Bonnett & David Ha.",108053674
1411,2019-02-25T07:45:55Z,2019-05-02T01:17:58Z,,,"Added Jupyter Notebook Detailing how to implement a Deep Exponential Family. This was based on an example from the edward1 library.

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/deep_exponential_family.py) and [this paper](http://www.cs.toronto.edu/~lcharlin/papers/def_aistats.pdf), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/)",108053674
1412,2019-02-25T07:35:49Z,2019-04-30T19:16:43Z,,,"Added Jupyter notebook detailing how to do supervised learning with Gaussian Processes. Ported from edward1 example. 

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/notebooks/supervised_classification.ipynb), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/)",108053674
1413,2019-02-25T07:27:24Z,2019-05-02T00:50:05Z,,,"Added Jupyter notebook detailing how to set up a Cox Process with Tensorflow Probability. This is ported from an example from the Edward1 Library.

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/cox_process.py), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/)",108053674
1414,2019-02-25T07:23:07Z,2019-02-25T08:22:19Z,,,"Added Jupyter notebook detailing how to do Factor Analysis with Tensorflow Probability. This is ported from an example from the Edward1 Library.

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/factor_analysis.py), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/)",108053674
1415,2019-02-25T07:16:11Z,2019-03-27T19:36:15Z,,,"Added Jupyter notebook detailing how to do Probabilistic Matrix Factorization with Tensorflow Probability. This is ported from an example from the Edward1 Library

Original content [this Repository](https://github.com/blei-lab/edward/blob/master/examples/probabilistic_matrix_factorization.py), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/)",108053674
1416,2018-12-07T14:06:01Z,2019-06-16T19:28:44Z,,,"TL;DR new version of `MultivariateNormalFullCovariance` that is backwards-compatible but also able to take `LinearOperator`-type covariance matrices as input.

I have previously touched on the need for a way to work with structured covariance matrices in both issue #161 and the closed PR #184 which demonstrated a prototype of this design. The concerns raised in the PR was that there were already too many variants of the `MultivariateNormal`. As suggested in the PR, this new version instead revamps the standard `MultivariateNormalFullCovariance` with a version that can take both tensors and linear operators as input when specifying the covariance. I have not touched the existing tests, so as it stands this new version should be completely compatible with existing code (assuming test coverage).
### Design decisions
I was unsure whether it would be worthwhile to inherit from any of the existing multivariate normal classes as `MultivariateNormalFullCovariance` used to do. I ultimately decided not to, basing it on the similar `MultivariateStudentT` instead, which was already using a `LinearOperator`-based design. This was mostly to ensure that a cholesky-factor was not computed in cases where that would not be efficient or desirable.  

 ### Incorporating `MultivariateNormalTril`
This could eventually subsume the need for `MultivariateNormalTril`, but currently `MultivariateNormalFullCovariance` depends on it for sampling which is most easily done with a Cholesky factor. In addition, `MultivariateNormalTril` is slightly more efficient since it only has to solve the system `inv(L)*x` once and then take a norm, as opposed to a linear operator-based solve which will always solve twice even if it knows the cholesky decomposition `inv(L*L')*x`. Since `LinearOperator` is getting cholesky support, it might make sense to also have special methods for `x'*A*x` and `x'*inv(A)*x` as well as a PSD linear operator specified via a cholesky factor.",108053674
1417,2018-11-15T21:28:51Z,2019-02-26T23:13:03Z,,,"Added a Jupyter notebook for a doubly-stochastic Poisson process, with the example being simulating the numbers of shots taken from positions on a Basketball Court by 308 NBA Players.",108053674
1418,2018-10-01T03:58:05Z,2018-10-29T21:05:26Z,,,Fixes issue #69 ,108053674
1419,2018-09-24T23:13:43Z,2019-03-18T19:15:16Z,,,Change from a bash script to a python script. Bazel build works correctly on windows. See discussion here: https://github.com/tensorflow/probability/pull/162,108053674
1420,2018-07-05T20:38:19Z,2018-10-29T23:47:05Z,,,"I'm adding a tutorial for probabilistic PCA by basically adapting the original Edward's tutorial but with HMC inference instead of VI. I would appreciate reviews as I'm new to both Tensorflow and Bayesian statistics. I haven't understood how to use VI framework of Tensorflow Probability yet, but I'll add that too in the inference section, once I figure it out.",108053674
1421,2018-06-20T07:22:40Z,2018-11-29T08:04:57Z,,,"I took the example from Edward and tried to implement it with TFP. Since I could not find an implementation of MAP, I implemented the computation of the loss as well.

However, I could not get the optimization to converge. I must be doing something wrong (first time using tensorflow), and I started this PR to get some help.",108053674
1422,2020-03-27T01:43:16Z,2020-03-27T10:57:21Z,,,"A ipython notebook which benchmarks Sklearn vs Shogun and highlights our API. Does this suffice for our ipython notebook for ""RandomFourierFeatures""? @karlnapf. 

",1555094
1423,2020-03-26T19:13:50Z,2020-03-27T05:14:50Z,,,"Refer #3555 
All tests passed!!",1555094
1424,2020-03-26T14:05:54Z,2020-03-27T07:35:25Z,,,fix #4847 ,1555094
1425,2020-03-24T23:23:57Z,2020-03-27T06:52:46Z,,,"remove `add` from SGVector
remove operator`+` from SGVector
remove operator`+=` from SGVector
refactor code to call `linalg::add` method
#2747
#1139
#2582

Code compiles successfully
While running `make test`, a few unit tests failed but they were failing on the original code(before making these changes) as well.",1555094
1426,2020-03-24T01:05:37Z,2020-03-27T09:45:17Z,,,"Hello, I have made a fix required in this issue https://github.com/shogun-toolbox/shogun/issues/4954 
I hope this helps.",1555094
1427,2020-03-23T05:32:17Z,2020-03-27T04:11:20Z,,,"Tried to refactor some code using our linalg interface.I've deliberately left some parts as I've some doubts regarding it. I've done the benchmarks, here are the results

**Without changes**
![wihtoutchange](https://user-images.githubusercontent.com/45007169/77284907-d2582200-6cf5-11ea-8883-c07f3167f873.png)

**With changes**
![withchanges](https://user-images.githubusercontent.com/45007169/77284923-dbe18a00-6cf5-11ea-8fdb-d04bd920c579.png)

As we can see, the gain isn't substantial, more like +- 1 seconds.


**NOTE**: This PR needs rebase, the current build is breaking in my computer, so I've made PR w.r.t to the latest build which was working.



Thoughts? @karlnapf , @gf712 ?",1555094
1428,2020-03-20T12:24:06Z,2020-03-25T18:32:18Z,,,"Fix so that get_log_density() works even after passing Feature.  @vigsterkr , comments?",1555094
1429,2020-03-20T09:49:35Z,2020-03-20T12:59:23Z,,,"Shogun depends on graph, not the other way around",1555094
1430,2020-03-19T05:09:19Z,2020-03-25T18:29:18Z,,,"Removed `.py` file.
Added `.sg.in` file.
Compiled successfully on local machine.
#3555 #3000 #4942 
Sorry for creating a new PR, had deleted the old branch by mistake.",1555094
1431,2020-03-16T17:51:14Z,2020-03-17T11:52:49Z,,,"Hashes required:
- [x] Abstract node -> may contain runtime values
- [ ] Runtime node -> knows static shape and input. The graph can use this to find out if it requires recomputation of the graph
- [ ] Tensor hash with fingerprint (might be handled in PR by @vigsterkr)

The hash of the nodes is a combination of all hashes of all previous nodes. This might be too expensive for very simple graphs?",1555094
1432,2020-03-14T06:14:21Z,2020-03-20T11:05:15Z,,,,1555094
1433,2020-03-14T00:17:52Z,2020-03-18T07:56:58Z,,,"Tada
──────────▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
────────█═════════════════█
──────█═════════════════════█
─────█════════▄▄▄▄▄▄▄════════█
────█════════█████████════════█
────█═══════██▀─────▀██═══════█
───███████████──█▀█──███████████
───███████████──▀▀▀──███████████
────█═══════▀█▄─────▄█▀═══════█
────█═════════▀█████▀═════════█
────█═════════════════════════█
────█══════▀▄▄▄▄▄▄▄▄▄▀════════█
───▐▓▓▌═════════════════════▐▓▓▌
───▐▐▓▓▌▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▐▓▓▌▌
───█══▐▓▄▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▄▓▌══█
──█══▌═▐▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▌═▐══█
──█══█═▐▓▓▓▓▓▓▄▄▄▄▄▄▄▓▓▓▓▓▓▌═█══█
──█══█═▐▓▓▓▓▓▓▐██▀██▌▓▓▓▓▓▓▌═█══█
──█══█═▐▓▓▓▓▓▓▓▀▀▀▀▀▓▓▓▓▓▓▓▌═█══█
──█══█▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓█══█
─▄█══█▐▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▌█══█▄
─█████▐▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▌─█████
─██████▐▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▌─██████
──▀█▀█──▐▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▌───█▀█▀
─────────▐▓▓▓▓▓▓▌▐▓▓▓▓▓▓▌
──────────▐▓▓▓▓▌──▐▓▓▓▓▌
─────────▄████▀────▀████▄
─────────▀▀▀▀────────▀▀▀▀",1555094
1434,2020-03-13T15:03:44Z,2020-03-14T18:58:09Z,,,,1555094
1435,2020-03-13T09:22:30Z,2020-03-20T04:26:21Z,,,"This is a feature that was required to be added to CI of shogun to make checking of broken links in Ipython notebooks more streamlined
(Usability add the path of the Ipython notebook file as a command line argument ,OUTPUT- Links that are broken)
also closes issue no. #3702 ",1555094
1436,2020-03-12T11:39:24Z,2020-03-25T18:23:18Z,,,"@gf712  @vigsterkr , Any fixes recommended??",1555094
1437,2020-03-10T02:33:31Z,2020-03-19T05:47:41Z,,,In response to https://github.com/shogun-toolbox/shogun/issues/4259,1555094
1438,2020-03-06T17:23:03Z,2020-03-06T17:34:13Z,,,,1555094
1439,2020-03-04T19:29:24Z,2020-03-10T02:39:43Z,,,Changed Kmeans parameter to cluster_centers.,1555094
1440,2020-03-03T21:04:34Z,2020-03-03T21:52:39Z,,,testing on the fly 🤷‍♂ ,1555094
1441,2020-02-29T21:18:06Z,2020-03-03T20:36:15Z,,,"I'm having some doubts please help me clarify that. In kernel_linear.py, it has ```kernel.set_normalizer(AvgDiagKernelNormalizer(1.2))``` so I created ```Normalizer avg_diag_normalizer = normalizer(""AvgDiagKernelNormalize"") ``` which I think may be wrong. Could you guide me from here? :) ",1555094
1442,2020-02-27T20:06:55Z,2020-03-01T08:45:03Z,,,,1555094
1443,2020-02-27T19:19:15Z,2020-03-03T05:03:39Z,,,"Fixed typo in the section ""Adding new examples""",1555094
1444,2020-02-26T17:36:33Z,2020-03-23T15:48:35Z,,,"Updated conf.py.in and braycurtis.rst to reflect changes in the hyperlink of the **BrayCurtis Distance** in the reference  section of 
 https://www.shogun-toolbox.org/examples/latest/examples/distance/braycurtis.html ",1555094
1445,2020-02-26T10:46:38Z,2020-02-29T08:23:54Z,,,"Helps in Fixing #4817

This adds the functionality to print entire code from a file generated by a metaexample, I have tested it and rendered cookbooks and it works well.

The command to be added to .rst files is 
```.. sgexample:: filename.sg:entire```

I will create a python script to add this to each file if this is merged.",1555094
1446,2020-02-01T09:18:36Z,2020-02-26T18:03:49Z,,,"This allows us to integrate with SciRuby stack, see the example notebook here: https://gist.github.com/vigsterkr/48a5f0523528bbab85bec04464ca2b6c

- [x] define typemaps for SGVector and SGMatrix
- [ ] fix SGMatrix -> NMatrix conversion: SGMatrix is column major while NMatrix is row-major, hence the `rb_nmatrix_dense_create` will not give the right output.
- [ ] Fix unsigned type (see FIXME's) mapping
- [ ] allow switching between `narray` typemap and `nmatrix` (compile time, i.e. cmake option)
- [ ] change meta example generator for ruby to use NMatrix instead of NArray

",1555094
1447,2020-01-10T07:32:25Z,2020-02-26T17:29:23Z,,,This PR resolves https://github.com/shogun-toolbox/shogun/issues/4797. Currently I have added the forward pass computation of the Mish Function. ,1555094
1448,2020-01-03T13:36:27Z,2020-02-26T11:55:27Z,,,Fix #4795 ,1555094
1449,2019-11-09T12:37:49Z,2019-11-16T15:55:00Z,,,,1555094
1450,2019-10-23T12:28:19Z,2020-02-14T05:17:37Z,,,fix #4783,1555094
1451,2019-10-21T16:07:39Z,2019-10-22T12:59:17Z,,,This kernel will be useful for Bayesian optimisation. ,1555094
1452,2019-10-06T05:46:36Z,2019-10-06T05:46:36Z,,,,1555094
1453,2019-10-04T22:25:06Z,2019-10-04T22:25:06Z,,,"Requesting review
Adding LevensteinDistance.{cpp/h} and corresponding unit-tests for plain strings
Files:
<root>/src/shogun/distance/LevensteinDistance.cpp
<root>/src/shogun/distance/LevensteinDistance.h
<root>/tests/unit/distance/Levenstein_unittest.cc",1555094
1454,2019-10-04T19:08:05Z,2019-10-22T18:24:55Z,,,"fixes #4776

[The documentation of cmake_minimum_required](https://cmake.org/cmake/help/latest/command/cmake_minimum_required.html) says:

> Note: Call the cmake_minimum_required() command at the beginning of the top-level CMakeLists.txt file even before calling the project() command. It is important to establish version and policy settings before invoking other commands whose behavior they may affect. See also policy CMP0000.",1555094
1455,2019-09-11T10:18:54Z,2019-12-26T09:55:09Z,,,"do not merge as needs to be tested with cmake that actually supports `target_precompile_headers` command (3.16 or later, which as of this point has not been released yet)",1555094
1456,2019-07-09T09:46:34Z,2020-02-26T16:06:19Z,,,"## testing
* [x] use sgobject iterator to instantiate and test all machines from a given list
* [x] individual ignore lists for each test (runtime)
* [ ] automate header and machine name extraction from `trained_model_serialization.cc.py`
* [ ] separate data generating code into environment
* [ ] clean up

## tests
* [x] consistency of views/subset with subsampled(no-view, same data)
* [x] training/apply leaves data unchanged
* [x] training thread consistency
* [x] test cross-validation thread consistency
* [x] import trained_model_serialization tests
* [ ] delete old trained_model_serialization tests
* [x] training without initializing does throw an exception (and doesnt crash)
* [ ] document non-working machines in ignore lists
",1555094
1457,2019-06-12T23:19:23Z,2020-02-26T16:17:23Z,,,,1555094
1458,2019-06-09T15:47:23Z,2019-11-11T10:43:04Z,,,,1555094
1459,2019-06-08T14:29:55Z,2020-02-26T16:18:42Z,,,,1555094
1460,2019-06-06T17:44:47Z,2020-02-26T17:45:58Z,,,"This is done by creating another class `CachedKernel` that is composed of the original kernel, in addition to the caching data/methods.",1555094
1461,2019-05-30T14:54:13Z,2020-02-26T16:21:26Z,,,,1555094
1462,2019-05-28T15:30:11Z,2020-02-03T21:44:22Z,,,"Here is a simple first draft of how the SG_ADD macros could be replaced with methods. I try to make the variadic argument pack somewhat type safe by adding some checks. This could probably be done inside a macro, but this way we can remove all the macro ""overloads"" of SG_ADD which are annoying to deal with and borderline portable imo because the preprocessor behaviour is not specified in the standard...

The variadic pack is required because a parameter could require both a constraint function #4647 and an automatic initialiser:
```cpp
sg_add(&k, ""k"", ""k, the number of clusters"", 
ParameterProperties::HYPER | ParameterProperties::AUTO | ParameterProperties::CONSTRAIN,
std::make_shared<my_kmeans_heuristic>(this), positive<>);
``` 

This also brings into question whether ParameterProperties::AUTO and ParameterProperties::CONSTRAIN have to be explicitly passed to the method",1555094
1463,2019-05-28T11:00:52Z,2020-02-26T16:23:05Z,,,"Enables mapping user-provided labels into the required internal algorithm representation.

This roughly resembles what we discussed @lisitsyn @gf712 I created a drop-in replacement for `CBinaryLabels`, which replaces the provided labels inside `CMachine::train`. These perform shared-memory (if possible) on-the-fly mappings to the required internal label space when the usual access functions of `CDenseLabels` are used. The mapping code is self-contained and the drop-in functionality is achieved via adding a mixing to the desired label classes.

The user gets back prediction labels in the same space/class as she provided in `train`.

I am looking for high level feedback for now, will tune this stuff later on.",1555094
1464,2019-05-03T13:50:50Z,2020-02-04T14:24:47Z,,,"adds [OpenML](https://www.openml.org/home) natively to shogun.

TODO:
- C++ specific
- [x] write GET
- [ ] write POST
- [x] write OpenML flow class
- [x] write OpenML task
- Shogun specific
- [x] write OpenML to SGObject
- [ ] write SGObject to OpenML
- [ ] expose OpenML to all interfaces
- [x] write shogun extension for runs with flows 
- Maintenance stuff
- [x] figure out how to split all the classes, i.e. files and directories in shogun",1555094
1465,2019-05-02T14:26:22Z,2020-02-04T16:29:26Z,,,"Simple ARFF deserialiser. See the spec [here](https://waikato.github.io/weka-wiki/arff_stable/).

@karlnapf shall I add a ARFF file as an example and a unittest/meta example?

TODO:
- [x] datetime format
- [x] string
- [x] handle missing data",1555094
1466,2019-03-31T14:35:15Z,2020-02-26T16:28:03Z,,,"Proposed API (in python):

OPTION 1: change nested object parameter values with a string 
```python
import numpy as np   
import shogun as sg   
kernel = sg.kernel(""GaussianKernel"")   
svm = sg.machine(""LibSVM"", kernel=kernel)   
ps = sg.GridParameters(svm) 
ps.attach(""kernel::log_width"", np.array([1., 2.])) 
ps.attach(""kernel::combined_kernel_weight"", np.array([1., 2.])) 
ps.attach(""C1"", np.array([1., 2.])) 
ps.attach(""C2"", np.array([1., 2.])) 
for x in range(16):
    print(ps.next_combination()) 
```
OPTION 2: change nested object parameters by manipulating the nested node directly, before adding it to the tree
```python
import numpy as np   
import shogun as sg   
kernel = sg.kernel(""GaussianKernel"")   
svm = sg.machine(""LibSVM"")
ps = sg.GridParameters(svm) 
kernel1 = sg.kernel(""PolyKernel"") 
node1 = sg.GridParameters(kernel1) 
node1.attach(""degree"", np.array([1.,2,3])) 
ps.attach(""kernel"", node1) 
ps.attach(""C1"", np.array([1., 2.])) 
ps.attach(""C2"", np.array([1., 2.]))
for x in range(12):
    print(ps.next_combination())
```
OPTION 1 and 2: combine both options, but option 1 becomes unavailable when there are several nodes representing a single object, i.e. kernel=[GaussianKernel, PolyKernel]
```python
import numpy as np 
import shogun as sg 

kernel1 = sg.kernel(""GaussianKernel"")  
kernel2 = sg.kernel(""PolyKernel"") 
svm = sg.machine(""LibSVM"", kernel=kernel1)

node = sg.GridParameters(svm)
node.attach(""kernel::log_width"", np.array([1.]))
node.attach(""C1"", np.array([1., 2., 3.])) 
node.attach(""C2"", np.array([1., 2., 3.])) 


node2=sg.GridParameters(kernel2)
node2.attach(""degree"", np.array([1.,2.]))

node.attach(""kernel"", node2)

for x in range(20):
    print(node.next_combination()) 
```

- [x] implement option 1
- [x] implement option 2 
- [x] implement GridSearch engine",1555094
1467,2019-03-18T12:06:08Z,2020-02-26T16:29:04Z,,,Estimate log_width as described in https://arxiv.org/pdf/1707.07269.pdf,1555094
1468,2019-03-16T11:37:15Z,2020-02-26T16:30:05Z,,,@karlnapf this is what I did in the openml branch to get binary features from multiclass. Is this what you meant?  ,1555094
1469,2019-01-20T19:36:26Z,2020-02-26T16:34:00Z,,,"First stab at #4455 using Concepts for vectorized rng.

Currently this will only work with GCC which provides Concepts TS support based on certain flags. I've tried to make the impl compatible with either Concepts TS or C++20 concepts.",1555094
1470,2019-01-09T11:58:18Z,2020-02-26T16:41:30Z,,,,1555094
1471,2018-12-15T21:03:23Z,2020-02-26T16:39:18Z,,,"Addresses #4430 

Only adds Uniform Random capability for now:
- [x] SGMatrix
- [x] SGVector
- [x] Benchmark

Edit: Will add Normal random in a different PR.",1555094
1472,2018-05-31T11:02:02Z,2020-02-26T16:44:36Z,,,"[WIP]
Adapt machine api to fit + predict
This need some heavy refactor, so let's start from linear machines and iterate a few times.
The idea is to keep both old and new apis working and then remove old apis gradually. 
`train_machine(CFeatures*, CLabels*)` is added to machines that need labels. The old method, `train_machine(CFeatures*)` will redirect to the new api and pass `m_labels` as the labels argument. In this way, the old api (`set_labels` + `train(CFeatures*)`) still works. 

## Roadmap 
- [x] Add `void fit(CFeatures*)`, `void fit(CFeatures*, CLabels*)` to `CMachine`
- [x] Redirect `bool train_machine(CFeatures*)` to `void train_machine(CFeatures*, CLabels*)` in LinearMachine
- [ ] Calling new api in unittests
- [ ] Move api redirection to `LinearMachine` as a base class method after the new api works for all `LinearMachine` subclasses
- [ ] Make `bool train_machine(CFeatures*)` and `bool train_machine_templated(CFeatures*)` return void (The latter one should be easier as it is added last year and we haven't used in many places)

Known issues for moving to const methods:
* ref counting
* labels factory (e.g. `regression_labels`) doesn't accept const args",1555094
1473,2018-01-26T06:26:44Z,2020-02-26T16:47:56Z,,,"According to SGObjectAll test case, registered parameters in tags to match old parameters for all classes. 

Exceptions:
For class LatentSOSVM & SVMOcas, old parameters and tag parameters don't match yet.(I am not able to find those classes, please help me find those). 

@karlnapf There are many classes in this PR where static_cast on parameters preset. you asked in PR #4116 not add watch_param call & to comment add tags call in separate PR.
Would you like me to do this all together in separate PR? (I am curious what is the exact reason for that suggestion)",1555094
1474,2016-06-23T13:32:52Z,2020-02-26T17:31:49Z,,,"- `CSparseFeatures` instances now work with plain `CEuclideanDistance`. This would enable us using sparse features in GaussianKernel, for example.
",1555094
1475,2018-06-06T22:23:37Z,2018-06-06T22:23:37Z,,,"In file server.py 
```python 
from gevent.wsgi import WSGIServer
```
Has to be changed to:

```python 
from gevent.pywsgi import WSGIServer
```

http://www.gevent.org/api/gevent.pywsgi.html

",84231689
1476,2019-06-20T00:43:14Z,2019-06-20T02:14:51Z,,,Adds support for django 2.2 and improves setup.py so Django 2+ is not installed on python2.7,31502883
1477,2017-04-07T12:58:58Z,2017-04-07T13:19:45Z,,,This doc looks great in dillinger.io but doesnt seem to work in my github fork...you may need to adjust the markdown further.,31502883
1478,2015-08-27T15:36:52Z,2016-08-16T03:15:12Z,,,"This commit supports loading scripts from remote URLs
and zip or gzip files. Archives are automatically de-compressed
and all contained files imported.

Folders are now walked recursively (as archives commonly have a
first level folder) and all found scripts imported.

Will need to handle the case of pre-existing scripts (by name?)
to be able to support automatic updating without creating
multiple script objects.
",31502883
1479,2020-03-12T21:38:25Z,2020-03-13T17:06:37Z,,,"### Description
This PR is to implement both use_clones and fit_base_estimators (previously called refit in EnsembleVoteClassifier) for EnsembleVoteClassifier and StackingClassifier. As well as added corresponding tests for these two parameters.

Will have another PR to address StackingRegressor and StackingCVRegressor.
<!--  
Please insert a brief description of the Pull request here
-->

### Related issues or pull requests
Towards #627. See also #383.
<!--  
If applicable, please link related issues/pull request here. E.g.,   
Fixes #366
-->

### Pull Request Checklist

- [x] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file (if applicable)
- [x] Added appropriate unit test functions in the `./mlxtend/*/tests` directories (if applicable)
- [x] Modify documentation in the corresponding Jupyter Notebook under `mlxtend/docs/sources/` (if applicable)
- [x] Ran `PYTHONPATH='.' pytest ./mlxtend -sv` and make sure that all unit tests pass (for small modifications, it might be sufficient to only run the specific test file, e.g., `PYTHONPATH='.' pytest ./mlxtend/classifier/tests/test_stacking_cv_classifier.py -sv`)
- [x] Checked for style issues by running `flake8 ./mlxtend`


<!--NOTE  
Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).
For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/  
-->
",22937749
1480,2019-12-22T18:57:00Z,2019-12-29T15:00:52Z,,,"add fixed_features

### Description

<!--  
Adds a new fixed_features parameter to Exhaustive Feature Selection for features to be forced to be included in the results.
-->

### Related issues or pull requests

<!--  #579  
Fixes #579 
https://github.com/rasbt/mlxtend/issues/579
-->

### Pull Request Checklist

- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file (if applicable)
- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories (if applicable)
- [ ] Modify documentation in the corresponding Jupyter Notebook under `mlxtend/docs/sources/` (if applicable)
- [ ] Ran `PYTHONPATH='.' pytest ./mlxtend -sv` and make sure that all unit tests pass (for small modifications, it might be sufficient to only run the specific test file, e.g., `PYTHONPATH='.' pytest ./mlxtend/classifier/tests/test_stacking_cv_classifier.py -sv`)
- [ ] Checked for style issues by running `flake8 ./mlxtend`


<!--NOTE  
Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).
For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/  
-->
",22937749
1481,2019-12-21T22:04:00Z,2019-12-22T16:39:55Z,,,"### Description

<!--  
added random subsampling during feature selection
-->

### Related issues or pull requests

Fixes #47 

<!--  
If applicable, please link related issue to #47 
-->

### Pull Request Checklist

- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file (if applicable)
- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories (if applicable)
- [ ] Modify documentation in the corresponding Jupyter Notebook under `mlxtend/docs/sources/` (if applicable)
- [ ] Ran `PYTHONPATH='.' pytest ./mlxtend -sv` and make sure that all unit tests pass (for small modifications, it might be sufficient to only run the specific test file, e.g., `PYTHONPATH='.' pytest ./mlxtend/classifier/tests/test_stacking_cv_classifier.py -sv`)
- [ ] Checked for style issues by running `flake8 ./mlxtend`


<!--NOTE  
Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).
For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/  
-->
",22937749
1482,2019-12-20T19:02:54Z,2019-12-21T21:51:03Z,,,"

### Description

－Added EnsembleVotingRegressor in regressor folder
－also including my own test file in the test folder.

### Related issues or pull requests

<!--  
Fix #602
-->

### Pull Request Checklist

- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file (if applicable)
- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories (if applicable)
- [ ] Modify documentation in the corresponding Jupyter Notebook under `mlxtend/docs/sources/` (if applicable)
- [ ] Ran `PYTHONPATH='.' pytest ./mlxtend -sv` and make sure that all unit tests pass (for small modifications, it might be sufficient to only run the specific test file, e.g., `PYTHONPATH='.' pytest ./mlxtend/classifier/tests/test_stacking_cv_classifier.py -sv`)
- [ ] Checked for style issues by running `flake8 ./mlxtend`


<!--NOTE  
Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).
For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/  
-->",22937749
1483,2019-12-18T10:31:20Z,2020-01-29T02:38:51Z,,,"### Description

Implement apriori-gen as in original Apriori paper.
This is a draft PR for discussion; different changes are proposed, they should be benchmarked.

- First commit implements join step
- Second commit implements prune step
- Third commit enforces `low_memory=True` processing; thanks to previous optimizations, it is now as fast as `low_memory=False` and requires less memory; frequent itemsets are stored as list of tuples instead of Numpy arrays
- Fourth commit replaces trie implementation by set
- Fifth commit replaces `_support` function, which was really slow on some test cases

### Related issues or pull requests

Reported in #644.

### Pull Request Checklist

- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file (if applicable)
- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories (if applicable)
- [ ] Modify documentation in the corresponding Jupyter Notebook under `mlxtend/docs/sources/` (if applicable)
- [ ] Ran `PYTHONPATH='.' pytest ./mlxtend -sv` and make sure that all unit tests pass (for small modifications, it might be sufficient to only run the specific test file, e.g., `PYTHONPATH='.' pytest ./mlxtend/classifier/tests/test_stacking_cv_classifier.py -sv`)
- [ ] Checked for style issues by running `flake8 ./mlxtend`

Checklist empty for now since this is a draft pull request.
<!--NOTE  
Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).
For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/  
-->
",22937749
1484,2019-12-09T03:51:01Z,2019-12-09T04:06:17Z,,,"### Description
This PR will automatically set the cross-validation score of an ordinary least-squares fit to `-np.inf` whenever the fit fails during sequential feature selection.
This ensures that whenever a least-squares fit fails, it does not fail the entire regression process, but simply goes to the next one (after all, if a fit fails, it simply means that that fit is terrible and should never be used, not that no fit ever will work).

Also, I made the modification in `master`, simply because it is a one-liner and I will be deleting the fork anyways afterward.
I can add the change to the `CHANGELOG` is required of course.

### Pull Request Checklist

- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file (if applicable)
- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories (if applicable)
- [ ] Modify documentation in the corresponding Jupyter Notebook under `mlxtend/docs/sources/` (if applicable)
- [ ] Ran `PYTHONPATH='.' pytest ./mlxtend -sv` and make sure that all unit tests pass (for small modifications, it might be sufficient to only run the specific test file, e.g., `PYTHONPATH='.' pytest ./mlxtend/classifier/tests/test_stacking_cv_classifier.py -sv`)
- [ ] Checked for style issues by running `flake8 ./mlxtend`
",22937749
1485,2018-12-10T16:40:58Z,2019-03-27T18:37:36Z,,,"### Description

A module for photometric image quality measure

- Contrast
- Brightness
- Focus and sharpness
- Illumination
- Face image quality index (FQI) (Ref: proposed in [here](https://ieeexplore.ieee.org/document/6985846))




### Pull Request Checklist

- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file (if applicable)
- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories (if applicable)
- [ ] Modify documentation in the corresponding Jupyter Notebook under `mlxtend/docs/sources/` (if applicable)
- [ ] Ran `nosetests ./mlxtend -sv` and make sure that all unit tests pass (for small modifications, it might be sufficient to only run the specific test file, e.g., `nosetests ./mlxtend/classifier/tests/test_stacking_cv_classifier.py -sv`)
- [ ] Checked for style issues by running `flake8 ./mlxtend`

",22937749
1486,2018-09-25T01:53:45Z,2018-09-28T16:14:05Z,,,"### Description

**This is work-in-progress**

Refactors existing stacking estimators with class inheritance structure.  More specifically, classes are related as below.  The real code is a bit more complex with multiple inheritance.

```
StackingEstimator (new, ""abstract"")
   |-- StackingRegressor
   |-- StackingClassifier
   |-- StackingCVEstimator (new, ""abstract"")
       |-- StackingCVRegressor
       |-- StackingCVClassifier
```

In addition, following arguments are renamed:

* `refit` --> `use_clones` in `StackingRegressor` and `StackingCVRegressor`
Following option is added
* `verbose` to `StackingRegressor`

Although there are still some tests fail and bug fix is going, I would like to have your thoughts as of now.

<!--  
Please insert a brief description of the Pull request here
-->

Fixes #444 


### Related issues or pull requests

<!--  
If applicable, please link related issues/pull request here. E.g.,   
Fixes #366
-->



### Pull Request Checklist

- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file (if applicable)
- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories (if applicable)
- [ ] Modify documentation in the corresponding Jupyter Notebook under `mlxtend/docs/sources/` (if applicable)
- [ ] Ran `nosetests ./mlxtend -sv` and make sure that all unit tests pass (for small modifications, it might be sufficient to only run the specific test file, e.g., `nosetests ./mlxtend/classifier/tests/test_stacking_cv_classifier.py -sv`)
- [ ] Checked for style issues by running `flake8 ./mlxtend`




<!--NOTE  
Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).
For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/  
-->
",22937749
1487,2017-11-02T06:38:44Z,2017-11-02T21:38:24Z,,,"Adds a `loadings_` attribute to `LinearDiscriminantAnalysis` to compute the factor loadings of the features on the components (discrimnants).

Adds a `tol` parameter to `LinearDiscriminantAnalysis` to threshold small eigenvalues, which are due to floating point imprecision, to zero for numerical stability",22937749
1488,2017-10-31T19:55:49Z,2017-11-01T17:55:45Z,,,"### Description

Added ‘n_cv_repeats’ parameter and extended functionality with RepeatedStratifiedKFold and RepeatedKFold to allow repeating cross-validation for both classifiers and regressors respectively.

### Related issues or pull requests

Fixes #271 

### Pull Request requirements

- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories
- [x] Ran `nosetests ./mlxtend -sv` and make sure that all unit tests pass
- [x] Checked the test coverage by running `nosetests ./mlxtend --with-coverage`
- [ ] Checked for style issues by running `flake8 ./mlxtend`
- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file
- [ ] Modify documentation in the appropriate location under `mlxtend/docs/sources/` (optional)
- [ ] Checked that the Travis-CI build passed at https://travis-ci.org/rasbt/mlxtend




<!--
NOTE

Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).

For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/
-->
",22937749
1489,2017-09-22T04:16:27Z,2019-10-03T03:23:19Z,,,"<!-- Please read the following guidelines for new Pull Requests -- thank you! -->

<!--
Make sure that you submit this pull request as a separate topic or feature branch and not as master branch. The new feature branch of your fork will then be merged to the master branch of the original repository, following the ""Fork-and-Branch Git Workflow:""

1. Fork the original GitHub project
2. Clone the fork to your local machine
3. Create a new topic branch
4. Make your code changes to this new topic branch
5. Commit the changes and push the commit to the topic branch to your fork upstream on GitHub
6. Create a new pull request from the upstream topic branch to the master branch of the original repo
-->

<!-- Provide a small summary describing the Pull Request below -->

### Description

This PR aims to add fit parameter support for `StackingClassifier`, `StackingCVClassifier`, and `EnsembleVoteClassifier`. 

### Related issues or pull requests

<!-- Please provide a link to the respective issue on the [Issue Tracker](https://github.com/rasbt/mlxtend/issues) if one exists. E.g.,

Fixes #<ISSUE_NUMBER> -->

Fixes #177, fixes #178, fixes #179 

<!-- Below is a general todo list for typical pull request -->

### Pull Request requirements

- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories
- [ ] Ran `nosetests ./mlxtend -sv` and make sure that all unit tests pass
- [ ] Checked the test coverage by running `nosetests ./mlxtend --with-coverage`
- [ ] Checked for style issues by running `flake8 ./mlxtend`
- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file
- [ ] Modify documentation in the appropriate location under `mlxtend/docs/sources/` (@rasbt will take care of that)
- [ ] Checked that the Travis-CI build passed at https://travis-ci.org/rasbt/mlxtend




<!--
NOTE

Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).

For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/
-->
",22937749
1490,2017-05-13T21:19:48Z,2019-10-02T20:41:37Z,,,"<!-- Please read the following guidelines for new Pull Requests -- thank you! -->

<!--
Make sure that you submit this pull request as a separate topic or feature branch and not as master branch. The new feature branch of your fork will then be merged to the master branch of the original repository, following the ""Fork-and-Branch Git Workflow:""

1. Fork the original GitHub project
2. Clone the fork to your local machine
3. Create a new topic branch
4. Make your code changes to this new topic branch
5. Commit the changes and push the commit to the topic branch to your fork upstream on GitHub
6. Create a new pull request from the upstream topic branch to the master branch of the original repo
-->

<!-- Provide a small summary describing the Pull Request below -->

Fix for issue #176 

### Description

1. Uses the appropriate `predict` or `predict_proba` method for a classifier depending on which model scoring function is being examined via hints in the function definition.
2. Uses `make_scorer` to provide an interface for the scoring methods.
3. Potential TODO (not implemented in this PR): allow the caller to explicitly specify which of `predict`, `predict_proba`, `fit_and_predict` methods to use instead of doing the inference for it.

### Related issues or pull requests

<!-- Please provide a link to the respective issue on the [Issue Tracker](https://github.com/rasbt/mlxtend/issues) if one exists. E.g.,

Fixes #176  -->

Also possibly addresses issue #192 

<!-- Below is a general todo list for typical pull request -->

### Pull Request requirements

- [x] Added appropriate unit test functions in the `./mlxtend/*/tests` directories
- [x] Ran `nosetests ./mlxtend -sv` and make sure that all unit tests pass
- [x] Checked the test coverage by running `nosetests ./mlxtend --with-coverage`
- [x] Checked for style issues by running `flake8 ./mlxtend`
- [x] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file
- [ ] Modify documentation in the appropriate location under `mlxtend/docs/sources/` (optional)
- [x] Checked that the Travis-CI build passed at https://travis-ci.org/rasbt/mlxtend




<!--
NOTE

Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).

For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/
-->
",22937749
1491,2017-05-10T02:49:00Z,2017-06-13T02:44:04Z,,,"…sifier

<!-- Please read the following guidelines for new Pull Requests -- thank you! -->

<!--
Make sure that you submit this pull request as a separate topic or feature branch and not as master branch. The new feature branch of your fork will then be merged to the master branch of the original repository, following the ""Fork-and-Branch Git Workflow:""

1. Fork the original GitHub project
2. Clone the fork to your local machine
3. Create a new topic branch
4. Make your code changes to this new topic branch
5. Commit the changes and push the commit to the topic branch to your fork upstream on GitHub
6. Create a new pull request from the upstream topic branch to the master branch of the original repo
-->

<!-- Provide a small summary describing the Pull Request below -->

### Description

Insert Description Here

### Related issues or pull requests

<!-- Please provide a link to the respective issue on the [Issue Tracker](https://github.com/rasbt/mlxtend/issues) if one exists. E.g.,

Fixes #<ISSUE_NUMBER> -->

Link related issues/pull requests here

<!-- Below is a general todo list for typical pull request -->

### Pull Request requirements

- [ ] Added appropriate unit test functions in the `./mlxtend/*/tests` directories
- [ ] Ran `nosetests ./mlxtend -sv` and make sure that all unit tests pass
- [ ] Checked the test coverage by running `nosetests ./mlxtend --with-coverage`
- [ ] Checked for style issues by running `flake8 ./mlxtend`
- [ ] Added a note about the modification or contribution to the `./docs/sources/CHANGELOG.md` file
- [ ] Modify documentation in the appropriate location under `mlxtend/docs/sources/` (optional)
- [ ] Checked that the Travis-CI build passed at https://travis-ci.org/rasbt/mlxtend




<!--
NOTE

Due to the improved GitHub UI, the squashing of commits is no longer necessary.
Please DO NOT SQUASH commits since they help with keeping track of the changes during the discussion).

For more information and instructions, please see http://rasbt.github.io/mlxtend/contributing/
-->
",22937749
1492,2020-03-21T19:18:12Z,2020-03-24T05:50:33Z,,,"Allow get centroids from clusters when calculate a cluster with Kmeans.
Necesary fix to run 
https://github.com/php-ai/php-ml-examples/blob/master/clustering/robberySquaredErrors.php",51325497
1493,2019-08-01T19:01:01Z,2019-08-01T19:01:01Z,,,"I posted a [comment](https://github.com/php-ai/php-ml/issues/113#issuecomment-517374221) on #113, here is my attempt to resolve that issue.

Take this scenario. You have a large dataset that you do not want to be touching at all in production. You train your models in a CI pipeline, use the `\Phpml\ModelManager` class to save the trained model, and deploy that file to production.

Here is the catch, the ""var"" and ""bin"" paths for the class were set during the training process and serialized into the saved model. This means when restoring the model, it is still looking in the path specified by the CI process which likely does not exist in production.

I know that you can manually set the bin and var paths like so:
```
$classifier->setBinPath('vendor folder somewhere');
$classifier->setVarPath('also vendor folder');
```
That seems a bit touchy though, as the library could change where those items are stored at any time and the hard coded values would need updated.

With these changes, the default behaviour will be to use the ""default"" directories unless specified otherwise.",51325497
1494,2019-07-10T16:43:03Z,2019-08-29T02:52:33Z,,,"This addresses #162, at least partially, by doing a few things:
 * Allowing for matrices with a zero word count (fixes divide by zero error)
 * Adding the functionality to filter sparse matrices given a minimum TF and IDF value.

The first item is just a bug fix. The second helps with memory consumption by helping filter out irrelevant data from matrices given the proper parameters. Existing tests pass and the default values given should ensure backwards compatibility. 

My apologies if I have made any mistakes or am misunderstanding a concept. I am learning as I go and attempting to solve problems as I reach them. If criticism is warranted, definitely share it!",51325497
1495,2020-03-12T00:00:40Z,2020-03-12T00:01:41Z,,,"### Status
**IN DEVELOPMENT**

### Todo list
- [ ] Documentation
- [x] Tests added and passed
- [ ] Issue: closes #56

### Description of the changes proposed in the pull request
Adapt code here and there to accommodate future versions of dependencies

### Where should the reviewer start?
Follow the [contributing guide](https://fklearn.readthedocs.io/en/latest/contributing.html) and run the tests

### Remaining problems or questions
There are still some warnings from `shap` for example that are not so clear about what should be done.

Numpy related warnings were ignored due to my lack of experience with it.

Any kind of suggestion/help is more than welcome :) ",172931222
1496,2020-02-12T14:06:46Z,2020-03-26T19:18:00Z,,,"### Status
**READY**

### Todo list
- [x] Documentation
- [x] Tests added and passed
- [x] Issue: closes https://github.com/nubank/fklearn/issues/124

### Description of the changes proposed in the pull request
- add `fill_value` to `imputer`
- add to `.gitignore` some files created by the tests",172931222
1497,2020-01-14T22:12:54Z,2020-03-26T20:02:44Z,,,"### Status
**READY**

### Todo list
- [x] Documentation
- [x] Tests added and passed
- [X] Issue: closes #116 

### Description of the changes proposed in the pull request
- Add a new transformer `feature_duplicator`
- Add new meta decorator `column_duplicatable` to transformers and learners to duplicate columns

### Background
Currently, the transformations (such as `target_categorizer`) replace the value in the column. A better approach would be to allow the user to preserve the original value, outputting the encoded feature to a new column. This would also enable users to apply more than one transformation to the same column (example: frequency and target encoding of the same feature) without needing to duplicate the column first.

Usage examples in comments.",172931222
1498,2020-01-16T21:30:40Z,2020-01-21T15:56:32Z,,,"## What does this PR do?

Fixes an edge case that arises when using tpot on dataset with extreme class imbalance where the pretest sample would cause an error to be thrown since the sample resulted in only one class being represented in the pretest_sample

## Where should the reviewer start?
See the changes in the tpot base with the introduction of a init_pretest function and the corresponding implementation in the TPotClassifier


## How should this PR be tested?
a unit test was added to verify that the pretest function works as intended


## Any background context you want to provide?
see PR description
## Questions:

- Do the docs need to be updated? no
- Does this PR add new (Python) dependencies? no
",45495679
1499,2020-01-14T14:51:53Z,2020-02-14T14:58:34Z,,,"## What does this PR do?

Slight variation of #903 depending on the shape of the target array.

## What are the relevant issues?

#971 ",45495679
1500,2019-08-14T12:39:44Z,2019-12-06T14:10:51Z,,,"[please review the [Contribution Guidelines](http://epistasislab.github.io/tpot/contributing/) prior to submitting your pull request. go ahead and delete this line if you've already reviewed said guidelines.]

## What does this PR do?
Allows multi-output models to run with TPOT.


## Where should the reviewer start?
It's only a couple of small changes.


## How should this PR be tested?
Try running TPOT with this flag. See if anything breaks.


## Any background context you want to provide?
Not all SK models support multiple regression. Therefore the user should also add MultiOutputRegressor in the config if they want to maximise the utility of the change.


## What are the relevant issues?

https://github.com/EpistasisLab/tpot/issues/747
https://github.com/EpistasisLab/tpot/issues/810

[you can link directly to issues by entering # then the number of the issue, for example, #3 links to issue 3]

## Screenshots (if appropriate)



## Questions:

- Do the docs need to be updated? - Yes.
- Does this PR add new (Python) dependencies? - No.
",45495679
1501,2019-07-27T22:50:09Z,2019-07-30T02:52:04Z,,,"[please review the [Contribution Guidelines](http://epistasislab.github.io/tpot/contributing/) prior to submitting your pull request. go ahead and delete this line if you've already reviewed said guidelines.]

## What does this PR do?

Too many things - so many things that I'm pretty sure there will be discussions on whether this is the way to proceed. Putting it up here before I invest more time on it. This PR addresses several items (possibly too many again) including

#507 
#836 
Handling of categorical data #771 - I know its not directly related; but it is the closest issue, and in my mind it would allow for extending it to using different encodings like using this: https://github.com/scikit-learn-contrib/categorical-encoding

Which are all related to how TPOT does preprocessing. This PR does a number of things including re-introducing ""RandomTree"" option in templates that allows specifying templates in the form `Transformer-RandomTree`; which will then allow things like `My_Preprocessing-RandomTree`.

The high level approach is to inject additional preprocessing steps when `_fit_init` is called which then alters the behaviour of TPOT. 

## Where should the reviewer start?

`tpot/base.py` - will add comments in files as part of this PR with my thoughts...

Also can't get relative imports working - so that would be appreciated for the `tpot.drivers.load_scoring_function` part


## How should this PR be tested?

Happy to add new tests later; when the design is approved...

## Any background context you want to provide?

NIL see above

## What are the relevant issues?

#507 
#836 
Handling of categorical data #771 

## Screenshots (if appropriate)

Example of API using modified Iris dataset

```py
from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=1, population_size=5, verbosity=2, template=""PCA-RandomTree"")

X_train_df = pd.DataFrame(X_train, columns=[""num1"", ""num2"", ""num3"", ""num4""])
X_train_df['text'] = np.random.choice([""hello"", ""world"", ""foo"", ""bar world"", ""bar hello""], X_train.shape[0])

tpot2 = TPOTClassifier(generations=1, population_size=5, verbosity=2, template=""PCA-LogisticRegression"", 
                      preprocess_config_dict = {
                          'numeric_columns': [""num2""]
                      })
tpot2.fit(X_train_df, y_train)

tpot2 = TPOTClassifier(generations=1, population_size=5, verbosity=2, template=""PCA-LogisticRegression"", 
                      preprocess_config_dict = {
                          'numeric_columns': [""num2"", ""num3"", ""num4""]
                      })
tpot2.fit(X_train_df, y_train)

tpot2 = TPOTClassifier(generations=1, population_size=5, verbosity=2, template=""PCA-LogisticRegression"", 
                      preprocess_config_dict = {
                          'numeric_columns': [""num2""],
                          'text_columns': ['text']
                      })
tpot2.fit(X_train_df, y_train)
```

```
Generation 1 - Current best internal CV score: 0.5908385093167702

Best pipeline: LogisticRegression(PCA(PreprocessTransformer(input_matrix, numeric_columns=['num2']), iterated_power=2, svd_solver=randomized), C=20.0, dual=True, penalty=l2)

Generation 1 - Current best internal CV score: 0.9556935817805383

Best pipeline: LogisticRegression(PCA(PreprocessTransformer(input_matrix, numeric_columns=['num2', 'num3', 'num4']), iterated_power=9, svd_solver=randomized), C=20.0, dual=False, penalty=l1)

Generation 1 - Current best internal CV score: 0.5096273291925466
```


## Questions:

- Do the docs need to be updated? Yes - will do later
- Does this PR add new (Python) dependencies? No
",45495679
1502,2019-07-26T21:18:23Z,2019-10-04T14:20:43Z,,,"[please review the [Contribution Guidelines](http://epistasislab.github.io/tpot/contributing/) prior to submitting your pull request. go ahead and delete this line if you've already reviewed said guidelines.]

## What does this PR do?
Attempts to resolve #809 

## Where should the reviewer start?

Have a look at `tpot.builtins.embedding_estimator`


## How should this PR be tested?
It has been tested on Python 3.7 on two environments; one with `tensorflow 2.0.0a` installed and one without `tensorflow` installed at all. It was tested via:

```
python3 -m nose tests/embedding_estimator_tests.py
```

## Any background context you want to provide?

See also #809 - as requested. I've made an attempt to ensure the API is compliant and sensible

## What are the relevant issues?

#809 

## Screenshots (if appropriate)



## Questions:

- Do the docs need to be updated? Maybe - let me know what you need me to do.
- Does this PR add new (Python) dependencies? Adds new optional dependencies. You can use the MLPClassifier as part of existing dependencies (scikit-learn)
",45495679
1503,2020-03-26T04:33:17Z,2020-03-27T00:18:46Z,,,"This PR does the following things:
 - Issue #4587 asks for a feature that was added way back in #721 but that somehow disappeared in the intervening work; this PR puts that feature back in.
 - That feature had previously used a popup dialog, but @captainsafia said to use `sendNotification()` this time instead, so that's a small upgrade.
 - In order to do that stuff, it was also necessary to fix #4968, so this PR fixes that one, too.
 - The code in `applications/desktop/src/notebook/menu.tsx` had some vestigial stuff that needed to be cleaned out; I did that while I was in there.

Includes changes made in #4989.",37496521
1504,2020-03-17T19:25:51Z,2020-03-27T06:10:28Z,,,"<!-- If this is your first PR for nteract, please mark these boxes to confirm (otherwise you can exclude them) -->

- [x] I have read the [Contributor Guide](https://github.com/nteract/nteract/blob/master/CONTRIBUTING.md)

<!-- Questions? Feel free to ping us on https://nteract.slack.com/ -->
For Linux System Only ;
Tested on Nautilus and Nemo File Manager (Working)

Work on Issue ( https://github.com/nteract/nteract/issues/2780 )",37496521
1505,2020-03-16T03:59:28Z,2020-03-16T04:10:28Z,,,"Partially addresses #4902 

",37496521
1506,2020-03-15T19:20:30Z,2020-03-24T01:00:06Z,,,"Addresses #4415 
",37496521
1507,2020-02-28T19:37:32Z,2020-03-26T02:06:54Z,,,"This PR is for issue #4956.

Changes made so far by this PR.
- Implemented following components
     - Sidebar
     - BinderMenu
     - StatusLine
     - Console
- Implemented GitHub API to fetch the files and contents.

### Demo
![final](https://user-images.githubusercontent.com/29037312/77571890-16187a80-6ef4-11ea-9f7c-d505d22c3b7d.gif)


",37496521
1508,2020-01-15T22:26:23Z,2020-02-10T04:08:45Z,,,"Before, the function was taking a custom payload object. Typing now allows
JupyterMessage to be accepted as payload.

<!-- If this is your first PR for nteract, please mark these boxes to confirm (otherwise you can exclude them) -->

- [x] I have read the [Contributor Guide](https://github.com/nteract/nteract/blob/master/CONTRIBUTING.md)

<!-- Questions? Feel free to ping us on https://nteract.slack.com/ -->
",37496521
1509,2020-03-24T14:52:25Z,2020-03-26T07:59:10Z,,,I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=ru.,97556265
1510,2020-03-24T01:35:08Z,2020-03-26T06:33:37Z,,,I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=ru.,97556265
1511,2020-03-22T14:44:00Z,2020-03-23T23:07:56Z,,,"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=ru.

This request contains implementation of approximate SHAP values calculation for oblivious trees (with catfeatures support).",97556265
1512,2020-03-22T01:45:59Z,2020-03-22T01:45:59Z,,,"* Updated iris model
* Test datasets were saved in the directory. We do not need to download it
every time.
",97556265
1513,2020-03-15T21:57:24Z,2020-03-20T01:00:16Z,,,"Implemented IDisposable interface for classes:
* CatBoostModel
* CatBoostModelEvaluator 
",97556265
1514,2020-03-10T01:43:39Z,2020-03-25T04:56:33Z,,,"Before submitting a pull request, please do the following steps:

1. Read [instructions for contributors](https://catboost.ai/docs/concepts/development-and-contributions.html).
2. Run `ya make` in catboost folder to make sure the code builds.
3. Add tests that test your change.
4. Run tests using `ya make -t -A` command.
5. If you haven't already, complete the [CLA](https://yandex.ru/legal/cla/).
",97556265
1515,2020-03-05T21:11:17Z,2020-03-05T21:11:23Z,,,"It is test commit made by Anna Volodkevich working on AUC tutorial

I hereby agree to the terms of the CLA available at https://yandex.ru/legal/cla/?lang=ru
",97556265
1516,2020-02-19T17:41:53Z,2020-02-19T17:48:39Z,,,"## PR Objective:
Allow for users to use a custom defined metric as regression objective

## What Changed:
* Added a constant of ELossFunction::UserPerObjMetric to the constants vector holding all the RegressionObjectives

## Other Notes:
This is necessary to add to allow custom objective functions in c++ to be used in the CatBoostRegressor model in python. Python checks if the specified loss function is inside the RegressionObjectives by way of `is_regression_objective` in core.py that calls on the .so file",97556265
1517,2020-01-03T01:42:08Z,2020-01-16T09:54:08Z,,,,97556265
1518,2020-01-02T14:48:19Z,2020-02-14T17:12:04Z,,,"New version of scikit-learn implements an _get_tags method in an Estimator base class. Some of the sklearn's methods use it, most prominently - [RFE/RFECV methods](https://github.com/scikit-learn/scikit-learn/blob/70b0ddea992c01df1a41588fa9e2d130fb6b13f8/sklearn/feature_selection/_rfe.py). CatBoost used to work with this method, but now it fails, as _get_tags method isn't implemented.

I am proposing to add this method with default parameters, as suggested in sklearn.

I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en (in English) or https://yandex.ru/legal/cla/?lang=ru (in Russian).",97556265
1519,2020-01-21T12:15:41Z,2020-03-21T23:32:11Z,,,"## Description of proposed changes

The fit() method of LabelModel runs several epochs, with default being 100. 
Added code to print epoch number before running each epoch, the same way that keras does. 

```
Epoch 1/100
Epoch 2/100
....
Epoch 100/100
```


## Related issue(s)
",52581991
1520,2020-03-26T12:37:06Z,2020-03-27T00:13:11Z,,,"…ailed GDrive API call.

Related to #2865 

- Default client_id and client_secret values in DVC config ( from DVC's GCP project ).
- Validate pair of `client_id` and `client_secret` (both non-default should be specified by user).
- Debug message on retry of failed GDrive API call.
",83878269
1521,2020-03-26T07:45:34Z,2020-03-26T07:48:43Z,,,"* [x] ❗ I have followed the [Contributing to DVC](https://dvc.org/doc/user-guide/contributing/core) checklist.

* [ ] 📖 If this PR requires [documentation](https://dvc.org/doc) updates, I have created a separate PR (or issue, at least) in [dvc.org](https://github.com/iterative/dvc.org) and linked it here. If the CLI API is changed, I have updated [tab completion scripts](https://github.com/iterative/dvc/tree/master/scripts/completion).

* [x] ❌ I will check DeepSource, CodeClimate, and other sanity checks below. (We consider them recommendatory and don't expect everything to be addressed. Please fix things that actually improve code or fix bugs.)

Thank you for the contribution - we'll try to review it as soon as possible. 🙏

Will close #3530 ",83878269
1522,2020-03-25T12:40:59Z,2020-03-27T04:05:35Z,,,"* [x] ❗ I have followed the [Contributing to DVC](https://dvc.org/doc/user-guide/contributing/core) checklist.

* [x] 📖 If this PR requires [documentation](https://dvc.org/doc) updates, I have created a separate PR (or issue, at least) in [dvc.org](https://github.com/iterative/dvc.org) and linked it here. If the CLI API is changed, I have updated [tab completion scripts](https://github.com/iterative/dvc/tree/master/scripts/completion).

* [x] ❌ I will check DeepSource, CodeClimate, and other sanity checks below. (We consider them recommendatory and don't expect everything to be addressed. Please fix things that actually improve code or fix bugs.)

Thank you for the contribution - we'll try to review it as soon as possible. 🙏

Fixes #3386 
",83878269
1523,2020-03-25T06:13:38Z,2020-03-27T10:42:28Z,,,"* [x] ❗ I have followed the [Contributing to DVC](https://dvc.org/doc/user-guide/contributing/core) checklist.

* [x] 📖 If this PR requires [documentation](https://dvc.org/doc) updates, I have created a separate PR (or issue, at least) in [dvc.org](https://github.com/iterative/dvc.org) and linked it here. If the CLI API is changed, I have updated [tab completion scripts](https://github.com/iterative/dvc/tree/master/scripts/completion).

* [x] ❌ I will check DeepSource, CodeClimate, and other sanity checks below. (We consider them recommendatory and don't expect everything to be addressed. Please fix things that actually improve code or fix bugs.)

Thank you for the contribution - we'll try to review it as soon as possible. 🙏

* During size estimation, status is displayed via `BAR_FMT_NOTOTAL` progress bar, as current # of estimated total remote size
* During threaded traverse, status is displayed via a single progress bar, as # of objects fetched out of the estimated total remote size

Will close #3526 ",83878269
1524,2020-03-24T13:46:32Z,2020-03-24T13:46:32Z,,,"Fixes #1050

* [x] ❗ I have followed the [Contributing to DVC](https://dvc.org/doc/user-guide/contributing/core) checklist.

* [x] 📖 If this PR requires [documentation](https://dvc.org/doc) updates, I have created a separate PR (or issue, at least) in [dvc.org](https://github.com/iterative/dvc.org) and linked it here. If the CLI API is changed, I have updated [tab completion scripts](https://github.com/iterative/dvc/tree/master/scripts/completion).

* [x] ❌ I will check DeepSource, CodeClimate, and other sanity checks below. (We consider them recommendatory and don't expect everything to be addressed. Please fix things that actually improve code or fix bugs.)

Thank you for the contribution - we'll try to review it as soon as possible. 🙏
",83878269
1525,2020-03-24T13:00:40Z,2020-03-25T15:37:33Z,,,"* [x] ❗ I have followed the [Contributing to DVC](https://dvc.org/doc/user-guide/contributing/core) checklist.

* [x] 📖 If this PR requires [documentation](https://dvc.org/doc) updates, I have created a separate PR (or issue, at least) in [dvc.org](https://github.com/iterative/dvc.org) and linked it here. If the CLI API is changed, I have updated [tab completion scripts](https://github.com/iterative/dvc/tree/master/scripts/completion).

* [x] ❌ I will check DeepSource, CodeClimate, and other sanity checks below. (We consider them recommendatory and don't expect everything to be addressed. Please fix things that actually improve code or fix bugs.)

Thank you for the contribution - we'll try to review it as soon as possible. 🙏

This resolves #3523. But, we should try not to make directory path objects if we really don't need to.",83878269
1526,2020-03-23T14:38:46Z,2020-03-23T16:23:55Z,,,"- [x] ensure main bar appear above nested bars (init main bar before workers)
  + closes #3521
  + [x] add `wrap_fn` helper
  + [x] use `wrapattr` and `wrap_fn` in all appropriate places
- related tqdm/tqdm#583",83878269
1527,2020-03-20T12:30:34Z,2020-03-26T06:32:48Z,,,"* [x] ❗ I have followed the [Contributing to DVC](https://dvc.org/doc/user-guide/contributing/core) checklist.

* [x] 📖 If this PR requires [documentation](https://dvc.org/doc) updates, I have created a separate PR (or issue, at least) in [dvc.org](https://github.com/iterative/dvc.org) and linked it here. If the CLI API is changed, I have updated [tab completion scripts](https://github.com/iterative/dvc/tree/master/scripts/completion).

* [x] ❌ I will check DeepSource, CodeClimate, and other sanity checks below. (We consider them recommendatory and don't expect everything to be addressed. Please fix things that actually improve code or fix bugs.)

Thank you for the contribution - we'll try to review it as soon as possible. 🙏

####  TODO
- [ ] Manually check with SSH and local storage if I have broken anything.
- [ ] See if hardlink/symlink is really getting verified.
- [ ] See if I haven't broken s3 for the very least (as I have changed `_cache_is_copy`).
- [ ] Try to cover the change.",83878269
1528,2020-03-18T01:37:01Z,2020-03-23T14:44:15Z,,,"- [x] introduce `Tqdm.MAX_BARS = 20` for parallel display
  + [x] add ` ... (more hidden) ...` message for overflow bars
- [ ] auto-detect screen height? Should be done in upstream https://github.com/tqdm/tqdm/issues/918
- TODO (future): combine sub bars into a single high-level bar
- fixes #2905
- Replaces/closes #3453
- related #3452

issue & fix:

[![](https://asciinema.org/a/wpM1zYFg9TfH53uiNPMsKjiUt.svg)](https://asciinema.org/a/wpM1zYFg9TfH53uiNPMsKjiUt?speed=2)

(from #3453 (cc @JIoJIaJIu): [script for reproducing issue & fix](https://gist.github.com/JIoJIaJIu/05f3b72982373771da88f313bb5cad9f))",83878269
1529,2020-03-06T07:33:52Z,2020-03-18T01:37:26Z,,,"## Disclaimer
I would like to change the implementation completely but shared for collaboration

## How to try
There is [a script](https://gist.github.com/JIoJIaJIu/05f3b72982373771da88f313bb5cad9f) for simulating behavior
[Before](https://asciinema.org/a/0JYsWtiSuOkQ1xVGkmNpVBKng)
[After](https://asciinema.org/a/j36cX6XYPTLBbduKs9KZJBUOY)


## TLDR
I tried an approach with limiting amount of visible pbars but would like to change the solution to the single bar that contains information about all files.
The idea was show only **N** pbars, and when one is finished - add new from the background. It allows to avoid flickering, but it has problems:
1. Pbars hierarchy (main/nested). At this moment main pbar is defined as pbar with [0 position](https://github.com/iterative/dvc/blob/f05e6e0529ae147e82e3c392951f03c01b6b0c64/dvc/progress.py#L96), but the order is not guaranteed. Can be addressed by more smart main bar definition, relates https://github.com/iterative/dvc/issues/3452
2. When files are small - there is flickering
3. the solution interacts with a lot of internal `Tqdm` stuff 

Close #2905

<hr/>

* [ ] ❗ Have you followed the guidelines in the [Contributing to DVC](https://dvc.org/doc/user-guide/contributing/core) list?

* [ ] 📖 Check this box if this PR **does not** require [documentation](https://dvc.org/doc) updates, or if it does **and** you have created a separate PR in [dvc.org](https://github.com/iterative/dvc.org) with such updates (or at least opened an issue about it in that repo). Please link below to your PR (or issue) in the [dvc.org](https://github.com/iterative/dvc.org) repo.

* [ ] ❌ Have you checked DeepSource, CodeClimate, and other sanity checks below? We consider their findings recommendatory and don't expect everything to be addressed. Please review them carefully and fix those that actually improve code or fix bugs.

Thank you for the contribution - we'll try to review it as soon as possible. 🙏
",83878269
1530,2019-12-15T06:56:37Z,2020-03-15T06:57:26Z,,,"The idea is to simplify function signatures and make intermediate calls not need to know when some high-level code is talking some low-level one. 

Showcased on annoying enough `no_progress_bar` and `progress_callback`. Might be also used to simplify:
- combining pbars
- jobs
- force
- other cli or config options only low level code needs to know about ",83878269
1531,2020-03-26T22:57:17Z,2020-03-27T00:32:18Z,,,,23653453
1532,2020-03-26T22:51:47Z,2020-03-26T22:54:02Z,,,"Fixes https://github.com/pachyderm/pachyderm/issues/4535

Just a basic check to make sure spouts don't have input",23653453
1533,2020-03-25T23:28:37Z,2020-03-25T23:31:44Z,,,,23653453
1534,2020-03-25T17:46:14Z,2020-03-25T17:55:23Z,,,"TODO
- [ ] Figure out how to handle `WORKER_SIDECAR_IMAGE` and `WORKER_IMAGE` (Should match pach version set in kustomize images section
- [ ] How to handle `CLUSTER_DEPLOYMENT_ID`
- [ ] Use secrets handlers to generate storage secret per env?
- [ ] Use kustomize vars to match port names?
- [ ] Set `ClusterIP` as base and set to `NodePort` only for local deployment

- Finish and test
  - [ ] GCP
  - [ ] AWS 
  - [ ] Microsoft
  - [ ] Local
  - [ ] Minio
",23653453
1535,2020-03-24T17:15:29Z,2020-03-24T17:18:03Z,,,,23653453
1536,2020-03-23T22:52:38Z,2020-03-23T22:54:56Z,,,"- Added Go API client example for:
   - Creating a repo
   - Listing a repo
   - Deleting a repo
   - Putting a File into a repo",23653453
1537,2020-03-23T19:44:59Z,2020-03-26T21:05:59Z,,,,23653453
1538,2020-03-13T23:49:11Z,2020-03-26T14:59:23Z,,,…eaned up Makefile targets.  Please read Makefile.testing for instructions.,23653453
1539,2020-03-06T23:01:53Z,2020-03-12T16:05:19Z,,,"Fixes https://github.com/pachyderm/pachyderm/issues/4327

Named pipes have issues when the reading end is opened and closed rapidly (see https://stackoverflow.com/questions/47948952/named-pipe-dropping-data-in-c-program-using-fgets-in-linux). We realized that we didn't need to do it for spouts, so I simply moved the open and close out of the loop.

I also changed the kafka test to reproduce the bug.",23653453
1540,2020-03-06T20:39:16Z,2020-03-12T16:05:21Z,,,"Enforce semvar release naming.

Fixes #4189 
",23653453
1541,2020-02-28T20:59:50Z,2020-03-12T16:05:25Z,,,,23653453
1542,2020-02-22T01:12:23Z,2020-03-12T16:05:58Z,,,,23653453
1543,2020-02-12T20:24:52Z,2020-03-12T16:06:12Z,,,"Right now if we stop a pipeline with standby field set to true, its status will still be ""standby"" instead of ""failure"", thus blocks the garbage collection.",23653453
1544,2020-02-07T21:51:30Z,2020-03-12T16:06:13Z,,,,23653453
1545,2020-02-06T03:28:56Z,2020-03-13T08:30:56Z,,,"There are several loose ends, still, but this PR has the major architectural changes for the worker refactor and parallel jobs.  Navigating the worker code has changed significantly, so to summarize:

* `src/server/worker` files have been broken up into several subpackages:
* `worker/cache` - a dumb interface for managing multiple hashtree caches across jobs, may delete later idk
* `worker/common` - common functions and protobuf definitions for worker code that do not have associated state (anything that did was put in the `Driver`) 
* `worker/datum` - the `Iterator` interface used for datum iteration over inputs
* `worker/driver` - the `Driver` interface used across most worker code, provides commonly-used functions
* `worker/logs` - the `TaggedLogger` interface used by the worker for logging
* `worker/pipeline/service` - code specific to service pipelines
* `worker/pipeline/spout` - code specific to spout pipelines
* `worker/pipeline/transform` - code specific to transform pipelines, this is where most of the parallel jobs work went
* `worker/server` - the grpc API server exposed by the worker process (not the pachd sidecar)
* `worker/stats` - the `Prometheus` stats interface used by worker code

Most of the above were broken out of the existing `worker/api_server.go` and `worker/master.go` files, although some are entirely new and some had their own file in the `worker` directory.  `worker/datum` is mostly unchanged (although I dropped the redundant `Datum` in its exported identifiers).

I will add comments around specific changes, although feel free to ask questions and raise concerns while I get this out of WIP.",23653453
1546,2020-02-05T00:30:58Z,2020-03-12T16:06:06Z,,,Fixes #4116,23653453
1547,2020-02-04T19:07:22Z,2020-03-12T16:06:02Z,,,https://github.com/pachyderm/pachyderm/issues/4558,23653453
1548,2020-01-24T21:41:22Z,2020-03-12T16:06:05Z,,,,23653453
1549,2020-01-20T02:40:50Z,2020-03-12T13:04:24Z,,,"Setting pachd address is no longer required for TLS connections to pachd. If serverCAs is set, TLS is assumed, and a server name override will permit a host check to succeed against localhost when port forwarding.

Implements #4465",23653453
1550,2020-01-10T21:32:35Z,2020-03-12T16:06:12Z,,,,23653453
1551,2019-11-19T20:36:58Z,2020-03-12T13:04:40Z,,,force unix style paths for pachctl put so that functionality is same when used on native windows.,23653453
1552,2019-11-05T11:02:00Z,2020-03-12T13:48:49Z,,,,23653453
1553,2019-09-27T21:54:26Z,2020-03-12T16:06:23Z,,,,23653453
1554,2019-09-24T00:22:09Z,2020-03-12T16:06:30Z,,,Will be useful for debugging join inputs that are behaving unexpectedly.,23653453
1555,2019-08-27T23:42:42Z,2020-03-12T16:06:30Z,,,,23653453
1556,2019-08-12T15:35:59Z,2020-03-12T16:06:15Z,,,Add skaffold config and new docker images for skaffold. Let me know if there's a better spot for those docs,23653453
1557,2019-08-06T20:33:39Z,2020-03-12T16:06:39Z,,,"DeleteAll done in a single transaction will often have a transactions size exceeding etcd's limit. Since we're trying to delete everything, we're not as concerned about possible data races, and instead find it worth it to split it into separate transactions.",23653453
1558,2019-08-05T15:41:55Z,2020-03-12T13:04:43Z,,,Fix verb tense.,23653453
1559,2019-06-19T19:43:17Z,2020-03-12T16:06:32Z,,,"Example:
```
$ kc apply -f ./src/server/crd/pipeline-crd-definition.yaml 
customresourcedefinition.apiextensions.k8s.io/pipelines.pachyderm.io created

$ kc apply -f ./src/server/crd/edges.yaml 
pipeline.pachyderm.io/edges created

$ kc get pipeline/edges -o yaml
apiVersion: pachyderm.io/v1
kind: Pipeline
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiVersion"":""pachyderm.io/v1"",""kind"":""Pipeline"",""metadata"":{""annotations"":{},""name"":""edges"",""namespace"":""default""},""spec"":{""input"":{""pfs"":{""glob"":""/*"",""repo"":""images""}},""transform"":{""cmd"":[""python3"",""/edges.py""],""image"":""pachyderm/opencv""}}}
  creationTimestamp: ""2019-06-19T19:41:37Z""
  generation: 1
  name: edges
  namespace: default
  resourceVersion: ""14090""
  selfLink: /apis/pachyderm.io/v1/namespaces/default/pipelines/edges
  uid: 405d5f67-92ca-11e9-b37b-0800271abf09
spec:
  input:
    pfs:
      glob: /*
      repo: images
  transform:
    cmd:
    - python3
    - /edges.py
    image: pachyderm/opencv
```",23653453
1560,2019-06-12T20:27:14Z,2020-03-12T16:06:34Z,,,"Right now, only CreateRepo has been changed:
https://github.com/pachyderm/pachyderm/pull/3822/files#diff-013fc8aac894793fb95d86dc816adf28L80

But this should in theory let us get rid of all of our logging boilerplate.

Other things we might be able to do fairly easily with GRPC interceptors:
- Structured errors
- Auth (we could go back to passing around ctx without worrying about whether auth data had been copied into the outgoing GRPC metadata. In fact, any time we're passing around info in GRPC metadata, we could use GRPC interceptors to copy it from the incoming metadata to the outgoing metadata)
- Remove some of the tracing boilerplate. Tracing already uses interceptors, but they're 3rd party. We could fork that code and get more of what we want
",23653453
1561,2019-06-07T18:55:14Z,2020-03-12T16:06:47Z,,,Fixes #626,23653453
1562,2019-05-27T03:11:06Z,2020-03-12T16:06:49Z,,,,23653453
1563,2019-05-04T17:29:40Z,2020-03-12T16:06:58Z,,,"This PR will be used for tracking iterations and conversation about the storage format spec.

#3690",23653453
1564,2019-04-10T20:53:56Z,2020-03-12T16:06:51Z,,,"This implements a simple plugin interface and one use case for plugins: datum filtering. Which gives you a way to filter out datums from an input, I've included a test that implements joins by filtering out all of the datums for which the file names do not match.

This isn't meant to be merged (although it wouldn't be a ton of work to get it to a state where it could be) so much as it's meant to demonstrate the concept and act as an RFC.",23653453
1565,2019-03-25T18:59:46Z,2020-03-12T16:06:56Z,,,This is just a scratch space that I am using for iterating through the storage layer design. Stuff to merge in will be pulled from here as it is completed. The completed pieces will be in separate branches/PRs.,23653453
1566,2019-03-08T22:52:25Z,2020-03-12T16:06:51Z,,,,23653453
1567,2019-02-20T20:56:20Z,2020-03-12T16:06:56Z,,,,23653453
1568,2019-01-08T02:02:38Z,2020-03-12T16:07:04Z,,,"…cent compatibility file.

This PR changes the build process such that it pulls the default dash image from the most recent and compatible version based on the compatibility files rather than a hard coded value that would need to be constantly updated.",23653453
1569,2020-03-27T01:34:26Z,2020-03-27T10:14:20Z,,,"This PR proposes of drop Travis CI from now on.

IMO, I don't think there is a need to keep two management points for CI.

The following changes are includes in this PR.

- Removed the `.travis.yml`
- Added the existing tests in the `.travis.yml` to `master.yml` which is used for GitHub Actions.
- Removed badge for the Travis CI from `README.md`
- Renamed the filename from `download_travis_dependencies.sh` to `download_spark.sh` since this script is downloading the Spark only.
- Uncomment the test which was failing on a Travis CI.
- Renamed job names",164026325
1570,2020-03-25T15:35:55Z,2020-03-25T15:35:55Z,,,"This PR proposes `Series.asof()`
(https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.asof.html#pandas.Series.asof)

```python
>>> s = ks.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])
>>> s
10    1.0
20    2.0
30    NaN
40    4.0
Name: 0, dtype: float64
```

A scalar `where`.

```python
>>> s.asof(20)
2.0
```

For a sequence `where`, a Series is returned. The first value is
NaN, because the first element of `where` is before the first
index value.

```python
>>> s.asof([5, 20]).sort_index()
5     NaN
20    2.0
Name: 0, dtype: float64
```

Missing values are not considered. The following is ``2.0``, not
NaN, even though NaN is at the index location for ``30``.

```python
>>> s.asof(30)
2.0
```",164026325
1571,2020-03-20T13:08:59Z,2020-03-27T09:16:44Z,,,"Manage broadcast hint for dataframes.
Provides `broadcast` function which, from a given koalas DataFrame, returns a new one with broadcast hint.
Broadcast join can now be performed in `DataFrame.join`, `DataFrame.merge` and `DataFrame.update.`
A broadcast join may be more efficient than sort merge join (Spark default) between a small dataframe and a big daframe. It gives every node a copy of a the small dataframe, which reduces the number of shuffle between partitions. By default, Spark performs it if a dataframe is smaller than ~10MB, but the user should be able to force it.
",164026325
1572,2020-03-20T00:17:32Z,2020-03-20T23:22:59Z,,,"If the protocol in the specified path is not supported by Spark, Koalas fails even if pandas supports the protocol, e.g., `http` or `https`.
This PR proposes to add a config to enable to fallback to pandas with warnings.",164026325
1573,2020-02-17T19:00:20Z,2020-03-22T08:55:16Z,,,"Implement [Series.combine](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.combine.html#pandas.Series.combine)


- Basic example
```python
>>> s1 = ks.Series({'falcon': 330.0, 'eagle': 160.0})
>>> s1
falcon    330.0
eagle     160.0
Name: 0, dtype: float64
>>> s2 = ks.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})
>>> s2
falcon    345.0
eagle     200.0
duck       30.0
Name: 0, dtype: float64

>>> s1.combine(s2, max).sort_index()
duck        NaN
eagle     200.0
falcon    345.0
Name: 0, dtype: float64

>>> s1.combine(s2, max, fill_value=0).sort_index()
duck       30.0
eagle     200.0
falcon    345.0
Name: 0, dtype: float64

>>> s1.combine(s2, max, fill_value=50).sort_index()
duck       50.0
eagle     200.0
falcon    345.0
Name: 0, dtype: float64
```

- MultiIndex
```python
>>> midx1 = pd.MultiIndex([['lama', 'cow', 'falcon', 'koala'],
...                        ['speed', 'weight', 'length', 'power']],
...                       [[0, 3, 1, 1, 1, 2, 2, 2],
...                        [0, 2, 0, 3, 2, 0, 1, 3]])
>>> midx2 = pd.MultiIndex([['lama', 'cow', 'falcon'],
...                        ['speed', 'weight', 'length']],
...                       [[0, 0, 0, 1, 1, 1, 2, 2, 2],
...                        [0, 1, 2, 0, 1, 2, 0, 1, 2]])
>>> kser1 = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1], index=midx1)
>>> kser2 = ks.Series([-45, 200, -1.2, 30, -250, 1.5, 320, 1, -0.3], index=midx2)

>>> kser1
lama    speed      45.0
koala   length    200.0
cow     speed       1.2
        power      30.0
        length    250.0
falcon  speed       1.5
        weight    320.0
        power       1.0
Name: 0, dtype: float64
>>> kser2
lama    speed     -45.0
        weight    200.0
        length     -1.2
cow     speed      30.0
        weight   -250.0
        length      1.5
falcon  speed     320.0
        weight      1.0
        length     -0.3
Name: 0, dtype: float64

>>> kser1.combine(kser2, max).sort_index()
cow     length    250.0
        power      30.0
        speed      30.0
        weight      NaN
falcon  length      NaN
        power       1.0
        speed     320.0
        weight    320.0
koala   length    200.0
lama    length      NaN
        speed      45.0
        weight      NaN
Name: 0, dtype: float64
```

- with scala value
```python
>>> kser1.combine(100, max)
cow     length    250.0
falcon  weight    320.0
cow     speed     100.0
falcon  speed     100.0
lama    speed     100.0
koala   length    200.0
cow     power     100.0
falcon  power     100.0
Name: 0, dtype: float64
```",164026325
1574,2020-02-10T13:23:38Z,2020-02-12T21:36:32Z,,,"Inspired by #1261 , we'd better to have the list of APIs which are not planned to implement to the official docs.",164026325
1575,2020-01-02T13:43:51Z,2020-03-27T02:55:21Z,,,"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.delete.html#pandas.Index.delete

```python
>>> kidx = ks.Index([10, 10, 9, 8, 4, 2, 4, 4, 2, 2, 10, 10])
>>> kidx
Int64Index([10, 10, 9, 8, 4, 2, 4, 4, 2, 2, 10, 10], dtype='int64')

>>> kidx.delete(0)
Int64Index([10, 9, 8, 4, 2, 4, 4, 2, 2, 10, 10], dtype='int64')

>>> kidx.delete([0, 1, 2, 3, 10, 11])
Int64Index([4, 2, 4, 4, 2, 2], dtype='int64')
```",164026325
1576,2019-12-05T03:59:51Z,2019-12-05T04:41:00Z,,,"This PR adds PyArrow 0.15 support back with a better documentation and error message.
Error message will look as below:

```python
>>> import databricks.koalas as ks
WARNING:root:'ARROW_PRE_0_15_IPC_FORMAT' environment variable was not set. It is required to set this environment variable to '1' if you use pyarrow>=0.15 and pyspark<=3.0. Koalas will set it for you but it does not work if there is a Spark context already launched.
```

Resolves #1109",164026325
1577,2019-11-26T18:21:53Z,2020-01-06T08:52:49Z,,,"Resolves #1067 

now, it can compute a simple cross tabulation of two factors.

```python
>>> a = np.array([""foo"", ""foo"", ""foo"", ""foo"", ""bar"", ""bar"",
...               ""bar"", ""bar"", ""foo"", ""foo"", ""foo""], dtype=object)
>>> b = np.array([""one"", ""one"", ""one"", ""two"", ""one"", ""one"",
...               ""one"", ""two"", ""two"", ""two"", ""one""], dtype=object)

>>> ks.crosstab(a, b).sort_index()
0    one  two
0
bar    3    1
foo    4    3

>>> ks.crosstab(a, b, rownames=['koalas'], colnames=['hello']).sort_index()
hello   one  two
koalas
bar       3    1
foo       4    3
```

also support with Series

```python
>>> kdf
      regiment   company experience      name  preTestScore  postTestScore
0   Nighthawks  infantry    veteran    Miller             4             25
1   Nighthawks  infantry     rookie  Jacobson            24             94
2   Nighthawks   cavalry    veteran       Ali            31             57
3   Nighthawks   cavalry     rookie    Milner             2             62
4     Dragoons  infantry    veteran     Cooze             3             70
5     Dragoons  infantry     rookie     Jacon             4             25
6     Dragoons   cavalry    veteran    Ryaner            24             94
7     Dragoons   cavalry     rookie      Sone            31             57
8       Scouts  infantry    veteran     Sloan             2             62
9       Scouts  infantry     rookie     Piger             3             70
10      Scouts   cavalry    veteran     Riani             2             62
11      Scouts   cavalry     rookie       Ali             3             70

>>> ks.crosstab(kdf.company, kdf.name).sort_index()
name      Ali  Cooze  Jacobson  Jacon  Miller  Milner  Piger  Riani  Ryaner  Sloan  Sone
company
cavalry     2      0         0      0       0       1      0      1       1      0     1
infantry    0      1         1      1       1       0      1      0       0      1     0
```

i'm still working for supporting more than two factors.
",164026325
1578,2019-11-06T21:35:07Z,2019-11-08T21:52:01Z,,,#1010 ,164026325
1579,2019-11-04T21:47:06Z,2019-11-06T11:51:08Z,,,"Since in new pandas, named aggregation for MultiIndex is working, so add this in this PR.

```python
kdf = ks.DataFrame({""group"": ['a', 'a', 'b', 'b'], ""A"": [0, 1, 2, 3], ""B"": [5, 6, 7, 8]})
kdf.columns = pd.MultiIndex.from_tuples([('x', 'group'), ('y', 'A'), ('y', 'B')])
```

![Screen Shot 2019-11-04 at 10 46 48 PM](https://user-images.githubusercontent.com/9269816/68160898-03a5a080-ff55-11e9-81fd-228f5968fafa.png)
",164026325
1580,2019-10-29T09:08:21Z,2019-11-11T18:58:44Z,,,"This PR is blocked by #973 where `levels` property is implemented, and by #971 where seems @itholic is into fixing it soon ^^

^ above is done!!

```python
mi = pd.MultiIndex.from_arrays((list('abc'), list('def')))
mi.names = ['level_1', 'level_2']
kdf = ks.DataFrame({'a': [1, 2, 3]}, index=mi)

# get values by level number
kdf.index.get_level_values(0)

# get values by level number
kdf.index.get_level_values('level_1')
```
",164026325
1581,2020-03-27T10:28:31Z,2020-03-27T10:59:09Z,,,"## What does this PR do?
adding missing deprecation warnings in loggers

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1582,2020-03-27T09:03:09Z,2020-03-27T09:51:16Z,,,"## What does this PR do?
Improving mergify
- adjust the condition of required check (in case we add a check in the future, so it does not merge incomplete)
- lower merge strategy
- [WIP] add auto reviewer assignment https://github.com/Mergifyio/mergify-engine/issues/876

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1583,2020-03-26T23:50:54Z,2020-03-27T00:22:42Z,,,removes unecessary syncs in ddp caused by unecessary .item(),178626720
1584,2020-03-26T22:28:32Z,2020-03-26T22:58:01Z,,,"## What does this PR do?
Fixes #1238 - updating do according to https://github.com/PyTorchLightning/pytorch-lightning/issues/1238#issuecomment-604616476

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1585,2020-03-26T15:42:46Z,2020-03-27T01:50:36Z,,,"## What does this PR do?
After initial setting we lower timeouts for inactive issues

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1586,2020-03-26T11:53:50Z,2020-03-27T10:02:02Z,,,"## What does this PR do?
Simplify the example structure and make it more shallow...
This is a reaction to creating `lightning-bolts` so here we keep just the minimal example set to illustrate the Lightning potential :]

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1587,2020-03-25T23:00:16Z,2020-03-26T15:18:31Z,,,"## What does this PR do?
Remove redundant Tox config since we do not use it any CI

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1588,2020-03-25T22:54:21Z,2020-03-26T00:42:05Z,,,"## What does this PR do?
Fixes # (issue).

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1589,2020-03-25T22:27:34Z,2020-03-27T09:46:27Z,,,"Remove unnecessary parameters to `super()` in the documentation and source code

## What does this PR do?
Removes unnecessary params in super() to conform to the Python 3 style. Reduces code on the page to aid readability.

Also cleans up some unwanted line endings along the way.
",178626720
1590,2020-03-25T17:49:26Z,2020-03-27T03:50:28Z,,,"# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [ ] Did you make sure to update the docs?   
- [ ] Did you write any new necessary tests?  
- [ ] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Fixes #1201 (right now it's just just a check that the proposed fix by @Dunrar passes the unit tests).

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1591,2020-03-25T15:30:05Z,2020-03-26T11:10:13Z,,,"## What does this PR do?
As many bug reports are missing relevant information, we have customized the pytorch env script
Sample output:
```
cuda:
	GPU:
		GeForce GTX 1050
	available:           True
	version:             10.0
packages:
	numpy:               1.18.1
	pyTorch_debug:       False
	pyTorch_version:     1.4.0+cu100
	pytorch-lightning:   0.7.2-dev
	tensorboard:         1.15.0
	tqdm:                4.41.1
system:
	OS:                  Linux
	architecture:
		64bit
		ELF
	processor:           x86_64
	python:              3.6.9
	version:             #34~18.04.1-Ubuntu SMP Fri Feb 28 13:42:26 UTC 2020
```

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1592,2020-03-25T07:25:01Z,2020-03-26T23:30:57Z,,,"DQN RL Agent using Lightning. Model uses an IterableDataset to wrap the ReplayBuffer, providing mini batches of past experiences to train on during each train_step. During each train_step, the agent carries out a step through the environment and updates the ReplayBuffer within the Dataset.

# Before submitting

- [X] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [X] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [x] Did you make sure to update the docs?   
- [ ] Did you write any new necessary tests?  
- [x] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Fixes #713
Provides a basic domain example of using Lightning for Reinforcement Learning

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1593,2020-03-24T23:52:11Z,2020-03-26T06:50:50Z,,,"## What does this PR do?
This is aiming at compatibility issue, as he deprecated some arguments the new placed just below the old ones it will crash for a user who is using positional Trainer setting

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1594,2020-03-24T21:24:40Z,2020-03-25T18:23:45Z,,,"# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?
- [ ] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?

Fix outdated docs: `save_best_only` was changed to `save_top_k` (#128).

## PR review

Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?

Make sure you had fun coding 🙃",178626720
1595,2020-03-24T16:04:55Z,2020-03-27T07:35:38Z,,,"# Before submitting

- [ ] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [ ] Did you make sure to update the docs?
- [ ] Did you write any new necessary tests?
- [ ] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?

Fixes typo in method's Docstrings.

## PR review

Anyone in the community is free to review the PR once the tests have passed.
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?

Make sure you had fun coding 🙃",178626720
1596,2020-03-20T15:06:43Z,2020-03-27T02:30:30Z,,,"# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [x] Did you make sure to update the docs?   
- [x] Did you write any new necessary tests?  
- [x] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Fixes https://github.com/PyTorchLightning/pytorch-lightning/issues/1171 and https://github.com/PyTorchLightning/pytorch-lightning/issues/508

This PR adds checks, that secure that the users model is correctly configured before training. This includes:
* Checking that `training_step` has been overridden
* Checking that `training_dataloader` has been overridden
* Warning if `configure_optimizers` has not been overridden, will tell user that program is using default optimizer (adam with lr=0.0001)
* Error if a `validation_dataloader` is overridden but no `validation_step` is defined (and vise verse)
* Error if a `test_dataloader` is overriden but no `test_step` is defined (and vise verse)
* Warning if `validation_dataloader, validation_step` is overridden but no `validation_epoch_end`
* Warning if `test_dataloader, test_step` is overridden but no `test_epoch_end`

The most fundamental change is the requirement of `validation_step` and `test_step` when there respective dataloaders are defined. This will probably not be backward compatible with some users code. ",178626720
1597,2020-03-19T23:31:03Z,2020-03-27T07:22:33Z,,,"## What does this PR do?
Fixes #1181 (issue).
",178626720
1598,2020-03-19T19:22:57Z,2020-03-25T12:36:50Z,,,"between training / eval

# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [x] Did you write any new necessary tests?  
- [ ] Did you make sure to update the docs?   
- [ ] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Temporary fix for #1096 

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1599,2020-03-19T01:21:31Z,2020-03-26T17:30:25Z,,,"# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [ ] Did you make sure to update the docs?   
- [ ] Did you write any new necessary tests?  
- [ ] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Fixes #1116 and #1220 

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃",178626720
1600,2020-03-18T22:38:31Z,2020-03-26T10:36:17Z,,,"Fix #1161 - when using ddp/ddpd2, the validation and training loops run the full respective dataset on each gpu. This costs time, and changes batch counts for any statistics being collected.

The fix just makes sure that for ddp and ddp2, `auto_add_sampler()` creates a `DistributedSampler` for each data set.

This passes all the tests on my machine except for slurm and apex related as I do not have either. I don't think this needs any doc changes. I can look into writing a test for this ... if needed. Let me know.",178626720
1601,2020-03-16T06:08:42Z,2020-03-26T11:12:08Z,,,"# Before submitting

- [x] Was this discussed/approved via a Github issue? #596 
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [ ] Did you make sure to update the docs?   
- [ ] Did you write any new necessary tests?  
- [ ] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Improves some use-cases where saving all models is desired. See #596 .
Removes deleting of models on existing save filepath and drops delete warning if save_top_k == -1.
Enables saving of all epochs where validation occurs.
Does not address saving all epochs regardless of check_val_every_n_epochs > 1

Not tested with NVIDIA/apex

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Yes, but 2nd ever contribution (be gentle : )
",178626720
1602,2020-03-15T21:39:22Z,2020-03-26T15:38:33Z,,,"New feature: DDP on CPU. This can be used for distributed training with out GPUs, or to test/debug DDP on single machine without GPUs.. Since PyTorch already makes good use of multiple cores under the hood, this *will not* provide any speedup over normal CPU training if you're only using a single node.

API changes:

* New distributed backend: `distributed_backend=""ddp_cpu""`
* New `Trainer` argument: `num_processes`. Controls the number or processes per node. Is currently only used by `ddp_cpu`.

Still need to update the documentation, but I wanted to get some eyes on this before I got too far. Right now everything if functional as far as I can tell, and I've added a new test that covers this feature.",178626720
1603,2020-03-14T23:19:10Z,2020-03-16T23:22:29Z,,,"# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [ ] Did you make sure to update the docs?   
- [ ] Did you write any new necessary tests?  
- [ ] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Fixes #981 

When using multiprocessing the default behavior of wandb is to return None for multiple calls to init.  Passing reinit=True ensure we always create an experiment.  This could lead to cases where a single experiment has multiple runs in wandb, but it's a much better default than throwing an exception.

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1604,2020-03-14T01:00:32Z,2020-03-25T14:48:02Z,,,"# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [x] Did you make sure to update the docs?   
- [x] Did you write any new necessary tests?  
- [x] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Fixes #1143 

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1605,2020-03-10T00:53:17Z,2020-03-26T20:34:15Z,,,"# Before submitting

- [X] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [X] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [X] Did you make sure to update the docs?   
- [X] Did you write any new necessary tests?  
- [ ] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Fixes #1102 suggestion to collapse `show_progress_bar` into `progress_bar_refresh_rate`

Adds show_progress_bar to ""to be deprecated"" Trainer args (unsure on best approach here for community)

Removes show_progress_bar from tests that used it

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1606,2020-03-08T20:17:34Z,2020-03-27T11:08:13Z,,,"# Before submitting

- [X] Was this discussed/approved via a Github issue? (no need for typos and docs improvements)
- [X] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section?
- [ ] Did you make sure to update the docs?   
- [ ] Did you write any new necessary tests?  
- [ ] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Fixes #958 related to memory leak. Pretty old PyTorch issue related to setting device. 
1. Set the device directly while finding the root gpu in `determine_root_gpu_device` in trainer.distrib_parts.py
2. Remove also the hack from [trainer.evaluation_loop](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/evaluation_loop.py#L424)

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃 lol. Yeah I did! 
",178626720
1607,2020-03-07T17:26:07Z,2020-03-27T07:15:15Z,,,"# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos and docs improvements) 
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md), Pull Request section? Yes
- [x] Did you make sure to update the docs?   Yes
- [x] Did you write any new necessary tests?  Not Applicable
- [x] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)? I can update if required. It is a minor change. 

## What does this PR do?
Fixes #1081

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1608,2020-02-29T17:40:27Z,2020-03-26T17:36:40Z,,,"# Before submitting

- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)
- [x] Did you read the [contributor guideline](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md)?
- [ ] Did you make sure to update the docs?   
- [ ] Did you write any new necessary tests?  
- [x] If you made a notable change (that affects users), did you update the [CHANGELOG](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.github/CHANGELOG.md)?

## What does this PR do?
Fixes #942 and follow up to #990.

- Added a custom MNIST class that does not rely on torchvision. 
- Added download links to MNIST on S3 
- Removed torchvision dependency from tests
- PL examples still use the real torchvision

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1609,2020-02-22T21:10:36Z,2020-03-25T14:35:20Z,,,"## What does this PR do?
Fixes #910. I have run CI for 3.8 and it seems that not all packages are ported to 3.8 yet, e.g. `torchvision` Which comes we shall drop `torchvision` even from test dependencies #859 #942 #986 

## PR review    
Anyone in the community is free to review the PR once the tests have passed.     
If we didn't discuss your PR in Github issues there's a high chance it will not be merged.

## Did you have fun?
Make sure you had fun coding 🙃
",178626720
1610,2020-03-19T01:49:06Z,2020-03-24T21:18:02Z,,,"**Thanks for contributing to Prefect!**

Please describe your work and make sure your PR:

- [x] adds new tests (if appropriate)
- [x] updates `CHANGELOG.md` (if appropriate)
- [x] updates docstrings for any new functions or function arguments, including `docs/outline.toml` for API reference docs (if appropriate)

Note that your PR will not be reviewed unless all three boxes are checked.

## What does this PR change?
Closes #2144. Adds a validate() method onto the Result baseclass that will call all validators for a Result. 

Note that this method isn't hooked up to be called by anything yet, but should be called by the TaskRunner eventually.


## Why is this PR important?
Overall, this is the building block to support arbitrary functions of the signature `validation_fn(r: Result) -> bool` to be produced by users and passed to Results to be run automatically for them. They will not actually be called until we hook this into the TaskRunner logic.
",139199684
1611,2020-03-16T19:23:15Z,2020-03-19T17:55:52Z,,,"**Thanks for contributing to Prefect!**

Please describe your work and make sure your PR:

- [x] adds new tests (if appropriate)
- [x] updates `CHANGELOG.md` (if appropriate)
- [x] updates docstrings for any new functions or function arguments, including `docs/outline.toml` for API reference docs (if appropriate)

Note that your PR will not be reviewed unless all three boxes are checked.

## What does this PR change?
Adds `S3Result` for writing/reading results to/from an S3 bucket.

Addresses the S3 component for #2141

Since this is a longer-term PIN, we should update the documentation closer to release in a separate PR.

Note: since PR #2161 reorgs the results dir, it should be merged first, then this branch will be rebased.
",139199684
1612,2020-03-15T23:43:55Z,2020-03-15T23:43:57Z,,,"Solid progress on #2038 #2039 (Local storage of log messages and exceptions.)

Per our discussions last week and getting more comfortable poking around in Prefect-Core I got a working prototype for redacting messages and exceptions locally and only sending a URI reference to the cloud.  Here is what the log looks link in Cloud.   The first reference is a redacted log message and the second is the redacted exception.

![redacted-cloud](https://user-images.githubusercontent.com/28061825/76712772-2a34ce80-66e1-11ea-915e-db7914d08286.png)

The tasks and local logfiles are:

**Log Social Security Number**
```python
@task
def redacted_log():
    logger = prefect.context.get(""logger"")
    logger.info(""SSN: 123-45-6789"")
```

```text
[2020-03-15 16:44:06,512] INFO - prefect.Task: redacted_log | SSN: 123-45-6789
```

**Raise Exception**
```python
@task
def redacted_exception():
    1 / 0
```

```text
[2020-03-15 16:44:07,617] ERROR - prefect.CloudTaskRunner | Unexpected error: ZeroDivisionError('division by zero')
Traceback (most recent call last):
  File ""/home/xxx/projects/prefect/src/prefect/engine/runner.py"", line 48, in inner
    new_state = method(self, state, *args, **kwargs)
  File ""/home/xxx/projects/prefect/src/prefect/engine/task_runner.py"", line 884, in get_task_run_state
    self.task.run, timeout=self.task.timeout, **raw_inputs
  File ""/home/xxx/projects/prefect/src/prefect/utilities/executors.py"", line 184, in timeout_handler
    return fn(*args, **kwargs)
  File ""/home/xxx/projects/prefect/redactor-poc/example/redactor.py"", line 13, in redacted_exception
    1 / 0
ZeroDivisionError: division by zero
```

I can see scenarios where people might want to redact the task and flow name or use regex to mask certain character strings.   It also seems like we can get from the above log to some simple tool/process to make the links clickable to review the locally stored logs.  Here locally means inside of the uses controlled environment.

## Implementation
In the end I was able to keep all the changes in [logging.py](https://github.com/nathan5280/prefect/blob/standalone-redact/src/prefect/utilities/logging.py) and the additions in (local_file_redactor.py)[https://github.com/nathan5280/prefect/blob/standalone-redact/src/prefect/utilities/local_file_redactor.py].

### Configuration
```yaml
[logging]
# The logging level: NOTSET, DEBUG, INFO, WARNING, ERROR, or CRITICAL
level = ""INFO""

    [logging.redactor]
    redactor_class = ""prefect.utilities.local_file_redactor.LocalFileRedactor""
    redactor_format = ""%(asctime)s - %(name)s - %(levelname)s - Redacted""
    redactor_root_dir = ""/tmp""
```
The RedactorFilter, RedactorHandler classes and redactor_mapper() function are all wrapped into the LocalFileRedactor class to make it easier to load them dynamically.   Nothing special about this other than it keeps everything together with the helper functions to build the paths.

### Strategy
1. All the loggers and handlers roll up to the ""prefect"" logger so that is where things get attached.  Logging configuration is modified in logging._add_redactor() which is called at the end of logging._create_logger.  I haven't confirmed if this works for anything other than the local agent.

1. Add a RedactorHandler to the prefect logger.
    1.  Select log messages from 'prefect Task: <task-name>' logger and write the message to the local file system.
    1. Select log messages from 'prefect CloudTaskRunner' logger that have exc_info associated with them and write the message to the local file.
1. Replace the formatter on the CloudHandler with the RedactorFormatter.  This formatter replaces messages for records that meet the same criteria as the Handler above with the local file URI.
1. Replace the default logic in CloudHandler that maps the log record to the log dictionary that is sent to the Cloud.

",139199684
1613,2020-03-12T21:19:05Z,2020-03-20T18:19:59Z,,,"**Thanks for contributing to Prefect!**

Please describe your work and make sure your PR:

- [x] adds new tests (if appropriate)
- [x] updates `CHANGELOG.md` (if appropriate)
- [x] updates docstrings for any new functions or function arguments, including `docs/outline.toml` for API reference docs (if appropriate)

Note that your PR will not be reviewed unless all three boxes are checked.

Closes #2145

## What does this PR change?
This PR allows for the option of directly using the `ConstantResult` subclass with PIN-16 in mind. Method implementations for `read` and `write` were copied from `ConstantResultHandler`, and `exists` hard-coded as returning True. If users can / do specify `NoResultType` instances as values for constants, we'd probably like to change the `exists` method to take that into account. However, if there's a rework of the `NoResultType` to inherit from `Result` and always return False, maybe we create a separate issue?

Also, slightly related, this PR adds tests for how the current `ConstantResultHandler` behaves to ensure the `ConstantResult` subclass behaves the same way.


## Why is this PR important?
This PR is important for migrating to use of a unified result interface internally for use with the flow-runner.

",139199684
1614,2020-03-10T04:56:43Z,2020-03-26T03:31:28Z,,,"**Thanks for contributing to Prefect!**

Please describe your work and make sure your PR:

WIP PR for #2124 

- [x] adds new tests (if appropriate)
- [ ] updates `CHANGELOG.md` (if appropriate)
- [x] updates docstrings for any new functions or function arguments, including `docs/outline.toml` for API reference docs (if appropriate)

Note that your PR will not be reviewed unless all three boxes are checked.

## What does this PR change?
Adds new task that enables users to execute and fetch queries from a MySQL DB. 


## Why is this PR important?
Increase useability due to integration with popular DB. 

",139199684
1615,2020-03-09T01:02:18Z,2020-03-12T15:47:49Z,,,"**Thanks for contributing to Prefect!**

Please describe your work and make sure your PR:

- [n/a] adds new tests (if appropriate)
- [ ] updates `CHANGELOG.md` (if appropriate)
- [n/a] updates docstrings for any new functions or function arguments, including `docs/outline.toml` for API reference docs (if appropriate)

Note that your PR will not be reviewed unless all three boxes are checked.

## What does this PR change?
This PR adds a new PIN focused on how to address obfuscating data passed to Cloud.
This is currently a DRAFT seeking comment from the community. Note that the prose is expanded into individual bullets to enable per-line Github comments for debate.
For more context, see a prior conversation on a POC PR, #2048 

## Why is this PR important?
@nathan5280 recently reinvigorated conversation about privacy in Cloud for compliance-focused users, which I understand is at least anecdotally known to be of interest to more Cloud users.
This PR seeks to consolidate the conversation on this into one place and open up the concept for discussion and debate to the broader Core community.

cc @nathan5280, @wagoodman ",139199684
1616,2020-03-08T19:56:36Z,2020-03-15T23:55:49Z,,,"Prototype for storing log messages locally and only sending a reference to the local data to Prefect Cloud.   This is still work in progress.   I'll add updates to the open issue.
",139199684
1617,2020-03-04T03:11:07Z,2020-03-14T00:55:46Z,,,"**Thanks for contributing to Prefect!**

Please describe your work and make sure your PR:

- [x] adds new tests (if appropriate)
- [x] updates `CHANGELOG.md` (if appropriate)
- [x] updates docstrings for any new functions or function arguments, including `docs/outline.toml` for API reference docs (if appropriate)

Note that your PR will not be reviewed unless all three boxes are checked.

## What does this PR change?
This PR implements a new result handler that saves and loads Pandas DataFrames using Panda's built-in tools. It's based on the same handler in my [`prefect_ds`](https://github.com/AndrewRook/prefect_ds) library. Discussion in [this Slack thread](https://prefect-community.slack.com/archives/CL09KU1K7/p1582684214035000) may also be useful as a reference for why this PR is coming into Prefect looking like it does. 

**This PR, in its current form, is not something I would consider mergeable into Prefect,** but rather is intended to start the conversation about what modifications would need to be made in order to make it mergeable. There are three main reasons for this:

1. `PandasResultHandler` requires the full filepath, including the file name, to be specified. This is in contrast (I believe) to the rest of the file handlers, which have the user specify the directory/bucket/container/etc and then generate the actual filename themselves. My interpretation of this design choice is to make using the result handlers with `map` easier, because with the full filepath specified a mapped task would just write to the same file for every iteration of the map.
2. `PandasResultHandler` has hooks for formatting the filepath based on the inputs to the task, to handle the aforementioned mapping problem. But obviously Prefect does not recognize these hooks. (For reference, to get around this in `prefect_ds` I subclassed the `TaskRunner` and created a new state handler. This was a bit of a kludge, and could probably be implemented more elegantly in Prefect proper if that was a direction folks wanted to go.)
3. `PandasResultHandler` would add an optional Pandas dependency, and currently that's not included in any of the docs, installation tools, or CI (although I did try to account for it in the importing of the handler itself and in the tests). That's not a particularly challenging thing to resolve, but I wanted guidance on how y'all would like that to be implemented.

## Why is this PR important?
A lot of data workloads, especially data analysis/modeling workflows, revolve around Pandas DataFrames. Furthermore, when analyzing data it's important to be able to manually inspect and validate what you're dealing with, which is hard to do with the `LocalResultHandler` writing files to randomly named pickles. 

@cicdw (and/or anyone else) take a look and let me know what you think (no rush through 😄). ",139199684
1618,2020-02-28T20:00:11Z,2020-03-24T16:13:57Z,,,"**Thanks for contributing to Prefect!**

Please describe your work and make sure your PR:

- [x] adds new tests (if appropriate)
- [x] updates `CHANGELOG.md` (if appropriate)
- [x] updates docstrings for any new functions or function arguments, including `docs/outline.toml` for API reference docs (if appropriate)

Note that your PR will not be reviewed unless all three boxes are checked.

## What does this PR change?
This PR changes the Prefect configuration mechanism to support loading multiple configuration files; previously it accepted two: the default config and a user config. Now it supports arbitrarily many as a single `paths` parameter.

This allows us to easily split the config into multiple files, as appropriate - in this case, a new `cloud-config.toml` file to contain Prefect Cloud-specific settings.

## Why is this PR important?


",139199684
1619,2020-02-18T22:56:26Z,2020-03-09T01:11:50Z,,,"This is a stab at addressing:
* https://github.com/PrefectHQ/prefect/issues/2039
* https://github.com/PrefectHQ/prefect/issues/2038

It was mostly a PoC to see what code would need to be updated.   It is still dependent on a reliable across executor hook or configuration item to turn on local only mode.  

# Changes
1. Addition off prefect/utilities/local_only.py.  This is where the work is done to replace all of the handlers and loggers to enable local only mode.
1. Updates to prefect/utilities/logging.py.  These changes check to see if we are in Log to Cloud and Local Only is enabled.   If so then the log message is replaced with a URL to a local file where the local only data is written.   The log message extra={""_local_only"": <something>} must be set to trigger this formatting.   Information in this extra{_local_only} is also written to the file.
1. prefect/engine/runner.py is also updated to make the above check.   If the check is true, it adds the extra{_local_only} key to trigger local only logging of the exception.

This has only be tried with the default local executor and the cloud local executor.   On to Dask tomorrow.

Net result in cloud log:
```code
February 18th 2020 at 3:42:26pm MST | prefect.CloudTaskRunner
ERROR 
Unexpected error: ZeroDivisionError
file:////<some path>/.logs/local-only-logging/216f392f-278b-4519-8dd5-79f15a5b831d--2020-02-18T22-42-24.942057-00-00.txt
```
# Configuration
PREFECT__LOCAL_ONLY__ENABLED=True
PREFECT__LOCAL_ONLY__CLS=prefect.utilities.local_only.FileLocalOnly
PREFECT__LOCAL_ONLY__KWARGS={\""root_directory\"":\"".logs\""}

# Example to drive the PoC
```python
from prefect import task, Flow, Client
from prefect.engine.executors import LocalExecutor
from prefect.environments import RemoteEnvironment

from prefect.utilities.local_only import LocalOnly


@task
def uncaught_exception(numerator: int):
    # Get the special logger until the PoC has a reliable hook to
    # configure the local only logging and exception mode.
    logger = LocalOnly.logger()
    x = numerator / 0


@task
def log_info():
    # Get the special logger until the PoC has a reliable hook to
    # configure the local only logging and exception mode.
    logger = LocalOnly.logger()
    logger.info(""Public message."", extra={LocalOnly.EXTRA_KEY: ""private message.""})


def build_flow() -> Flow:
    with Flow(""local-only-logging"") as flow:
        log_info()
        uncaught_exception(numerator=10)
    return flow


def run_local():
    flow = build_flow()
    executor = LocalExecutor()
    flow.run(executor=executor)


def run_cloud_local():
    flow = build_flow()
    flow_id = flow.register(project_name=""tutorial"")
    client = Client()
    client.create_flow_run(flow_id=flow_id)


def run_cloud_dask():
    flow = build_flow()
    flow.environment = RemoteEnvironment(
        executor=""prefect.engine.executors.DaskExecutor"",
        executor_kwargs={""address"": ""tcp://192.168.84.52:8786""},
        # on_start=apply_safe_logging,
    )
    flow_id = flow.register(project_name=""tutorial"")
    client = Client()
    client.create_flow_run(flow_id=flow_id)


if __name__ == ""__main__"":
    # run_local()
    run_cloud_local()
    # run_cloud_dask()
```",139199684
1620,2020-02-07T03:01:29Z,2020-03-03T19:54:35Z,,,"**Thanks for contributing to Prefect!**

Please describe your work and make sure your PR:

- [x] adds new tests (if appropriate)
- [x] updates `CHANGELOG.md` (if appropriate)
- [x] updates docstrings for any new functions or function arguments, including `docs/outline.toml` for API reference docs (if appropriate)

Note that your PR will not be reviewed unless all three boxes are checked.

## What does this PR change?
This is a minor but possibly important change and closes #1430. 

Previously, if `flow.run(run_on_schedule=False)` was called, then a `FlowRunner` was instantiated and and executed once, visiting each task only once. If `flow.run(run_on_schedule=True)` was called, then a much more rich set of logic was invoked and the flow itself was run to completion -- even if it didn't have a schedule attached. This includes task retries and other features only made possible by calling a `FlowRunner` multiple times. If the flow has a schedule, it also sleeps until the next run. 

This means the following experiences were actually very different:
1. a flow with no schedule that starts running right now: the user calls `flow.run()` and gets the benefits of retries (among other things)
2. a flow with a schedule that starts running right now: the user must call `flow.run(run_on_schedule=False)` and gets no retries

This PR unifies the two routes, so that no matter whether a flow has a schedule or not, it gets the ""full"" treatment. `run_on_schedule=False` can still be passed to start a run immediately. 

The old behavior (of running each task only once) can be recovered by using a `FlowRunner` directly.


## Why is this PR important?
To avoid the old Airflow issue where backfills and scheduled runs used completely different logic pipelines

",139199684
1621,2019-10-29T21:04:22Z,2019-12-13T04:51:39Z,,,"**Thanks for contributing to Prefect!**

Please describe your work and make sure your PR:

- [x] adds new tests (if appropriate)
- [x] updates `CHANGELOG.md` (if appropriate)
- [x] updates docstrings for any new functions or function arguments, including `docs/outline.toml` for API reference docs (if appropriate)

Note that your PR will not be reviewed unless all three boxes are checked.

## What does this PR change?
This PR adds support for type-casting Parameters, which are usually provided as strings/JSON.


## Why is this PR important?


",139199684
1622,2020-03-25T15:05:47Z,2020-03-25T15:05:47Z,,,fixes #243 - reproduced a problem with unit test and fixed it.,8307391
1623,2020-03-18T19:30:23Z,2020-03-25T14:28:11Z,,,"Hi!

As as discussed in #242, here is my suggestion to add an edge case to the usage of `funcutils.wraps`. 
The situation is that if the wrapped and wrapper function have the same arguments but differ as to which are keyword-only or positional-only, the function returned by `wraps` will fail as the wrong ""invocation string"" was inserted in the process. A solution to this is to create the internal `FunctionBuilder`instance from the wrapper instead of the wrapped. As it didn't make sense to simply add an argument to `wraps`, I decided to replicate the structure of the built-in `functools` and add a `update_wrapper` function, of which `wraps` is only a `partial` call.

This solves a problem that originated from using partials. Code that now works to wrap those is as follow:
```python
import functools
from boltons import funcutils

def func(a, b=1, c=1):  # Has the signature ""a, b=1, c=1""
    return a, b, c

wrapper = partial(func, b=2)
functools.update_wrapper(wapper, func)  # Needed as FunctionBuilder will look for some attributes like __name__ that are not updated directly with `partial`.
# wrapper has the signature; ""a, *, b=2, c=1""

new_func = funcutils.update_wrapper(wrapper, func, build_from=wrapper, injected='b')
# new_func now has the signature : ""a, *, c=1""
new_func(1)  # Prints : 1, 2, 1, 
```

Also, another small improvement I made here is to add an option to completely hide the wrapped function in the new one returned. Most IDEs use `inspect.signature` to extract the signature functions that they display to the user. However, by default, `signature` follows all wrapped functions until the innermost, so the user doesn't see the updated signature. One could use the `follow_wrapped=False` option, but I believe offering the possibility here could be nice.

Closes #242",8307391
1624,2019-11-28T21:40:37Z,2019-11-29T00:45:10Z,,,"This is a bigger PR, even though it only contains 2 functions: cmd and enable_flatapi. However the first of these functions is one of my most useful tools as measured by the latest [ubelt](https://github.com/Erotemic/ubelt) documentation: (in clocks in at [82 times used in a non-ubelt project]( https://ubelt.readthedocs.io/en/latest/), placing at 14 / 127 of my most frequently used functions). The second is a developer convenience that incurs minimal overhead during runtime.  

------

# New Function: cmd


Lets start with cmd first. The `boltons/cmdutils.py` and `tests\test_cmdutils.py` are the key contributions of this PR. There are a few other minor changes, but I can remove those if need-be. 

At its most basic it just executes some shell code. The reason this is better than subprocess.Popen (other than the name is much easier to type) is because it allows you to easily ""tee"" the output of your shell code. You can get the output in a dictionary and have it print to stdout at the same time and in real time (mostly, up to buffering issues that are frankly over my head). This is really useful when developing because you want to see the output of your long running process, but you might also want to be able to interact with it. In addition to these features you can also ""detach"" the process and run it asynchronously in the background. 


I think cmd would be a really useful addition to boltons. I could talk about `cmd` a lot, but I'm just going to let its signature speak for itself: 

```python
def cmd(command, shell=False, detach=False, cwd=None,
        env=None, tee=None, tee_backend='auto', verbose=0):
    """"""
    Executes a command in a subprocess.

    The advantage of this wrapper around subprocess is that
    (1) you control if the subprocess prints to stdout,
    (2) the text written to stdout and stderr is returned for parsing,
    (3) cross platform behavior that lets you specify the command as a string
    or tuple regardless of whether or not shell=True.
    (4) ability to detach, return the process object and allow the process to
    run in the background (eventually we may return a Future object instead).

    Implementation is based on the collection of code samples on stackoverflow
    [1]_ [2]_ [3]_.


    Args:
        command (str or Sequence): bash-like command string or tuple of
            executable and args

        shell (bool, default=False): if True, process is run in shell.

        detach (bool, default=False):
            if True, process is detached and run in background.

        cwd (PathLike, optional): path to run command

        env (str, optional): environment passed to Popen

        tee (bool, optional): if True, simultaneously writes to stdout while
            capturing output from the command. If not specified, defaults to
            True if verbose > 0.  If detach is True, then this argument is
            ignored.

        tee_backend (str, optional): backend for tee output.
            Valid choices are: ""auto"", ""select"" (POSIX only), and ""thread"".

        verbose (int, default=0): verbosity mode. Can be 0, 1, 2, or 3.
            0 is quiet, 3 is loud.

    Returns:
        dict: info - information about command status.
            if detach is False ``info`` contains captured standard out,
            standard error, and the return code
            if detach is False ``info`` contains a reference to the process.

    Notes:

        * While this function strives to be cross-platform, there are certain
          insurmountable issues that arise when handling multiple shell
          languages.

        * Inputs can either be text or tuple based. On UNIX we ensure
            conversion to text if shell=True, and to tuple if shell=False. On
            windows, the input is always text based.  See [3]_ for a potential
            cross-platform shlex solution for windows.

    References:
        .. [1] https://stackoverflow.com/questions/11495783/redirect-subprocess-stderr-to-stdout
        .. [2] https://stackoverflow.com/questions/7729336/display-subprocess-stdout-without-distorti
        .. [3] https://stackoverflow.com/questions/33560364/python-windows-parsing-command-lines-with-shlex

    Example:
        >>> info = cmd(('echo', 'simple cmdline interface'), verbose=1)
        >>> print(info)  # xdoctest: +IGNORE_WANT
        simple cmdline interface
        {'out': 'simple cmdline interface\n',
         'err': '',
         'ret': 0,
         'proc': <subprocess.Popen at 0x7f0a4723a2e8>,
         'cwd': None,
         'command': ""echo 'simple cmdline interface'""}

        >>> # The following commands demonstrate multiple ways co call cmd
        >>> info = cmd('echo str noshell', verbose=0)
        >>> assert info['out'].strip() == 'str noshell'

        >>> # windows echo will output extra single quotes
        >>> info = cmd(('echo', 'tuple noshell'), verbose=0)
        >>> assert info['out'].strip().strip(""'"") == 'tuple noshell'

        >>> # Note this command is formatted to work on win32 and unix
        >>> info = cmd('echo str&&echo shell', verbose=0, shell=True)
        >>> assert info['out'].strip() == 'str' + chr(10) + 'shell'

        >>> info = cmd(('echo', 'tuple shell'), verbose=0, shell=True)
        >>> assert info['out'].strip().strip(""'"") == 'tuple shell'
    """"""
```



------

# New Function: enable_flatapi

Next is ""enable_flatapi"", which you'll have to tell me if you like or not. It lazily populates the global boltons API so everything is top-level (which helps reduce coder keystrokes). I used my [mkinit](https://github.com/Erotemic/mkinit) tool to autogenerate the import statements needed to declare all boltons functions in a local scope, then I simply write to the globals scope. By default it does nothing but define one function in `__init__`, however if you call that function, then all of a sudden you can do 

```python
import boltons
boltons.enable_flatapi()
print(boltons.get_python_info())
```

You can also set `BOLTONS_ENABLE_FLATAPI=TRUE` in your environment to enable the behavior by default. I personally like this feature a lot; I think it adds minimal complexity to achieve a useful behavior. 


------

# Notes

Also note a pretty cool refactoring technique I did. I left the command line in. 
When I needed to port my tests for cmd from ubelt to boltons, I needed to remove the dependencies on ubelt. I did this with a refactoring tool that currently exists in my netharn package on gitlab. I basically used this shell command:

```bash
xdoctest -m netharn.export.closer _closefile --fpath=$HOME/code/boltons/tests/test_cmdutils.py --modnames=ubelt,
```

to execute a ""[Closer](https://gitlab.kitware.com/computer-vision/netharn/blob/dev/0.5.3/netharn/export/closer.py#L922)"" that recursively goes finds all dependencies on a particular package and recursively goes through that package to copy and paste all the relevant bits of code statically to the top of the file  (note: my package does not execute the code to do this). Because its auto-generated its not perfect, so I did a bit of manual cleanup at the end to tie up the lose ends. It definitely made the task of porting code much less time consuming than it usually is. I mention it because I think it has the potential to become a really powerful refactoring tool, and I'd be interested in your opinion.",8307391
1625,2019-08-16T02:32:24Z,2019-11-09T05:22:38Z,,,"Builds off of #218, in which there was discussion of moving to xdoctest as pytest doctest driver. This patch should accomplish that. It adds the appropriate dependency and sets the flag to enable xdoctest in the test command. I think that should be sufficient (lets see if the CI likes it).

----

On an unrelated note, I discovered this library due to its similarity to a package I maintain: [ubelt]( https://github.com/Erotemic/ubelt). It exists in much the same spirit of boltons, but it has 90% less exposure, and is developed by 97% fewer people. I'm interested in merging parts of my toolset into this library. I think the organization of this package is well aligned with my tastes, and my tools might be of some benefit.

I'll make the appropriate PRs as necessary, moving only a single unit of functionality at a time (there are some things in ubelt that I don't see making it over here), but my entire API is in the README, so let me know if there is anything specific that looks interesting, otherwise I'll choose somewhere to start (assume you think this sounds like a good idea). 
",8307391
1626,2019-06-21T10:47:24Z,2019-06-22T20:07:00Z,,,"Use idiomatic Python code instead of `try-except`.

That also makes `iterutils` consistent -- some methods used `isinstance(obj, Iterable)` and some used `try-except`.

Also for common methods (`is_scalar` and `is_collection`) use code directly, without `is_iterable`.",8307391
1627,2019-01-29T14:14:13Z,2019-01-29T14:14:13Z,,,"Pretty much like php's implode() (lol) but better. The energy to contribute this came from [this article](https://mail.python.org/pipermail/python-ideas/2019-January/054995.html).

Request for comments ;)",8307391
1628,2018-11-01T12:00:07Z,2018-12-23T02:21:11Z,,,"Now you can create a Dict for a folder and access their files like a dictionary.
The opening of a file, closing of a file is done automatically.
Also, the listing of files is supported by dict.keys()
File deletion is supported by del dict[""File_name""] or dict.pop(""File_Name"")
",8307391
1629,2018-05-31T22:09:03Z,2019-02-11T07:27:17Z,,,"This makes only one more test pass (now only 3 failing) but at least prevents the `TypeError` when attempting to add a `list` and a `tuple`.

pytest output attached
[pytest-out-2.txt](https://github.com/mahmoud/boltons/files/2060262/pytest-out-2.txt)
",8307391
1630,2018-05-31T21:57:12Z,2019-02-11T07:26:55Z,,,"[pytest-out.txt](https://github.com/mahmoud/boltons/files/2060236/pytest-out.txt)
",8307391
1631,2017-05-24T18:37:51Z,2017-05-24T18:37:51Z,,,"A small change to allow using `ipdb` instead of `pdb` in `debugutils.py`, as mentioned in #92, *if* it's pre-installed in the user's environment.
",8307391
1632,2016-09-21T05:20:45Z,2016-09-21T19:14:25Z,,,"This is a function that tries to map data to one of the handful of primitive types supported by JSON. I have included tests using it both for the _default_ parameter of `json.dump` and with `remap`.

There is a bit more work to be done but I wanted to first see if there way any interest in it before I put more effort into it.  (I am also open to suggestions for the name.)
",8307391
1633,2016-09-20T23:34:07Z,2016-09-21T17:21:02Z,,,"The nested tuple/list structures that are returned by `get_format_args` are fairly cumbersome; returning data `namedtuple` makes accessing the data much cleaner.

I am a bit uncertain about the field names themselves - - I chose 'positional' and 'named' because the documentation for the function describes them that way, but I chose 'key' and 'index' for the component (rather than 'position' and 'name') because those are the standard way to refer to them. It seems a bit inconsistent, however -- perhaps the `FormatArgs` fields should be `indexed` and `keyed`? (or ""...s"" instead of ""...ed""?)
",8307391
1634,2016-05-25T18:01:17Z,2016-08-03T09:11:14Z,,,"might be handy to have this in a CLI type interface like stdlib in trace as well

python -m tracemod json socket main.py
",8307391
1635,2015-06-19T06:03:48Z,2016-08-03T09:11:14Z,,,"So I'm trying again with my other favorite iterator function.  I've googled this time (somewhat) to make sure there's not an existing one out there (or right in itertools!)

The docstring describes it pretty well, let me know what you think!

Thanks,
Taylor
",8307391
